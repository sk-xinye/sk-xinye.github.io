<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sk-xinye.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.yml"};
  </script>

  <meta name="description" content="愿所有努力都不被辜负">
<meta property="og:type" content="website">
<meta property="og:title" content="sk-xinyeの博客">
<meta property="og:url" content="https://sk-xinye.github.io/index.html">
<meta property="og:site_name" content="sk-xinyeの博客">
<meta property="og:description" content="愿所有努力都不被辜负">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="sk-xinye">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://sk-xinye.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>sk-xinyeの博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">sk-xinyeの博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录学习的脚步</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">0</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">13</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">96</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/07/03/8-%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/03/8-%E7%BD%91%E7%BB%9C/" class="post-title-link" itemprop="url">8.网络</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-03 16:52:50" itemprop="dateCreated datePublished" datetime="2021-07-03T16:52:50+08:00">2021-07-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-04 11:35:39" itemprop="dateModified" datetime="2021-07-04T11:35:39+08:00">2021-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="容器网络"><a href="#容器网络" class="headerlink" title="容器网络"></a>容器网络</h2><p>网络栈”，实际上是被隔离在它自己的 Network Namespace 当中的。</p>
<p>而所谓“网络栈”，就包括了：</p>
<ul>
<li>网卡（Network Interface）</li>
<li>回环设备（Loopback Device）</li>
<li>路由表（Routing Table）</li>
<li>iptables 规则。</li>
<li>对于一个进程来说，这些要素，其实就构成了它发起和响应网络请求的基本环境。</li>
</ul>
<p>需要指出的是，作为一个容器，它可以声明直接使用宿主机的网络栈（–net=host），即：不开启 Network Namespace，比如：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">docker run –d –net=host --name nginx-host nginx</span></span><br></pre></td></tr></table></figure>

<ul>
<li>在这种情况下，这个容器启动后，直接监听的就是宿主机的 80 端口。</li>
</ul>
<p>问题：</p>
<ul>
<li>像这样直接使用宿主机网络栈的方式，虽然可以为容器提供良好的网络性能，但也会不可避免地引入共享网络资源的问题，比如端口冲突。所以，在大多数情况下，我们都希望容器进程能使用自己 Network Namespace 里的网络栈，即：拥有属于自己的 IP 地址和端口。</li>
<li>这个被隔离的容器进程，该如何跟其他 Network Namespace 里的容器进程进行交互呢？</li>
</ul>
<h3 id="网桥"><a href="#网桥" class="headerlink" title="网桥"></a>网桥</h3><p>而为了实现上述目的，Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连接在 docker0 网桥上的容器，就可以通过它来进行通信。</p>
<ul>
<li>我们就需要使用一种名叫Veth Pair的虚拟设备了。</li>
<li>Veth Pair 设备的特点是：它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里。</li>
<li>这就使得 Veth Pair 常常被用作连接不同 Network Namespace 的“网线”。</li>
</ul>
<h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><ul>
<li>当进入容器后，查看容器ifconfig网络，发现eth0的网卡，它正是一个 Veth Pair 设备在容器里的这一端。</li>
<li>通过 route 命令查看 nginx-1 容器的路由表，我们可以看到，这个 eth0 网卡是这个容器里的默认路由设备；所有对 172.17.0.0/16 网段的请求，也会被交给 eth0 来处理（第二条 172.17.0.0 路由规则）。</li>
<li>而这个 Veth Pair 设备的另一端，则在宿主机上。你可以通过查看宿主机的网络设备看到它，</li>
<li>通过 ifconfig 命令的输出，你可以看到，nginx-1 容器对应的 Veth Pair 设备，在宿主机上是一张虚拟网卡。它的名字叫作 veth9c02e56。</li>
<li>并且，通过 brctl show 的输出，你可以看到这张网卡被“插”在了 docker0 上。</li>
<li>如果我们再在这台宿主机上启动另一个 Docker 容器，比如 nginx-2：你就会发现一个新的、名叫 vethb4963f3 的虚拟网卡，也被“插”在了 docker0 网桥上。</li>
</ul>
<p>这其中的原理其实非常简单：</p>
<ul>
<li>当你在 nginx-1 容器里访问 nginx-2 容器的 IP 地址（比如 ping 172.17.0.3）的时候，这个目的 IP 地址会匹配到 nginx-1 容器里的第二条路由规则。可以看到，这条路由规则的网关（Gateway）是 0.0.0.0，这就意味着这是一条直连规则，即：凡是匹配到这条规则的 IP 包，应该经过本机的 eth0 网卡，通过二层网络直接发往目的主机。</li>
<li>而要通过二层网络到达 nginx-2 容器，就需要有 172.17.0.3 这个 IP 地址对应的 MAC 地址。所以 nginx-1 容器的网络协议栈，就需要通过 eth0 网卡发送一个 ARP 广播，来通过 IP 地址查找对应的 MAC 地址。<ul>
<li>ARP（Address Resolution Protocol），是通过三层的 IP 地址找到对应的二层 MAC 地址的协议。</li>
</ul>
</li>
<li>这个 eth0 网卡，是一个 Veth Pair，它的一端在这个 nginx-1 容器的 Network Namespace 里，而另一端则位于宿主机上（Host Namespace），并且被“插”在了宿主机的 docker0 网桥上。</li>
<li>一旦一张虚拟网卡被“插”在网桥上，它就会变成该网桥的“从设备”。从设备会被“剥夺”调用网络协议栈处理数据包的资格，从而“降级”成为网桥上的一个端口。</li>
<li>而这个端口唯一的作用，就是接收流入的数据包，然后把这些数据包的“生杀大权”（比如转发或者丢弃），全部交给对应的网桥。</li>
<li>所以，在收到这些 ARP 请求之后，docker0 网桥就会扮演二层交换机的角色，把 ARP 广播转发到其他被“插”在 docker0 上的虚拟网卡上。</li>
<li>这样，同样连接在 docker0 上的 nginx-2 容器的网络协议栈就会收到这个 ARP 请求，从而将 172.17.0.3 所对应的 MAC 地址回复给 nginx-1 容器。</li>
<li>有了这个目的 MAC 地址，nginx-1 容器的 eth0 网卡就可以将数据包发出去。</li>
<li>而根据 Veth Pair 设备的原理，这个数据包会立刻出现在宿主机上的 veth9c02e56 虚拟网卡上。不过，此时这个 veth9c02e56 网卡的网络协议栈的资格已经被“剥夺”，所以这个数据包就直接流入到了 docker0 网桥里。</li>
<li>docker0 处理转发的过程，则继续扮演二层交换机的角色。此时，docker0 网桥根据数据包的目的 MAC 地址（也就是 nginx-2 容器的 MAC 地址），在它的 CAM 表（即交换机通过 MAC 地址学习维护的端口和 MAC 地址的对应表）里查到对应的端口（Port）为：vethb4963f3，然后把数据包发往这个端口。</li>
<li>而这个端口，正是 nginx-2 容器“插”在 docker0 网桥上的另一块虚拟网卡，当然，它也是一个 Veth Pair 设备。这样，数据包就进入到了 nginx-2 容器的 Network Namespace 里。</li>
<li>所以，nginx-2 容器看到的情况是，它自己的 eth0 网卡上出现了流入的数据包。这样，nginx-2 的网络协议栈就会对请求进行处理，最后将响应（Pong）返回到 nginx-1。<img src="/2021/07/03/8-%E7%BD%91%E7%BB%9C/%E5%AE%B9%E5%99%A8%E9%80%9A%E4%BF%A1.png" class=""></li>
<li>熟悉了 docker0 网桥的工作方式，你就可以理解，在默认情况下，被限制在 Network Namespace 里的容器进程，实际上是通过 Veth Pair 设备 + 宿主机网桥的方式，实现了跟同其他容器的数据交换。</li>
<li>与之类似地，当你在一台宿主机上，访问该宿主机上的容器的 IP 地址时，这个请求的数据包，也是先根据路由规则到达 docker0 网桥，然后被转发到对应的 Veth Pair 设备，最后出现在容器里。这个过程的示意图，<img src="/2021/07/03/8-%E7%BD%91%E7%BB%9C/%E5%AE%BF%E4%B8%BB%E6%9C%BA%E8%AE%BF%E9%97%AEdocker.png" class=""></li>
<li>同样地，当一个容器试图连接到另外一个宿主机时，比如：ping 10.168.0.3，它发出的请求数据包，首先经过 docker0 网桥出现在宿主机上。然后根据宿主机的路由表里的直连路由规则（10.168.0.0/24 via eth0)），对 10.168.0.3 的访问请求就会交给宿主机的 eth0 处理。</li>
<li>所以接下来，这个数据包就会经宿主机的 eth0 网卡转发到宿主机网络上，最终到达 10.168.0.3 对应的宿主机上。<img src="/2021/07/03/8-%E7%BD%91%E7%BB%9C/docker%E8%AE%BF%E9%97%AE%E5%85%B6%E4%BB%96%E7%BD%91%E7%BB%9C.png" class=""></li>
<li>当你遇到容器连不通“外网”的时候，你都应该先试试 docker0 网桥能不能 ping 通，然后查看一下跟 docker0 和 Veth Pair 设备相关的 iptables 规则是不是有异常，往往就能够找到问题的答案了。</li>
<li>跨主机通信Overlay Network（覆盖网络）。<img src="/2021/07/03/8-%E7%BD%91%E7%BB%9C/%E8%B7%A8%E4%B8%BB%E6%9C%BA%E9%80%9A%E4%BF%A1.png" class=""></li>
</ul>
<h2 id="深入解析容器跨主机网络-Flannel项目说起"><a href="#深入解析容器跨主机网络-Flannel项目说起" class="headerlink" title="深入解析容器跨主机网络 Flannel项目说起"></a>深入解析容器跨主机网络 Flannel项目说起</h2><p>Flannel 项目是 CoreOS 公司主推的容器网络方案。事实上，Flannel 项目本身只是一个框架，真正为我们提供容器网络功能的，是 Flannel 的后端实现。目前，Flannel 支持三种后端实现，分别是：</p>
<ul>
<li>VXLAN；</li>
<li>host-gw；</li>
<li>UDP。</li>
</ul>
<h3 id="UDP-模式开始，来为你讲解容器“跨主网络”的实现原理"><a href="#UDP-模式开始，来为你讲解容器“跨主网络”的实现原理" class="headerlink" title="UDP 模式开始，来为你讲解容器“跨主网络”的实现原理"></a>UDP 模式开始，来为你讲解容器“跨主网络”的实现原理</h3><p>例子：我有两台宿主机。</p>
<ul>
<li><p>宿主机 Node 1 上有一个容器 container-1，它的 IP 地址是 100.96.1.2，对应的 docker0 网桥的地址是：100.96.1.1/24。</p>
</li>
<li><p>宿主机 Node 2 上有一个容器 container-2，它的 IP 地址是 100.96.2.3，对应的 docker0 网桥的地址是：100.96.2.1/24。</p>
</li>
<li><p>我们现在的任务，就是让 container-1 访问 container-2。</p>
</li>
<li><p>container-1 容器里的进程发起的 IP 包，其源地址就是 100.96.1.2，目的地址就是 100.96.2.3。</p>
</li>
<li><p>由于目的地址 100.96.2.3 并不在 Node 1 的 docker0 网桥的网段里，所以这个 IP 包会被交给默认路由规则，通过容器的网关进入 docker0 网桥（如果是同一台宿主机上的容器间通信，走的是直连规则），从而出现在宿主机上。</p>
</li>
<li><p>这时候，这个 IP 包的下一个目的地，就取决于宿主机上的路由规则了。此时，Flannel 已经在宿主机上创建出了一系列的路由规则，以 Node 1 为例，如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在 Node 1 上</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip route</span></span><br><span class="line">default via 10.168.0.1 dev eth0</span><br><span class="line">100.96.0.0/16 dev flannel0  proto kernel  scope link  src 100.96.1.0</span><br><span class="line">100.96.1.0/24 dev docker0  proto kernel  scope link  src 100.96.1.1</span><br><span class="line">10.168.0.0/24 dev eth0  proto kernel  scope link  src 10.168.0.2</span><br></pre></td></tr></table></figure></li>
<li><p>可以看到，由于我们的 IP 包的目的地址是 100.96.2.3，它匹配不到本机 docker0 网桥对应的 100.96.1.0/24 网段，只能匹配到第二条、也就是 100.96.0.0/16 对应的这条路由规则，从而进入到一个叫作 flannel0 的设备中。</p>
</li>
<li><p>而这个 flannel0 设备的类型就比较有意思了：它是一个 TUN 设备（Tunnel 设备）。</p>
<ul>
<li>在 Linux 中，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能非常简单，即：在操作系统内核和用户应用程序之间传递 IP 包。</li>
<li>像上面提到的情况，当操作系统将一个 IP 包发送给 flannel0 设备之后，flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 Flannel 进程。这是一个从内核态（Linux 操作系统）向用户态（Flannel 进程）的流动方向。</li>
<li>反之，如果 Flannel 进程向 flannel0 设备发送了一个 IP 包，那么这个 IP 包就会出现在宿主机网络栈中，然后根据宿主机的路由表进行下一步处理。这是一个从用户态向内核态的流动方向。</li>
</ul>
</li>
<li><p>所以，当 IP 包从容器经过 docker0 出现在宿主机，然后又根据路由表进入 flannel0 设备后，宿主机上的 flanneld 进程（Flannel 项目在每个宿主机上的主进程），就会收到这个 IP 包。然后，flanneld 看到了这个 IP 包的目的地址，是 100.96.2.3，就把它发送给了 Node 2 宿主机。</p>
</li>
<li><p>flanneld 通过子网Subnet来知道IP 地址对应的容器，是运行在 Node 2 上</p>
<ul>
<li><p>在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。</p>
</li>
<li><p>在我们的例子中，Node 1 的子网是 100.96.1.0/24，container-1 的 IP 地址是 100.96.1.2。</p>
</li>
<li><p>Node 2 的子网是 100.96.2.0/24，container-2 的 IP 地址是 100.96.2.3。</p>
</li>
<li><p>而这些子网与宿主机的对应关系，正是保存在 Etcd 当中，如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> etcdctl ls /coreos.com/network/subnets</span></span><br><span class="line">/coreos.com/network/subnets/100.96.1.0-24</span><br><span class="line">/coreos.com/network/subnets/100.96.2.0-24</span><br><span class="line">/coreos.com/network/subnets/100.96.3.0-24</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>所以，flanneld 进程在处理由 flannel0 传入的 IP 包时，就可以根据目的 IP 的地址（比如 100.96.2.3），匹配到对应的子网（比如 100.96.2.0/24），从 Etcd 中找到这个子网对应的宿主机的 IP 地址是 10.168.0.3，如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> etcdctl get /coreos.com/network/subnets/100.96.2.0-24</span></span><br><span class="line">&#123;&quot;PublicIP&quot;:&quot;10.168.0.3&quot;&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>所以说，flanneld 在收到 container-1 发给 container-2 的 IP 包之后，就会把这个 IP 包直接封装在一个 UDP 包里，然后发送给 Node 2。</p>
<ul>
<li>不难理解，这个 UDP 包的源地址，就是 flanneld 所在的 Node 1 的地址，而目的地址，则是 container-2 所在的宿主机 Node 2 的地址。</li>
<li>当然，这个请求得以完成的原因是，每台宿主机上的 flanneld，都监听着一个 8285 端口，所以 flanneld 只要把 UDP 包发往 Node 2 的 8285 端口即可。</li>
</ul>
</li>
<li><p>Node 2 上监听 8285 端口的进程也是 flanneld，所以这时候，flanneld 就可以从这个 UDP 包里解析出封装在里面的、container-1 发来的原 IP 包。</p>
</li>
<li><p>而接下来 flanneld 的工作就非常简单了：flanneld 会直接把这个 IP 包发送给它所管理的 TUN 设备，即 flannel0 设备。</p>
</li>
<li><p>Linux 内核网络栈就会负责处理这个 IP 包，具体的处理方法，就是通过本机的路由表来寻找这个 IP 包的下一步流向。</p>
</li>
<li><p>而 Node 2 上的路由表，跟 Node 1 非常类似，如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在 Node 2 上</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip route</span></span><br><span class="line">default via 10.168.0.1 dev eth0</span><br><span class="line">100.96.0.0/16 dev flannel0  proto kernel  scope link  src 100.96.2.0</span><br><span class="line">100.96.2.0/24 dev docker0  proto kernel  scope link  src 100.96.2.1</span><br><span class="line">10.168.0.0/24 dev eth0  proto kernel  scope link  src 10.168.0.3</span><br></pre></td></tr></table></figure></li>
<li><p>由于这个 IP 包的目的地址是 100.96.2.3，它跟第三条、也就是 100.96.2.0/24 网段对应的路由规则匹配更加精确。所以，Linux 内核就会按照这条路由规则，把这个 IP 包转发给 docker0 网桥。</p>
</li>
<li><p>docker0 网桥会扮演二层交换机的角色，将数据包发送给正确的端口，进而通过 Veth Pair 设备进入到 container-2 的 Network Namespace 里。</p>
</li>
<li><p>需要注意的是，上述流程要正确工作还有一个重要的前提，那就是 docker0 网桥的地址范围必须是 Flannel 为宿主机分配的子网。这个很容易实现，以 Node 1 为例，你只需要给它上面的 Docker Daemon 启动时配置如下所示的 bip 参数即可：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">FLANNEL_SUBNET=100.96.1.1/24</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> dockerd --bip=<span class="variable">$FLANNEL_SUBNET</span> ...</span></span><br></pre></td></tr></table></figure></li>
</ul>
<img src="/2021/07/03/8-%E7%BD%91%E7%BB%9C/UDP%E6%A8%A1%E5%BC%8F.png" class="">

<p>UDP被弃用原因：</p>
<ul>
<li>实际上，相比于两台宿主机之间的直接通信，基于 Flannel UDP 模式的容器通信多了一个额外的步骤，即 flanneld 的处理过程。而这个过程，由于使用到了 flannel0 这个 TUN 设备，仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝，如下所示：<img src="/2021/07/03/8-%E7%BD%91%E7%BB%9C/UDP%E5%BC%83%E7%94%A8.png" class=""><ul>
<li>第一次：用户态的容器进程发出的 IP 包经过 docker0 网桥进入内核态；</li>
<li>第二次：IP 包根据路由表进入 TUN（flannel0）设备，从而回到用户态的 flanneld 进程；</li>
<li>第三次：flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去。</li>
</ul>
</li>
<li>此外，我们还可以看到，Flannel 进行 UDP 封装（Encapsulation）和解封装（Decapsulation）的过程，也都是在用户态完成的。在 Linux 操作系统中，上述这些上下文切换和用户态操作的代价其实是比较高的，这也正是造成 Flannel UDP 模式性能不好的主要原因</li>
</ul>
<p>我们在进行系统级编程的时候，有一个非常重要的优化原则，就是要减少用户态到内核态的切换次数，并且把核心的处理逻辑都放在内核态进行</p>
<h3 id="VXLAN"><a href="#VXLAN" class="headerlink" title="VXLAN"></a>VXLAN</h3><p>VXLAN，即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。所以说，VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network）。</p>
<p>VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。</p>
<p>而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。</p>
<p>而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。</p>
<p>上述基于 VTEP 设备进行“隧道”通信的流程，我也为你总结成了一幅图，如下所示：</p>
<img src="/2021/07/03/8-%E7%BD%91%E7%BB%9C/vxlan%E5%8E%9F%E7%90%86.png" class="">

<ul>
<li><p>可以看到，图中每台宿主机上名叫 flannel.1 的设备，就是 VXLAN 所需的 VTEP 设备，它既有 IP 地址，也有 MAC 地址。</p>
</li>
<li><p>现在，我们的 container-1 的 IP 地址是 10.1.15.2，要访问的 container-2 的 IP 地址是 10.1.16.3。</p>
</li>
<li><p>那么，与前面 UDP 模式的流程类似，当 container-1 发出请求之后，这个目的地址是 10.1.16.3 的 IP 包，会先出现在 docker0 网桥，然后被路由到本机 flannel.1 设备进行处理。</p>
</li>
<li><p>为了能够将“原始 IP 包”封装并且发送到正确的宿主机，VXLAN 就需要找到这条“隧道”的出口，即：目的宿主机的 VTEP 设备。</p>
</li>
<li><p>而这个设备的信息，正是每台宿主机上的 flanneld 进程负责维护的。</p>
</li>
<li><p>比如，当 Node 2 启动并加入 Flannel 网络之后，在 Node 1（以及所有其他节点）上，flanneld 就会添加一条如下所示的路由规则：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> route -n</span></span><br><span class="line">Kernel IP routing table</span><br><span class="line">Destination     Gateway         Genmask         Flags Metric Ref    Use Iface</span><br><span class="line">...</span><br><span class="line">10.1.16.0       10.1.16.0       255.255.255.0   UG    0      0        0 flannel.1</span><br></pre></td></tr></table></figure></li>
<li><p>这条规则的意思是：凡是发往 10.1.16.0/24 网段的 IP 包，都需要经过 flannel.1 设备发出，并且，它最后被发往的网关地址是：10.1.16.0。</p>
</li>
<li><p>10.1.16.0 正是 Node 2 上的 VTEP 设备（也就是 flannel.1 设备）的 IP 地址。</p>
</li>
<li><p>“源 VTEP 设备”收到“原始 IP 包”后，就要想办法把“原始 IP 包”加上一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”（当然，这么做还是因为这个 IP 包的目的地址不是本机）。</p>
</li>
<li><p>这里需要解决的问题就是：“目的 VTEP 设备”的 MAC 地址是什么？</p>
<ul>
<li><p>此时，根据前面的路由记录，我们已经知道了“目的 VTEP 设备”的 IP 地址。而要根据三层 IP 地址查询对应的二层 MAC 地址，这正是 ARP（Address Resolution Protocol ）表的功能。</p>
</li>
<li><p>而这里要用到的 ARP 记录，也是 flanneld 进程在 Node 2 节点启动时，自动添加在 Node 1 上的。我们可以通过 ip 命令看到它，如下所示：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 在 Node 1 上</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ip neigh show dev flannel.1</span></span><br><span class="line">10.1.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>有了这个“目的 VTEP 设备”的 MAC 地址，Linux 内核就可以开始二层封包工作了。这个二层帧的格式，如下所示：<img src="/2021/07/03/8-%E7%BD%91%E7%BB%9C/vxlan%E5%86%85%E9%83%A8%E5%B8%A7.png" class=""></p>
</li>
<li><p>然后，Linux 内核会把这个数据帧封装进一个 UDP 包里发出去。</p>
</li>
<li><p>…</p>
</li>
<li><p>接下来，Node 1 上的 flannel.1 设备就可以把这个数据帧从 Node 1 的 eth0 网卡发出去。显然，这个帧会经过宿主机网络来到 Node 2 的 eth0 网卡。</p>
</li>
<li><p>这时候，Node 2 的内核网络栈会发现这个数据帧里有 VXLAN Header，并且 VNI=1。所以 Linux 内核会对它进行拆包，拿到里面的内部数据帧，然后根据 VNI 的值，把它交给 Node 2 上的 flannel.1 设备。</p>
</li>
<li><p>而 flannel.1 设备则会进一步拆包，取出“原始 IP 包”。接下来就回到了我在上一篇文章中分享的单机容器网络的处理流程。最终，IP 包就进入到了 container-2 容器的 Network Namespace 里。</p>
</li>
</ul>
<p><strong>不难看到，上面的例子有一个共性，那就是用户的容器都连接在 docker0 网桥上。而网络插件则在宿主机上创建了一个特殊的设备（UDP 模式创建的是 TUN 设备，VXLAN 模式创建的则是 VTEP 设备），docker0 与这个设备之间，通过 IP 转发（路由表）进行协作。<br>然后，网络插件真正要做的事情，则是通过某种方法，把不同宿主机上的特殊设备连通，从而达到容器跨主机通信的目的。</strong></p>
<p>实际上，上面这个流程，也正是 Kubernetes 对容器网络的主要处理方法。只不过，Kubernetes 是通过一个叫作 CNI 的接口，维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/07/02/7-pod/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/02/7-pod/" class="post-title-link" itemprop="url">7.pod</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-02 09:53:13" itemprop="dateCreated datePublished" datetime="2021-07-02T09:53:13+08:00">2021-07-02</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-04 11:35:39" itemprop="dateModified" datetime="2021-07-04T11:35:39+08:00">2021-07-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Pod，是 Kubernetes 项目中最小的 API 对象。如果换一个更专业的说法，我们可以这样描述：Pod，是 Kubernetes 项目的原子调度单位。</p>
<h2 id="为什么我们会需要-Pod"><a href="#为什么我们会需要-Pod" class="headerlink" title="为什么我们会需要 Pod"></a>为什么我们会需要 Pod</h2><ul>
<li>容器的本质到底是什么？ <strong>容器的本质是进程。</strong></li>
<li>容器，就是未来云计算系统中的进程；容器镜像就是这个系统里的“.exe”安装包。那么 Kubernetes 呢？</li>
<li>你应该也能立刻回答上来：Kubernetes 就是操作系统！</li>
<li>部署的应用，往往都存在着类似于“进程和进程组”的关系。更具体地说，就是这些应用之间有着密切的协作关系，使得它们必须部署在同一台机器上。</li>
</ul>
<p>当选择用pod做部署时，会选择资源合适的机器启动该pod。也就是说pod是基本单位</p>
<h2 id="pod-实现原理"><a href="#pod-实现原理" class="headerlink" title="pod 实现原理"></a>pod 实现原理</h2><ul>
<li><p>首先，关于 Pod 最重要的一个事实是：它只是一个逻辑概念。</p>
<ul>
<li>也就是说，Kubernetes 真正处理的，还是宿主机操作系统上 Linux 容器的 Namespace 和 Cgroups，而并不存在一个所谓的 Pod 的边界或者隔离环境。</li>
<li>Pod，其实是一组共享了某些资源的容器。</li>
<li><strong>Pod 里的所有容器，共享的是同一个 Network Namespace，并且可以声明共享同一个 Volume。</strong></li>
</ul>
</li>
<li><p>在 Kubernetes 项目里，Pod 的实现需要使用一个中间容器，这个容器叫作 Infra 容器。</p>
<ul>
<li><p>在这个 Pod 中，Infra 容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。</p>
</li>
<li><img src="/2021/07/02/7-pod/infra.png" class=""></li>
<li><p>Infra 容器一定要占用极少的资源，所以它使用的是一个非常特殊的镜像，叫作：k8s.gcr.io/pause。这个镜像是一个用汇编语言编写的、永远处于“暂停”状态的容器，解压后的大小也只有 100~200 KB 左右。</p>
</li>
<li><p>这也就意味着，对于 Pod 里的容器 A 和容器 B 来说：</p>
<ul>
<li>它们可以直接使用 localhost 进行通信；</li>
<li>它们看到的网络设备跟 Infra 容器看到的完全一样；</li>
<li>一个 Pod 只有一个 IP 地址，也就是这个 Pod 的 Network Namespace 对应的 IP 地址；</li>
<li>当然，其他的所有网络资源，都是一个 Pod 一份，并且被该 Pod 中的所有容器共享；</li>
<li>Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。</li>
</ul>
</li>
<li><p>而对于同一个 Pod 里面的所有用户容器来说，它们的进出流量，也可以认为都是通过 Infra 容器完成的。这一点很重要，因为将来如果你要为 Kubernetes 开发一个网络插件时，应该重点考虑的是如何配置这个 Pod 的 Network Namespace，而不是每一个用户容器如何使用你的网络配置，这是没有意义的。</p>
</li>
<li><p>有了这个设计之后，共享 Volume 就简单多了：Kubernetes 项目只要把所有 Volume 的定义都设计在 Pod 层级即可。</p>
</li>
<li><p>这样，一个 Volume 对应的宿主机目录对于 Pod 来说就只有一个，Pod 里的容器只要声明挂载这个 Volume，就一定可以共享这个 Volume 对应的宿主机目录。</p>
<pre><code><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">two-containers</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">restartPolicy:</span> <span class="string">Never</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">shared-data</span></span><br><span class="line">    <span class="attr">hostPath:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/data</span></span><br><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">shared-data</span></span><br><span class="line">    <span class="attr">mountPath:</span> <span class="string">/usr/share/nginx/html</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">debian-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">debian</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">shared-data</span></span><br><span class="line">    <span class="attr">mountPath:</span> <span class="string">/pod-data</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>]</span><br><span class="line">    <span class="attr">args:</span> [<span class="string">&quot;-c&quot;</span>, <span class="string">&quot;echo Hello from the debian container &gt; /pod-data/index.html&quot;</span>]</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>在这个例子中，debian-container 和 nginx-container 都声明挂载了 shared-data 这个 Volume。而 shared-data 是 hostPath 类型。所以，它对应在宿主机上的目录就是：/data。而这个目录，其实就被同时绑定挂载进了上述两个容器当中。</p>
</li>
</ul>
</li>
</ul>
<h3 id="容器设计模式"><a href="#容器设计模式" class="headerlink" title="容器设计模式"></a>容器设计模式</h3><p>Pod 这种“超亲密关系”容器的设计思想，实际上就是希望，当用户想在一个容器里跑多个功能并不相关的应用时，应该优先考虑它们是不是更应该被描述成一个 Pod 里的多个容器。</p>
<ol>
<li><p>通过initContainers 挂载目录 sidecar</p>
<p> 实际上，有了 Pod 之后，这样的问题就很容易解决了。我们可以把 WAR 包和 Tomcat 分别做成镜像，然后把它们作为一个 Pod 里的两个容器“组合”在一起。这个 Pod 的配置文件如下所示：</p>
<pre><code> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">javaweb-2</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">initContainers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">image:</span> <span class="string">geektime/sample:v2</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">war</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;cp&quot;</span>, <span class="string">&quot;/sample.war&quot;</span>, <span class="string">&quot;/app&quot;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/app</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">app-volume</span></span><br><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">image:</span> <span class="string">geektime/tomcat:7.0</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">tomcat</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;sh&quot;</span>,<span class="string">&quot;-c&quot;</span>,<span class="string">&quot;/root/apache-tomcat-7.0.42-v2/bin/start.sh&quot;</span>]</span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/root/apache-tomcat-7.0.42-v2/webapps</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">app-volume</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">8080</span></span><br><span class="line">    <span class="attr">hostPort:</span> <span class="number">8001</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">app-volume</span></span><br><span class="line">    <span class="attr">emptyDir:</span> &#123;&#125;</span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li>在 Pod 中，所有 Init Container 定义的容器，都会比 spec.containers 定义的用户容器先启动。并且，Init Container 容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动。</li>
<li>所以，这个 Init Container 类型的 WAR 包容器启动后，我执行了一句”cp /sample.war /app”，把应用的 WAR 包拷贝到 /app 目录下，然后退出。</li>
<li>而后这个 /app 目录，就挂载了一个名叫 app-volume 的 Volume。</li>
<li>接下来就很关键了。Tomcat 容器，同样声明了挂载 app-volume 到自己的 webapps 目录下。</li>
<li>等 Tomcat 容器启动时，它的 webapps 目录下就一定会存在 sample.war 文件：这个文件正是 WAR 包容器启动时拷贝到这个 Volume 里面的，而这个 Volume 是被这两个容器共享的。</li>
</ul>
<p> <strong>这个所谓的“组合”操作，正是容器设计模式里最常用的一种模式，它的名字叫：sidecar。</strong><br> 所以，我们用 Init Container 的方式优先运行 WAR 包容器，扮演了一个 sidecar 的角色。</p>
</li>
<li><p>日志收集，通过sidecar</p>
</li>
</ol>
<ul>
<li>比如，我现在有一个应用，需要不断地把日志文件输出到容器的 /var/log 目录中。</li>
<li>我就可以把一个 Pod 里的 Volume 挂载到应用容器的 /var/log 目录上。</li>
<li>然后，我在这个 Pod 里同时运行一个 sidecar 容器，它也声明挂载同一个 Volume 到自己的 /var/log 目录上。</li>
<li>接下来 sidecar 容器就只需要做一件事儿，那就是不断地从自己的 /var/log 目录里读取日志文件，转发到 MongoDB 或者 Elasticsearch 中存储起来。这样，一个最基本的日志收集工作就完成了</li>
</ul>
<p>Pod，实际上是在扮演传统基础设施里“虚拟机”的角色；而容器，则是这个虚拟机里运行的用户程序。</p>
<p>你就可以把整个虚拟机想象成为一个 Pod，把这些进程分别做成容器镜像，把有顺序关系的容器，定义为 Init Container。这才是更加合理的、松耦合的容器编排诀窍，也是从传统应用架构，到“微服务架构”最自然的过渡方式。</p>
<p>注意：Pod 这个概念，提供的是一种编排思想，而不是具体的技术方案。所以，如果愿意的话，你完全可以使用虚拟机来作为 Pod 的实现，然后把用户容器都运行在这个虚拟机里。比如，Mirantis 公司的virtlet 项目就在干这个事情。甚至，你可以去实现一个带有 Init 进程的容器项目，来模拟传统应用的运行方式。这些工作，在 Kubernetes 中都是非常轻松的，也是我们后面讲解 CRI 时会提到的内容。相反的，如果强行把整个应用塞到一个容器里，甚至不惜使用 Docker In Docker 这种在生产环境中后患无穷的解决方案，恐怕最后往往会得不偿失。</p>
<h2 id="pod基本概念"><a href="#pod基本概念" class="headerlink" title="pod基本概念"></a>pod基本概念</h2><ul>
<li>Pod，而不是容器，才是 Kubernetes 项目中的最小编排单位。</li>
<li>将这个设计落实到 API 对象上，容器（Container）就成了 Pod 属性里的一个普通的字段。</li>
</ul>
<p>那么，一个很自然的问题就是：到底哪些属性属于 Pod 对象，而又有哪些属性属于 Container 呢？</p>
<ul>
<li>凡是调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的。</li>
<li>这些属性的共同特征是，它们描述的是“机器”这个整体，而不是里面运行的“程序”。</li>
</ul>
<h3 id="Pod-中几个重要字段的含义和用法"><a href="#Pod-中几个重要字段的含义和用法" class="headerlink" title="Pod 中几个重要字段的含义和用法"></a>Pod 中几个重要字段的含义和用法</h3><ol>
<li><p>NodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段，用法如下所示：</p>
<pre><code> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nodeSelector:</span></span><br><span class="line">    <span class="attr">disktype:</span> <span class="string">ssd</span></span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li>这样的一个配置，意味着这个 Pod 永远只能运行在携带了“disktype: ssd”标签（Label）的节点上；否则，它将调度失败。</li>
</ul>
</li>
<li><p>NodeName：</p>
<ul>
<li>一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到。</li>
</ul>
</li>
<li><p>HostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容，用法如下：</p>
<pre><code> <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">hostAliases:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">ip:</span> <span class="string">&quot;10.1.2.3&quot;</span></span><br><span class="line">    <span class="attr">hostnames:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;foo.remote&quot;</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;bar.remote&quot;</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li>在这个 Pod 的 YAML 文件中，我设置了一组 IP 和 hostname 的数据。修改的就是/etc/hosts中的内容</li>
</ul>
</li>
</ol>
<p>凡是跟容器的 Linux Namespace 相关的属性，也一定是 Pod 级别的。</p>
<ul>
<li><p>原因也很容易理解：Pod 的设计，就是要让它里面的容器尽可能多地共享 Linux Namespace，仅保留必要的隔离和限制能力。这样，Pod 模拟出的效果，就跟虚拟机里程序间的关系非常类似了。</p>
</li>
<li><p>在下面这个 Pod 的 YAML 文件中，我定义了 shareProcessNamespace=true：</p>
<pre><code>  <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">shareProcessNamespace:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">shell</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">stdin:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">tty:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li><p>这就意味着这个 Pod 里的容器要共享 PID Namespace。</p>
</li>
<li><p>tty 和 stdin 在 Pod 的 YAML 文件里声明开启它们俩，其实等同于设置了 docker run 里的 -it（-i 即 stdin，-t 即 tty）参数。</p>
</li>
<li><p>如果你还是不太理解它们俩的作用的话，可以直接认为 tty 就是 Linux 给用户提供的一个常驻小程序，用于接收用户的标准输入，返回操作系统的标准输出。当然，为了能够在 tty 中输入信息，你还需要同时开启 stdin（标准输入流）。</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">kubectl create -f nginx.yaml</span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>接下来，我们使用 kubectl attach 命令，连接到 shell 容器的 tty 上</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">kubectl attach -it nginx -c shell</span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>这样，我们就可以在 shell 容器里执行 ps 指令，查看所有正在运行的进程：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl attach -it nginx -c shell</span></span><br><span class="line">/ # ps ax</span><br><span class="line">PID   USER     TIME  COMMAND</span><br><span class="line">    1 root      0:00 /pause</span><br><span class="line">    8 root      0:00 nginx: master process nginx -g daemon off;</span><br><span class="line">14 101       0:00 nginx: worker process</span><br><span class="line">15 root      0:00 sh</span><br><span class="line">21 root      0:00 ps ax</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>可以看到，在这个容器里，我们不仅可以看到它本身的 ps ax 指令，还可以看到 nginx 容器的进程，以及 Infra 容器的 /pause 进程。这就意味着，整个 Pod 里的每个容器的进程，对于所有容器来说都是可见的：它们共享了同一个 PID Namespace。</p>
</li>
</ul>
</li>
</ul>
<p>凡是 Pod 中的容器要共享宿主机的 Namespace，也一定是 Pod 级别的定义，比如：</p>
<pre><code><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">hostIPC:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">hostPID:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">shell</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">stdin:</span> <span class="literal">true</span></span><br><span class="line">    <span class="attr">tty:</span> <span class="literal">true</span></span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li>在这个 Pod 中，我定义了共享宿主机的 Network、IPC 和 PID Namespace。这就意味着，这个 Pod 里的所有容器，会直接使用宿主机的网络、直接与宿主机进行 IPC 通信、看到宿主机里正在运行的所有进程。</li>
</ul>
<h3 id="Containers"><a href="#Containers" class="headerlink" title="Containers"></a>Containers</h3><p>Init Containers 的生命周期，会先于所有的 Containers，并且严格按照定义的顺序执行,其他一样。</p>
<p>Container 的定义，和 Docker 相比并没有什么太大区别。我在前面的容器技术概念入门系列文章中，和你分享的 Image（镜像）、Command（启动命令）、workingDir（容器的工作目录）、Ports（容器要开发的端口），以及 volumeMounts（容器要挂载的 Volume）都是构成 Kubernetes 项目中 Container 的主要字段。不过在这里，还有这么几个属性值得你额外关注。</p>
<ol>
<li><p>首先，是 ImagePullPolicy 字段。</p>
<ul>
<li>它定义了镜像拉取的策略。而它之所以是一个 Container 级别的属性，是因为容器镜像本来就是 Container 定义中的一部分。</li>
<li>ImagePullPolicy 的值默认是 Always，即每次创建 Pod 都重新拉取一次镜像。另外，当容器的镜像是类似于 nginx 或者 nginx:latest 这样的名字时，ImagePullPolicy 也会被认为 Always。</li>
<li>如果它的值被定义为 Never 或者 IfNotPresent，则意味着 Pod 永远不会主动拉取这个镜像，或者只在宿主机上不存在这个镜像时才拉取。</li>
</ul>
</li>
<li><p>其次，是 Lifecycle 字段 Container Lifecycle Hooks</p>
<ul>
<li><p>顾名思义，Container Lifecycle Hooks 的作用，是在容器状态发生变化时触发一系列“钩子”。我们来看这样一个例子：</p>
<pre><code>   <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">lifecycle-demo</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">lifecycle-demo-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">lifecycle:</span></span><br><span class="line">    <span class="attr">postStart:</span></span><br><span class="line">        <span class="attr">exec:</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;/bin/sh&quot;</span>, <span class="string">&quot;-c&quot;</span>, <span class="string">&quot;echo Hello from the postStart handler &gt; /usr/share/message&quot;</span>]</span><br><span class="line">    <span class="attr">preStop:</span></span><br><span class="line">        <span class="attr">exec:</span></span><br><span class="line">        <span class="attr">command:</span> [<span class="string">&quot;/usr/sbin/nginx&quot;</span>,<span class="string">&quot;-s&quot;</span>,<span class="string">&quot;quit&quot;</span>]</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>先说 postStart 吧。它指的是，在容器启动后，立刻执行一个指定的操作。需要明确的是，postStart 定义的操作，虽然是在 Docker 容器 ENTRYPOINT 执行之后，但它并不严格保证顺序。也就是说，在 postStart 启动时，ENTRYPOINT 有可能还没有结束。</p>
</li>
<li><p>而类似地，preStop 发生的时机，则是容器被杀死之前（比如，收到了 SIGKILL 信号）。而需要明确的是，preStop 操作的执行，是同步的。所以，它会阻塞当前的容器杀死流程，直到这个 Hook 定义操作完成之后，才允许容器被杀死，这跟 postStart 不一样。</p>
</li>
</ul>
</li>
</ol>
<h3 id="Pod-对象在-Kubernetes-中的生命周期"><a href="#Pod-对象在-Kubernetes-中的生命周期" class="headerlink" title="Pod 对象在 Kubernetes 中的生命周期"></a>Pod 对象在 Kubernetes 中的生命周期</h3><p>Pod 生命周期的变化，主要体现在 Pod API 对象的Status 部分，这是它除了 Metadata 和 Spec 之外的第三个重要字段。其中，pod.status.phase，就是 Pod 的当前状态，它有如下几种可能的情况：</p>
<ul>
<li>Pending。这个状态意味着，Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod 里有些容器因为某种原因而不能被顺利创建。比如，调度不成功。</li>
<li>Running。这个状态下，Pod 已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中。</li>
<li>Succeeded。这个状态意味着，Pod 里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。</li>
<li>Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状态的出现，意味着你得想办法 Debug 这个容器的应用，比如查看 Pod 的 Events 和日志。</li>
<li>Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。</li>
</ul>
<p>更进一步地，Pod 对象的 Status 字段，还可以再细分出一组 Conditions。这些细分状态的值包括：PodScheduled、Ready、Initialized，以及 Unschedulable。它们主要用于描述造成当前 Status 的具体原因是什么。</p>
<ul>
<li>比如，Pod 当前的 Status 是 Pending，对应的 Condition 是 Unschedulable，这就意味着它的调度出现了问题。</li>
<li>而其中，Ready 这个细分状态非常值得我们关注：它意味着 Pod 不仅已经正常启动（Running 状态），而且已经可以对外提供服务了。这两者之间（Running 和 Ready）是有区别的，你不妨仔细思考一下。</li>
</ul>
<p>更多信息可以看 $GOPATH/src/k8s.io/kubernetes/vendor/k8s.io/api/core/v1/types.go 里，type Pod struct ，尤其是 PodSpec 部分的内容。</p>
<h2 id="特殊的-Volume，叫作-Projected-Volume，你可以把它翻译为“投射数据卷”"><a href="#特殊的-Volume，叫作-Projected-Volume，你可以把它翻译为“投射数据卷”" class="headerlink" title="特殊的 Volume，叫作 Projected Volume，你可以把它翻译为“投射数据卷”"></a>特殊的 Volume，叫作 Projected Volume，你可以把它翻译为“投射数据卷”</h2><p>Kubernetes 支持的 Projected Volume 一共有四种：</p>
<ul>
<li>Secret；</li>
<li>ConfigMap；</li>
<li>Downward API；</li>
<li>ServiceAccountToken。</li>
</ul>
<h3 id="Secret"><a href="#Secret" class="headerlink" title="Secret"></a>Secret</h3><p>它的作用，是帮你把 Pod 想要访问的加密数据，存放到 Etcd 中。然后，你就可以通过在 Pod 的容器里挂载 Volume 的方式，访问到这些 Secret 里保存的信息了。</p>
<p>Secret 最典型的使用场景，莫过于存放数据库的 Credential 信息，比如下面这个例子：</p>
<pre><code><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">test-projected-volume</span> </span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">test-secret-volume</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">args:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">sleep</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">&quot;86400&quot;</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mysql-cred</span></span><br><span class="line">    <span class="attr">mountPath:</span> <span class="string">&quot;/projected-volume&quot;</span></span><br><span class="line">    <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">mysql-cred</span></span><br><span class="line">    <span class="attr">projected:</span></span><br><span class="line">    <span class="attr">sources:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">secret:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">user</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">secret:</span></span><br><span class="line">        <span class="attr">name:</span> <span class="string">pass</span></span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li><p>在这个 Pod 中，我定义了一个简单的容器。它声明挂载的 Volume，并不是常见的 emptyDir 或者 hostPath 类型，而是 projected 类型。</p>
</li>
<li><p>而这个 Volume 的数据来源（sources），则是名为 user 和 pass 的 Secret 对象，分别对应的是数据库的用户名和密码。</p>
</li>
<li><p>这里用到的数据库的用户名、密码，正是以 Secret 对象的方式交给 Kubernetes 保存的。完成这个操作的指令，如下所示：</p>
<pre><code>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat ./username.txt</span></span><br><span class="line">admin</span><br><span class="line"><span class="meta">$</span><span class="bash"> cat ./password.txt</span></span><br><span class="line">c1oudc0w!</span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl create secret generic user --from-file=./username.txt</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl create secret generic pass --from-file=./password.txt</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get secrets</span></span><br><span class="line">NAME           TYPE                                DATA      AGE</span><br><span class="line">user          Opaque                                1         51s</span><br><span class="line">pass          Opaque                                1         51s</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">要求base64转码，避免明文</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> -n <span class="string">&#x27;admin&#x27;</span> | base64</span></span><br><span class="line">YWRtaW4=</span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">echo</span> -n <span class="string">&#x27;1f2d1e2e67df&#x27;</span> | base64</span></span><br><span class="line">MWYyZDFlMmU2N2Rm</span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li><p>还可以通过yml方式</p>
<pre><code><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Secret</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">mysecret</span></span><br><span class="line"><span class="attr">type:</span> <span class="string">Opaque</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">    <span class="attr">user:</span> <span class="string">YWRtaW4=</span></span><br><span class="line">    <span class="attr">pass:</span> <span class="string">MWYyZDFlMmU2N2Rm</span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>启动pod,验证</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">kubectl create -f test-projected-volume.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl <span class="built_in">exec</span> -it test-projected-volume -- /bin/sh</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> ls /projected-volume/</span></span><br><span class="line">user</span><br><span class="line">pass</span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /projected-volume/user</span></span><br><span class="line">root</span><br><span class="line"><span class="meta">$</span><span class="bash"> cat /projected-volume/pass</span></span><br><span class="line">1f2d1e2e67df</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
<h3 id="ConfigMap"><a href="#ConfigMap" class="headerlink" title="ConfigMap"></a>ConfigMap</h3><p>它与 Secret 的区别在于，ConfigMap 保存的是不需要加密的、应用所需的配置信息。</p>
<p>而 ConfigMap 的用法几乎与 Secret 完全相同：你可以使用 kubectl create configmap 从文件或者目录创建 ConfigMap，也可以直接编写 ConfigMap 对象的 YAML 文件。</p>
<p>比如，一个 Java 应用所需的配置文件（.properties 文件），就可以通过下面这样的方式保存在 ConfigMap 里：</p>
<pre><code>    <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> .properties 文件的内容</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat example/ui.properties</span></span><br><span class="line">color.good=purple</span><br><span class="line">color.bad=yellow</span><br><span class="line">allow.textmode=true</span><br><span class="line">how.nice.to.look=fairlyNice</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 从.properties 文件创建 ConfigMap</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl create configmap ui-config --from-file=example/ui.properties</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看这个 ConfigMap 里保存的信息 (data)</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get configmaps ui-config -o yaml</span></span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">ui.properties: |</span><br><span class="line">    color.good=purple</span><br><span class="line">    color.bad=yellow</span><br><span class="line">    allow.textmode=true</span><br><span class="line">    how.nice.to.look=fairlyNice</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">name: ui-config</span><br><span class="line">...</span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li>备注：kubectl get -o yaml 这样的参数，会将指定的 Pod API 对象以 YAML 的方式展示出来。</li>
</ul>
<h3 id="Downward-API"><a href="#Downward-API" class="headerlink" title="Downward API"></a>Downward API</h3><p>它的作用是：让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。</p>
<pre><code><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">test-downwardapi-volume</span></span><br><span class="line"><span class="attr">labels:</span></span><br><span class="line">    <span class="attr">zone:</span> <span class="string">us-est-coast</span></span><br><span class="line">    <span class="attr">cluster:</span> <span class="string">test-cluster1</span></span><br><span class="line">    <span class="attr">rack:</span> <span class="string">rack-22</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">containers:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">client-container</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">k8s.gcr.io/busybox</span></span><br><span class="line">    <span class="attr">command:</span> [<span class="string">&quot;sh&quot;</span>, <span class="string">&quot;-c&quot;</span>]</span><br><span class="line">    <span class="attr">args:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">while</span> <span class="literal">true</span><span class="string">;</span> <span class="string">do</span></span><br><span class="line">        <span class="string">if</span> [[ <span class="string">-e</span> <span class="string">/etc/podinfo/labels</span> ]]<span class="string">;</span> <span class="string">then</span></span><br><span class="line">            <span class="string">echo</span> <span class="string">-en</span> <span class="string">&#x27;\n\n&#x27;</span><span class="string">;</span> <span class="string">cat</span> <span class="string">/etc/podinfo/labels;</span> <span class="string">fi;</span></span><br><span class="line">        <span class="string">sleep</span> <span class="number">5</span><span class="string">;</span></span><br><span class="line">        <span class="string">done;</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">podinfo</span></span><br><span class="line">        <span class="attr">mountPath:</span> <span class="string">/etc/podinfo</span></span><br><span class="line">        <span class="attr">readOnly:</span> <span class="literal">false</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">podinfo</span></span><br><span class="line">    <span class="attr">projected:</span></span><br><span class="line">        <span class="attr">sources:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">downwardAPI:</span></span><br><span class="line">            <span class="attr">items:</span></span><br><span class="line">            <span class="bullet">-</span> <span class="attr">path:</span> <span class="string">&quot;labels&quot;</span></span><br><span class="line">                <span class="attr">fieldRef:</span></span><br><span class="line">                <span class="attr">fieldPath:</span> <span class="string">metadata.labels</span></span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li><p>在这个 Pod 的 YAML 文件中，我定义了一个简单的容器，声明了一个 projected 类型的 Volume。只不过这次 Volume 的数据来源，变成了 Downward API。而这个 Downward API Volume，则声明了要暴露 Pod 的 metadata.labels 信息给容器。</p>
</li>
<li><p>通过这样的声明方式，当前 Pod 的 Labels 字段的值，就会被 Kubernetes 自动挂载成为容器里的 /etc/podinfo/labels 文件。</p>
</li>
<li><p>而这个容器的启动命令，则是不断打印出 /etc/podinfo/labels 里的内容。所以，当我创建了这个 Pod 之后，就可以通过 kubectl logs 指令，查看到这些 Labels 字段被打印出来，如下所示：</p>
<pre><code>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl create -f dapi-volume.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl logs test-downwardapi-volume</span></span><br><span class="line">cluster=&quot;test-cluster1&quot;</span><br><span class="line">rack=&quot;rack-22&quot;</span><br><span class="line">zone=&quot;us-est-coast&quot;</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
<p>目前，Downward API 支持的字段已经非常丰富了，比如：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">1. 使用 fieldRef 可以声明使用:</span><br><span class="line">spec.nodeName - 宿主机名字</span><br><span class="line">status.hostIP - 宿主机 IP</span><br><span class="line">metadata.name - Pod 的名字</span><br><span class="line">metadata.namespace - Pod 的 Namespace</span><br><span class="line">status.podIP - Pod 的 IP</span><br><span class="line">spec.serviceAccountName - Pod 的 Service Account 的名字</span><br><span class="line">metadata.uid - Pod 的 UID</span><br><span class="line">metadata.labels[&#x27;&lt;KEY&gt;&#x27;] - 指定 &lt;KEY&gt; 的 Label 值</span><br><span class="line">metadata.annotations[&#x27;&lt;KEY&gt;&#x27;] - 指定 &lt;KEY&gt; 的 Annotation 值</span><br><span class="line">metadata.labels - Pod 的所有 Label</span><br><span class="line">metadata.annotations - Pod 的所有 Annotation</span><br><span class="line">2. 使用 resourceFieldRef 可以声明使用:</span><br><span class="line">容器的 CPU limit</span><br><span class="line">容器的 CPU request</span><br><span class="line">容器的 memory limit</span><br><span class="line">容器的 memory request</span><br></pre></td></tr></table></figure>
</code></pre>
<h3 id="Service-Account-ServiceAccountToken"><a href="#Service-Account-ServiceAccountToken" class="headerlink" title="Service Account (ServiceAccountToken)"></a>Service Account (ServiceAccountToken)</h3><p>我现在有了一个 Pod，我能不能在这个 Pod 里安装一个 Kubernetes 的 Client，这样就可以从容器里直接访问并且操作这个 Kubernetes 的 API 了呢？</p>
<ul>
<li>你首先要解决 API Server 的授权问题。<ul>
<li>Service Account 对象的作用，就是 Kubernetes 系统内置的一种“服务账户”，它是 Kubernetes 进行权限分配的对象。</li>
<li>比如，Service Account A，可以只被允许对 Kubernetes API 进行 GET 操作，</li>
<li>而 Service Account B，则可以有 Kubernetes API 的所有操作的权限。</li>
</ul>
</li>
</ul>
<h2 id="再来看-Pod-的另一个重要的配置：容器健康检查和恢复机制"><a href="#再来看-Pod-的另一个重要的配置：容器健康检查和恢复机制" class="headerlink" title="再来看 Pod 的另一个重要的配置：容器健康检查和恢复机制"></a>再来看 Pod 的另一个重要的配置：容器健康检查和恢复机制</h2><p>我们一起来看一个 Kubernetes 文档中的例子</p>
<pre><code><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">labels:</span></span><br><span class="line">    <span class="attr">test:</span> <span class="string">liveness</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">test-liveness-exec</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">liveness</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">busybox</span></span><br><span class="line">    <span class="attr">args:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">/bin/sh</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">-c</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">touch</span> <span class="string">/tmp/healthy;</span> <span class="string">sleep</span> <span class="number">30</span><span class="string">;</span> <span class="string">rm</span> <span class="string">-rf</span> <span class="string">/tmp/healthy;</span> <span class="string">sleep</span> <span class="number">600</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">    <span class="attr">exec:</span></span><br><span class="line">        <span class="attr">command:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">cat</span></span><br><span class="line">        <span class="bullet">-</span> <span class="string">/tmp/healthy</span></span><br><span class="line">    <span class="attr">initialDelaySeconds:</span> <span class="number">5</span></span><br><span class="line">    <span class="attr">periodSeconds:</span> <span class="number">5</span></span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li>在这个 Pod 中，我们定义了一个有趣的容器。它在启动之后做的第一件事，就是在 /tmp 目录下创建了一个 healthy 文件，以此作为自己已经正常运行的标志。而 30 s 过后，它会把这个文件删除掉。</li>
<li>与此同时，我们定义了一个这样的 livenessProbe（健康检查）。它的类型是 exec，这意味着，它会在容器启动后，在容器里面执行一句我们指定的命令，比如：“cat /tmp/healthy”。这时，如果这个文件存在，这条命令的返回值就是 0，Pod 就会认为这个容器不仅已经启动，而且是健康的。这个健康检查，在容器启动 5 s 后开始执行（initialDelaySeconds: 5），每 5 s 执行一次（periodSeconds: 5）</li>
</ul>
<p>你还可以通过设置 restartPolicy，改变 Pod 的恢复策略。除了 Always，它还有 OnFailure 和 Never 两种情况：</p>
<ul>
<li><p>Always：在任何情况下，只要容器不在运行状态，就自动重启容器；</p>
</li>
<li><p>OnFailure: 只在容器 异常时才自动重启容器；</p>
</li>
<li><p>Never: 从来不重启容器。</p>
</li>
<li><p>只要 Pod 的 restartPolicy 指定的策略允许重启异常的容器（比如：Always），那么这个 Pod 就会保持 Running 状态，并进行容器重启。否则，Pod 就会进入 Failed 状态 。</p>
</li>
<li><p>对于包含多个容器的 Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态。</p>
</li>
</ul>
<p>PodPreset 里定义的内容，只会在 Pod API 对象被创建之前追加在这个对象本身上，而不会影响任何 Pod 的控制器的定义。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/07/01/6-%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84Kubernetes%E9%9B%86%E7%BE%A4/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/01/6-%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84Kubernetes%E9%9B%86%E7%BE%A4/" class="post-title-link" itemprop="url">6.搭建一个完整的Kubernetes集群</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-01 20:56:08" itemprop="dateCreated datePublished" datetime="2021-07-01T20:56:08+08:00">2021-07-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-03 10:13:29" itemprop="dateModified" datetime="2021-07-03T10:13:29+08:00">2021-07-03</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><ul>
<li>在所有节点上安装 Docker 和 kubeadm；</li>
<li>部署 Kubernetes Master；</li>
<li>部署容器网络插件；</li>
<li>部署 Kubernetes Worker；</li>
<li>部署 Dashboard 可视化插件；</li>
<li>部署容器存储插件。</li>
</ul>
<h2 id="安装-kubeadm-和-Docker"><a href="#安装-kubeadm-和-Docker" class="headerlink" title="安装 kubeadm 和 Docker"></a>安装 kubeadm 和 Docker</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> cat &lt;&lt;<span class="string">EOF &gt; /etc/apt/sources.list.d/kubernetes.list</span></span></span><br><span class="line">deb http://apt.kubernetes.io/ kubernetes-xenial main</span><br><span class="line">EOF</span><br><span class="line"><span class="meta">$</span><span class="bash"> apt-get update</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> apt-get install -y docker.io kubeadm</span></span><br></pre></td></tr></table></figure>

<h2 id="部署-Kubernetes-的-Master-节点"><a href="#部署-Kubernetes-的-Master-节点" class="headerlink" title="部署 Kubernetes 的 Master 节点"></a>部署 Kubernetes 的 Master 节点</h2><p>这里我编写了一个给 kubeadm 用的 YAML 文件（名叫：kubeadm.yaml）：</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">MasterConfiguration</span></span><br><span class="line"><span class="attr">controllerManagerExtraArgs:</span></span><br><span class="line">  <span class="attr">horizontal-pod-autoscaler-use-rest-clients:</span> <span class="string">&quot;true&quot;</span></span><br><span class="line">  <span class="attr">horizontal-pod-autoscaler-sync-period:</span> <span class="string">&quot;10s&quot;</span></span><br><span class="line">  <span class="attr">node-monitor-grace-period:</span> <span class="string">&quot;10s&quot;</span></span><br><span class="line"><span class="attr">apiServerExtraArgs:</span></span><br><span class="line">  <span class="attr">runtime-config:</span> <span class="string">&quot;api/all=true&quot;</span></span><br><span class="line"><span class="attr">kubernetesVersion:</span> <span class="string">&quot;stable-1.11&quot;</span></span><br></pre></td></tr></table></figure>

<p>然后，我们只需要执行一句指令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">kubeadm init --config kubeadm.yaml</span></span><br></pre></td></tr></table></figure>

<p>部署完成后，kubeadm 会生成一行指令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubeadm join 10.168.0.2:6443 --token 00bwbx.uvnaa2ewjflwu1ry --discovery-token-ca-cert-hash sha256:00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711</span><br></pre></td></tr></table></figure>

<p>这个 kubeadm join 命令，就是用来给这个 Master 节点添加更多工作节点（Worker）的命令。我们在后面部署 Worker 节点的时候马上会用到它，所以找一个地方把这条命令记录下来。</p>
<p>现在，我们就可以使用 kubectl get 命令来查看当前唯一一个节点的状态了：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get nodes</span></span><br><span class="line">NAME      STATUS     ROLES     AGE       VERSION</span><br><span class="line">master    NotReady   master    1d        v1.11.1</span><br></pre></td></tr></table></figure>

<p>kubectl describe node master对象的详细信息、状态和事件（Event）</p>
<h2 id="部署网络插件"><a href="#部署网络插件" class="headerlink" title="部署网络插件"></a>部署网络插件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$kubectl apply -f https:&#x2F;&#x2F;git.io&#x2F;weave-kube-1.6</span><br></pre></td></tr></table></figure>

<p>部署完成后，我们可以通过 kubectl get 重新检查 Pod 的状态：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods -n kube-system</span></span><br><span class="line"></span><br><span class="line">NAME                             READY     STATUS    RESTARTS   AGE</span><br><span class="line">coredns-78fcdf6894-j9s52         1/1       Running   0          1d</span><br><span class="line">coredns-78fcdf6894-jm4wf         1/1       Running   0          1d</span><br><span class="line">etcd-master                      1/1       Running   0          9s</span><br><span class="line">kube-apiserver-master            1/1       Running   0          9s</span><br><span class="line">kube-controller-manager-master   1/1       Running   0          9s</span><br><span class="line">kube-proxy-xbd47                 1/1       Running   0          1d</span><br><span class="line">kube-scheduler-master            1/1       Running   0          9s</span><br><span class="line">weave-net-cmk27                  2/2       Running   0          19s</span><br></pre></td></tr></table></figure>

<h2 id="部署-Kubernetes-的-Worker-节点"><a href="#部署-Kubernetes-的-Worker-节点" class="headerlink" title="部署 Kubernetes 的 Worker 节点"></a>部署 Kubernetes 的 Worker 节点</h2><p>Kubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 Pod。</p>
<p>所以，相比之下，部署 Worker 节点反而是最简单的，只需要两步即可完成。</p>
<ul>
<li>第一步，在所有 Worker 节点上执行“安装 kubeadm 和 Docker”一节的所有步骤。</li>
<li>第二步，执行部署 Master 节点时生成的 kubeadm join 指令：<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">kubeadm join 10.168.0.2:6443 --token 00bwbx.uvnaa2ewjflwu1ry --discovery-token-ca-cert-hash sha256:00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="通过-Taint-Toleration-调整-Master-执行-Pod-的策略"><a href="#通过-Taint-Toleration-调整-Master-执行-Pod-的策略" class="headerlink" title="通过 Taint/Toleration 调整 Master 执行 Pod 的策略"></a>通过 Taint/Toleration 调整 Master 执行 Pod 的策略</h3><ul>
<li><p>我在前面提到过，默认情况下 Master 节点是不允许运行用户 Pod 的。而 Kubernetes 做到这一点，依靠的是 Kubernetes 的 Taint/Toleration 机制。</p>
</li>
<li><p>它的原理非常简单：一旦某个节点被加上了一个 Taint，即被“打上了污点”，那么所有 Pod 就都不能在这个节点上运行，因为 Kubernetes 的 Pod 都有“洁癖”</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">为节点打上“污点”（Taint）的命令</span></span><br><span class="line"><span class="meta">$</span><span class="bash">kubectl taint nodes node1 foo=bar:NoSchedule</span></span><br></pre></td></tr></table></figure></li>
<li><p>除非，有个别的 Pod 声明自己能“容忍”这个“污点”，即声明了 Toleration，它才可以在这个节点上运行。</p>
<ul>
<li><p>我们只要在 Pod 的.yaml 文件中的 spec 部分，加入 tolerations 字段即可：</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">tolerations:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">key:</span> <span class="string">&quot;foo&quot;</span></span><br><span class="line">    <span class="attr">operator:</span> <span class="string">&quot;Equal&quot;</span></span><br><span class="line">    <span class="attr">value:</span> <span class="string">&quot;bar&quot;</span></span><br><span class="line">    <span class="attr">effect:</span> <span class="string">&quot;NoSchedule&quot;</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="部署-Dashboard-可视化插件"><a href="#部署-Dashboard-可视化插件" class="headerlink" title="部署 Dashboard 可视化插件"></a>部署 Dashboard 可视化插件</h2><p>在 Kubernetes 社区中，有一个很受欢迎的 Dashboard 项目，它可以给用户提供一个可视化的 Web 界面来查看当前集群的各种信息。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml</span></span><br></pre></td></tr></table></figure>

<p>部署完成之后，我们就可以查看 Dashboard 对应的 Pod 的状态了：</p>
<h2 id="部署容器存储插件"><a href="#部署容器存储插件" class="headerlink" title="部署容器存储插件"></a>部署容器存储插件</h2><p>用两条指令，Rook 就可以把复杂的 Ceph 存储后端部署起来：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml</span></span><br><span class="line"><span class="meta">$</span><span class="bash">kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods -n rook-ceph-system</span></span><br><span class="line">NAME                                  READY     STATUS    RESTARTS   AGE</span><br><span class="line">rook-ceph-agent-7cv62                 1/1       Running   0          15s</span><br><span class="line">rook-ceph-operator-78d498c68c-7fj72   1/1       Running   0          44s</span><br><span class="line">rook-discover-2ctcv                   1/1       Running   0          15s</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods -n rook-ceph</span></span><br><span class="line">NAME                   READY     STATUS    RESTARTS   AGE</span><br><span class="line">rook-ceph-mon0-kxnzh   1/1       Running   0          13s</span><br><span class="line">rook-ceph-mon1-7dn2t   1/1       Running   0          2s</span><br></pre></td></tr></table></figure>

<h2 id="我的第一个容器化应用"><a href="#我的第一个容器化应用" class="headerlink" title="我的第一个容器化应用"></a>我的第一个容器化应用</h2><p>通过编写yml文件通过指令（$ kubectl create -f 我的配置文件）运行他们</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<ul>
<li>所谓 Deployment，是一个定义多副本应用（即多个副本 Pod）的对象</li>
<li>此外，Deployment 还负责在 Pod 定义发生变化时，对每个副本进行滚动更新（Rolling Update）。</li>
<li>我给它定义的 Pod 副本个数 (spec.replicas) 是：2。</li>
<li>Pod 具体的又长什么样子,我定义了一个 Pod 模版（spec.template），这个模版描述了我想要创建的 Pod 的细节。</li>
<li>这个 Pod 里只有一个容器，这个容器的镜像（spec.containers.image）是 nginx:1.7.9，这个容器监听端口（containerPort）是 80。</li>
<li><strong>Pod 就是 Kubernetes 世界里的“应用”；而一个应用，可以由多个容器组成。</strong></li>
<li>像这样使用一种 API 对象（Deployment）管理另一种 API 对象（Pod）的方法，在 Kubernetes 中，叫作“控制器”模式（controller pattern）。</li>
<li>每一个 API 对象都有一个叫作 Metadata 的字段，这个字段就是 API 对象的“标识”，即元数据，它也是我们从 Kubernetes 里找到这个对象的主要依据。这其中最主要使用到的字段是 Labels。</li>
<li>而像 Deployment 这样的控制器对象，就可以通过这个 Labels 字段从 Kubernetes 中过滤出它所关心的被控制对象。</li>
<li>比如，在上面这个 YAML 文件中，Deployment 会把所有正在运行的、携带“app: nginx”标签的 Pod 识别为被管理的对象，并确保这些 Pod 的总数严格等于两个。</li>
<li>而这个过滤规则的定义，是在 Deployment 的“spec.selector.matchLabels”字段。我们一般称之为：Label Selector。</li>
<li>另外，在 Metadata 中，还有一个与 Labels 格式、层级完全相同的字段叫 Annotations，它专门用来携带 key-value 格式的内部信息</li>
<li>所谓内部信息，指的是对这些信息感兴趣的，是 Kubernetes 组件本身，而不是用户。所以大多数 Annotations，都是在 Kubernetes 运行过程中，被自动加在这个 API 对象上。</li>
<li>一个 Kubernetes 的 API 对象的定义，大多可以分为 Metadata 和 Spec 两个部分。前者存放的是这个对象的元数据，对所有 API 对象来说，这一部分的字段和格式基本上是一样的；而后者存放的，则是属于这个对象独有的定义，用来描述它所要表达的功能。</li>
</ul>
<p>运行 查看yml运行状态 查看API对象细节</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">kubectl create -f nginx-deployment.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl get pods -l app=nginx</span></span><br><span class="line">NAME                                READY     STATUS    RESTARTS   AGE</span><br><span class="line">nginx-deployment-67594d6bf6-9gdvr   1/1       Running   0          10m</span><br><span class="line">nginx-deployment-67594d6bf6-v6j7w   1/1       Running   0          10m</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl describe pod nginx-deployment-67594d6bf6-9gdvr</span></span><br></pre></td></tr></table></figure>

<p>kubectl get 指令的作用，就是从 Kubernetes 里面获取（GET）指定的 API 对象。可以看到，在这里我还加上了一个 -l 参数，即获取所有匹配 app: nginx 标签的 Pod。需要注意的是，在命令行中，所有 key-value 格式的参数，都使用“=”而非“:”表示。</p>
<p>describe很重要，如果有异常发生，你一定要第一时间查看这些其中的Events，往往可以看到非常详细的错误信息。</p>
<h3 id="升级"><a href="#升级" class="headerlink" title="升级"></a>升级</h3><p>修改yml即可</p>
<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.8</span> <span class="comment"># 这里被从 1.7.9 修改为 1.8</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br><span class="line">      <span class="attr">volumeMounts:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">&quot;/usr/share/nginx/html&quot;</span></span><br><span class="line">          <span class="attr">name:</span> <span class="string">nginx-vol</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-vol</span></span><br><span class="line">        <span class="attr">emptyDir:</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment"># 或者：</span></span><br><span class="line">      <span class="attr">volumes:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx-vol</span></span><br><span class="line">          <span class="attr">hostPath:</span></span><br><span class="line">            <span class="attr">path:</span> <span class="string">/var/data</span></span><br></pre></td></tr></table></figure>

<p>我们可以使用 kubectl replace 或者apply指令来完成这个更新：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash">kubectl replace -f nginx-deployment.yaml</span></span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f nginx-deployment.yaml</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 修改 nginx-deployment.yaml 的内容</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> kubectl apply -f nginx-deployment.yaml</span></span><br></pre></td></tr></table></figure>

<p>而 Pod 中的容器，使用的是 volumeMounts 字段来声明自己要挂载哪个 Volume，并通过 mountPath 字段来定义容器内的 Volume 目录，比如：/usr/share/nginx/html。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/07/01/5-Kubernetes%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E5%88%A9%E5%99%A8kubeadm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/01/5-Kubernetes%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E5%88%A9%E5%99%A8kubeadm/" class="post-title-link" itemprop="url">5.Kubernetes一键部署利器kubeadm</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-01 15:25:57" itemprop="dateCreated datePublished" datetime="2021-07-01T15:25:57+08:00">2021-07-01</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-02 08:09:54" itemprop="dateModified" datetime="2021-07-02T08:09:54+08:00">2021-07-02</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p><strong>要真正发挥容器技术的实力，你就不能仅仅局限于对 Linux 容器本身的钻研和使用。</strong></p>
<p>更深入的学习容器技术的关键在于，如何使用这些技术来“容器化”你的应用。</p>
<ul>
<li>我们的应用既可能是 Java Web 和 MySQL 这样的组合</li>
<li>也可能是 Cassandra 这样的分布式系统。哪些 Cassandra 容器是主，哪些是从？主从容器如何区分？它们之间又如何进行自动发现和通信？Cassandra 容器的持久化数据又如何保持，等等。</li>
<li>这也是为什么我们要反复强调 Kubernetes 项目的主要原因：这个项目体现出来的容器化“表达能力”，具有独有的先进性和完备性。这就使得它不仅能运行 Java Web 与 MySQL 这样的常规组合，还能够处理 Cassandra 容器集群等复杂编排问题。所以，对这种编排能力的剖析、解读和最佳实践，将是本专栏最重要的一部分内容。</li>
</ul>
<h2 id="Kubernetes-项目简单的部署方法-kubeadm"><a href="#Kubernetes-项目简单的部署方法-kubeadm" class="headerlink" title="Kubernetes 项目简单的部署方法 kubeadm"></a>Kubernetes 项目简单的部署方法 kubeadm</h2><h3 id="部署方式"><a href="#部署方式" class="headerlink" title="部署方式"></a>部署方式</h3><p>这个项目的目的，就是要让用户能够通过这样两条指令完成一个 Kubernetes 集群的部署：</p>
<pre><code># 创建一个 Master 节点
$ kubeadm init
# 将一个 Node 节点加入到当前集群中
$ kubeadm join &lt;Master 节点的 IP 和端口 &gt;
</code></pre>
<h2 id="kubeadm-的工作原理"><a href="#kubeadm-的工作原理" class="headerlink" title="kubeadm 的工作原理"></a>kubeadm 的工作原理</h2><h3 id="Kubernetes-部署问题"><a href="#Kubernetes-部署问题" class="headerlink" title="Kubernetes 部署问题"></a>Kubernetes 部署问题</h3><ul>
<li>到目前为止，在容器里运行 kubelet，依然没有很好的解决办法，我也不推荐你用容器去部署 Kubernetes 项目。</li>
<li>把 kubelet 直接运行在宿主机上，然后使用容器部署其他的 Kubernetes 组件。</li>
</ul>
<ol>
<li><p>你使用 kubeadm 的第一步，是在机器上手动安装 kubeadm、kubelet 和 kubectl 这三个二进制文件。当然，kubeadm 的作者已经为各个发行版的 Linux 准备好了安装包，所以你只需要执行：</p>
<ul>
<li>$ apt-get install kubeadm</li>
</ul>
</li>
<li><p>使用“kubeadm init”部署 Master 节点了。</p>
</li>
</ol>
<h3 id="kubeadm-init-的工作流程"><a href="#kubeadm-init-的工作流程" class="headerlink" title="kubeadm init 的工作流程"></a>kubeadm init 的工作流程</h3><ol>
<li><p>kubeadm 首先要做的，是一系列的检查工作，以确定这台机器可以用来部署 Kubernetes。这一步检查，我们称为“Preflight Checks”，它可以为你省掉很多后续的麻烦。</p>
<p> Preflight Checks 包括了很多方面，比如：</p>
<ul>
<li>Linux 内核的版本必须是否是 3.10 以上？</li>
<li>Linux Cgroups 模块是否可用？</li>
<li>机器的 hostname 是否标准？在 Kubernetes 项目里，机器的名字以及一切存储在 Etcd 中的 API 对象，都必须使用标准的 DNS 命名（RFC 1123）。</li>
<li>用户安装的 kubeadm 和 kubelet 的版本是否匹配？</li>
<li>机器上是不是已经安装了 Kubernetes 的二进制文件？</li>
<li>Kubernetes 的工作端口 10250/10251/10252 端口是不是已经被占用？</li>
<li>ip、mount 等 Linux 指令是否存在？</li>
<li>Docker 是否已经安装？</li>
</ul>
</li>
<li><p>在通过了 Preflight Checks 之后，kubeadm 要为你做的，是生成 Kubernetes 对外提供服务所需的各种证书和对应的目录。</p>
<ul>
<li>Kubernetes 对外提供服务时，除非专门开启“不安全模式”，否则都要通过 HTTPS 才能访问 kube-apiserver。这就需要为 Kubernetes 集群配置好证书文件。</li>
<li>kubeadm 为 Kubernetes 项目生成的证书文件都放在 Master 节点的 /etc/kubernetes/pki 目录下。在这个目录下，最主要的证书文件是 ca.crt 和对应的私钥 ca.key。</li>
<li>用户使用 kubectl 获取容器日志等 streaming 操作时，需要通过 kube-apiserver 向 kubelet 发起请求，这个连接也必须是安全的。</li>
<li>kubeadm 为这一步生成的是 apiserver-kubelet-client.crt 文件，对应的私钥是 apiserver-kubelet-client.key。</li>
</ul>
</li>
<li><p>证书生成后，kubeadm 接下来会为其他组件生成访问 kube-apiserver 所需的配置文件。这些文件的路径是：/etc/kubernetes/xxx.conf：</p>
<ul>
<li>ls /etc/kubernetes/   admin.conf  controller-manager.conf  kubelet.conf  scheduler.conf</li>
<li>这些文件里面记录的是，当前这个 Master 节点的服务器地址、监听端口、证书目录等信息。这样，对应的客户端（比如 scheduler，kubelet 等），可以直接加载相应的文件，使用里面的信息与 kube-apiserver 建立安全连接。</li>
</ul>
</li>
<li><p>接下来，kubeadm 会为 Master 组件生成 Pod 配置文件。</p>
<ul>
<li><p>Kubernetes 有三个 Master 组件 kube-apiserver、kube-controller-manager、kube-scheduler，而它们都会被使用 Pod 的方式部署起来。</p>
</li>
<li><p>在 Kubernetes 中，有一种特殊的容器启动方法叫做“Static Pod”。它允许你把要部署的 Pod 的 YAML 文件放在一个指定的目录里。这样，当这台机器上的 kubelet 启动时，它会自动检查这个目录，加载所有的 Pod YAML 文件，然后在这台机器上启动它们。</p>
</li>
<li><p>kubelet 在 Kubernetes 项目中的地位非常高，在设计上它就是一个完全独立的组件，而其他 Master 组件，则更像是辅助性的系统容器。</p>
</li>
<li><p>在 kubeadm 中，Master 组件的 YAML 文件会被生成在 /etc/kubernetes/manifests 路径下。</p>
<pre><code>   <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Pod</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">annotations:</span></span><br><span class="line">    <span class="attr">scheduler.alpha.kubernetes.io/critical-pod:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="attr">creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">labels:</span></span><br><span class="line">    <span class="attr">component:</span> <span class="string">kube-apiserver</span></span><br><span class="line">    <span class="attr">tier:</span> <span class="string">control-plane</span></span><br><span class="line"><span class="attr">name:</span> <span class="string">kube-apiserver</span></span><br><span class="line"><span class="attr">namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">containers:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">command:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">kube-apiserver</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--authorization-mode=Node,RBAC</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--runtime-config=api/all=true</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--advertise-address=10.168.0.2</span></span><br><span class="line">    <span class="string">...</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--tls-cert-file=/etc/kubernetes/pki/apiserver.crt</span></span><br><span class="line">    <span class="bullet">-</span> <span class="string">--tls-private-key-file=/etc/kubernetes/pki/apiserver.key</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">k8s.gcr.io/kube-apiserver-amd64:v1.11.1</span></span><br><span class="line">    <span class="attr">imagePullPolicy:</span> <span class="string">IfNotPresent</span></span><br><span class="line">    <span class="attr">livenessProbe:</span></span><br><span class="line">    <span class="string">...</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">kube-apiserver</span></span><br><span class="line">    <span class="attr">resources:</span></span><br><span class="line">    <span class="attr">requests:</span></span><br><span class="line">        <span class="attr">cpu:</span> <span class="string">250m</span></span><br><span class="line">    <span class="attr">volumeMounts:</span></span><br><span class="line">    <span class="bullet">-</span> <span class="attr">mountPath:</span> <span class="string">/usr/share/ca-certificates</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">usr-share-ca-certificates</span></span><br><span class="line">    <span class="attr">readOnly:</span> <span class="literal">true</span></span><br><span class="line">    <span class="string">...</span></span><br><span class="line"><span class="attr">hostNetwork:</span> <span class="literal">true</span></span><br><span class="line"><span class="attr">priorityClassName:</span> <span class="string">system-cluster-critical</span></span><br><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">hostPath:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/etc/ca-certificates</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">DirectoryOrCreate</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">etc-ca-certificates</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li>这个 Pod 里只定义了一个容器，它使用的镜像是：k8s.gcr.io/kube-apiserver-amd64:v1.11.1 。这个镜像是 Kubernetes 官方维护的一个组件镜像。</li>
<li>这个容器的启动命令（commands）是 kube-apiserver –authorization-mode=Node,RBAC …，这样一句非常长的命令。其实，它就是容器里 kube-apiserver 这个二进制文件再加上指定的配置参数而已。</li>
<li>如果你要修改一个已有集群的 kube-apiserver 的配置，需要修改这个 YAML 文件。</li>
<li>这些组件的参数也可以在部署时指定。</li>
</ul>
</li>
</ul>
</li>
<li><p>在这一步完成后，kubeadm 还会再生成一个 Etcd 的 Pod YAML 文件，用来通过同样的 Static Pod 的方式启动 Etcd。所以，最后 Master 组件的 Pod YAML 文件如下所示</p>
<ul>
<li>$ ls /etc/kubernetes/manifests/  etcd.yaml  kube-apiserver.yaml  kube-controller-manager.yaml  kube-scheduler.yaml</li>
<li>一旦这些 YAML 文件出现在被 kubelet 监视的 /etc/kubernetes/manifests 目录下，kubelet 就会自动创建这些 YAML 文件中定义的 Pod，即 Master 组件的容器。</li>
</ul>
</li>
<li><p>Master 容器启动后，kubeadm 会通过检查 localhost:6443/healthz 这个 Master 组件的健康检查 URL，等待 Master 组件完全运行起来。</p>
</li>
<li><p>然后，kubeadm 就会为集群生成一个 bootstrap token。在后面，只要持有这个 token，任何一个安装了 kubelet 和 kubadm 的节点，都可以通过 kubeadm join 加入到这个集群当中。</p>
<ul>
<li>这个 token 的值和使用方法会，会在 kubeadm init 结束后被打印出来。</li>
<li>在 token 生成之后，kubeadm 会将 ca.crt 等 Master 节点的重要信息，通过 ConfigMap 的方式保存在 Etcd 当中，供后续部署 Node 节点使用。</li>
<li>这个 ConfigMap 的名字是 cluster-info。</li>
</ul>
</li>
<li><p>kubeadm init 的最后一步，就是安装默认插件。Kubernetes 默认 kube-proxy 和 DNS 这两个插件是必须安装的。它们分别用来提供整个集群的服务发现和 DNS 功能。</p>
<ul>
<li>其实，这两个插件也只是两个容器镜像而已，所以 kubeadm 只要用 Kubernetes 客户端创建两个 Pod 就可以了。</li>
</ul>
</li>
</ol>
<h3 id="kubeadm-join-的工作流程"><a href="#kubeadm-join-的工作流程" class="headerlink" title="kubeadm join 的工作流程"></a>kubeadm join 的工作流程</h3><p>这个流程其实非常简单，kubeadm init 生成 bootstrap token 之后，你就可以在任意一台安装了 kubelet 和 kubeadm 的机器上执行 kubeadm join 了。</p>
<p>cluster-info 里的 kube-apiserver 的地址、端口、证书，kubelet 就可以以“安全模式”连接到 apiserver 上，这样一个新的节点就部署完成了。</p>
<h3 id="配置-kubeadm-的部署参数"><a href="#配置-kubeadm-的部署参数" class="headerlink" title="配置 kubeadm 的部署参数"></a>配置 kubeadm 的部署参数</h3><p>kubeadm 确实简单易用，可是我又该如何定制我的集群组件参数呢？</p>
<p>在这里，我强烈推荐你在使用 kubeadm init 部署 Master 节点时，使用下面这条指令：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> kubeadm init --config kubeadm.yaml</span></span><br></pre></td></tr></table></figure>
</code></pre>
<p>这时，你就可以给 kubeadm 提供一个 YAML 文件（比如，kubeadm.yaml），它的内容如下所示（我仅列举了主要部分）</p>
<pre><code><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1alpha2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">MasterConfiguration</span></span><br><span class="line"><span class="attr">kubernetesVersion:</span> <span class="string">v1.11.0</span></span><br><span class="line"><span class="attr">api:</span></span><br><span class="line"><span class="attr">advertiseAddress:</span> <span class="number">192.168</span><span class="number">.0</span><span class="number">.102</span></span><br><span class="line"><span class="attr">bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">etcd:</span></span><br><span class="line"><span class="attr">local:</span></span><br><span class="line">    <span class="attr">dataDir:</span> <span class="string">/var/lib/etcd</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="attr">imageRepository:</span> <span class="string">k8s.gcr.io</span></span><br><span class="line"><span class="attr">kubeProxy:</span></span><br><span class="line"><span class="attr">config:</span></span><br><span class="line">    <span class="attr">bindAddress:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">    <span class="string">...</span></span><br><span class="line"><span class="attr">kubeletConfiguration:</span></span><br><span class="line"><span class="attr">baseConfig:</span></span><br><span class="line">    <span class="attr">address:</span> <span class="number">0.0</span><span class="number">.0</span><span class="number">.0</span></span><br><span class="line">    <span class="string">...</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line"><span class="attr">dnsDomain:</span> <span class="string">cluster.local</span></span><br><span class="line"><span class="attr">podSubnet:</span> <span class="string">&quot;&quot;</span></span><br><span class="line"><span class="attr">serviceSubnet:</span> <span class="number">10.96</span><span class="number">.0</span><span class="number">.0</span><span class="string">/12</span></span><br><span class="line"><span class="attr">nodeRegistration:</span></span><br><span class="line"><span class="attr">criSocket:</span> <span class="string">/var/run/dockershim.sock</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>
</code></pre>
<p>然后，kubeadm 就会使用上面这些信息替换 /etc/kubernetes/manifests/kube-apiserver.yaml 里的 command 字段里的参数了。</p>
<p>而这个 YAML 文件提供的可配置项远不止这些。比如，你还可以修改 kubelet 和 kube-proxy 的配置，修改 Kubernetes 使用的基础镜像的 URL（默认的k8s.gcr.io/xxx镜像 URL 在国内访问是有困难的），指定自己的证书文件，指定特殊的容器运行时等等。这些配置项，就留给你在后续实践中探索了。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul>
<li>kubeadm 能够用于生产环境吗？<ul>
<li>到目前为止（2018 年 9 月），这个问题的答案是：不能。</li>
<li>如果你有部署规模化生产环境的需求，我推荐使用kops或者 SaltStack 这样更复杂的部署工具</li>
</ul>
</li>
<li>一方面，作为 Kubernetes 项目的原生部署工具，kubeadm 对 Kubernetes 项目特性的使用和集成，确实要比其他项目“技高一筹”，非常值得我们学习和借鉴；</li>
<li>另一方面，kubeadm 的部署方法，不会涉及到太多的运维工作，也不需要我们额外学习复杂的部署工具。而它部署的 Kubernetes 集群，跟一个完全使用二进制文件搭建起来的集群几乎没有任何区别。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/07/01/4-%E5%88%9D%E8%AF%86k8s/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/01/4-%E5%88%9D%E8%AF%86k8s/" class="post-title-link" itemprop="url">4.初识k8s</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-07-01 09:12:11 / 修改时间：20:45:07" itemprop="dateCreated datePublished" datetime="2021-07-01T09:12:11+08:00">2021-07-01</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="回顾docker"><a href="#回顾docker" class="headerlink" title="回顾docker"></a>回顾docker</h2><ul>
<li>一个“容器”，实际上是一个由 Linux Namespace、Linux Cgroups 和 rootfs 三种技术构建出来的进程的隔离环境。</li>
<li>一个正在运行的 Linux 容器，其实可以被“一分为二”地看待：<ul>
<li>一组联合挂载在 /var/lib/docker/aufs/mnt 上的 rootfs，这一部分我们称为“容器镜像”（Container Image），是容器的静态视图；</li>
<li>一个由 Namespace+Cgroups 构成的隔离环境，这一部分我们称为“容器运行时”（Container Runtime），是容器的动态视图。</li>
</ul>
</li>
</ul>
<h2 id="初识k8s"><a href="#初识k8s" class="headerlink" title="初识k8s"></a>初识k8s</h2><p>容器就从一个开发者手里的小工具，一跃成为了云计算领域的绝对主角；而能够定义容器组织和管理规范的“容器编排”技术，则当仁不让地坐上了容器技术领域的“头把交椅”。</p>
<ul>
<li>最具代表性的容器编排工具，当属 Docker 公司的 Compose+Swarm 组合，以及 Google 与 RedHat 公司共同主导的 Kubernetes 项目。</li>
<li>Kubernetes 项目 前身是google 的Borg 系统，</li>
</ul>
<h3 id="Kubernetes-项目要解决的问题是什么"><a href="#Kubernetes-项目要解决的问题是什么" class="headerlink" title="Kubernetes 项目要解决的问题是什么"></a>Kubernetes 项目要解决的问题是什么</h3><ul>
<li>在不同的发展阶段，Kubernetes 需要着重解决的问题是不同的。</li>
<li>但是，对于大多数用户来说，他们希望 Kubernetes 项目带来的体验是确定的：现在我有了应用的容器镜像，请帮我在一个给定的集群上把这个应用运行起来。</li>
<li>更进一步地说，我还希望 Kubernetes 能给我提供路由网关、水平扩展、监控、备份、灾难恢复等一系列运维能力。</li>
<li><strong>运行在大规模集群中的各种任务之间，实际上存在着各种各样的关系。这些关系的处理，才是作业编排和管理系统最困难的地方。</strong></li>
</ul>
<h3 id="基础架构"><a href="#基础架构" class="headerlink" title="基础架构"></a>基础架构</h3><img src="/2021/07/01/4-%E5%88%9D%E8%AF%86k8s/k8s%E5%9F%BA%E7%A1%80%E6%9E%B6%E6%9E%84.png" class="">

<ul>
<li>跟它的原型项目 Borg 非常类似，都由 Master 和 Node 两种节点组成，而这两种角色分别对应着控制节点和计算节点。</li>
<li>控制节点，即 Master 节点，由三个紧密协作的独立组件组合而成，它们分别是负责 API 服务的 kube-apiserver、负责调度的 kube-scheduler，以及负责容器编排的 kube-controller-manager。</li>
<li>整个集群的持久化数据，则由 kube-apiserver 处理后保存在 Etcd 中。</li>
<li>在 Kubernetes 项目中，kubelet 主要负责同容器运行时（比如 Docker 项目）打交道。<ul>
<li>而这个交互所依赖的，是一个称作 CRI（Container Runtime Interface）的远程调用接口，这个接口定义了容器运行时的各项核心操作，比如：启动一个容器需要的所有参数。</li>
</ul>
</li>
<li>而具体的容器运行时，比如 Docker 项目，则一般通过 OCI 这个容器运行时规范同底层的 Linux 操作系统进行交互，即：把 CRI 请求翻译成对 Linux 操作系统的调用（操作 Linux Namespace 和 Cgroups 等）。</li>
<li>kubelet 还通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互。<ul>
<li>这个插件，是 Kubernetes 项目用来管理 GPU 等宿主机物理设备的主要组件，也是基于 Kubernetes 项目进行机器学习训练、高性能作业支持等工作必须关注的功能。</li>
</ul>
</li>
<li>kubelet 的另一个重要功能，则是调用网络插件和存储插件为容器配置网络和持久化存储。<ul>
<li>这两个插件与 kubelet 进行交互的接口，分别是 CNI（Container Networking Interface）和 CSI（Container Storage Interface）。</li>
</ul>
</li>
</ul>
<h3 id="Borg-对于-Kubernetes-项目的指导作用又体现在哪里呢"><a href="#Borg-对于-Kubernetes-项目的指导作用又体现在哪里呢" class="headerlink" title="Borg 对于 Kubernetes 项目的指导作用又体现在哪里呢"></a>Borg 对于 Kubernetes 项目的指导作用又体现在哪里呢</h3><p>答案是，Master 节点。即：如何编排、管理、调度用户提交的作业？</p>
<p>Kubernetes 项目最主要的设计思想是，从更宏观的角度，以统一的方式来定义任务之间的各种关系，并且为将来支持更多种类的关系留有余地。</p>
<h3 id="pod"><a href="#pod" class="headerlink" title="pod"></a>pod</h3><ul>
<li>Kubernetes 项目对容器间的“访问”进行了分类，首先总结出了一类非常常见的“紧密交互”的关系，即：这些应用之间需要非常频繁的交互和访问；又或者，它们会直接通过本地文件进行信息交换。</li>
<li>在常规环境下，这些应用往往会被直接部署在同一台机器上，通过 Localhost 通信，通过本地磁盘目录交换文件。而在 Kubernetes 项目中，这些容器则会被划分为一个“Pod”，Pod 里的容器共享同一个 Network Namespace、同一组数据卷，从而达到高效率交换信息的目的。</li>
<li>而对于另外一种更为常见的需求，比如 Web 应用与数据库之间的访问关系，Kubernetes 项目则提供了一种叫作“Service”的服务。像这样的两个应用，往往故意不部署在同一台机器上，这样即使 Web 应用所在的机器宕机了，数据库也完全不受影响。可是，我们知道，对于一个容器来说，它的 IP 地址等信息不是固定的，那么 Web 应用又怎么找到数据库容器的 Pod 呢？</li>
<li>所以，Kubernetes 项目的做法是给 Pod 绑定一个 Service 服务，而 Service 服务声明的 IP 地址等信息是“终生不变”的。这个Service 服务的主要作用，就是作为 Pod 的代理入口（Portal），从而代替 Pod 对外暴露一个固定的网络地址。</li>
</ul>
<img src="/2021/07/01/4-%E5%88%9D%E8%AF%86k8s/pod.png" class="">

<ul>
<li>按照这幅图的线索，我们从容器这个最基础的概念出发，首先遇到了容器间“紧密协作”关系的难题，于是就扩展到了 Pod；</li>
<li>有了 Pod 之后，我们希望能一次启动多个应用的实例，这样就需要 Deployment 这个 Pod 的多实例管理器；</li>
<li>而有了这样一组相同的 Pod 后，我们又需要通过一个固定的 IP 地址和端口以负载均衡的方式访问它，于是就有了 Service。</li>
<li>如果现在两个不同 Pod 之间不仅有“访问关系”，还要求在发起时加上授权信息。Kubernetes 项目提供了一种叫作 Secret 的对象，它其实是一个保存在 Etcd 里的键值对数据。</li>
<li>运行形态也是关键因素，比如 Job，用来描述一次性运行的 Pod（比如，大数据任务）；再比如 DaemonSet，用来描述每个宿主机上必须且只能运行一个副本的守护进程服务；又比如 CronJob，则用于描述定时任务等等。</li>
</ul>
<p>可以看到，Kubernetes 项目并没有像其他项目那样，为每一个管理功能创建一个指令，然后在项目中实现其中的逻辑。这种做法，的确可以解决当前的问题，但是在更多的问题来临之后，往往会力不从心。</p>
<p>相比之下，在 Kubernetes 项目中，我们所推崇的使用方法是：</p>
<ul>
<li>首先，通过一个“编排对象”，比如 Pod、Job、CronJob 等，来描述你试图管理的应用；</li>
<li>然后，再为它定义一些“服务对象”，比如 Service、Secret、Horizontal Pod Autoscaler（自动水平扩展器）等。这些对象，会负责具体的平台级功能。</li>
</ul>
<h3 id="声明式-API"><a href="#声明式-API" class="headerlink" title="声明式 API"></a>声明式 API</h3><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">apps/v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">nginx-deployment</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">replicas:</span> <span class="number">2</span></span><br><span class="line">  <span class="attr">selector:</span></span><br><span class="line">    <span class="attr">matchLabels:</span></span><br><span class="line">      <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">  <span class="attr">template:</span></span><br><span class="line">    <span class="attr">metadata:</span></span><br><span class="line">      <span class="attr">labels:</span></span><br><span class="line">        <span class="attr">app:</span> <span class="string">nginx</span></span><br><span class="line">    <span class="attr">spec:</span></span><br><span class="line">      <span class="attr">containers:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">name:</span> <span class="string">nginx</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">nginx:1.7.9</span></span><br><span class="line">        <span class="attr">ports:</span></span><br><span class="line">        <span class="bullet">-</span> <span class="attr">containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>

<p>在上面这个 YAML 文件中，我们定义了一个 Deployment 对象，它的主体部分（spec.template 部分）是一个使用 Nginx 镜像的 Pod，而这个 Pod 的副本数是 2（replicas=2）。</p>
<p>执行 $ kubectl create -f nginx-deployment.yaml</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/30/3-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/30/3-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F/" class="post-title-link" itemprop="url">3.深入理解容器镜像</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-30 09:44:21" itemprop="dateCreated datePublished" datetime="2021-06-30T09:44:21+08:00">2021-06-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-01 20:45:07" itemprop="dateModified" datetime="2021-07-01T20:45:07+08:00">2021-07-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Namespace 的作用是“隔离”，它让应用进程只能看到该 Namespace 内的“世界”；而 Cgroups 的作用是“限制”，它给这个“世界”围上了一圈看不见的墙。</p>
<h2 id="容器里的进程看到的文件系统又是什么样子的呢"><a href="#容器里的进程看到的文件系统又是什么样子的呢" class="headerlink" title="容器里的进程看到的文件系统又是什么样子的呢"></a>容器里的进程看到的文件系统又是什么样子的呢</h2><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><ul>
<li>Mount Namespace 修改的，是容器进程对文件系统“挂载点”的认知。</li>
<li>这也就意味着，只有在“挂载”这个操作发生之后，进程的视图才会被改变。</li>
<li>而在此之前，新创建的容器会直接继承宿主机的各个挂载点。</li>
</ul>
<p>这就是 Mount Namespace 跟其他 Namespace 的使用略有不同的地方：<strong>它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效。</strong></p>
<p>挂载“/”目录，使容器内的文件系统单一</p>
<ul>
<li><p>在 Linux 操作系统里，有一个名为 chroot 的命令change root file system 即改变进程的根目录到你指定的位置。</p>
<ul>
<li><p>假设，我们现在有一个 $HOME/test 目录，想要把它作为一个 /bin/bash 进程的根目录。</p>
</li>
<li><p>首先，创建一个 test 目录和几个 lib 文件夹：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p <span class="variable">$HOME</span>/<span class="built_in">test</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p <span class="variable">$HOME</span>/<span class="built_in">test</span>/&#123;bin,lib64,lib&#125;</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> <span class="variable">$T</span></span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>然后，把 bash 命令拷贝到 test 目录对应的 bin 路径下：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cp -v /bin/&#123;bash,ls&#125; <span class="variable">$HOME</span>/<span class="built_in">test</span>/bin</span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>接下来，把 bash 命令需要的所有 so 文件，也拷贝到 test 目录对应的 lib 路径下。找到 so 文件可以用 ldd 命令：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> T=<span class="variable">$HOME</span>/<span class="built_in">test</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> list=<span class="string">&quot;<span class="subst">$(ldd /bin/ls | egrep -o &#x27;/lib.*\.[0-9]&#x27;)</span>&quot;</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="variable">$list</span>; <span class="keyword">do</span> cp -v <span class="string">&quot;<span class="variable">$i</span>&quot;</span> <span class="string">&quot;<span class="variable">$&#123;T&#125;</span><span class="variable">$&#123;i&#125;</span>&quot;</span>; <span class="keyword">done</span></span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>最后，执行 chroot 命令，告诉操作系统，我们将使用 $HOME/test 目录作为 /bin/bash 进程的根目录：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chroot <span class="variable">$HOME</span>/<span class="built_in">test</span> /bin/bash</span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>这时，你如果执行 “ls /“，就会看到，它返回的都是 $HOME/test 目录下面的内容，而不是宿主机的内容。</p>
</li>
</ul>
</li>
<li><p>实际上，Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统里的第一个 Namespace。</p>
</li>
<li><p>我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如 Ubuntu16.04 的 ISO。这样，在容器启动之后，我们在容器里通过执行 “ls /“ 查看根目录下的内容，就是 Ubuntu 16.04 的所有目录和文件。</p>
</li>
<li><p>而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。</p>
</li>
</ul>
<p>对 Docker 项目来说，它最核心的原理实际上就是为待创建的用户进程：</p>
<ul>
<li>启用 Linux Namespace 配置；</li>
<li>设置指定的 Cgroups 参数；</li>
<li>切换进程的根目录（Change Root）。</li>
<li>不过，Docker 项目在最后一步的切换上会优先使用 pivot_root 系统调用，如果系统不支持，才会使用 chroot。</li>
</ul>
<p>需要明确的是，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。</p>
<ul>
<li>/usr/src/一般用于存放内核源代码</li>
<li>/boot一般用于存放可引导内核</li>
<li>/usr/lib/modules/kernel/存放内核内置的已编译好的驱动程序</li>
<li>所以说，rootfs 只包括了操作系统的“躯壳”，并没有包括操作系统的“灵魂”。</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>容器相比于虚拟机的主要缺陷之一：毕竟后者不仅有模拟出来的硬件机器充当沙盒，而且每个沙盒里还运行着一个完整的 Guest OS 给应用随便折腾。</p>
<ul>
<li>由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。</li>
<li>对一个应用来说，操作系统本身才是它运行所需要的最完整的“依赖库”。</li>
</ul>
<h3 id="联合文件系统（Union-File-System）"><a href="#联合文件系统（Union-File-System）" class="headerlink" title="联合文件系统（Union File System）"></a>联合文件系统（Union File System）</h3><pre><code>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tree</span></span><br><span class="line">.</span><br><span class="line">├── A</span><br><span class="line">│  ├── a</span><br><span class="line">│  └── x</span><br><span class="line">└── B</span><br><span class="line">├── b</span><br><span class="line">└── x</span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir C</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mount -t aufs -o <span class="built_in">dirs</span>=./A:./B none ./C</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tree ./C</span></span><br><span class="line">./C</span><br><span class="line">├── a</span><br><span class="line">├── b</span><br><span class="line">└── x</span><br></pre></td></tr></table></figure>
</code></pre>
<p>层的概念就是在联合操作系统中</p>
<ul>
<li>镜像的层都放置在 /var/lib/docker/image/overlay2 目录下，然后被联合挂载在 /var/lib/docker/aufs/mnt 里面。</li>
<li>在/sys/fs/aufs 有个si=972c6d361e6b32ba。</li>
<li>然后使用这个 ID，你就可以在 /sys/fs/aufs 下查看被联合挂载在一起的各个层的信息：</li>
</ul>
<h3 id="分层"><a href="#分层" class="headerlink" title="分层"></a>分层</h3><ul>
<li>第一部分，只读层。<ul>
<li>它是这个容器的 rootfs 最下面的五层，对应的正是 ubuntu:latest 镜像的五层。可以看到，它们的挂载方式都是只读的（ro+wh，即 readonly+whiteout，至于什么是 whiteout，我下面马上会讲到）。</li>
<li>为了实现这样的删除操作，AuFS 会在可读写层创建一个 whiteout 文件，把只读层里的文件“遮挡”起来。</li>
</ul>
</li>
<li>第二部分，可读写层。<ul>
<li>它是这个容器的 rootfs 最上面的一层（6e3be5d2ecccae7cc），它的挂载方式为：rw，即 read write。在没有写入文件之前，这个目录是空的。而一旦在容器里做了写操作，你修改产生的内容就会以增量的方式出现在这个层中。</li>
</ul>
</li>
<li>第三部分，Init 层。<ul>
<li>它是一个以“-init”结尾的层，夹在只读层和读写层之间。Init 层是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。</li>
</ul>
</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>rootfs。它只是一个操作系统的所有文件和目录，并不包含内核，最多也就几百兆。而相比之下，传统虚拟机的镜像大多是一个磁盘的“快照”，磁盘有多大，镜像就至少有多大。</li>
<li>通过结合使用 Mount Namespace 和 rootfs，容器就能够为进程构建出一个完善的文件系统隔离环境。当然，这个功能的实现还必须感谢 chroot 和 pivot_root 这两个系统调用切换进程根目录的能力。</li>
<li>而在 rootfs 的基础上，Docker 公司创新性地提出了使用多个增量 rootfs 联合挂载一个完整 rootfs 的方案，这就是容器镜像中“层”的概念。</li>
<li>通过“分层镜像”的设计，以 Docker 镜像为核心，来自不同公司、不同团队的技术人员被紧密地联系在了一起。而且，由于容器镜像的操作是增量式的，这样每次镜像拉取、推送的内容，比原本多个完整的操作系统的大小要小得多；而共享层的存在，可以使得所有这些容器镜像需要的总空间，也比每个镜像的总和要小。这样就使得基于容器镜像的团队协作，要比基于动则几个 GB 的虚拟机磁盘镜像的协作要敏捷得多。</li>
<li>更重要的是，一旦这个镜像被发布，那么你在全世界的任何一个地方下载这个镜像，得到的内容都完全一致，可以完全复现这个镜像制作者当初的完整环境。这，就是容器技术“强一致性”的重要体现。</li>
</ul>
<h2 id="制作"><a href="#制作" class="headerlink" title="制作"></a>制作</h2><ul>
<li><p>安装docker</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sSL https://get.daocloud.io/docker | sh</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>用 Docker 部署一个用 Python 编写的 Web 应用</p>
<ul>
<li><p>app.py</p>
<pre><code>    <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span>():</span></span><br><span class="line">html = <span class="string">&quot;&lt;h3&gt;Hello &#123;name&#125;!&lt;/h3&gt;&quot;</span> \</span><br><span class="line">      <span class="string">&quot;&lt;b&gt;Hostname:&lt;/b&gt; &#123;hostname&#125;&lt;br/&gt;&quot;</span></span><br><span class="line"><span class="keyword">return</span> html.<span class="built_in">format</span>(name=os.getenv(<span class="string">&quot;NAME&quot;</span>, <span class="string">&quot;world&quot;</span>), hostname=socket.gethostname())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">app.run(host=<span class="string">&#x27;0.0.0.0&#x27;</span>, port=<span class="number">80</span>)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>requirements</p>
<pre><code>    <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat requirements.txt Flask</span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>制作容器镜像</p>
<pre><code>    <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 使用官方提供的 Python 开发镜像作为基础镜像</span></span><br><span class="line">FROM python:2.7-slim</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将工作目录切换为 /app</span></span><br><span class="line">WORKDIR /app</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将当前目录下的所有内容复制到 /app 下</span></span><br><span class="line">ADD . /app</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用 pip 命令安装这个应用所需要的依赖</span></span><br><span class="line">RUN pip install --trusted-host pypi.python.org -r requirements.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许外界访问容器的 80 端口</span></span><br><span class="line">EXPOSE 80</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置环境变量</span></span><br><span class="line">ENV NAME World</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置容器进程为：python app.py，即：这个 Python 应用的启动命令</span></span><br><span class="line">CMD [&quot;python&quot;, &quot;app.py&quot;]</span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li><p>这里，app.py 的实际路径是 /app/app.py。所以，CMD [“python”, “app.py”] 等价于 “docker run python app.py”。</p>
</li>
<li><p>另外，在使用 Dockerfile 时，你可能还会看到一个叫作 ENTRYPOINT 的原语。实际上，它和 CMD 都是 Docker 容器进程启动所必需的参数，完整执行格式是：“ENTRYPOINT CMD”。</p>
</li>
<li><p>默认情况下，Docker 会为你提供一个隐含的 ENTRYPOINT，即：/bin/sh -c。所以，在不指定 ENTRYPOINT 时，比如在我们这个例子里，实际上运行在容器里的完整进程是：/bin/sh -c “python app.py”，即 CMD 的内容就是 ENTRYPOINT 的参数。</p>
</li>
<li><p>Dockerfile 里的原语并不都是指对容器内部的操作。就比如 ADD，它指的是把当前目录（即 Dockerfile 所在的目录）里的文件，复制到指定容器内的目录当中。</p>
</li>
<li><p>读懂这个 Dockerfile 之后，我再把上述内容，保存到当前目录里一个名叫“Dockerfile”的文件中：$ ls  Dockerfile  app.py   requirements.txt</p>
</li>
<li><p>接下来，我就可以让 Docker 制作这个镜像了，在当前目录执行：$ docker build -t helloworld .</p>
</li>
<li><p>其中，-t 的作用是给这个镜像加一个 Tag，即：起一个好听的名字。docker build 会自动加载当前目录下的 Dockerfile 文件，然后按照顺序，执行文件中的原语。</p>
</li>
<li><p>需要注意的是，Dockerfile 中的每个原语执行后，都会生成一个对应的镜像层。即使原语本身并没有明显地修改文件的操作（比如，ENV 原语），它对应的层也会存在。只不过在外界看来，这个层是空的。</p>
</li>
<li><p>接下来，我使用这个镜像，通过 docker run 命令启动容器：$ docker run -p 4000:80 helloworld</p>
</li>
<li><p>在这一句命令中，镜像名 helloworld 后面，我什么都不用写，因为在 Dockerfile 中已经指定了 CMD。</p>
</li>
<li><p>我已经通过 -p 4000:80 告诉了 Docker，请把容器内的 80 端口映射在宿主机的 4000 端口上。</p>
</li>
<li><p>用 docker tag 命令给容器镜像起一个完整的名字：$ docker tag helloworld geektime/helloworld:v1</p>
</li>
<li><p>然后，我执行 docker push：$ docker push geektime/helloworld:v1</p>
</li>
<li><p>我还可以使用 docker commit 指令，把一个正在运行的容器，直接提交为一个镜像。</p>
</li>
<li><p>$ docker push geektime/helloworld:v2</p>
<pre><code>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> -it 4ddf4638572d /bin/sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在容器内部新建了一个文件</span></span><br><span class="line">root@4ddf4638572d:/app# touch test.txt</span><br><span class="line">root@4ddf4638572d:/app# exit</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将这个新建的文件提交到镜像中保存</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker commit 4ddf4638572d geektime/helloworld:v2</span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="docker-exec-是怎么做到进入容器里的呢"><a href="#docker-exec-是怎么做到进入容器里的呢" class="headerlink" title="docker exec 是怎么做到进入容器里的呢"></a>docker exec 是怎么做到进入容器里的呢</h3><ul>
<li><p>实际上，Linux Namespace 创建的隔离空间虽然看不见摸不着，但一个进程的 Namespace 信息在宿主机上是确确实实存在的，并且是以一个文件的方式存在。</p>
</li>
<li><p>通过如下指令，你可以看到当前正在运行的 Docker 容器的进程号（PID）是 25686：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker inspect --format <span class="string">&#x27;&#123;&#123; .State.Pid &#125;&#125;&#x27;</span>  4ddf4638572d</span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>这时，你可以通过查看宿主机的 proc 文件，看到这个 25686 进程的所有 Namespace 对应的文件 $ ls -l  /proc/25686/ns</p>
</li>
<li><p>这也就意味着：一个进程，可以选择加入到某个进程已有的 Namespace 当中，从而达到“进入”这个进程所在容器的目的，这正是 docker exec 的实现原理。</p>
</li>
<li><p>而这个操作所依赖的，乃是一个名叫 setns() 的 Linux 系统调用。</p>
</li>
<li><p>$ docker run -it –net container:4ddf4638572d busybox ifconfig  共享network namespace</p>
</li>
<li><p>而如果我指定–net=host，就意味着这个容器不会为进程启用 Network Namespace。共享主机网络</p>
</li>
</ul>
<h3 id="docker-commit"><a href="#docker-commit" class="headerlink" title="docker commit"></a>docker commit</h3><ul>
<li>docker commit，实际上就是在容器运行起来后，把最上层的“可读写层”，加上原先容器镜像的只读层，打包组成了一个新的镜像。当然，下面这些只读层在宿主机上是共享的，不会占用额外的空间。</li>
<li>而由于使用了联合文件系统，你在容器里对镜像 rootfs 所做的任何修改，都会被操作系统先复制到这个可读写层，然后再修改。这就是所谓的：Copy-on-Write。</li>
<li>而正如前所说，Init 层的存在，就是为了避免你执行 docker commit 时，把 Docker 自己对 /etc/hosts 等文件做的修改，也一起提交掉。</li>
</ul>
<h3 id="Volume（数据卷）"><a href="#Volume（数据卷）" class="headerlink" title="Volume（数据卷）"></a>Volume（数据卷）</h3><ul>
<li>Volume 机制，允许你将宿主机上指定的目录或者文件，挂载到容器里面进行读取和修改操作。</li>
<li>宿主机文件映射到容器中 $ docker run -v /test …   $ docker run -v /home:/test …<ul>
<li>在第一种情况下，由于你并没有显示声明宿主机目录，那么 Docker 就会默认在宿主机上创建一个临时目录 /var/lib/docker/volumes/[VOLUME_ID]/_data，然后把它挂载到容器的 /test 目录上。</li>
<li>而在第二种情况下，Docker 就直接把宿主机的 /home 目录挂载到容器的 /test 目录上。</li>
</ul>
</li>
</ul>
<h3 id="总结2"><a href="#总结2" class="headerlink" title="总结2"></a>总结2</h3><img src="/2021/06/30/3-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F/%E5%AE%B9%E5%99%A8%E6%80%BB%E8%A7%88.png" class="">

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/29/2-%E9%99%90%E5%88%B6%E4%B8%8E%E9%9A%94%E7%A6%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/29/2-%E9%99%90%E5%88%B6%E4%B8%8E%E9%9A%94%E7%A6%BB/" class="post-title-link" itemprop="url">2.限制与隔离</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-29 21:42:37" itemprop="dateCreated datePublished" datetime="2021-06-29T21:42:37+08:00">2021-06-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-06-30 21:08:13" itemprop="dateModified" datetime="2021-06-30T21:08:13+08:00">2021-06-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h2><p>一旦“程序”被执行起来，它就从磁盘上的二进制文件，变成了计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合。</p>
<h2 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h2><p>Cgroups 技术是用来制造约束的主要手段，而Namespace 技术则是用来修改进程视图的主要方法。</p>
<h3 id="隔离"><a href="#隔离" class="headerlink" title="隔离"></a>隔离</h3><ul>
<li>通过int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL) 系统调用，创建进程号为“1”的独立隔离进程空间，达到隔离的作用</li>
<li>除了PID Namespace，Linux 操作系统还提供了 Mount、UTS、IPC、Network 和 User 这些 Namespace，用来对各种不同的进程上下文进行“障眼法”操作。<ul>
<li>Mount Namespace，用于让被隔离进程只看到当前 Namespace 里的挂载点信息；Network Namespace，用于让被隔离进程看到当前 Namespace 里的网络设备和配置。</li>
</ul>
</li>
</ul>
<p>以上就是 Linux 容器最基本的实现原理了。</p>
<p>Docker 容器这个听起来玄而又玄的概念，实际上是在创建容器进程时，指定了这个进程所需要启用的一组 Namespace 参数。这样，容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。而对于宿主机以及其他不相关的程序，它就完全看不到了。</p>
<p>所以说，容器，其实是一种特殊的进程而已。</p>
<p>使用虚拟化技术作为应用沙盒，就必须要由 Hypervisor 来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的 Guest OS 才能执行用户的应用进程。这就不可避免地带来了额外的资源消耗和占用。</p>
<p>隔离得不彻底</p>
<ul>
<li>首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。<ul>
<li>尽管你可以在容器里通过 Mount Namespace 单独挂载其他不同版本的操作系统文件，比如 CentOS 或者 Ubuntu，但这并不能改变共享宿主机内核的事实。</li>
<li>这意味着，如果你要在 Windows 宿主机上运行 Linux 容器，或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器，都是行不通的。</li>
</ul>
</li>
<li>其次，在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。<ul>
<li>如果你的容器中的程序使用 settimeofday(2) 系统调用修改了时间，整个宿主机的时间都会被随之修改，这显然不符合用户的预期。</li>
</ul>
</li>
</ul>
<h3 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h3><ul>
<li>Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。</li>
<li>Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。</li>
<li>Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。</li>
<li>在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下</li>
</ul>
<p> /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls /sys/fs/cgroup/cpu</span></span><br><span class="line">cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release</span><br><span class="line">cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks</span><br></pre></td></tr></table></figure>

可以用来限制进程在长度为 cfs_period 的一段时间内，只能被分配到总量为 cfs_quota 的 CPU 时间。
</code></pre>
<p>使用：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:/sys/fs/cgroup/cpu$ mkdir container</span><br><span class="line">root@ubuntu:/sys/fs/cgroup/cpu$ ls container/</span><br><span class="line">cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release</span><br><span class="line">cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks</span><br></pre></td></tr></table></figure>
这个目录就称为一个“控制组”。你会发现，操作系统会在你新创建的 container 目录下，自动生成该子系统对应的资源限制文件。
$ echo 20000 &gt; /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us
</code></pre>
<p>其他：</p>
<pre><code>blkio，为​​​块​​​设​​​备​​​设​​​定​​​I/O 限​​​制，一般用于磁盘等设备；
cpuset，为进程分配单独的 CPU 核和对应的内存节点；
memory，为进程设定内存使用的限制。
</code></pre>
<p>docker run -it –cpu-period=100000 –cpu-quota=20000 ubuntu /bin/bash  就是对资源使用的限制</p>
<p>Cgroups 对资源的限制能力也有很多不完善的地方，被提及最多的自然是 /proc 文件系统的问题。</p>
<ul>
<li>Linux 下的 /proc 目录存储的是记录当前内核运行状态的一系列特殊文件，用户可以通过访问这些文件，查看系统以及当前正在运行的进程的信息，比如 CPU 使用情况、内存占用率等，这些文件也是 top 指令查看系统信息的主要数据来源。</li>
<li>但是，你如果在容器里执行 top 指令，就会发现，它显示的信息居然是宿主机的 CPU 和内存数据，而不是当前容器的数据。</li>
<li>造成这个问题的原因就是，/proc 文件系统并不知道用户通过 Cgroups 给这个容器做了什么样的资源限制，即：/proc 文件系统不了解 Cgroups 限制的存在。</li>
<li>top 是从 /prof/stats 目录下获取数据，所以道理上来讲，容器不挂载宿主机的该目录就可以了。lxcfs就是来实现这个功能的，做法是把宿主机的 /var/lib/lxcfs/proc/memoinfo 文件挂载到Docker容器的/proc/meminfo位置后。容器中进程读取相应文件内容时，LXCFS的FUSE实现会从容器对应的Cgroup中读取正确的内存限制。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/29/1-%E5%8F%91%E5%B1%95%E5%8F%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/29/1-%E5%8F%91%E5%B1%95%E5%8F%B2/" class="post-title-link" itemprop="url">1.发展史</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-29 21:33:16" itemprop="dateCreated datePublished" datetime="2021-06-29T21:33:16+08:00">2021-06-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-06-30 08:10:35" itemprop="dateModified" datetime="2021-06-30T08:10:35+08:00">2021-06-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li>2013~2014 年，以 Cloud Foundry 为代表的 PaaS 项目，逐渐完成了教育用户和开拓市场的艰巨任务，也正是在这个将概念逐渐落地的过程中，应用“打包”困难这个问题，成了整个后端技术圈子的一块心病。</li>
<li>2013年Docker 项目的出现，则为这个根本性的问题提供了一个近乎完美的解决方案。dotCloud 公司则在 2013 年底大胆改名为 Docker 公司。</li>
<li>Fig 项目之所以受欢迎，在于它在开发者面前第一次提出了“容器编排”（Container Orchestration）的概念，后Fig 项目被收购后改名为 Compose。</li>
<li>谷歌开源Kubernetes，成为受欢迎项目</li>
<li>容器技术的兴起源于 PaaS 技术的普及；</li>
<li>Docker 公司发布的 Docker 项目具有里程碑式的意义；</li>
<li>Docker 项目通过“容器镜像”，解决了应用打包这个根本性难题。</li>
<li>容器本身没有价值，有价值的是“容器编排”</li>
<li>最终以 Kubernetes 项目和 CNCF 社区的胜利而告终</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/29/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/29/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">常见问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-06-29 16:00:43 / 修改时间：21:22:13" itemprop="dateCreated datePublished" datetime="2021-06-29T16:00:43+08:00">2021-06-29</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kafka/" itemprop="url" rel="index"><span itemprop="name">kafka</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ol>
<li><p>Kafka 如何做到高吞吐、低延迟的呢？</p>
<p> 这里提下 Kafka 写数据的大致方式：先写操作系统的页缓存（Page Cache）,然后由操作系统自行决定何时刷到磁盘。</p>
<p> 因此 Kafka 达到高吞吐、低延迟的原因主要有以下 4 点：</p>
<ul>
<li>页缓存是在内存中分配的，所以消息写入的速度很快。</li>
<li>Kafka 不必和底层的文件系统进行交互，所有繁琐的 I/O 操作都由操作系统来处理。</li>
<li>Kafka 采用追加写的方式，避免了磁盘随机写操作。</li>
<li>使用以 sendfile 为代表的零拷贝技术提高了读取数据的效率。</li>
<li>PS: 使用页缓存而非堆内存还有一个好处，就是当 Kafka broker 的进程崩溃时，堆内存的数据会丢失，但是页缓存的数据依然存在，重启 Kafka broker 后可以继续提供服务。</li>
</ul>
</li>
<li><p>Kafka 的 producer 工作流程？</p>
 <img src="/2021/06/29/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/%E7%94%9F%E4%BA%A7%E8%80%85%E5%86%99%E5%85%A5.png" class="">

<ul>
<li>封装为 ProducerRecord 实例</li>
<li>序列化</li>
<li>由 partitioner 确定具体分区</li>
<li>发送到内存缓冲区</li>
<li>由 producer 的一个专属 I/O 线程去取消息，并将其封装到一个批次 ，发送给对应分区的 kafka broker</li>
<li>leader 将消息写入本地 log</li>
<li>followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK</li>
<li>leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK</li>
</ul>
</li>
<li><p>Kafka 的 consumer 工作流程？</p>
<ul>
<li>连接 ZK 集群，拿到对应 topic 的 partition 信息和 partition 的 leader 的相关信息</li>
<li>连接到对应 leader 对应的 broker</li>
<li>consumer 将自己保存的 offset 发送给 leader</li>
<li>leader 根据 offset 等信息定位到 segment（索引文件和日志文件）</li>
<li>根据索引文件中的内容，定位到日志文件中该偏移量对应的开始位置读取相应长度的数据并返回给 consumer</li>
</ul>
</li>
<li><p>重要参数有哪些？</p>
<ul>
<li>acks<ul>
<li>acks = 0 : 不接收发送结果</li>
<li>acks = all 或者 -1: 表示发送消息时，不仅要写入本地日志，还要等待所有副本写入成功。</li>
<li>acks = 1: 写入本地日志即可，是上述二者的折衷方案，也是默认值。</li>
</ul>
</li>
<li>retries<ul>
<li>默认为 0，即不重试，立即失败。</li>
<li>一个大于 0 的值，表示重试次数。</li>
</ul>
</li>
<li>buffer.memory<ul>
<li>指定 producer 端用于缓存消息的缓冲区的大小，默认 32M；</li>
<li>适当提升该参数值，可以增加一定的吞吐量。</li>
</ul>
</li>
<li>batch.size<ul>
<li>producer 会将发送分区的多条数据封装在一个 batch 中进行发送，这里的参数指的就是 batch 的大小。</li>
<li>该参数值过小的话，会降低吞吐量，过大的话，会带来较大的内存压力。</li>
<li>默认为 16K，建议合理增加该值。</li>
</ul>
</li>
</ul>
</li>
<li><p>丢失数据的场景？</p>
<ul>
<li>consumer 端：不是严格意义的丢失，其实只是漏消费了。<ul>
<li>设置了 auto.commit.enable=true ，当 consumer fetch 了一些数据但还没有完全处理掉的时候，刚好到 commit interval 触发了提交 offset 操作，接着 consumer 挂掉。这时已经fetch的数据还没有处理完成但已经被commit掉，因此没有机会再次被处理，数据丢失。</li>
</ul>
</li>
<li>producer 端：<ul>
<li>I/O 线程发送消息之前，producer 崩溃， 则 producer 的内存缓冲区的数据将丢失。</li>
</ul>
</li>
</ul>
</li>
<li><p>producer 端丢失数据如何解决？</p>
<ul>
<li>同步发送，性能差，不推荐。</li>
<li>仍然异步发送，通过“无消息丢失配置”（来自胡夕的《Apache Kafka 实战》）极大降低丢失的可能性：<ul>
<li>block.on.buffer.full = true 尽管该参数在0.9.0.0已经被标记为“deprecated”，但鉴于它的含义非常直观，所以这里还是显式设置它为true，使得producer将一直等待缓冲区直至其变为可用。否则如果producer生产速度过快耗尽了缓冲区，producer将抛出异常</li>
<li>acks=all 很好理解，所有follower都响应了才认为消息提交成功，即”committed”</li>
<li>retries = MAX 无限重试，直到你意识到出现了问题:)</li>
<li>max.in.flight.requests.per.connection = 1 限制客户端在单个连接上能够发送的未响应请求的个数。设置此值是1表示kafka broker在响应请求之前client不能再向同一个broker发送请求。注意：设置此参数是为了避免消息乱序</li>
<li>使用KafkaProducer.send(record, callback)而不是send(record)方法 自定义回调逻辑处理消息发送失败</li>
<li>callback逻辑中最好显式关闭producer：close(0) 注意：设置此参数是为了避免消息乱序</li>
<li>unclean.leader.election.enable=false 关闭unclean leader选举，即不允许非ISR中的副本被选举为leader，以避免数据丢失</li>
<li>replication.factor &gt;= 3 这个完全是个人建议了，参考了Hadoop及业界通用的三备份原则</li>
<li>min.insync.replicas &gt; 1 消息至少要被写入到这么多副本才算成功，也是提升数据持久性的一个参数。与acks配合使用</li>
<li>保证replication.factor &gt; min.insync.replicas 如果两者相等，当一个副本挂掉了分区也就没法正常工作了。通常设置replication.factor = min.insync.replicas + 1即可</li>
</ul>
</li>
</ul>
</li>
<li><p>consumer 端丢失数据如何解决？</p>
<ul>
<li>enable.auto.commit=false 关闭自动提交位移，在消息被完整处理之后再手动提交位移</li>
</ul>
</li>
<li><p>重复数据的场景？</p>
<ul>
<li>网络抖动导致 producer 误以为发送错误，导致重试，从而产生重复数据，可以通过幂等性配置避免。</li>
</ul>
</li>
<li><p>分区策略（即生产消息时如何选择哪个具体的分区）？</p>
<ul>
<li>指定了 key ，相同的 key 会被发送到相同的分区，通过key计算哈希值，（采用MurmurHash2算法，具备高运算性能及低碰撞率）；</li>
<li>没有指定 key，通过轮询保证各个分区上的均匀分配。</li>
</ul>
</li>
<li><p>乱序的场景？</p>
<ul>
<li>消息重试发送</li>
</ul>
</li>
<li><p>乱序如何解决？</p>
<ul>
<li>参数配置 max.in.flight.requests.per.connection = 1 ，但同时会限制 producer 未响应请求的数量，即造成在 broker 响应之前，producer 无法再向该 broker 发送数据。</li>
</ul>
</li>
<li><p>如何选择 Partiton 的数量？</p>
<ul>
<li>在创建 Topic 的时候可以指定 Partiton 数量，也可以在创建完后手动修改。但 Partiton 数量只能增加不能减少。中途增加 Partiton 会导致各个 Partiton 之间数据量的不平等。</li>
<li>Partition 的数量直接决定了该 Topic 的并发处理能力。但也并不是越多越好。Partition 的数量对消息延迟性会产生影响。</li>
<li>一般建议选择 Broker Num * Consumer Num ，这样平均每个 Consumer 会同时读取 Broker 数目个 Partition ， 这些 Partition 压力可以平摊到每台 Broker 上。</li>
</ul>
</li>
<li><p>可重试的异常情况有哪些？</p>
<ul>
<li>分区的 leader 副本不可用，一般发生再 leader 换届选举时。</li>
<li>controller 当前不可用，一般是 controller 在经历新一轮的选举。</li>
<li>网络瞬时故障。</li>
</ul>
</li>
<li><p>controller 的职责有哪些？</p>
<p>在 kafka 集群中，某个 broker 会被选举承担特殊的角色，即控制器（controller），用于管理和协调 kafka 集群，具体职责如下：</p>
<ul>
<li>管理副本和分区的状态</li>
<li>更新集群元数据信息</li>
<li>创建、删除 topic</li>
<li>分区重分配</li>
<li>leader 副本选举</li>
<li>topic 分区扩展</li>
<li>broker 加入、退出集群</li>
<li>受控关闭</li>
<li>controller leader 选举</li>
</ul>
</li>
<li><p>leader 挂了会怎样？（leader failover）</p>
<ul>
<li>当 leader 挂了之后，controller 默认会从 ISR 中选择一个 replica 作为 leader 继续工作，条件是新 leader 必须有挂掉 leader 的所有数据。</li>
<li>如果为了系统的可用性，而容忍降低数据的一致性的话，可以将 unclean.leader.election.enable = true ，开启 kafka 的”脏 leader 选举”。当 ISR 中没有 replica，则会从 OSR 中选择一个 replica 作为 leader 继续响应请求，如此操作提高了 Kafka 的分区容忍度，但是数据一致性降低了。</li>
</ul>
</li>
<li><p>broker 挂了会怎样？（broker failover）</p>
<p>broker上面有很多 partition 和多个 leader 。因此至少需要处理如下内容：</p>
<ul>
<li>更新该 broker 上所有 follower 的状态</li>
<li>从新给 leader 在该 broker 上的 partition 选举 leader</li>
<li>选举完成后，要更新 partition 的状态，比如谁是 leader 等<br>kafka 集群启动后，所有的 broker 都会被 controller 监控，一旦有 broker 宕机，ZK 的监听机制会通知到 controller， controller 拿到挂掉 broker 中所有的 partition，以及它上面的存在的 leader，然后从 partition的 ISR 中选择一个 follower 作为 leader，更改 partition 的 follower 和 leader 状态。</li>
</ul>
</li>
<li><p>controller 挂了会怎样？（controller failover）</p>
<ul>
<li>由于每个 broker 都会在 zookeeper 的 “/controller” 节点注册 watcher，当 controller 宕机时 zookeeper 中的临时节点消失</li>
<li>所有存活的 broker 收到 fire 的通知，每个 broker 都尝试创建新的 controller path，只有一个竞选成功并当选为 controller。</li>
</ul>
</li>
<li><p>Zookeeper 为 Kafka 做了什么？</p>
<ul>
<li>管理 broker 与 consumer 的动态加入与离开。（Producer 不需要管理，随便一台计算机都可以作为Producer 向 Kakfa Broker 发消息）</li>
<li>触发负载均衡，当 broker 或 consumer 加入或离开时会触发负载均衡算法，使得一个 consumer group 内的多个 consumer 的消费负载平衡。（因为一个 comsumer 消费一个或多个partition，一个 partition 只能被一个 consumer 消费）</li>
<li>维护消费关系及每个 partition 的消费信息。</li>
</ul>
</li>
<li><p>Page Cache 带来的好处。</p>
<ul>
<li>Linux 总会把系统中还没被应用使用的内存挪来给 Page Cache，在命令行输入free，或者 cat /proc/meminfo ，“Cached”的部分就是 Page Cache。</li>
<li>Page Cache 中每个文件是一棵 Radix 树（又称 PAT 位树, 一种多叉搜索树），节点由 4k 大小的 Page 组成，可以通过文件的偏移量（如 0x1110001）快速定位到某个Page。</li>
<li>当写操作发生时，它只是将数据写入 Page Cache 中，并将该页置上 dirty 标志。</li>
<li>当读操作发生时，它会首先在 Page Cache 中查找，如果有就直接返回，没有的话就会从磁盘读取文件写入 Page Cache 再读取。</li>
<li>可见，只要生产者与消费者的速度相差不大，消费者会直接读取之前生产者写入Page Cache的数据，大家在内存里完成接力，根本没有磁盘访问。</li>
<li>而比起在内存中维护一份消息数据的传统做法，这既不会重复浪费一倍的内存，Page Cache 又不需要 GC （可以放心使用60G内存了），而且即使 Kafka 重启了，Page Cache 还依然在。</li>
</ul>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/27/8-%E5%8F%AF%E9%9D%A0%E6%80%A7%E6%8E%A2%E7%A9%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/27/8-%E5%8F%AF%E9%9D%A0%E6%80%A7%E6%8E%A2%E7%A9%B6/" class="post-title-link" itemprop="url">8.可靠性探究</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-27 19:08:21" itemprop="dateCreated datePublished" datetime="2021-06-27T19:08:21+08:00">2021-06-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-06-29 21:22:13" itemprop="dateModified" datetime="2021-06-29T21:22:13+08:00">2021-06-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kafka/" itemprop="url" rel="index"><span itemprop="name">kafka</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="副本剖析"><a href="#副本剖析" class="headerlink" title="副本剖析"></a>副本剖析</h2><h3 id="回忆概念"><a href="#回忆概念" class="headerlink" title="回忆概念"></a>回忆概念</h3><ul>
<li>副本是相对于分区而言的，即副本是特定分区的副本。</li>
<li>一个分区中包含一个或多个副本，其中一个为leader副本，其余为follower副本，各个副本位于不同的broker节点中。只有leader副本对外提供服务，follower副本只负责数据同步。</li>
<li>分区中的所有副本统称为 AR，而ISR 是指与leader 副本保持同步状态的副本集合，当然leader副本本身也是这个集合中的一员。</li>
<li>LEO标识每个分区中最后一条消息的下一个位置，分区的每个副本都有自己的LEO，ISR中最小的LEO即为HW，俗称高水位，消费者只能拉取到HW之前的消息。</li>
<li>从生产者发出的一条消息首先会被写入分区的leader副本，不过还需要等待ISR集合中的所有 follower 副本都同步完之后才能被认为已经提交，之后才会更新分区的 HW，进而消费者可以消费到这条消息。</li>
</ul>
<h3 id="失效副本"><a href="#失效副本" class="headerlink" title="失效副本"></a>失效副本</h3><p>replica.lag.time.max.ms来抉择，当ISR集合中的一个follower副本滞后leader副本的时间超过此参数指定的值时则判定为同步失败，需要将此follower副本剔除出ISR集合，replica.lag.time.max.ms参数的默认值为10000。</p>


<ul>
<li>follower副本进程卡住，在一段时间内根本没有向leader副本发起同步请求，比如频繁的Full GC。</li>
<li>follower副本进程同步过慢，在一段时间内都无法追赶上leader副本，比如I/O开销过大。</li>
</ul>
<h3 id="ISR的伸缩"><a href="#ISR的伸缩" class="headerlink" title="ISR的伸缩"></a>ISR的伸缩</h3><h3 id="LEO与HW"><a href="#LEO与HW" class="headerlink" title="LEO与HW"></a>LEO与HW</h3><p>对于副本而言，还有两个概念：本地副本（Local Replica）和远程副本（Remote Replica），本地副本是指对应的Log分配在当前的broker节点上，远程副本是指对应的Log分配在其他的broker节点上。</p>
<p>整个消息追加的过程可以概括如下：</p>
<ul>
<li>生产者客户端发送消息至leader副本（副本1）中。</li>
<li>消息被追加到leader副本的本地日志，并且会更新日志的偏移量。</li>
<li>follower副本（副本2和副本3）向leader副本请求同步数据。</li>
<li>leader副本所在的服务器读取本地日志，并更新对应拉取的follower副本的信息。</li>
<li>leader副本所在的服务器将拉取结果返回给follower副本。</li>
<li>follower副本收到leader副本返回的拉取结果，将消息追加到本地日志中，并更新日志的偏移量信息。</li>
</ul>
<h3 id="为什么不支持读写分离"><a href="#为什么不支持读写分离" class="headerlink" title="为什么不支持读写分离"></a><strong>为什么不支持读写分离</strong></h3><p>可以实现，但是主写从读也有2个很明显的缺点：</p>
<ul>
<li>数据一致性问题。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间窗口会导致主从节点之间的数据不一致。某一时刻，在主节点和从节点中A数据的值都为X，之后将主节点中A的值修改为Y，那么在这个变更通知到从节点之前，应用读取从节点中的A数据的值并不为最新的Y，由此便产生了数据不一致的问题。</li>
<li>延时问题。类似Redis这种组件，数据从写入主节点到同步至从节点中的过程需要经历网络→主节点内存→网络→从节点内存这几个阶段，整个过程会耗费一定的时间。而在Kafka中，主从同步会比 Redis 更加耗时，它需要经历网络→主节点内存→主节点磁盘→网络→从节点内存→从节点磁盘这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/10/">10</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">sk-xinye</p>
  <div class="site-description" itemprop="description">愿所有努力都不被辜负</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">96</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">13</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">sk-xinye</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
