{"meta":{"title":"sk-xinyeの博客","subtitle":"记录学习的脚步","description":"愿所有努力都不被辜负","author":"sk-xinye","url":"https://sk-xinye.github.io","root":"/"},"pages":[{"title":"categories","date":"2021-05-05T09:11:02.000Z","updated":"2021-05-08T13:59:05.046Z","comments":false,"path":"categories/index.html","permalink":"https://sk-xinye.github.io/categories/index.html","excerpt":"","text":""},{"title":"欢迎到来的奋斗者~","date":"2021-05-05T09:11:16.000Z","updated":"2023-07-16T13:33:19.169Z","comments":true,"path":"index.html","permalink":"https://sk-xinye.github.io/index.html","excerpt":"","text":"Welcome to [sk-xinye] blog (https://sk-xinye.github.io/)!House-moving in labor day. 欢迎到来的奋斗者们，原所有努力都不被辜负 查看更多，请点击左侧分类"},{"title":"tags","date":"2021-05-05T09:11:16.000Z","updated":"2023-07-16T13:26:37.920Z","comments":false,"path":"tags/index.html","permalink":"https://sk-xinye.github.io/tags/index.html","excerpt":"","text":"aaa"}],"posts":[{"title":"3.operator测试","slug":"3-operator测试","date":"2023-06-02T09:20:27.000Z","updated":"2023-07-16T13:14:55.064Z","comments":true,"path":"2023/06/02/3-operator测试/","link":"","permalink":"https://sk-xinye.github.io/2023/06/02/3-operator%E6%B5%8B%E8%AF%95/","excerpt":"","text":"hadoop环境搭建下载 地址： https://archive.apache.org/dist/hadoop/common/ hadoop-2.7.7.tar.gz 因要使用hive 2.1.1所以，使用对应的hadoop版本2.7.7 tar -xvf hadoop-2.7.7.tar.gz 配置 路径 hadoop-2.7.7/etc/hadoop hadoop-env.sh 修改 JAVA_HOME 修改core-site.xml 1234567891011121314151617181920212223242526272829303132333435363738394041&lt;configuration&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://node-193:9000&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/ksun/k8s-env/hadoop/hadoop-2.7.7/data/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.proxyuser.root.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt; &lt;/property&gt; &lt;!--以下为 开启 Kerberos --&gt; &lt;property&gt; &lt;name&gt;hadoop.security.authentication&lt;/name&gt; &lt;value&gt;kerberos&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.security.authorization&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.rpc.protection&lt;/name&gt; &lt;value&gt;authentication&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.security.auth_to_local&lt;/name&gt; &lt;value&gt; RULE:[2:$1/$2@$0](hadoop/.*@CONNECT.COM)s/.*/root/ DEFAULT &lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105&lt;configuration&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;1&lt;/value&gt; &lt;/property&gt; &lt;!--以下为 开启 Kerberos --&gt; &lt;property&gt; &lt;name&gt;dfs.premissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 2 NameNode --&gt; &lt;property&gt; &lt;name&gt;dfs.block.access.token.enable&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.kerberos.principal&lt;/name&gt; &lt;value&gt;hadoop/node-193@CONNECT.COM&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.keytab.file&lt;/name&gt; &lt;value&gt;/home/ksun/k8s-env/hadoop/hadoop-2.7.7/etc/hadoop/hadoop.keytab&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.kerberos.internal.spnego.principal&lt;/name&gt; &lt;value&gt;hadoop/node-193@CONNECT.COM&lt;/value&gt; &lt;/property&gt; &lt;!-- 3 Secondary NameNode --&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;0.0.0.0:9868&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.https.address&lt;/name&gt; &lt;value&gt;0.0.0.0:9869&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.namenode.keytab.file&lt;/name&gt; &lt;value&gt;/home/ksun/k8s-env/hadoop/hadoop-2.7.7/etc/hadoop/hadoop.keytab&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.namenode.kerberos.principal&lt;/name&gt; &lt;value&gt;hadoop/node-193@CONNECT.COM&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.secondary.namenode.kerberos.internal.spnego.principal&lt;/name&gt; &lt;value&gt;hadoop/node-193@CONNECT.COM&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.http.policy&lt;/name&gt; &lt;value&gt;HTTPS_ONLY&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.https-address&lt;/name&gt; &lt;value&gt;node-193:9871&lt;/value&gt; &lt;/property&gt; &lt;!-- 5 DataNode --&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir.perm&lt;/name&gt; &lt;value&gt;700&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.address&lt;/name&gt; &lt;!--&lt;value&gt;0.0.0.0:1004&lt;/value&gt;--&gt; &lt;value&gt;0.0.0.0:1104&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.http.address&lt;/name&gt; &lt;!--&lt;value&gt;0.0.0.0:1006&lt;/value&gt;--&gt; &lt;value&gt;0.0.0.0:1106&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.https.address&lt;/name&gt; &lt;value&gt;0.0.0.0:9865&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.data.transfer.protection&lt;/name&gt; &lt;value&gt;integrity&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.kerberos.principal&lt;/name&gt; &lt;value&gt;hadoop/node-193@CONNECT.COM&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.keytab.file&lt;/name&gt; &lt;value&gt;/home/ksun/k8s-env/hadoop/hadoop-2.7.7/etc/hadoop/hadoop.keytab&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.encrypt.data.transfer&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;!-- 6 WebHDFS --&gt; &lt;property&gt; &lt;name&gt;dfs.web.authentication.kerberos.principal&lt;/name&gt; &lt;value&gt;hadoop/node-193@CONNECT.COM&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.web.authentication.kerberos.keytab&lt;/name&gt; &lt;value&gt;/home/ksun/k8s-env/hadoop/hadoop-2.7.7/etc/hadoop/hadoop.keytab&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;&lt;/configuration&gt; 配置mapred-site.xml (kerberos时不配置该选项) 123456&lt;configuration&gt;&lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 配置yarn-site.xml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt;&lt;configuration&gt;&lt;!-- Site specific YARN configuration properties --&gt; &lt;!-- Reducer获取数据的方式 --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;!-- 指定YARN的ResourceManager的地址 --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt; &lt;value&gt;node-193&lt;/value&gt; &lt;/property&gt; &lt;!--以下为 开启 Kerberos --&gt; &lt;property&gt; &lt;name&gt;yarn.application.classpath&lt;/name&gt; &lt;value&gt; /home/ksun/k8s-env/hadoop/hadoop-2.7.7/etc/hadoop, /home/ksun/k8s-env/hadoop/hadoop-2.7.7/share/hadoop/common/*, /home/ksun/k8s-env/hadoop/hadoop-2.7.7/share/hadoop/common/lib/*, /home/ksun/k8s-env/hadoop/hadoop-2.7.7/share/hadoop/hdfs/*, /home/ksun/k8s-env/hadoop/hadoop-2.7.7/share/hadoop/hdfs/lib/*, /home/ksun/k8s-env/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/*, /home/ksun/k8s-env/hadoop/hadoop-2.7.7/share/hadoop/mapreduce/lib/*, /home/ksun/k8s-env/hadoop/hadoop-2.7.7/share/hadoop/yarn/*, /home/ksun/k8s-env/hadoop/hadoop-2.7.7/share/hadoop/yarn/lib/* &lt;/value&gt; &lt;/property&gt; &lt;!-- Kerberos --&gt; &lt;!-- 1 ResourceManager --&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.principal&lt;/name&gt; &lt;value&gt;hadoop/node-193@CONNECT.COM&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.keytab&lt;/name&gt; &lt;value&gt;/home/ksun/k8s-env/hadoop/hadoop-2.7.7/etc/hadoop/hadoop.keytab&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.https.address&lt;/name&gt; &lt;value&gt;$&#123;yarn.resourcemanager.hostname&#125;:8090&lt;/value&gt; &lt;/property&gt; &lt;!-- 2 NodeManager --&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.principal&lt;/name&gt; &lt;value&gt;hadoop/node-193@CONNECT.COM&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.keytab&lt;/name&gt; &lt;value&gt;/home/ksun/k8s-env/hadoop/hadoop-2.7.7/etc/hadoop/hadoop.keytab&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.webapp.https.address&lt;/name&gt; &lt;value&gt;0.0.0.0:8044&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt;&lt;/configuration&gt; 配置java /etc/profile 12345678910111213141516export JAVA_HOME=/home/ksun/k8s-env/hadoop/jdk1.8export HADOOP_HOME=/home/ksun/k8s-env/hadoop/hadoop-2.7.7export JRE_HOME=$JAVA_HOME/jre#export PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar#export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbinexport HADOOP_CONF_DIR=$&#123;HADOOP_HOME&#125;/etc/hadoopexport HADOOP_COMMON_LIB_NATIVE_DIR=$&#123;HADOOP_HOME&#125;/lib/nativeexport HADOOP_OPTS=&quot;-Djava.library.path=$&#123;HADOOP_HOME&#125;/lib&quot;export HIVE_HOME=/home/ksun/k8s-env/hadoop/hive-2.1.1export HIVE_CONF_DIR=$&#123;HIVE_HOME&#125;/conf#export PATH=.:$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$PATH#export PATH=/home/ksun/k8s-env/hadoop/jdk1.8/bin:/home/ksun/k8s-env/hadoop/jdk1.8/jre/bin:/home/ksun/k8s-env/hadoop/hive-2.1.1/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/local/bin:/usr/binexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/binexport PATH=.:$JAVA_HOME/bin:$JRE_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$HIVE_HOME/bin:$PATH 启动hadoop 123456hadoop namenode -format 格式化hadoopstart-dfs.shstart-yarn.shjps如果出现了SencondaryNameNode NameNode DataNode NodeManager ResourceManager则没什么问题，启动成功 root启动 123456789101112ERROR: Attempting to operate on hdfs namenode as root 等问题 在/hadoop/sbin路径下： 将start-dfs.sh，stop-dfs.sh两个文件顶部添加以下参数 HDFS_DATANODE_USER=root HADOOP_SECURE_DN_USER=hdfs HDFS_NAMENODE_USER=root HDFS_SECONDARYNAMENODE_USER=root start-yarn.sh，stop-yarn.sh顶部也需添加以下 YARN_RESOURCEMANAGER_USER=root HADOOP_SECURE_DN_USER=yarn YARN_NODEMANAGER_USER=root 相关命令 12345678910111213141516171819202122export KRB5CCNAME=/tmp/krb5cc_0[root@node-193 hadoop]# kinit -kt /home/ksun/k8s-env/hadoop/hadoop-2.7.7/etc/hadoop/hadoop.keytab hadoop/node-193@CONNECT.COM/home/ksun/k8s-env/hadoop/hadoop-2.7.7/data/hive beeline /opt/module/hive/bin/beeline -u &quot;jdbc:hive2://192.168.83.187:10000/;auth=KERBEROS;principal=hive1/node187@CONNECT.COM&quot; show databases; create database if not exists pp2_test; show tables; show create table e_mp_read_curve; load data local inpath &#x27;/home/robot/&#x27; overwrite into table pp2_test.yc_meter_archives partition(dt=&#x27;20221213&#x27;) insert overwrite local directory &#x27;/home/data/&#x27; select * from hive_table; cloudera-scm/admin cloudera-scm/adminhdfs: hdfs dfs -mkdir /demo1 hdfs dfs -ls /user/hive/csv/jibei_hubiao hdfs dfs -get /user/hive/csv/jibei_hubiao/e_mp_read_curve ./; hdfs dfs -put /root/sql_tmp/1wtg/ /user/pp2/binary_data/ hdfs dfs -rm -r 文件夹路径 hdfs dfs -chmod -R +w /user/hive/warehouse 设置权限导数据可以直接-get 然后-put就可以了 Hive环境搭建下载hive 地址：https://github.com/apache/hive/tree/rel/release-2.1.1 提前安装好mysql需要 tar -xvf apache-hive-2.1.1-bin.tar.gz 配置与启动 配置环境变量，如上述内容7所示 修改hive-env.sh文件 1234567cd /home/ksun/k8s-env/hadoop/hive-2.1.1/conf 下进行修改export HIVE_HOME=/home/ksun/k8s-env/hadoop/hive-2.1.1export HIVE_CONF_DIR=$&#123;HIVE_HOME&#125;/confexport HIVE_AUX_JARS_PATH=$&#123;HIVE_HOME&#125;/lib#export HADOOP_HOME=/home/ksun/k8s-env/hadoop/hadoop-3.0.0export HADOOP_HOME=/home/ksun/k8s-env/hadoop/hadoop-2.7.7 hive-site.xml 1234567# cp hive-default.xml.template hive-site.xml# vim hive-site.xml由于在该配置文件中有如下两个配置项注明了hive在HDFS中数据存储的目录，因此我们需要在HDFS上手动创建并赋权限，也就是需要在hdfs上创建/tmp/hive 和/user/hive/warehouse# hadoop fs -mkdir -p /user/hive/warehouse# hadoop fs -chmod -R 777 /user/hive/warehouse #递归赋予读写权限# hadoop fs -mkdir -p /tmp/hive/ #创建/tmp/hive/目录 # hadoop fs -chmod -R 777 /tmp/hive #目录赋予读写权限 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hive.default.fileformat&lt;/name&gt; &lt;value&gt;TextFile&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;!--端口改为你自己的端口，这里是连接数据库中zxhive数据库--&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://node187:3306/hive30&lt;/value&gt; &lt;description&gt;JDBC connect string for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;!--最新版本连接MySQL的jar包 所有写com.mysql.cj.jdbc.Driver,如果是旧版本用com.mysql.jdbc.Driver--&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;!--连接MySQL的用户名--&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;root&lt;/value&gt; &lt;description&gt;username to use against metastore database&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.authentication&lt;/name&gt; &lt;value&gt;KERBEROS&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.authentication.kerberos.principal&lt;/name&gt; &lt;value&gt;hadoop/node-193@CONNECT.COM&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.server2.authentication.kerberos.keytab&lt;/name&gt; &lt;value&gt;/home/ksun/k8s-env/hadoop/hadoop-2.7.7/etc/hadoop/hadoop.keytab&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.uris&lt;/name&gt; &lt;value&gt;thrift://node-193:9083&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.sasl.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.kerberos.keytab.file&lt;/name&gt; &lt;value&gt;/home/ksun/k8s-env/hadoop/hadoop-2.7.7/etc/hadoop/hadoop.keytab&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.kerberos.principal&lt;/name&gt; &lt;value&gt;hadoop/node-193@CONNECT.COM&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 启动测试 123456schematool -initSchema -dbType mysqlhivekinit -c /tmp/krb5cc_0 -k -t /zshield/BGconf/hive/conf/hive.keytab hive/node-192@ZSHIELD.REALMbeeline -u &quot;jdbc:hive2://192.168.81.192:10000/;principal=hive/node-192@ZSHIELD.REALM&quot;kinit -kt /home/ksun/k8s-env/hadoop/hadoop-2.7.7/etc/hadoop/hadoop.keytab hadoop/node-193@CONNECT.COM &amp;&amp; export KRB5CCNAME=/tmp/krb5cc_0","categories":[],"tags":[]},{"title":"2.k8s部署","slug":"2-k8s部署","date":"2023-04-25T06:15:33.000Z","updated":"2023-07-16T13:14:55.062Z","comments":true,"path":"2023/04/25/2-k8s部署/","link":"","permalink":"https://sk-xinye.github.io/2023/04/25/2-k8s%E9%83%A8%E7%BD%B2/","excerpt":"","text":"k8s搭建搭建 关闭防火墙 $ systemctl stop firewalld $ systemctl disable firewalld 由于 CentOS8 防火墙使用了 nftables，但 Docker 尚未支持 nftables， 我们可以使用如下设置使用 iptables： 更改 /etc/firewalld/firewalld.conf #FirewallBackend=nftables FirewallBackend=iptables 关闭selinux $ sed -i ‘s/enforcing/disabled/‘ /etc/selinux/config # 永久 $ setenforce 0 # 临时 关闭swap $ swapoff -a # 临时 $ vim /etc/fstab 注释掉 swap行挂载 # 永久 设置主机名： $ hostnamectl set-hostname 在master添加hosts： $ cat &gt;&gt; /etc/hosts &lt;&lt; EOF 192.168.81.192 node-192 192.168.81.193 node-193 192.168.81.194 node-194 EOF 将桥接的IPv4流量传递到iptables的链： $ cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt; EOF net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF $ sysctl –system # 生效 时间同步： $ yum install ntpdate -y $ ntpdate time.windows.com 安装dockeryum install -y yum-utilsyum-config-manager –add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.reposed -i ‘s/download.docker.com/mirrors.aliyun.com/docker-ce/g’ /etc/yum.repos.d/docker-ce.repoyum-config-manager –enable docker-ce-testyum install docker-ce docker-ce-cli containerd.iosystemctl enable dockersystemctl start dockersystemctl enable docker.service 问题发生时，清理镜像： docker system prune systemctl stop kubelet systemctl stop docker systemctl start docker systemctl start kubelet 配置镜像仓库 12345# cat &gt; /etc/docker/daemon.json &lt;&lt; EOF&#123;&quot;registry-mirrors&quot;: [&quot;https://b9pmyelo.mirror.aliyuncs.com&quot;]&#125;EOF 添加k8s yum源 123456789$ cat &gt; /etc/yum.repos.d/kubernetes.repo &lt;&lt; EOF[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOF 安装kubeadm，kubelet和kubectl 123yum install -y kubelet-1.18.0 kubeadm-1.18.0 kubectl-1.18.0systemctl enable kubeletkubeadm version #查看版本 加载镜像 kubeadm config images list 查看需要的镜像列表./load-images.sh 部署 K8S master 123456789101112131415161718kubeadm init \\ --apiserver-advertise-address=192.168.81.192 \\ --image-repository registry.aliyuncs.com/google_containers \\ 内网需要去掉这行 --kubernetes-version v1.18.0 \\ --service-cidr=10.92.0.0/12 \\ --pod-network-cidr=10.220.0.0/16 \\ --ignore-preflight-errors=allkubeadm join 192.168.81.192:6443 --token xivvbs.rdmeg2w6g8i0m1ww --discovery-token-ca-cert-hash sha256:3f475490aaf5a86a4d0a83e40579b73b5ac4debf17dece0300e910fc542c2b70mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config # 如果需要node节点使用kubectl ，可以将该文件复制到对应的node 节点位置sudo chown $(id -u):$(id -g) $HOME/.kube/config$ kubectl get nodeskubectl get pods -n kube-system 查看状态kubeadm token create --print-join-command 查看加入集群命令kubectl taint nodes --all node-role.kubernetes.io/master- 容忍master部署服务 node节点加入集群 kubeadm join 192.168.81.192:6443 –token xivvbs.rdmeg2w6g8i0m1ww –discovery-token-ca-cert-hash sha256:3f475490aaf5a86a4d0a83e40579b73b5ac4debf17dece0300e910fc542c2b70kubectl label node node-193 node-role.kubernetes.io/worker=worker 打标签 可做可不做kubectl label node node-194 node-role.kubernetes.io/worker=worker 安装网络插件 docker load -i mirrored-flannelcni-flannel-cni-plugin.xzdocker load -i flannel.xz docker apply -f kubectl apply -f kube-flannel.yml问题：open /etc/resolv.conf: no such file or directory 随意添加一个dns文件即可 nameserver 8.8.8.8 helm安装 问题 12345解决 Error: cannot re-use a name that is still in use解决方案如下：helm ls --all-namespaceskubectl delete namespace kuberhealthykubectl create namespace kuberhealthy python -m zbuild.command.cloud_pkg pkg.include=[‘zs_power’] cache=/home/zshield/zhaoyu/v3 pkg.name=test_0506 pkg.zip=true server.zs_power.git.branch=feature/feature/PBUZZSHYY-2854-tmp-0506 server.zs_power.git.tag=null server.zs_power.git.build_shell=container/aliyun_zs_power_3.6/build.sh server.zs_power.version=test0506-3 extra_vars=qa kubectl delete –all pods –namespace=fookubectl delete po kubectl get po -n spark-sk | grep test | awk &#39;&#123;print $1&#125;&#39; –grace-period=0 –force -n spark-sk 批量删除podkubectl delete pods –grace-period=0 –force","categories":[],"tags":[]},{"title":"1.cdh搭建","slug":"1-cdh搭建","date":"2023-04-25T06:00:11.000Z","updated":"2023-07-16T13:14:55.062Z","comments":true,"path":"2023/04/25/1-cdh搭建/","link":"","permalink":"https://sk-xinye.github.io/2023/04/25/1-cdh%E6%90%AD%E5%BB%BA/","excerpt":"","text":"CDH平台搭建搭建 配置初始化环境 3台机器 213 214 215 cd /etc/yum.repos.d mkdir bak mv *.repo back curl -O http://192.168.2.7/files/dev7.repo yum clean all yum makecache fast yum install docker-ce docker-ce-cli containerd.io cp ./docker-compose /usr/local/sbin/ systemctl start docker echo “vm.max_map_count=655360” &gt;&gt; /etc/sysctl.conf 解压缩 unzip CDH6.3.2.zip hosts配置 vim /etc/hosts 192.168.82.213 node-213 192.168.82.214 node-214 192.168.82.215 node-215 source /etc/hosts 关闭防火墙 systemctl stop firewalld systemctl disable firewalld 临时关闭：setenforce 0 永久关闭：vim /etc/selinux/config 修改SELINUX=disabled(重启生效) 配置免密登录（所有节点分别执行） ssh-keygen -t rsa ssh-copy-id node-214 时区、时间同步 找一个机器，作为时间服务器，所有的机器与这台集群时间进行定时的同步，生产环境根据任务对时间的准确程度要求周期同步。测试环境为了尽快看到效果，采用1分钟同步一次 时间服务器配置（必须root用户） 1.1 查看所有节点ntpd服务状态和开机自启动状态 查看：systemctl status ntpd 启动：systemctl start ntpd 开机启动：systemctl is-enabled ntpd 1.2 修改node-213的ntp.conf配置文件 vim /etc/ntp.conf 修改内容如下 （a）修改1（授权192.168.6.0-192.168.6.255网段上的所有机器可以从这台机器上查询和同步时间） #restrict 192.168.6.0 mask 255.255.255.0 nomodify notrap 为restrict 192.168.82.0 mask 255.255.255.0 nomodify notrap （b）修改2（注释掉默认的上层主机，局域网内访问不到）： server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst （c）添加3（当该节点丢失网络连接，依然可以采用本地时间作为时间服务器为集群中的其他节点提供时间同步） server 127.127.1.0 （d）设置本机作为时间服务器的等级（0为最高等级） fudge 127.127.1.0 stratum 10 1.3 修改node-213的/etc/sysconfig/ntpd 文件 vim /etc/sysconfig/ntpd 增加内容如下（让硬件时间与系统时间一起同步） SYNC_HWCLOCK=yes 1.4 重新启动ntpd服务 systemctl start ntpd 1.5 设置ntpd服务开机启动 systemctl enable ntpd 其他机器配置（必须root用户）方式二(1)systemctl stop ntpd(2)各台agent服务器编辑 /etc/ntp.conf文件b 修改内容如下 (a)修改1（注释掉默认的上层主机，局域网内访问不到）： server 0.centos.pool.ntp.org iburst server 1.centos.pool.ntp.org iburst server 2.centos.pool.ntp.org iburst server 3.centos.pool.ntp.org iburst (b)将node-213服务器作为时间服务器 server node-213service ntpd restart(3) 启动ntpd服务：systemctl start ntpd(4) 设置开机启动：systemctl enable ntpd(5)同步时间 ntpd -gq(6) 大约5分钟后查看是否时间同步，出现下图以 * 开头的包含本机服务器的行说明没问题； ntpdc -nc 安装java rpm -ivh oracle-j2sdk1.8-1.8.0+update181-1.x86_64.rpm #update-alternatives –install /usr/bin/java java /usr/java/jdk1.8.0_181-cloudera/bin/java 3 vim /etc/profile export JAVA_HOME=/usr/java/jdk1.8.0_181-cloudera export JRE_HOME=$JAVA_HOME/jre export PATH=$PATH:$JAVA_HOME/bin export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar source /etc/profile 安装mysql yum install krb5-devel cyrus-sasl-gssapi cyrus-sasl-deve libxml2-devel libxslt-devel mysql mysql-devel openldap-devel python-devel python-simplejson sqlite-devel yum install –skip-broken krb5-devel cyrus-sasl-gssapi cyrus-sasl-deve libxml2-devel libxslt-devel mysql mysql-devel openldap-devel python-devel python-simplejson sqlite-devel docker-compose up -d (需要注意映射文件目录) create database cmf default character set utf8; create database amon default character set utf8; create database hive default character set utf8; create database hue default character set utf8; create database oozie default character set utf8; grant all privileges on cmf.* to ‘cmf’@’%’ identified by ‘123456’; grant all privileges on amon.* to ‘amon’@’%’ identified by ‘123456’; grant all privileges on hive.* to ‘hive’@’%’ identified by ‘123456’; grant all privileges on hue.* to ‘hue’@’%’ identified by ‘123456’; flush privileges; create database hue default charset utf8 collate utf8_general_ci; grant all on hue.* to ‘root’@’%’ identified by ‘123456’; create database hive default charset utf8; create user ‘hive’@’%’ identified by ‘123456’; grant all on hive.* TO ‘hive’@’localhost’ identified by ‘123456’; grant all on hive.* TO ‘hive’@’%’ identified by ‘123456’; flush privileges; 2.5 重命名mysql jdbc jar 1.准备 mysql-connector-java.jar mkdir -p /usr/share/java 2.重命名 cp mysql-connector-java-5.1.47.jar /usr/share/java/mysql-connector-java.jar CM server部署 rpm -ivh cloudera-manager-daemons-6.3.1-1466458.el7.x86_64.rpm –nodeps –force rpm -ivh cloudera-manager-server-6.3.1-1466458.el7.x86_64.rpm –nodeps –force cd /etc/cloudera-scm-server vim db.properties com.cloudera.cmf.db.host=node-213:13306 com.cloudera.cmf.db.name=cmf com.cloudera.cmf.db.user=root com.cloudera.cmf.db.password=123456 com.cloudera.cmf.db.setupType=EXTERNAL 目录： /var/log/cloudera-scm-server service cloudera-scm-server start 部署CM agent（所有节点）:rpm -ivh cloudera-manager-daemons-6.3.1-1466458.el7.x86_64.rpm –nodeps –forcerpm -ivh cloudera-manager-agent-6.3.1-1466458.el7.x86_64.rpm –nodeps –force rpm -e –nodeps cloudera-manager-daemons-6.3.1-1466458.el7.x86_64.rpmrpm -e –nodeps cloudera-manager-agent-6.3.1-1466458.el7.x86_64.rpm #卸载 卸载一次在安装就可以了vim /etc/cloudera-scm-agent/conf.ini server_host=node-213 service cloudera-scm-agent start 多重启两次日志：/var/log/cloudera-manager-agent/cloudera-manager-agent.log 安装httpd服务 yum install httpd mkdir /var/www/html/cdh6_parcel 把parcel文件放到文件夹下 CDH-6.3.2-1.cdh6.3.2.p0.1600554-el7.parcel CDH-6.3.2-1.cdh6.3.2.p0.1600554-el7.parcel.sha Manifest.json 启动httpd服务 systemctl start httpd 访问 http://node-213/cdh6_parcel 访问安装即可 http://node-213:7180","categories":[],"tags":[]},{"title":"","slug":"README","date":"2023-02-04T02:34:09.242Z","updated":"2023-02-04T02:33:07.979Z","comments":true,"path":"2023/02/04/README/","link":"","permalink":"https://sk-xinye.github.io/2023/02/04/README/","excerpt":"","text":"文件 I/O 简明概述文件 I/O 性能是后台应用的主要瓶颈之一，一直以来想对文件 I/O 这个偌大的系统进行总结，故写此文。 文件 I/O 内容较多，书籍的意义在于能更系统地说明问题，避免博客文章散乱的问题。 书籍有涉及很大部分非原创内容，相关引用会在 REFERENCE 小节中指出。 书籍内容包括： [1.page cache](1. page cache.html) [2.DMA 与零拷贝技术](2. DMA 与零拷贝技术.html) [3.mmap](3. mmap.html) [4.文件分区](4. 文件分区.html) [5.Java ByteBuffer与 Channel](5. Java ByteBuffer与 Channel.html) [6.FileChannel](6. FileChannel.html) [7.JavaVisual 工具](7. Visual VM.html) [8.Java ByteBuffer 测试](8. Java ByteBuffer 测试.html) [9.如何实现顺序读写](9. 如何实现顺序读写.html) 一些章节可能会需要一定的 Java 语言基础，其中：14 小节完全不需要 Java 基础，而 59 小节会涉及一定的 Java 代码。读者朋友可以有选择性地阅读。 Figure1.Linux IO Stack Diagram 其他 个人博客地址：https://spongecaptain.cool 书籍 GitHub 地址：https://github.com/Spongecaptain/SimpleClearFileIO，如有意见或者建议，特别希望读者朋友能够提 issue&amp;pr。若有帮助，欢迎 star&amp;fork。 推荐阅读 On Disk IO, Part 1: Flavors of IO On Disk IO, Part 2: More Flavours of IO Read, write &amp; space amplification - pick 2 致谢 本书受到 文件 IO 操作的一些最佳实践 一文启发，很感谢阿里巴巴中间件团队的徐靖峰，其所写文章带来的启发意义非凡。","categories":[],"tags":[]},{"title":"","slug":"9. 如何实现顺序读写","date":"2023-02-04T02:34:09.237Z","updated":"2023-02-04T02:33:07.978Z","comments":true,"path":"2023/02/04/9. 如何实现顺序读写/","link":"","permalink":"https://sk-xinye.github.io/2023/02/04/9.%20%E5%A6%82%E4%BD%95%E5%AE%9E%E7%8E%B0%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99/","excerpt":"","text":"如何实现顺序读写1. 为什么顺序读写更快顺序读写快的原因主要有俩： 顺序写比随机写更节约磁盘指针的移动总长度； 顺序写最大化利用到了文件系统的缓存机制，提高了缓存命中率； 下面我们说一说原因 2 的由来： 如上图所示，以顺序读为例，当用户发起一个 fileChannel.read(4kb) 之后，实际发生了两件事： 操作系统从磁盘加载了 16kb 进入 PageCache，这被称为预读 操作通从 PageCache 拷贝 4kb 进入用户内存； 最终我们在用户内存访问到了 4kb，为什么顺序读快？很容量想到，当用户继续访问接下来的 [4kb,16kb] 的磁盘内容时，便是直接从 PageCache 去访问了。试想一下，当需要访问 16kb 的磁盘内容时，是发生 4 次磁盘 I/O 快，还是发生 1 次磁盘 I/O+4 次内存 I/O 快呢？答案是显而易见的，这一切都是 PageCache 带来的优化。 深度思考：当内存吃紧时，PageCache 的分配会受影响吗？PageCache 的大小如何确定，是固定的 16kb 吗？我可以监控 PageCache 的命中情况吗？ PageCache 会在哪些场景失效，如果失效了，我们又要哪些补救方式呢？ 我进行简单的自问自答，背后的逻辑还需要读者去推敲： 当内存吃紧时，PageCache 的预读会受到影响，实测，并没有搜到到文献支持 PageCache 是动态调整的，可以通过 linux 的系统参数进行调整，默认是占据总内存的 20% https://github.com/brendangregg/perf-tools github 上一款工具可以监控 PageCache 如果用 PageCache 做缓存不可控，不妨自己做预读。 顺序写的原理和顺序读一致，都是收到了 PageCache 的影响，留给读者自己推敲一下。 2. Java 上利用锁实现顺序读写一个公认的事实是：无论是传统机械磁盘，还是固体硬盘，顺序读写比随机读写效率更高，那么如何实现顺序读写呢？ 为了说明这个问题，我们首先需要分清读写过程涉及的两种顺序（以写操作为例）： 应用层的顺序性：接收端先后接收到两个消息：消息A、消息B，我们要求磁盘最终落盘时，消息就是以 A、B 次序保存的； 磁盘指针的顺序性：如果有两个线程负责写入 I/O 操作，线程 1 负责写消息 B，线程 2 负责写消息 A，但如果要确保磁盘指针的顺序移动，在消息 A 必须先于消息 B 落盘的大前提下，必然要求线程 2 先写，线程 B 后写。 我们再举一个代码上的例子，这里没有应用层的顺序要求，只有磁盘指针的顺序要求。 写入方式一：64 个线程，用户自己使用一个 atomic 变量记录写入指针的位置，并发写入： 12345678ExecutorService executor &#x3D; Executors.newFixedThreadPool(64);&#x2F;&#x2F;64 大小的线程池AtomicLong wrotePosition &#x3D; new AtomicLong(0);&#x2F;&#x2F;指针for(int i&#x3D;0;i&lt;1024;i++)&#123; final int index &#x3D; i; executor.execute(()-&gt;&#123; fileChannel.write(ByteBuffer.wrap(new byte[4*1024]),wrote.getAndAdd(4*1024)); &#125;)&#125; 写入方式二：给 write 加了锁，保证了同步： 123456789101112ExecutorService executor = Executors.newFixedThreadPool(64);AtomicLong wrotePosition = new AtomicLong(0);for(int i=0;i&lt;1024;i++)&#123; final int index = i; executor.execute(()-&gt;&#123; write(new byte[4*1024]); &#125;)&#125;public synchronized void write(byte[] data)&#123; fileChannel.write(ByteBuffer.wrap(new byte[4*1024]),wrote.getAndAdd(4*1024));&#125; 只有方式二才算顺序写，顺序读也是同理。在方式 1 中，可能有如下顺序的写入： 时序 1：thread1 write position[0~4096)； 时序 2：thread3 write position[8194~12288)； 时序 3：thread2 write position[4096~8194)； 所以，方式一并不是完全的“顺序写”。 对于文件操作，加锁并不是一件非常可怕的事，不敢同步 write/read 才可怕！ 读时加锁 —&gt; 读的阻塞耗时期间锁会迟迟不释放。但是即使如此，为了顺序 I/O，我们还是要使用锁机制。 有人会问：FileChannel 内部不是已经有 positionLock 保证写入的线程安全了吗，为什么还要自己加同步？ 确实如此，但是 FileChannel#write 方法入口处的 position 生成（即wrotePosition.getAndAdd(4*1024)）与 FileChannel#write 方法内部的上锁不是原子操作，此时如果不粗粒度地上锁，就会导致顺序写失效。 3. 扩展-锁操作系统提供的 write 系统调用并没有确保原子语义，这意味着如果多个线程同时执行多个 write 操作来对同一个文件的同一个部分进行修改操作，那么最终执行结果取决于实际的执行逻辑顺序（这是无法预计的）。这种写场景类似于多线程访问共享变量，都是线程不安全的操作。 不过，文件系统提供两个操作的原子性，即当 open 系统调用以 flag 为 O_CREAT 或 O_APPEND 打开文件时。 O_CREAT：文件不存在就创建； O_APPEND：每次写文件时把文件游标移动到文件最后追加写；Java 中使用追加式文件写的例子为 testAppendIoInJava（NFS等文件系统不保证这个flag）； 不过，即使确保原子操作，也无法确保并发安全性，因为原子性只是并发安全的一个条件。 我们通常使用锁来实现并发安全性，例如 Linux 有提供两种类型的文件锁： flock：只能对整个文件上锁，而不能对文件的某一部分上锁； fcntl：即可以锁住整个文件，又能只锁文件的某一部分； 但在事实上，我们通常会避免使用操作系统提供的文件锁，而是要么选择避免多线程同时写一个文件的情况，要么在应用层上实现锁，比如 Java 代码中的 synchronized 关键字。 REFERENCE 文件 IO 操作的一些最佳实践 Linux文件锁学习-flock, lockf, fcntl","categories":[],"tags":[]},{"title":"","slug":"8. Java ByteBuffer 测试","date":"2023-02-04T02:34:09.233Z","updated":"2023-02-04T02:33:07.978Z","comments":true,"path":"2023/02/04/8. Java ByteBuffer 测试/","link":"","permalink":"https://sk-xinye.github.io/2023/02/04/8.%20Java%20ByteBuffer%20%E6%B5%8B%E8%AF%95/","excerpt":"","text":"Java ByteBuffer 测试1. Java-HeapByteBuffer 的读/写操作时会自动构造一个 DirectByteBuffer 实例JVM 的 HeapByteBuffer 在进行读写操作时，JVM 会在堆外自动构造一个 DirectByteBuffer 实例（这里的意思是需要在堆外在开辟一个一样大小的内存区域），过程如下图所示： HeapByteBuffer(created by user) &lt;-automatically-&gt; DirectByteBuffer(created by JVM) &lt;-system call-&gt; pagecache &lt;-DMA-&gt; File on Disk 1.JVM 为何如此设计？ 这么设计的理由可以参考：https://www.zhihu.com/question/57374068/answer/152691891 2.测试向 为了测试 HeapByteBuffer 的这个性质，我们可以利用 Visual VM 工具来测试，我们需额外安装 VisualVM-BufferMonitor 插件。 测试代码如下： 123456789101112131415161718192021222324package cool.spongecaptain;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.FileChannel;import java.nio.file.FileSystems;import java.nio.file.Path;import java.nio.file.StandardOpenOption;import java.util.concurrent.CountDownLatch;public class TestHeapByteBufferWithWrite &#123; public static void main(String[] args) throws InterruptedException, IOException &#123; //allocate 1MB size HeapByteBuffer ByteBuffer heapByteBuffer = ByteBuffer.allocate(1024 * 1024); //make sure you have this file in the path Path path = FileSystems.getDefault().getPath(&quot;/Users/wjjiang/Desktop/temp.md&quot;); FileChannel fileChannel = FileChannel.open(path, StandardOpenOption.READ,StandardOpenOption.WRITE); //block on purpose System.in.read(); fileChannel.write(heapByteBuffer); //block on purpose new CountDownLatch(1).await(); &#125;&#125; 我们可以在 VisualVM-BufferMonitor 插件对应的页面中看到如下结果： 测试结果证明了这个说法。 2. DirectByteBuffer 会受到 GC 作用吗？虽然有“堆外内存的好处就在于不受 GC 影响”的说法，但在事实上，GC 能够回收堆外内存。 1.GC 如何管理堆外内存？ 当 JVM 发生 full GC 时，会找到指向堆外内存的 DirectByteBuffer 实例，然后进行回收。但是 GC 并不会自己去负责回收 DirectByteBuffer 实例，而是依赖于 DirectByteBuffer 的静态内部类 Deallocator，后者实现了一个 Runnable 接口，后者的执行逻辑是创建一个线程释放该 DirectByteBuffer 对象 malloc 申请的直接内存空间。 正是因为如此，虽然 GC 算法（除了 CMS 算法）之外在 GC 发生时都需要移动 DirectBuffer 对象，但由于回收 DirectByteBuffer 对象所指向的内存空间不是由 GC 线程完成的，所以保证了堆外内存地址的不变性。 注意区分堆内 DirectByteBuffer 与堆外 DirectByteBuffer 实例指向的内存地址空间： DirectByteBuffer 实例是由 GC 线程回收的，其地址会随着 GC 改变； DirectByteBuffer 实例指向的堆外地址由 Deallocator 线程回收，其地址不会随着回收改变。 在堆内的 DirectByteBuffer 实例指向堆外一个较大的内存区域，这种对象被称为冰山对象。 总之，堆外内存不是直接由 GC 线程负责回收，因此减少了 GC 线程的压力，但同时也受到了 GC 线程管理。 2.测试 为了证明堆外内存也受到 GC 的管理，写了如下的测试案例： 123456789101112131415161718192021package cool.spongecaptain;import java.io.IOException;import java.nio.ByteBuffer;import java.util.concurrent.CountDownLatch;public class TestDirectByteBufferWithGC &#123; public static void main(String[] args) throws InterruptedException, IOException &#123; //allocate 1MB size DirectByteBuffer ByteBuffer directBytebuffer = ByteBuffer.allocateDirect(1024 * 1024); System.in.read(); //help the GC to free directBytebuffer = null; System.gc(); //block on purpose new CountDownLatch(1).await(); &#125;&#125; 测试结果： 3. DirectByteBuffer 的主动内存释放1.DirectByteBuffer 不应依赖于 JVM 的 GC 进行回收 正如第二节所说，只有 JVM 进行 full GC 时，GC 线程才会触发 DirectByteBuffer 的“标记”，然后利用非 GC 线程去回收堆外内存。但在另一方面，DirectByteBuffer 实例在堆内所占内存空间非常小，因此 DirectByteBuffer 实例本身的构造难以导致 full GC。堆外内存使用再多，也无法触发 full GC。 如果 DirectByteBuffer 完全依赖于 JVM 的 GC 机制进行内存回收管理，那么很容易导致堆外内存分配过多，最终导致 OutOfMemoryError(&quot;Direct buffer memory”) 异常。 2.JVM 之上的 DirectByteBuffer 主动释放 DirectByteBuffer 的释放可以由用户代码主动触发。方法是利用 Cleaner#clean 方法。 3.测试 1234567891011121314151617181920package cool.spongecaptain;import sun.nio.ch.DirectBuffer;import java.io.IOException;import java.nio.ByteBuffer;import java.util.concurrent.CountDownLatch;public class TestDirectBufferCleaner &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; //allocate 1MB size DirectByteBuffer ByteBuffer directBytebuffer = ByteBuffer.allocateDirect(1024 * 1024); //block on purpose System.in.read(); //help the GC to free ((DirectBuffer)directBytebuffer).cleaner().clean(); //block on purpose new CountDownLatch(1).await(); &#125;&#125; 通过 VisualVM 工具观察结果如下： 4. MMAP 的内存大小可以远大于内存？我们通过 FileChannel#map 方法能够在 Java 中实现 MMAP 机制，我们可以发现，mmap 能够“申请”到远大于物理机内存大小的内存。 测试代码如下： 12345678910111213141516171819202122232425package cool.spongecaptain;import java.io.IOException;import java.nio.ByteBuffer;import java.nio.channels.FileChannel;import java.nio.file.FileSystems;import java.nio.file.Path;import java.nio.file.StandardOpenOption;import java.util.concurrent.CountDownLatch;public class TestMmapMomoryOccupy &#123; public static void main(String[] args) throws IOException, InterruptedException &#123; //allocate 1MB size HeapByteBuffer ByteBuffer heapByteBuffer = ByteBuffer.allocate(1024 * 1024); //make sure you have this file in the path Path path = FileSystems.getDefault().getPath(&quot;/Users/wjjiang/Desktop/temp.md&quot;); FileChannel fileChannel = FileChannel.open(path, StandardOpenOption.READ,StandardOpenOption.WRITE); //allocate 1000G size memory for (int i = 0; i &lt; 1000; i++) &#123; fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, 1024L * 1024 * 1024); &#125; //block on purpose new CountDownLatch(1).await(); &#125;&#125; 上述代码通过“申请”，映射得到了 1000G 大小的内存，但是我本机只有 16G 大小的内存，如何能够得到如此大的内存空间呢？ VisualVM 的监控结果如下图所示： 我们向操作系统询问一下该线程（pid 为 5950）一共占据了多大的内存空间，命令为 top -pid 5950。 可见，此 Java 线程仅仅占用了 106MB 大小的内存，远远不及 1000GB 大小。而且事实上，我的机器只有 16GB 的内存大小。 从测试结果中反映出如下两个事实： 内存映射并不等于内存占用，很多文章认为内存映射这种方式可以大幅度提升文件的读写速度，并宣称“写 MappedByteBuffer 就等于写内存”，实际是非常错误的认知。通过控制面板可以查看到该 Java 进程（pid 5950）实际占用的内存仅仅不到 100M。很多写操作会会触发页缺失异常，然后需要进行磁盘 I/O。 MappedByteBuffer 映射出一片文件内容之后，不会全部加载到内存中，而是会进行一部分的预读（体现在占用的那 100M 上），MappedByteBuffer 不是文件读写的银弹，它仍然依赖于 PageCache 异步刷盘的机制。 通过 Java VisualVM 可以监控到 mmap 总映射的大小，但并不是实际占用的物理内存大小。 REFERENCE 一文探讨堆外内存的监控与回收 Java NIO中，关于DirectBuffer，HeapBuffer的疑问？","categories":[],"tags":[]},{"title":"","slug":"7. Visual VM","date":"2023-02-04T02:34:09.229Z","updated":"2023-02-04T02:33:07.977Z","comments":true,"path":"2023/02/04/7. Visual VM/","link":"","permalink":"https://sk-xinye.github.io/2023/02/04/7.%20Visual%20VM/","excerpt":"","text":"JavaVisusal 工具1. 什么是 JavaVisusal 工具JavaVisusal 工具的介绍非常简洁直白，如下： VisualVM is a visual tool integrating commandline JDK tools and lightweight profiling capabilities. Designed for both development and production time use. 可见 JavaVisusal 工具的特性可以总结为：可视化、内置命令行 JDK 工具、轻量、同时适用于开发以及远程部署。 JDK8 中 JavaVisual 工具属于一款自带工具，而在 JDK11 中，JavaVisual 已经不自带，而需要我们从 https://visualvm.github.io/download.html 上自行下载，并进行安装。 2. 连接VisualVM 支持的连接远程节点有 2 种方式：jstatd 和 jmx。其中 jstatd 仅支持 monitor，jmx 才能支持 threads 和 sampler。 推荐使用 JMX 方式，不过，如果检测本地，那么完全不需要配置以及连接，使用 Local 节点即可。 3. 界面介绍VisualVM 一共分为四个页面，如下图所示： 其中 Local 下面显示了本地运行的 Java 应用，例如： VisualVM 软件自身就是一个 Java 应用； 坚果云也是一个 Java 应用； 我们以 VisualVM 应用为例，依次介绍这 4 个页面的作用。 3.1 Overview对当前应用的概述性信息，如下图所示： 应用有关信息：线程号、主机名称、入口类名称、运行参数 JVM 有关信息：JVM 版本号、$JAVA_HOME 参数、JVM Flags、JVM 参数、系统参数。 3.2 Monitor监视器界面如下图所示： 上图展示的信息有： CPU 栏：本机 CPU 使用率，JVM GC 活动频率； Heap/Metaspace 栏：这是受到 JVM 管理的内存空间信息，橙色代表总大小，蓝色代表实际使用的大小。肉眼可读的大小至少为 MB，我们可见粗略地去除后六位得到内存大小，例如上图中的 VisualVM 软件的最大内存大小为 104MB； Classes 栏：类的加载情况（统计单位为个数）； Threads 栏：线程的统计分为 Live 以及 Daemon，如果只有 Daemon 线程，那么意味着 JVM 会退出； 3.3 ThreadsThreads 界面如下图所示： 线程的排序方式是以名称的字母顺序，线程状态包括： Running：线程处于正常运行状态； Sleeping：线程休眠，通过调用 Thread#sleep 方法会进入此状态； Wait：线程等待一个条件的发生，例如通过在 synchronized 语句块中调用 Object#wait 方法可以进入此状态； Park：线程进入等待状态，例如通过调用 Unsafe#park 方法就会进入此状态。事实上，由于 JUC 包中的锁都是基于 Unsafe#park 方法实现，因此只要人为构造一个基于 ReentrantLock 的死锁状况，就能够使线程进入 Park 状态。 Monitor 状态：线程没有抢占到锁资源时会进入阻塞等待状态，此时线程的状态就是 Monitor 状态。 注意：Running 状态下的线程不一定都处于非阻塞状态，例如使用 Java 的 ServerSocket 会有阻塞问题，但是观察 JavaVisual 工具能够发现线程处于 Running 状态。 具体 Demo 可以参考个人项目：ThreadStateInJava 3.4 SamplerSampler 界面用于采样，如下图所示： 可以对 CPU 以及 Memory 进行采样，CPU 专注于对线程的执行时间的采样，Memory 专注于对内存中各种数据类型大小的采样。 3.5 其他插件除了 CPU、线程、堆、类等信息，还可以通过 Toos 安装插件，例如可以通过安装 VisualVM-BufferMonitor 插件来监控堆外内存（包含 DirectByteBuffer 和 MappedByteBuffer）。","categories":[],"tags":[]},{"title":"","slug":"6. FileChannel","date":"2023-02-04T02:34:09.225Z","updated":"2023-02-04T02:33:07.976Z","comments":true,"path":"2023/02/04/6. FileChannel/","link":"","permalink":"https://sk-xinye.github.io/2023/02/04/6.%20FileChannel/","excerpt":"","text":"FileChannel 学习1. FileChannel 的用途JDK 提供三种文件 I/O 方式，如下： 普通 I/O：存在于 java.io 包中的 FileWriter 与 FileReader 类； FileChannel（文件通道）：存在于 java.nio 包中的 FileChannel 类； MMAP（内存映射）：此方法较为特殊，由 FileChannel#map 方法衍生出的一种特殊的读写文件方式； 2. FileChannel 优势FileChannel 被 JavaDocs 称呼为：A channel for reading, writing, mapping, and manipulating a file，即 FileChannel 是用于读、写、映射、维护一个文件的通道。 本节有参考： Guide to Java FileChannel FileChannel 的优势如下： 可以在文件的特定位置进行读写操作； 可以直接将文件的一部分加载到内存中； 可以以更快的速度从一个通道传输文件数据到另一个通道； 可以锁定文件的一部分，以限制其他线程访问； 为了避免数据丢失，我们可以强制将对文件的写入更新立即写入存储； 总结起来有： FileInputStream/FileOutputStream FileChannel 单向 双向 面向字节的读写 面向 Buffer 读写 不支持 支持内存文件映射 不支持 支持转入或转出其他通道 不支持 支持文件锁 不支持操作文件元信息 不支持操作文件元信息 注意：虽然 FileChannel 类位于 java.nio 包下，FileChannel 只能运行在阻塞（blocking）模式下，而无法运行在非阻塞非模式（non-blocking）下。 3. FileChannel 的 API 本节参考于：https://www.cnblogs.com/lxyit/p/9170741.html FileChannel 类提供的重要 API 如下表所示： 方法 描述 open 创建 FileChannel read/write 基于 FileChannel 读写 force 强制将 FileChannel 中的数据刷入文件中 map 内存文件映射 transferTo 和 transferFrom 转入与转出通道 lock/tryLock 获取文件锁 3.1 得到一个 FileChannel 实例创建 FileChannel 实例的方式一共有三个： FileChanel#open 方法； RandomAccessFile#getChannel 方法； RandomAccessFile#getChannel 方法； 还可以设置文件的操作模式（OpenOption 操作符控制）： READ：只读方式； WRITE：只写方式； APPEND：只追加方式； CREATE：创建新文件； CREATE_NEW：创建新文件，如果存在则失败； TRUNCATE_EXISTING：如果以读方式访问文件，它的长度将被清除至 0； 示例1： 123//只读模式Path path = FileSystems.getDefault().getPath(&quot;/Users/wjjiang/Desktop/temp.md&quot;); FileChannel channel2 = FileChannel.open(path, StandardOpenOption.READ); 示例 2： 123456//通过 FileInputStream/FileOutputStream 得到FileInputStream inputStream = new FileInputStream(&quot;D:/test.txt&quot;);FileChannel channel = inputStream.getChannel();FileOutputStream outputStream = new FileOutputStream(&quot;D:/test.txt&quot;);FileChannel channel1 = outputStream.getChannel(); 示例 3： 123//通过 RandomAccessFile 得到，&quot;rw&quot; 代表可读可写RandomAccessFile randomAccessFile = new RandomAccessFile(&quot;./test.txt&quot;, &quot;rw&quot;);FileChannel channel2 = randomAccessFile.getChannel(); 3.2 读写通过调用 FileChannel#read/FileChannel#write 方法在文件上进行读写操作，示例如下： 12345678ByteBuffer byteBuffer = ByteBuffer.allocate(16);int count = channel2.read(byteBuffer);ByteBuffer byteBuffer = ByteBuffer.allocate(16);byte[] bs = &quot;s&quot;.getBytes();byteBuffer.put(bs);byteBuffer.flip();channel2.write(byteBuffer); 可见见得，FileChannel 与其他 java.nio 包下的类有一个最大的共同点，就是基于 ByteBuffer 类进行（而不是 byte[] 数组）。 在传统 I/O 中，流是基于字节的方式进行读写的。 在 NIO 中，使用通道（Channel）基于缓冲区数据块的读写。 注意：FileChannel 的 write 以及 read 是线程安全的，其内部有用到 synchronized 同步锁机制。 3.3 刷盘FileChannel#force 方法用于这个 Channel 更新的内容写入文件中， FileChannel #force 方法，接收一个布尔参数 metaData，表示是否需要确保文件元数据落盘，如果为 true，则调用 fsync。如果为 false，则调用 fdatasync。但无论如何，此方法都会确保有非元数据的文件内容落盘。 3.4 内存映射FileChannel#map 方法将外存文件某段映射至内存，返回 MappedByteBuffer，具有以下几种映射模式： READ_ONLY：以只读的方式映射，如果发生修改，则抛出 ReadOnlyBufferException； READ_WRITE：读写方式； PRIVATE：对这个 MappedByteBuffer 的修改不写入文件，且其他程序是不可见的； 注意：一旦经过 map 映射后，MappedByteBuffer 将与用于映射的 FileChannel 没有联系，即使 Channel 关闭，也对 MappedByteBuffer 没有影响。 map 通产应用在超大文件的处理中时使用，整体的性能才得以提升。对于数十 Kb 的文件处理，使用 map 的性能不一定比传统基于流式的读写好，因为直接映射进入内存的代价开销较大。需要在这两者之间进行权衡选择。 示例代码： 12345Path path = FileSystems.getDefault().getPath(&quot;./test.txt&quot;);FileChannel channel = FileChannel.open(path, StandardOpenOption.READ, StandardOpenOption.WRITE);MappedByteBuffer mappedByteBuffer = channel.map(MapMode.READ_ONLY, 0, 100000);byte[] bs = new byte[100];mappedByteBuffer.get(bs); 3.5 FileChannel 间数据传输FileChannel#transferTo 以及 FileChannel#transferFrom 用于文件通道的内容转出到另一个通道或者将另一个通道的内容转入当前通道。 案例： 12srcChannel.transferTo(0, Integer.MAX_VALUE, dstChannel);srcChannel.transferFrom(fromChannel, 0, Integer.MAX_VALUE); 该两个方法可以实现通道字节数据的快速转移，不仅在简化代码量（少了中间内存的数据拷贝转移）而且还大幅到提高了性能。 3.6 文件锁文件锁是作用在文件区域上的锁，即文件区域是同步资源，多个程序访问时，需要先获取该区域的锁，才能进入访问文件，访问结束释放锁，实现程序串行化访问文件。这里可以类比 Java 中的对象锁或者 lock 锁理解。 示例： 12FileChannel channel = rf.getChannel();FileLock lock = channel.lock(0L, 23L, false); 其中，false 意味着独占模式，true 则对应共享模式。 FileLock 的几个重要重点事项如下： 文件锁 FileLock 是被整个 Java Vitrual Machine 持有的，即 FileLock 是进程级别的，所以不可用于作为多线程安全控制的同步工具。 虽然上面提到 FileLock 不可用于多线程访问安全控制，但是多线程访问是安全的。如果线程 1 获取了文件锁 FileLock（共享或者独占），线程 2 再来请求获取该文件的文件锁，则会抛出 OverlappingFileLockException 一个程序获取到 FileLock 后，是否会阻止另一个程序访问相同文件具重叠内容的部分取决于操作系统的实现，具有不确定性。FileLock 的实现依赖于底层操作系统实现的本地文件锁设施。 以上所说的文件锁的作用域是文件的区域，可以时整个文件内容或者只是文件内容的一部分。独占和共享也是针对文件区域而言。程序（或者线程）获取文件 0 至 23 范围的锁，另一个程序（或者线程）仍然能获取文件 23 至以后的范围。只要作用的区域无重叠，都相互无影响。 4. FileChannel 的读写效率 本节参考于：https://www.cnkirito.moe/file-io-best-practise/ FileChannel 为什么比普通 I/O 要快呢？这么说可能不严谨，因为你要用对它，FileChannel 只有在一次写入 4kb 的整数倍时，才能发挥出实际的性能，这得益于 FileChannel 采用了 ByteBuffer 这样的内存缓冲区，让我们可以非常精准的控制写盘的大小，这是普通 I/O 无法实现的。4kb 一定快吗？也不严谨，这主要取决你机器的磁盘结构，并且受到操作系统，文件系统，CPU 的影响，例如中间件性能挑战赛时的那块盘，一次至少写入 64kb 才能发挥出最高的 IOPS。 中间件性能挑战复赛的盘 另外，正如 FileChannel API 中分别介绍了 FileChannel#write 以及 FileChannel#force 方法，因此ByteBuffer 中的数据和磁盘中的数据还隔了一层，这一层便是 PageCache，是用户内存和磁盘之间的一层缓存。我们都知道磁盘 IO 和内存 IO 的速度可是相差了好几个数量级。我们可以认为 filechannel.write 写入 PageCache 便是完成了落盘操作，但实际上，操作系统最终帮我们完成了 PageCache 到磁盘的最终写入。 同理，当我们使用 FileChannel 进行读操作时，同样经历了：磁盘 -&gt;PageCache-&gt; 用户内存这三个阶段，对于日常使用者而言，你可以忽略掉 PageCache，但作为挑战者参赛，PageCache 在调优过程中是万万不能忽视的，关于读操作这里不做过多的介绍，我们再下面的小结中还会再次提及，这里当做是引出 PageCache 的概念。 5. MMAP 的读写效率利用 FileChannel 进行 MMAP 读写示例如下图所示： 12345678910111213141516171819// 写byte[] data = new byte[4];int position = 8;// 从当前 mmap 指针的位置写入 4b 的数据mappedByteBuffer.put(data);// 指定 position 写入 4b 的数据MappedByteBuffer subBuffer = mappedByteBuffer.slice();subBuffer.position(position);subBuffer.put(data);// 读byte[] data = new byte[4];int position = 8;// 从当前 mmap 指针的位置读取 4b 的数据mappedByteBuffer.get(data);// 指定 position 读取 4b 的数据MappedByteBuffer subBuffer = mappedByteBuffer.slice();subBuffer.position(position);subBuffer.get(data); 我们通过执行如下语句： 1fileChannel.map(FileChannel.MapMode.READ_WRITE, 0, 1L * 1024 * 1024 * 1024); 观察一下磁盘上的变化，会立刻获得一个 1.5G 的文件，但此时文件的内容全部是 0（字节 0）。这符合 MMAP 的中文描述：内存映射文件，我们之后对内存中 MappedByteBuffer 做的任何操作，都会被最终映射到文件之中。如下图所示： 网上很多文章都在说，MMAP 操作大文件性能比 FileChannel 高出一个数量级！然而，通过我比赛的认识，MMAP 并非是文件 IO 的银弹，它只有在一次写入很小量数据的场景下才能表现出比 FileChannel 稍微优异的性能。紧接着我还要告诉你一些令你沮丧的事，至少在 JAVA 中使用 MappedByteBuffer 是一件非常麻烦并且痛苦的事，主要表现为三点： MMAP 使用时必须实现指定好内存映射的大小，并且一次 map 的大小限制在 1.5G 左右，重复 map 又会带来虚拟内存的回收、重新分配的问题，对于文件不确定大小的情形实在是太不友好了。 MMAP 使用的是虚拟内存，和 PageCache 一样是由操作系统来控制刷盘的，虽然可以通过 force() 来手动控制，但这个时间把握不好，在小内存场景下会很令人头疼。 MMAP 的回收问题，当 MappedByteBuffer 不再需要时，可以手动释放占用的虚拟内存，但…方式非常的诡异。 下面这么长的代码仅仅是为了干回收 MappedByteBuffer 这一件事。 123456789101112131415161718192021222324252627282930313233343536373839404142434445public static void clean(MappedByteBuffer mappedByteBuffer) &#123; ByteBuffer buffer = mappedByteBuffer; if (buffer == null || !buffer.isDirect() || buffer.capacity()== 0) return; invoke(invoke(viewed(buffer), &quot;cleaner&quot;), &quot;clean&quot;);&#125;private static Object invoke(final Object target, final String methodName, final Class&lt;?&gt;... args) &#123; return AccessController.doPrivileged(new PrivilegedAction&lt;Object&gt;() &#123; public Object run() &#123; try &#123; Method method = method(target, methodName, args); method.setAccessible(true); return method.invoke(target); &#125; catch (Exception e) &#123; throw new IllegalStateException(e); &#125; &#125; &#125;);&#125;private static Method method(Object target, String methodName, Class&lt;?&gt;[] args) throws NoSuchMethodException &#123; try &#123; return target.getClass().getMethod(methodName, args); &#125; catch (NoSuchMethodException e) &#123; return target.getClass().getDeclaredMethod(methodName, args); &#125;&#125;private static ByteBuffer viewed(ByteBuffer buffer) &#123; String methodName = &quot;viewedBuffer&quot;; Method[] methods = buffer.getClass().getMethods(); for (int i = 0; i &lt; methods.length; i++) &#123; if (methods[i].getName().equals(&quot;attachment&quot;)) &#123; methodName = &quot;attachment&quot;; break; &#125; &#125; ByteBuffer viewedBuffer = (ByteBuffer) invoke(buffer, methodName); if (viewedBuffer == null) return buffer; else return viewed(viewedBuffer);&#125; 所以建议优先使用 FileChannel 去完成初始代码的提交，在必须使用小数据量 (例如几个字节) 刷盘的场景下，再换成 MMAP 的实现，其他场景 FileChannel 完全可以 cover(前提是你理解怎么合理使用 FileChannel)。至于 MMAP 为什么在一次写入少量数据的场景下表现的比 FileChannel 优异，我还没有查到理论根据，如果你有相关的线索，欢迎留言。理论分析下，FileChannel 同样是写入内存，但是在写入小数据量时，MMAP 表现的更加优秀，所以在索引数据落盘时，大多数情况应该选择使用 MMAP。至于 MMAP 分配的虚拟内存是否就是真正的 PageCache 这一点，我觉得可以近似理解成 PageCache。","categories":[],"tags":[]},{"title":"","slug":"5. Java ByteBuffer与 Channel","date":"2023-02-04T02:34:09.220Z","updated":"2023-02-04T02:33:07.976Z","comments":true,"path":"2023/02/04/5. Java ByteBuffer与 Channel/","link":"","permalink":"https://sk-xinye.github.io/2023/02/04/5.%20Java%20ByteBuffer%E4%B8%8E%20Channel/","excerpt":"","text":"Java ByteBuffer与 Chanel引言Java NIO（Socket）由以下几个核心部分组成： Buffer Channel Selector Selector 重在实现 Socket 的事件驱动模型，是实现 NIO 的关键。而 Buffer 以及 Channel 则提供了一种面向缓冲区的尽力读写功能。 传统的 I/O 操作面向数据流，意味着每次从流中读一个或多个字节，直至完成，数据没有被缓存在任何地方。 NIO 操作面向缓冲区，数据从 Channel 读取到 Buffer 缓冲区，随后在 Buffer 中处理数据。 本节会结合 Netty 框架进行说明，因为其是一个出色的高性能 Java NIO 通信框架。 1. Buffer1.1 Buffer 概述 A buffer is a linear, finite sequence of elements of a specific primitive type. 一块缓存区，内部使用字节数组存储数据，并维护几个特殊变量，实现数据的反复利用。 下面虽然会介绍 Buffer 内部字段，但不用过于纠结 Buffer 内部的字段的含义，重要的是了解 Buffer 的特性： 同一个 Buffer 既可读又可写； Buffer 的读模式、写模式之间的相互转换需要调用具体方法，读模式式下写会报错，写模式写读也会报错。 当我们需要与 NIO Channel 进行交互时，我们就需要使用到 NIO Buffer，即数据从 Buffer 读取到 Channel 中，或者从 Channel 中写入到 Buffer 中。 实际上，一个 Buffer 其实就是一块内存区域，我们可以在这个内存区域中进行数据的读写。NIO Buffer 其实是这样的内存块的一个封装，并提供了一些操作方法让我们能够方便地进行数据的读写。 mark：初始值为 - 1，用于备份当前的 position; position：初始值为 0，position 表示当前可以写入或读取数据的位置，当写入或读取一个数据后，position 向前移动到下一个位置；因为仅仅支持顺序读写，所以当 Buffer 的读写模式切换时，position 会切换到可读的初始索引处以及可写的初始索引处。 limit：写模式下，limit 表示最多能往 Buffer 里写多少数据，等于 capacity 值；读模式下，limit 表示最多可以读取多少数据。 capacity：缓存数组大小。 **mark()**：把当前的 position 赋值给 mark 1234public final Buffer mark() &#123; mark = position; return this;&#125; **reset()**：把 mark 值还原给 position 1234567public final Buffer reset() &#123; int m = mark; if (m &lt; 0) throw new InvalidMarkException(); position = m; return this;&#125; **clear()**：一旦读完 Buffer 中的数据，需要让 Buffer 准备好再次被写入，clear 会恢复状态值，但不会擦除数据。 123456public final Buffer clear() &#123; position = 0; limit = capacity; mark = -1; return this;&#125; **flip()**：Buffer 有两种模式，写模式和读模式，flip 后 Buffer 从写模式变成读模式，或者相反。 123456public final Buffer flip() &#123; limit = position; position = 0; mark = -1; return this;&#125; **rewind()**：重置 position 为 0，从头读写数据。 12345public final Buffer rewind() &#123; position = 0; mark = -1; return this;&#125; 目前 Buffer 的实现类包括：ByteBuffer、CharBuffer、DoubleBuffer、FloatBuffer、IntBuffer、LongBuffer、ShortBuffer、MappedByteBuffer，继承关系如下图所示： 1.2 ByteBuffer 以及 HeapByteBuffer、DirectByteBuffer 它们本质上都是字节数组，只是一个在堆内，一个堆外（直接内存）。 ByteBuffer 的实现类包括 “HeapByteBuffer” 和 “DirectByteBuffer” 两种。 我们可以调用 ByteBuffer 类的两个静态方法得到上述两个子类实现： HeapByteBuffer#allocate 这个实例的构造实际上最终调用的是 Buffer 类的构造器，也就是说内存空间直接上分配在由 JVM 管理的堆空间中，其内部实际上为一个 byte 数组。 DirectByteBuffer#allocateDirect 这个实例通过 unsafe#allocateMemory 方法申请堆外内存，并在 ByteBuffer 的 address 变量中维护指向该内存的地址。 可见，这两种 Buffer 的最大区别于数据在内存何处，是位于 JVM 堆内还是堆外，Java 的 NIO 模式同时支持这两种 Buffer。 注意：Java 中的 Direct 并不是指操作系统的 Direct I/O，不过他们的作用是类似的。 1.3 为什么通常和 NIO 搭配的是 DirectByteBuffer 参考网址：知乎：Java NIO direct buffer 的优势在哪儿？ 其它一些平台可能是在错误地把一些概念整混了。 这个问题也是个伪命题，因为 NIO 和 HeapByteBuffer 以及 DirectByteBuffer 可以随意搭配，没有任何关系。NIO 对比于 BIO 的性能提升并不在于可以使用 DirectByteBuffer（这仅仅是一个影响非常小的优势）。更大的优势在于凭借单线程就能够处理在 BIO 中多线程才能够处理好的众多网络连接。 HeapByteBuffer： Socket&lt;—-&gt; 内核空间的缓存区 &lt;—-&gt; 堆外 &lt;—-&gt;堆内。 DirectByteBuffer： Socket &lt;—-&gt; 内核空间的缓存区 &lt;—-&gt; 堆外。 堆内的数据结构一般称为 Java Heap，而堆外的一般称为 C Heap，它们都处于用户空间。 Socket/文件到内核空间的缓冲区的数据拷贝现在一般由 DMA 机制负责完成，CPU 并不需要参与这个过程。CPU 可能参与的过程是内核空间到堆外（这里的堆外对于非 JVM 应用而言，就是普通用户空间），堆外到堆内。 结论：DirectByteBuffer 相对于 HeapByteBuffer 的优势仅仅在于少了一次堆内与堆外的数据拷贝过程。 为什么 HeapByteBuffer 也需要使用堆外的内存空间？ 因为 JVM 对内存中的实例实行 GC 管理，GC 可能会导致实例地址的变动，堆外内存的好处就在于其地址不受 GC 影响。地址被要求不能够改变的原因是：当我们把一个地址通过 JNI 传递给底层的 C 库的时候，有一个基本的要求，就是这个地址上的内容不能失效。所以无论我们是否使用 DirectByteBuffer，都需要使用一个堆外内存来保存和内核空间交互的数据。 如果要把一个 Java 里的 byte[] 对象的引用传给 native 代码，让 native 代码直接访问数组的内容的话，就必须要保证 native 代码在访问的时候这个 byte[] 对象不能被移动，也就是要被“pin”（钉）住。 可惜 HotSpot VM 出于一些取舍而决定不实现单个对象层面的 object pinning，要 pin 的话就得暂时禁用 GC——也就等于把整个 Java 堆都给 pin 住。 所以 Oracle/Sun JDK / OpenJDK 的这个地方就用了点绕弯的做法。它假设把 HeapByteBuffer 背后的 byte[] 里的内容拷贝一次是一个时间开销可以接受的操作，同时假设真正的 I/O 可能是一个很慢的操作。 因此，DirectByteBuffer 相对于 HeapByteBuffer 的优势是很有限的： 两种方式都有内核空间与用户空间之间的数据拷贝，只是 HeapByteBuffer 额外多一次堆内、堆外数据的拷贝； DirectByteBuffer 本身也是一个内存隐患，使用 DirectByteBuffer 并不能像 HeapByteBuffer 或 byte[] 一样任意使用可以被 GC 及时的回收。所以使用 DirectByteBuffer 最好是分配好缓存起来重复使用，否则很容易出现 OOM 错误（内存事实上管理非常复杂）。 描述 DirectByteBuffer 的精辟总结：DirectByteBuffer 只是给了用户(Java 程序员)一个操作堆外内存的机会，并不代表 JVM 没有堆外内存的管理（对于 JVM 来说堆外内一直是搓手可得的，因为本来就都在用户空间内，堆外内存并不是随着 JDK 推出 DirectByteBuffer 才可以进行管理的）。 1.4 使用 Buffer 的典型案例这里举一个使用 Buffer 的典型案例。 Buffer 由 ByteBuffer.allocate(100); 定义，所以是一个 Java 堆内的 HeapByteBuffer，当然也可以通过 DirectBuffer.allocateDirect() 来构造一个 DirectByteBuffer。 一个完成的 NIO 流程是这样的： 将 SocketChannel(与本地的一个 Socket 绑定的) 注册到 Selector 中，注册时的兴趣为可读事件。 当 Selector.slect() 方法返回时，预示着事件发生了，并且此方法停滞阻塞。 然后服务器线程去遍历 Socket 队列（epoll/select 遍历的范围有所区别），寻找可读的 Socket； 找到之后，服务器利用已经创建好的 （Buffer 一般要求是复用的，不要每个事件就创建一个）Buffer 进行非阻塞读取； 然后遍历 Socket 队列的下一个 Socket。 Socket 队列遍历完毕，继续调用 Selector.slect() 方法。 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122public class SimpleNioServer &#123; //这个 map 用于存储分批尽力读取的字节数据 final static HashMap&lt;SelectionKey, Object&gt; hashMap = new HashMap(); public static void main(String[] args) &#123; Selector selector = null; ServerSocketChannel serverSocket = null; ByteBuffer buffer = null; try &#123; //1. 创建 Selector 实例 selector = Selector.open(); //2. 创建 ServerSocketChannel 实例 serverSocket = ServerSocketChannel.open(); //3. 初始化 ServerSocketChannel 内部的 serverSocket 实例确定绑定的本地端口 serverSocket.bind(new InetSocketAddress(&quot;localhost&quot;, 2333)); //4. 将 ServerSocketChannel 配置成非阻塞模式，这是必要的， // 因为 ServerSocketChannel 和由其生产出来的 SocketChannel 实例都在 while 循环中被检查是否触发了事件， // 否则，没有新的请求导致 SocketChannel.accept() 阻塞，会影响 selector 去判别 SocketChannel 是否可读。 serverSocket.configureBlocking(false); //5. 将 ServerSocketChannel 注册到 Selector 中，事件是 OP_ACCEPT，一旦有 TCP 连接请求就会触发 serverSocket.register(selector, SelectionKey.OP_ACCEPT); // 测试性地尝试注册两次 serverSocket.register(selector, SelectionKey.OP_ACCEPT); //6. 创建 Buffer 用于从 SocketChannel 中读取字节数据 buffer = ByteBuffer.allocate(100); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; while (true) &#123; // 7. 如果迭代器内部有事件发生，那么不阻塞，否则阻塞 try &#123; selector.select(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; // 8. 阻塞结束，说明 selector 中有事件触发，所以获得其迭代器进行处理 Set&lt;SelectionKey&gt; selectedKeys = selector.selectedKeys(); Iterator&lt;SelectionKey&gt; iter = selectedKeys.iterator(); //9. 迭代器中全体元素的循环 while (iter.hasNext()) &#123; SelectionKey key = iter.next(); //10. 因为仅仅对 ServerSocketChannel 单例进行了 OP_ACCEPT 事件注册，所以断定地知道来了一个建立 TCP 连接请求。 //调用此 register 方法得到 ServerSocketChannel 中的一个 SocketChannel 并注册到 Selector 中，事件为 OP_READ if (key.isAcceptable()) &#123; try &#123; //这里的含义是将 ServerSocketChannel 实例 serverSocket 注册到 selector 内部 register(selector, serverSocket); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; //11. 我们为所有的 SocketChannel 注册了 OP_READ 事件，所以此事件发生时意味着可读了,于是调用 answerWithEcho 方法进行读 if (key.isReadable()) &#123; try &#123; read(buffer, key); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; &#125; //12. 将此 SelectableChannel 移出迭代器是必要的，否则会进行没有必要的事件是否准备好的询问 iter.remove(); &#125; &#125; &#125; private static void read(ByteBuffer buffer, SelectionKey key) throws IOException &#123; System.out.println(&quot;read 方法被调用了&quot;); //1. 得到 SelectionKey 实例内部的 SocketChannel 实例，因为可读状态下需要利用 Channel 进行读取字节数据 SocketChannel socketChannel = (SocketChannel) key.channel(); //2. 构造一组键值对，key 为 SelectionKey 实例，value 为 List，用于存储多次尽力读取的字节数组 if (!hashMap.containsKey(key)) hashMap.put(key, new ArrayList&lt;&gt;()); //3. 此标志用于判断： //0： UNIX 底层的缓冲字节数组被读完了或者 ByteBuffer 没有写一个字节，这里指的是前者，因为每次读取后都 clear 了； //-1: 意味这 EOF，即 HTTP 请求数据已经全部传输到服务端 Socket 了。 //其他大于 0 的数字：意味着这里从底层 UNIX 缓冲字节数组读取了几个字节 int count = 0; while (buffer.hasRemaining() &amp;&amp; (count = socketChannel.read(buffer)) &gt; 0) &#123; //使 Buffer 变成可读 buffer.flip(); ArrayList list = (ArrayList) hashMap.get(key); byte[] arr = new byte[buffer.remaining()]; buffer.get(arr); buffer.rewind(); list.add(arr); buffer.clear(); &#125; //当 socketChannel.read(buffer) 返回 -1 时，意味着此时彼通道数据传输已经完成，因为遇到了流传输中的 EOF 标志 // 如果没有读到字节流末尾，那么选择不关闭，因为下一次还是要继续读取 if (count == -1) &#123; //这些代码用于验证一次次的 Buffer 工作是否正确地转换为 byte 数组保存起来。 ArrayList list = (ArrayList) hashMap.get(key); final Iterator iterator = list.iterator(); while (iterator.hasNext()) &#123; final byte[] next = (byte[]) iterator.next(); System.out.println(next.length); &#125; socketChannel.close(); &#125; &#125; private static void register(Selector selector, ServerSocketChannel serverSocket) throws IOException &#123; //1. 利用 ServerSocketChannel accept 方法能够得到一个此次连接请求对应的 SocketChannel 实例 SocketChannel client = serverSocket.accept(); //2. 将此 SocketChannel 实例设置为非阻塞模式 client.configureBlocking(false); //3. 将此 SocketChannel 注册到 Selector 中，事件为 OP_READ，即可读事件，此方法返回的 SelectionKey 并不需要保存并引用起来 client.register(selector, SelectionKey.OP_READ); &#125;&#125; 1.5 Benchmark of DirectBuffer正如前文所述，DirectByteBuffer 与 HeapDirectBuffer 的性能差距不大，实际上应用层的 Buffer 缓存管理分配机制（内存池化）才是影响性能的最重要原因。当然，上述说法需要在具体应用场景下的 benchmark 来证明。 使用 HeapByteBuffer 的应用可以选择全权把内存管理交给 GC 线程，虽然使用应用层的 Buffer 缓存管理分配机制能得到更好的性能。 但是使用 DirectByteBuffer 的应用必须使用基于应用层的 Buffer 缓存管理分配机制，这是因为 DirectByteBuffer 在 GC 上存在问题：通常申请再多的 DirectByteBuffer 实例也难以触发 full GC，但是只有触发 full GC，GC 线程才会负责去触发 DirectByteBuffer 实例的回收。因此，使用 DirectByteBuffer 的应用通常需要在应用层实现一个 Buffer 缓存管理分配机制。 Netty 为 DirectByteBuffer 与 HeapDirectBuffer 都实现了 Buffer 缓存管理分配机制（内存池化）。 总之，事实上，使用 DirectByteBuffer 虽然可以提升性能，但实际上是出于不提升白不提升的目的，提升性能远远没有想象中那么大，因为其仅仅节约了堆外、堆内的数据复制操作，而这部分工作由 CPU 来完成是非常快的。 下图是 Twitter 对 Netty 的测试结果： 数值越大代表时间越长，性能越差。 Polled Heap 和 Polled Direct 相比几乎没有太大的区别； Unpolled Heap 和 unpolled Direct 相比区别略为大了一点，但是不够显著。 由图可知，DirectByteBuffer 相较于 HeapByteBuffer 可以提升一定性能，但是远远没有达到 Poll 相比于 Unpoll 的提升。 1.6 堆外内存的回收这里先回顾一下 JVM 堆内的内存回收的相关概念（粗略）： 新生代：一般来说新创建的对象都分配在这里。 年老代：经多次垃圾回收仍存活，新生代的对象就会放在年老代中。年老代中的对象保存的 GC 轮次更多。 永久代：这里面存放的是 class 相关的信息，一般是不会进行垃圾回收的。 JVM 的垃圾回收策略： Minor GC: 当新创建对象，内存空间不够的时候，就会执行这个垃圾回收。由于执行最频繁，因此一般采用标记复制机制。 Major GC: 清理年老代的内存，这里一般采用的是标记整理机制。 Full GC: 有的说与 Major GC 差不多，有的说相当于执行 minor+major 回收，但在这里我们暂且可以认为 Full GC 就是全面的垃圾回收吧。 需要我们回收内存的原因有 3 个： 内存空间有限：如果应用程序不断申请内存空间但是不回收，那么内存很快就会分配完毕，最终导致内存异常异常，包括： java.lang.OutOfMemoryError: Direct buffer memory java.lang.OutOfMemoryError: Requested array size exceeds VM limit 内存池化技术性能高：内存池化技术能够实现内存复用，因为反复的内存的申请以及分配代价高昂，池化技术能够提高整体系统性能； 堆外内存需要应用层自己实现一套内存回收机制，GC 对堆外内存的回收能力非常有限； 1.DirectByteBuffer 的内存回收 由前面的文章可知，堆外内存分配很简单，直接调用 ByteBuffer#allocateDirect 方法即可。 在 C 语言的内存分配和释放函数 malloc/free，必须要一一对应，否则就会出现内存泄露或者是野指针的非法访问。但是 JVM 拥有 GC 机制，通常并不需要应用层负责实例的内存释放。 DirectByteBuffer 的的内存回收非常受限，因为其所占内存只有发生 full GC 的情况下才会被回收（而且不是 GC 线程负责回收）。而 full GC 的触发是无法通过 Java 代码控制的，例如 System#gc 方法的语义无法确保 JVM 马上执行 GC 线程，它只是建议 JVM 进行垃圾回收。因此，很容易发送堆外内存溢出的问题。 DirectByteBuffer 的另一个缺陷是即使 GC 线程能够确保不发生堆外内存溢出，但是反复的堆外内存回收、分配会导致不尽人意的性能。 因此就像 Netty 一样，需要实现一套应用层 DirectByteBuffer 的池化技术，既负责内存回收，也负责内存缓存与分配。 补充说明： Netty 的堆外内存分配是直接基于 Unsafe 进行，而不是基于 DirectByteBuffer#allocate 方法。所以 GC 线程完全不会管理 Netty 框架中堆外内存的分配，后者全权由 Netty 应用层逻辑负责回收。 2.源码分析 先来看一个 jdk nio 的 DirectByteBuffer 类的 package-private 构造器，如下： 12345678910111213141516171819202122232425262728293031DirectByteBuffer(int cap) &#123; super(-1, 0, cap, cap); //是否页对齐 boolean pa = VM.isDirectMemoryPageAligned(); //页的大小4K int ps = Bits.pageSize(); //最小申请1K，若需要页对齐，那么多申请1页，以应对初始地址的页对齐问题 long size = Math.max(1L, (long)cap + (pa ? ps : 0)); //检查堆外内存是否够用, 并对分配的直接内存做一个记录 Bits.reserveMemory(size, cap); long base = 0; try &#123; //直接内存的初始地址, 返回初始地址 base = unsafe.allocateMemory(size); &#125; catch (OutOfMemoryError x) &#123; Bits.unreserveMemory(size, cap); throw x; &#125; //对直接内存初始化 unsafe.setMemory(base, size, (byte) 0); //若需要页对齐，并且不是页的整数倍，在需要将页对齐（默认是不需要进行页对齐的） if (pa &amp;&amp; (base % ps != 0)) &#123; // Round up to page boundary //初始地址取整页，注意申请的地址为取整数页 address = base + ps - (base &amp; (ps - 1)); &#125; else &#123; address = base; &#125; //声明一个Cleaner对象用于清理该DirectBuffer内存 cleaner = Cleaner.create(this, new Deallocator(base, size, cap)); att = null;&#125; 可以看到，DirectByteBuffer 通过直接调用 base=unsafe.allocateMemory (size) 操作堆外内存，返回的是该堆外内存的直接地址，存放在 address 中，以便通过 address 进行堆外数据的读取与写入。 我们需要了解下，Bits.reserveMemory() 如何判断堆外内存是否可用的： 12345678910111213141516171819202122232425262728293031323334353637static void reserveMemory(long size, int cap) &#123; //对分配的直接内存做一个记录 synchronized (Bits.class) &#123; if (!memoryLimitSet &amp;&amp; VM.isBooted()) &#123; //堆外直接内存默认等于堆内内存大小, 可以通过 maxMemory = VM.maxDirectMemory(); memoryLimitSet = true; &#125; // -XX:MaxDirectMemorySize limits the total capacity rather than the // actual memory usage, which will differ when buffers are page // aligned. //如果够分的话，则直接退出 if (cap &lt;= maxMemory - totalCapacity) &#123; reservedMemory += size; totalCapacity += cap; // count++; return; &#125; &#125; //不够分的话，则调用System.gc()进行一次full gc. 一般不要在线程启动时添加-XX:+DisableExplicitGC（禁止代码显示调用gc） System.gc(); //只是告知机器，这里应该GC一次， 但是实际并不一定进行垃圾回收 try &#123; //再等待100ms使gc有时间完成，然后再看是否够分配 Thread.sleep(100); &#125; catch (InterruptedException x) &#123; // Restore interrupt status Thread.currentThread().interrupt(); &#125; synchronized (Bits.class) &#123; //此时不够分的话，再调用向外抛出oom if (totalCapacity + cap &gt; maxMemory) throw new OutOfMemoryError(&quot;Direct buffer memory&quot;); reservedMemory += size; totalCapacity += cap; count++; &#125; &#125; 首先检查堆外内存是否够分 若不够分的话，再进行一次 full gc 显式推动对堆外内存的回收，再次尝试分配堆外内存，不够分的话，则抛出 OOM 异常。 堆外内存的回收-JDK-nio 的 DirectByteBuffer 在 DirectByteBuffer 的构造函数中，我们可以看到这样的一行代码 cleaner = Cleaner.create(this, new Deallocator(base, size, cap));, 没错，直接内存释放主要由 cleaner 来完成。我们知道 JVM GC 并不能直接释放直接内存，但是 GC 可以释放管理直接内存的 DirectByteBuffer 对象。我们需要注意下 cleaner 的类型: 1public class Cleaner extends PhantomReference&lt;Object&gt; PhantomReference 并不会对对象的垃圾回收产生任何影响，当进行 gc 完成后，当发现某个对象只剩下虚引用后，会将该引用迁移至 Reference 类的 pending 队列进行回收。这里可以看到 DirectByteBuffer 被 Cleaner 引用着。Reference 操作回收代码如下: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970static private class Lock &#123; &#125;;private static Lock lock = new Lock();/* List of References waiting to be enqueued. The collector adds * References to this list, while the Reference-handler thread removes * them. This list is protected by the above lock object. The * list uses the discovered field to link its elements. *///当gc时，发现DirectByteBuffer除了PhantomReference对象引用,没有其他对象引用， 会把DirectByteBuffer放入其中，等待被回收private static Reference&lt;Object&gt; pending = null;/* High-priority thread to enqueue pending References */private static class ReferenceHandler extends Thread &#123; ReferenceHandler(ThreadGroup g, String name) &#123; super(g, name); &#125; public void run() &#123; for (;;) &#123; Reference&lt;Object&gt; r; synchronized (lock) &#123; if (pending != null) &#123; r = pending; pending = r.discovered; r.discovered = null; &#125; else &#123; // The waiting on the lock may cause an OOME because it may try to allocate // exception objects, so also catch OOME here to avoid silent exit of the // reference handler thread. // // Explicitly define the order of the two exceptions we catch here // when waiting for the lock. // // We do not want to try to potentially load the InterruptedException class // (which would be done if this was its first use, and InterruptedException // were checked first) in this situation. // // This may lead to the VM not ever trying to load the InterruptedException // class again. try &#123; try &#123; //如果没有的话，会一直等待唤醒 lock.wait(); &#125; catch (OutOfMemoryError x) &#123; &#125; &#125; catch (InterruptedException x) &#123; &#125; continue; &#125; &#125; // Fast path for cleaners if (r instanceof Cleaner) &#123; //从头开始进行clena()调用 ((Cleaner)r).clean(); continue; &#125; ReferenceQueue&lt;Object&gt; q = r.queue; if (q != ReferenceQueue.NULL) q.enqueue(r); &#125; &#125;&#125;static &#123; ThreadGroup tg = Thread.currentThread().getThreadGroup(); for (ThreadGroup tgn = tg; tgn != null; tg = tgn, tgn = tg.getParent()); Thread handler = new ReferenceHandler(tg, &quot;Reference Handler&quot;); /* If there were a special system-only priority greater than * MAX_PRIORITY, it would be used here */ handler.setPriority(Thread.MAX_PRIORITY); handler.setDaemon(true); handler.start();&#125; 可以看出来，JVM 会新建名为 Reference Handler 的线程，时刻回收被挂到 pending 上面的虚拟引用 (该线程在 JVM 启动时就会产生)。 当 DirectByteBuff 对象仅被 Cleaner 引用时，Cleaner 被放入 pending 队列，之后调用 Cleaner.clean() 方法： 123456789101112131415161718192021222324252627282930313233343536373839public void clean() &#123; //这里的clean(）会在Reference回收时显示调用 if (!remove(this)) return; try &#123; thunk.run(); &#125; catch (final Throwable x) &#123; AccessController.doPrivileged(new PrivilegedAction&lt;Void&gt;() &#123; public Void run() &#123; if (System.err != null) new Error(&quot;Cleaner terminated abnormally&quot;, x) .printStackTrace(); System.exit(1); return null; &#125;&#125;); &#125;&#125;//就是一个释放直接内存的线程private static class Deallocator implements Runnable&#123; private static Unsafe unsafe = Unsafe.getUnsafe(); private long address; private long size; private int capacity; private Deallocator(long address, long size, int capacity) &#123; assert (address != 0); this.address = address; this.size = size; this.capacity = capacity; &#125; public void run() &#123; if (address == 0) &#123; // Paranoia return; &#125; unsafe.freeMemory(address); //释放地址 address = 0; Bits.unreserveMemory(size, capacity); //修改统计 &#125;&#125; 也就是说最终是通过调用 Unsafe.freeMemory() 方法来释放直接内存的。 所以如果简化 JDK nio 中 DirectByteBuffer 的创建与回收，那么步骤为： 利用静态方法 DirectByteBuffer.allocateDirect() 进行堆外内存的分配，本质上是利用 unsafe.allocateMemory(size); 来申请堆外内存。 DirectByteBuffer 的构造过程中，在最后将自己作为引用传入 Cleaner 实例中。 当 GC 完成后，当发现某个对象只剩下虚引用后，会将该 Cleaner 实例迁移至 Reference 类的 pending 队列进行回收。 GC 本身不负责 DirectByteBuffer 的回收，其只是负责将将 DirectByteBuffer 的引用迁移至 Reference 类的 pending 队列进行回收。 这个队列始终在进行回收工作。 在 pending 队列中，会负责回调 cleaner 实例的 clean() 方法，clean() 方法中会调用 Deallocator 实例的 run() 方法，在这个方法中会负责将 cleaner 对应的堆外内存释放。 1unsafe.freeMemory(address); DirectByteBuffer 就是如此依靠着 JVM 的 GC 进行内存回收的。实际上，通过下面两个组合就能够实现堆外内存的分配和回收： unsafe.allocateMemory(size); unsafe.freeMemory(address); 因为 Netty 的堆外内存管理自己使用了一套机制：jemalloc，目的就是在应用层实现一套自己的内存管理（内存分配和内存回收），而不依赖 JVM 的 GC 机制。 本质上，Netty 的内存管理也是依赖于 Unsafe 的两个方法，见上。Netty 选择自己实现另一套（完全没有继承、包含关系）的 ByteBuf，最终的目的就是为了使用自己的应用层内存分配管理机制。 1.7 ByteBuffer 的 JVM 配置堆内、堆外的内存空间都是有限的，当内存申请超过启动时的最大内存配置就会报错： HeapByteBuffer 的内存限制主要受到 JVM 内存配置的影响，-Xms 参数用于控制初始堆大小，-Xmx 参数用于控制最大堆大小。由于堆内还需要存放其他实例，因此实际上 HeapByteBuffer 能够申请的内存大小小于 -Xms 配置； DirectByteBuffer 的内存限制受到 -XX:MaxDirectMemorySize 参数的影响； 当然，上述内存配置都无法超过物理机的虚拟内存大小。 另一方面，单个 HeapByteBuffer 与 DirectByteBuffer 的最大数据量都为 Integer.MAXVALUE 个 byte，也就是 2000 MB 左右的内存大小。如果要申请 4GB 的内存，那么至少需要两个 ByteBuffer 实例。 2. Channel A channel represents an open connection to an entity such as a hardware device, a file, a network socket, or a program component that is capable of performing one or more distinct I/O operations, for example reading or writing. Channel 类位于 java.nio.channels 包中，但并不是 Channel 仅仅支持 NIO，其分为两种类型： FileChannel：完全不支持 NIO； SocketChannel/ServerSocketChannel 等 Channel 默认情况下并不支持 NIO，只有显式地调用配置方法才能够进入非阻塞模式（ServerSocketChannel.configBlocking(false)）。 下面主要以 SocketChannel 的角度来介绍 Channel 类。 Channel 我们可以理解为对应于 BIO 中的 Socket，也可以理解为 Scoket.inputStream/SocketOutputStream。如果认为是流，那么我们做一个比较： 传统 Socket：我们调用 Socket 的 getInputStream() 以及 getOutputStream() 进行数据的读和写。 Channel：我们不再需要得到输入输出流进行读和写，而是通过 Channel 的 read() 以及 write() 方法进行读和写。 Channel 如此实现也付出了代价（如下图所示）： 读写模式需要调用 flip() 方法进行切换，读模式下调用 write() 试图进行写操作会报错。 读写不再能够接受一个简单的字节数组，而是必须是封装了字节数组的 Buffer 类型。 目前已知 Channel 的实现类有： FileChannel 一个用来写、读、映射和操作文件的通道。 DatagramChannel SocketChannel SocketChannel 可以看做是具有非阻塞模式的 Socket。其可以运行在阻塞模式，也可以运行在非阻塞模式。其只能依靠 ByteBuffer 进行读写，而且是尽力读写，尽力的含义是： ByteBuffer 满了就不能在读了； 即使此次 Socket 流没有传输完毕，但是一旦 Channel 中的数据读完了，那么就返回了，这就是非阻塞读。所以读的方法有 -1（EOF），0（Channel 中的数据读完了，但是整个数据流本身没有消耗完），其他整数，此次读的数据（因为 ByteBuffer 并不是每次都是空的，原来就有数据时只能够尽力装满）。 ServerSocketChannel 这个类似于 ServerSocket 起到的作用。 3. 为什么 Netty 要封装原有的 Buffer 以及 Channel 原生的 Buffer + Channel 的读写方式容易导致错误，比如写和读共用一个 index，经常容易犯的错误是写完在读的时候忘记 flip()。Netty 的有 readerindex 和 writerindex，用起来更方便了。 提供更高层次的封装，提供更为丰富的功能。比如 Netty 的 Buffer 可以让我们方便的将两个小 Buffer 合并为一个大 Buffer 而不需要进行 byte 的复制。 Netty 提供了更好的内存管理：JDK 的 DirectBytebuffer 虽然也使用堆外内存，但是依赖 JVM 进行 GC。但是 Netty 自己实现了 ByteBuf 的内存管理 jemalloc，性能更好，且完全不受到 GC 的管理。 REFERENCE 知乎：Java NIO direct buffer 的优势在哪儿？ Java NIO中，关于DirectBuffer，HeapBuffer的疑问？ Java堆外内存之三：堆外内存回收方法 Netty - One Framework to rule them all","categories":[],"tags":[]},{"title":"","slug":"4. 文件分区","date":"2023-02-04T02:34:09.216Z","updated":"2023-02-04T02:33:07.975Z","comments":true,"path":"2023/02/04/4. 文件分区/","link":"","permalink":"https://sk-xinye.github.io/2023/02/04/4.%20%E6%96%87%E4%BB%B6%E5%88%86%E5%8C%BA/","excerpt":"","text":"文件分区1. 文件分区是指什么？文件分区很少有资料介绍，但是在众多数据库组件的最佳实践中通常会以 benchmark 告诉我们这一个建议。 文件分区包括两件事： 将原本一个大文件分为多个小文件。例如，将一个大数据量的文件分为多个小文件进行存储。例如 Kafka 中为避免 WAL Log 过大，使用了 LogSegment 概念，当某一个 LogSegment 足够大时，就创建一个新的 LogSegment，用于后续的日志写入； 将用途不同的文件存储于不同的磁盘上。例如，MySQL 通常推荐将事务日志文件与数据文件分别存储于本机挂载的不同磁盘上。 下面我们分析一下文件分区的意义。 2. 文件分区的意义2.1 减少文件锁粒度，提高并发 I/O 潜力正如 如何实现顺序读写中指出，为了确保实现物理上真正的顺序 I/O，在涉及多线并发读写时，我们必须使用锁机制。这里的锁通常是文件锁，一个文件一把锁。锁可以由操作系统提供，但更常见的是在用户应用中额外使用一把锁。 依赖于锁机制的线程并发能力提高可以通过细化锁粒度实现。一个大文件分为多个小文件，意味着将一个大锁分为多个小锁，这样一来，锁粒度细化，线程冲突可能性降低，系统的整体 I/O 能力得到提高。 2.2 简化索引实现一般的日志存储系统有如下特点： 每一条日志所占数据长度不一致（你无法事先确认用户试图写入 “Hello World” 还是 “Hi Spongecaptain”）； 日志顺序存储，例如在磁盘上编号 101 的日志顺序存储于编号 100 的日志之后； 为了提高日志存储系统的查询效率，我们必然需要实现索引。 在不进行日志文件分区的情况下，即只有一个日志文件，那么如果要提供随机日志查询与范围查询的能力，那么索引系统必须为每一条日志设计一个 Tree 上的节点。但是如果日志非常多，那么索引 Tree 将拥有非常多的节点，存在查询效率降低的问题。 如果进行日志分区，大日志文件分为多个小日志文件进行存储，那么如果要提供随机日志查询与范围查询能力，那么我们可以为每一个小日志文件分别设计一个索引文件，由于每一个索引 Tree 的节点数不会特别多，因此我们总是能够确保较好的查询效率。 Kafka 的日志索引就基于此思想实现。 2.3 分磁盘存储文件提高磁盘 I/O 效率文件分区的另一个含义是将不同文件存储于不同磁盘上，不过前提自然是你拥有多个磁盘与需要多个文件。正如前文所属，MySQL 通常推荐将事务日志文件（WAL）与数据文件（B+Tree）分别存储于本机上挂载的不同磁盘上。 分磁盘存储文件的优势主要有如下： 实现并行磁盘 I/O； 不同文件的 I/O 模式特质不同（顺序 or 随机）； 更可靠的持久化保证； 1.并行磁盘 I/O 多个磁盘具备属于各自的驱动，因此可以进行并行的磁盘 I/O（你可以想象多个磁盘的磁头在并行移动），比单磁盘的操作系统在磁盘 I/O 具备更好的性能。 2.屏蔽不同磁盘 I/O 方式之间的干扰 另一方面，不同文件的 I/O 模式特性不同。例如，MySQL 中的事务日志文件主要进行顺序 I/O 读写，而数据文件（B+Tree）主要进行随机 I/O 读写。如果随机 I/O 与顺序 I/O 共用一块磁盘，那么随机 I/O 会影响顺序 I/O 的磁盘调度，导致磁盘吞吐量下降。 3.更可靠的持久化保证 如果数据本身所在的磁盘损坏了，但是你有在另一个磁盘上存储着 tail-of-the-log backup，那么重新执行一遍写日志那么就能够得到完全一样的数据。这种情况下，持久化机制更可靠了。例如假设一个磁盘一年内发生损坏的概率为 2%，那么持久化的可靠性可以通过双磁盘提高至 1-2%*2% =99.96%。 REFERENCE 文件 IO 操作的一些最佳实践 Is there a performance benefit to placing transaction log files on a separate drive? Does Separating Data and Log Files Make Your Server More Reliable?","categories":[],"tags":[]},{"title":"","slug":"3. mmap","date":"2023-02-04T02:34:09.211Z","updated":"2023-02-04T02:33:07.974Z","comments":true,"path":"2023/02/04/3. mmap/","link":"","permalink":"https://sk-xinye.github.io/2023/02/04/3.%20mmap/","excerpt":"","text":"MMAP1. mmap 基础概念mmap 即 memory map，也就是内存映射。 mmap 是一种内存映射文件的方法，即将一个文件或者其它对象映射到进程的地址空间，实现文件磁盘地址和进程虚拟地址空间中一段虚拟地址的一一对映关系。实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用 read、write 等系统调用函数。相反，内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。如下图所示： mmap 具有如下的特点： mmap 向应用程序提供的内存访问接口是内存地址连续的，但是对应的磁盘文件的 block 可以不是地址连续的； mmap 提供的内存空间是虚拟空间（虚拟内存），而不是物理空间（物理内存），因此完全可以分配远远大于物理内存大小的虚拟空间（例如 16G 内存主机分配 1000G 的 mmap 内存空间）； mmap 负责映射文件逻辑上一段连续的数据（物理上可以不连续存储）映射为连续内存，而这里的文件可以是磁盘文件、驱动假造出的文件（例如 DMA 技术）以及设备； mmap 由操作系统负责管理，对同一个文件地址的映射将被所有线程共享，操作系统确保线程安全以及线程可见性； mmap 的设计很有启发性。基于磁盘的读写单位是 block（一般大小为 4KB），而基于内存的读写单位是地址（虽然内存的管理与分配单位是 4KB）。换言之，CPU 进行一次磁盘读写操作涉及的数据量至少是 4KB，但是进行一次内存操作涉及的数据量是基于地址的，也就是通常的 64bit（64 位操作系统）。mmap 下进程可以采用指针的方式进行读写操作，这是值得注意的。 2. mmap 的 I/O 模型mmap 也是一种零拷贝技术，其 I/O 模型如下图所示： mmap 技术有如下特点： 利用 DMA 技术来取代 CPU 来在内存与其他组件之间的数据拷贝，例如从磁盘到内存，从内存到网卡； 用户空间的 mmap file 使用虚拟内存，实际上并不占据物理内存，只有在内核空间的 kernel buffer cache 才占据实际的物理内存； mmap() 函数需要配合 write() 系统调动进行配合操作，这与 sendfile() 函数有所不同，后者一次性代替了 read() 以及 write()；因此 mmap 也至少需要 4 次上下文切换； mmap 仅仅能够避免内核空间到用户空间的全程 CPU 负责的数据拷贝，但是内核空间内部还是需要全程 CPU 负责的数据拷贝； 利用 mmap() 替换 read()，配合 write() 调用的整个流程如下： 用户进程调用 mmap()，从用户态陷入内核态，将内核缓冲区映射到用户缓存区； DMA 控制器将数据从硬盘拷贝到内核缓冲区（可见其使用了 Page Cache 机制）； mmap() 返回，上下文从内核态切换回用户态； 用户进程调用 write()，尝试把文件数据写到内核里的套接字缓冲区，再次陷入内核态； CPU 将内核缓冲区中的数据拷贝到的套接字缓冲区； DMA 控制器将数据从套接字缓冲区拷贝到网卡完成数据传输； write() 返回，上下文从内核态切换回用户态。 3. mmap 的优势1.简化用户进程编程 在用户空间看来，通过 mmap 机制以后，磁盘上的文件仿佛直接就在内存中，把访问磁盘文件简化为按地址访问内存。这样一来，应用程序自然不需要使用文件系统的 write（写入）、read（读取）、fsync（同步）等系统调用，因为现在只要面向内存的虚拟空间进行开发。 但是，这并不意味着我们不再需要进行这些系统调用，而是说这些系统调用由操作系统在 mmap 机制的内部封装好了。 （1）基于缺页异常的懒加载 出于节约物理内存以及 mmap 方法快速返回的目的，mmap 映射采用懒加载机制。具体来说，通过 mmap 申请 1000G 内存可能仅仅占用了 100MB 的虚拟内存空间，甚至没有分配实际的物理内存空间。当你访问相关内存地址时，才会进行真正的 write、read 等系统调用。CPU 会通过陷入缺页异常的方式来将磁盘上的数据加载到物理内存中，此时才会发生真正的物理内存分配。 （2）数据一致性由 OS 确保 当发生数据修改时，内存出现脏页，与磁盘文件出现不一致。mmap 机制下由操作系统自动完成内存数据落盘（脏页回刷），用户进程通常并不需要手动管理数据落盘。 2.读写效率提高：避免内核空间到用户空间的数据拷贝 简而言之，mmap 被认为快的原因是因为建立了页到用户进程的虚地址空间映射，以读取文件为例，避免了页从内核空间拷贝到用户空间。 3.避免只读操作时的 swap 操作 虚拟内存带来了种种好处，但是一个最大的问题在于所有进程的虚拟内存大小总和可能大于物理内存总大小，因此当操作系统物理内存不够用时，就会把一部分内存 swap 到磁盘上。 在 mmap 下，如果虚拟空间没有发生写操作，那么由于通过 mmap 操作得到的内存数据完全可以通过再次调用 mmap 操作映射文件得到。但是，通过其他方式分配的内存，在没有发生写操作的情况下，操作系统并不知道如何简单地从现有文件中（除非其重新执行一遍应用程序，但是代价很大）恢复内存数据，因此必须将内存 swap 到磁盘上。 4.节约内存 由于用户空间与内核空间实际上共用同一份数据，因此在大文件场景下在实际物理内存占用上有优势。 4. mmap 不是银弹mmap 不是银弹，这意味着 mmap 也有其缺陷，在相关场景下的性能存在缺陷： 由于 mmap 使用时必须实现指定好内存映射的大小，因此 mmap 并不适合变长文件； 如果更新文件的操作很多，mmap 避免两态拷贝的优势就被摊还，最终还是落在了大量的脏页回写及由此引发的随机 I/O 上，所以在随机写很多的情况下，mmap 方式在效率上不一定会比带缓冲区的一般写快； 读/写小文件（例如 16K 以下的文件），mmap 与通过 read 系统调用相比有着更高的开销与延迟；同时 mmap 的刷盘由系统全权控制，但是在小数据量的情况下由应用本身手动控制更好； mmap 受限于操作系统内存大小：例如在 32-bits 的操作系统上，虚拟内存总大小也就 2GB，但由于 mmap 必须要在内存中找到一块连续的地址块，此时你就无法对 4GB 大小的文件完全进行 mmap，在这种情况下你必须分多块分别进行 mmap，但是此时地址内存地址已经不再连续，使用 mmap 的意义大打折扣，而且引入了额外的复杂性； 5. mmap 的适用场景mmap 的适用场景实际上非常受限，在如下场合下可以选择使用 mmap 机制： 多个线程以只读的方式同时访问一个文件，这是因为 mmap 机制下多线程共享了同一物理内存空间，因此节约了内存。案例：多个进程可能依赖于同一个动态链接库，利用 mmap 可以实现内存仅仅加载一份动态链接库，多个进程共享此动态链接库。 mmap 非常适合用于进程间通信，这是因为对同一文件对应的 mmap 分配的物理内存天然多线程共享，并可以依赖于操作系统的同步原语； mmap 虽然比 sendfile 等机制多了一次 CPU 全程参与的内存拷贝，但是用户空间与内核空间并不需要数据拷贝，因此在正确使用情况下并不比 sendfile 效率差； 6. 补充mmap 代码案例可参考此 issue：mmap 代码补充 REFERENCE 认真分析mmap：是什么 为什么 怎么用 Linux 中 mmap() 函数的内存映射问题理解？ When should I use mmap for file access? Linux I/O 原理和 Zero-copy 技术全面揭秘 - 知乎","categories":[],"tags":[]},{"title":"","slug":"2. DMA 与零拷贝技术","date":"2023-02-04T02:34:09.206Z","updated":"2023-02-04T02:33:07.973Z","comments":true,"path":"2023/02/04/2. DMA 与零拷贝技术/","link":"","permalink":"https://sk-xinye.github.io/2023/02/04/2.%20DMA%20%E4%B8%8E%E9%9B%B6%E6%8B%B7%E8%B4%9D%E6%8A%80%E6%9C%AF/","excerpt":"","text":"DMA 与零拷贝技术 注意事项：除了 Direct I/O，与磁盘相关的文件读写操作都有使用到 page cache 技术。 1. 数据的四次拷贝与四次上下文切换很多应用程序在面临客户端请求时，可以等价为进行如下的系统调用： File.read(file, buf, len); Socket.send(socket, buf, len); 例如消息中间件 Kafka 就是这个应用场景，从磁盘中读取一批消息后原封不动地写入网卡（NIC，Network interface controller）进行发送。 在没有任何优化技术使用的背景下，操作系统为此会进行 4 次数据拷贝，以及 4 次上下文切换，如下图所示： 如果没有优化，读取磁盘数据，再通过网卡传输的场景性能比较差： 4 次 copy： 物理设备 &lt;-&gt; 内存： CPU 负责将数据从磁盘搬运到内核空间的 Page Cache 中； CPU 负责将数据从内核空间的 Socket 缓冲区搬运到的网络中； 内存内部拷贝： CPU 负责将数据从内核空间的 Page Cache 搬运到用户空间的缓冲区； CPU 负责将数据从用户空间的缓冲区搬运到内核空间的 Socket 缓冲区中； 4 次上下文切换： read 系统调用时：用户态切换到内核态； read 系统调用完毕：内核态切换回用户态； write 系统调用时：用户态切换到内核态； write 系统调用完毕：内核态切换回用户态； 我们不免发出抱怨： CPU 全程负责内存内部的数据拷贝还可以接受，因为内存的数据拷贝效率还行（不过还是比 CPU 慢很多），但是如果要 CPU 全程负责内存与磁盘、内存与网卡的数据拷贝，这将难以接受，因为磁盘、网卡的 I/O 速度远小于内存； 4 次 copy 太多了，4 次上下文切换也太频繁了； 2. DMA 参与下的数据四次拷贝DMA 技术很容易理解，本质上，DMA 技术就是我们在主板上放一块独立的芯片。在进行内存和 I/O 设备的数据传输的时候，我们不再通过 CPU 来控制数据传输，而直接通过 DMA 控制器（DMA Controller，简称 DMAC）。这块芯片，我们可以认为它其实就是一个协处理器（Co-Processor）。 DMAC 的价值在如下情况中尤其明显：当我们要传输的数据特别大、速度特别快，或者传输的数据特别小、速度特别慢的时候。 比如说，我们用千兆网卡或者硬盘传输大量数据的时候，如果都用 CPU 来搬运的话，肯定忙不过来，所以可以选择 DMAC。而当数据传输很慢的时候，DMAC 可以等数据到齐了，再发送信号，给到 CPU 去处理，而不是让 CPU 在那里忙等待。 注意：这里面的“协”字。DMAC 是在“协助”CPU，完成对应的数据传输工作。在 DMAC 控制数据传输的过程中，DMAC 还是被 CPU 控制，只是数据的拷贝行为不再由 CPU 来完成。 原本，计算机所有组件之间的数据拷贝（流动）必须经过 CPU。以磁盘读写为例，如下图所示： 现在，DMAC 代替了 CPU 负责内存与磁盘、内存与网卡之间的数据搬运，CPU 作为 DMAC 的控制者，如下图所示： 但是 DMAC 有其局限性，DMAC 仅仅能用于设备间交换数据时进行数据拷贝，但是设备内部的数据拷贝还需要 CPU 来亲力亲为。例如， CPU 需要负责内核空间与用户空间之间的数据拷贝（内存内部的拷贝），如下图所示： 上图中的 read buffer 也就是 page cache，socket buffer 也就是 Socket 缓冲区。 3. 零拷贝技术3.1 什么是零拷贝技术？零拷贝技术是一个思想[3]，指的是指计算机执行操作时，CPU 不需要先将数据从某处内存复制到另一个特定区域。 可见，零拷贝的特点是 CPU 不全程负责内存中的数据写入其他组件，CPU 仅仅起到管理的作用。但注意，零拷贝不是不进行拷贝，而是 CPU 不再全程负责数据拷贝时的搬运工作。如果数据本身不在内存中，那么必须先通过某种方式拷贝到内存中（这个过程 CPU 可以仅仅负责管理，DMAC 来负责具体数据拷贝），因为数据只有在内存中，才能被转移，才能被 CPU 直接读取计算。 零拷贝技术的具体实现方式有很多，例如： sendfile mmap 直接 Direct I/O splice 不同的零拷贝技术适用于不同的应用场景，下面依次进行 sendfile、mmap、Direct I/O 的分析。 不过，我们不妨先在这里做一个前瞻性的技术总结。 DMA 技术：DMA 负责内存与其他组件之间的数据拷贝，CPU 仅需负责管理，而无需负责全程的数据拷贝； 使用 page cache 的 zero copy： sendfile：一次代替 read/write 系统调用，通过使用 DMA 技术以及传递文件描述符，实现了 zero copy mmap：仅代替 read 系统调用，将内核空间地址映射为用户空间地址，write 操作直接作用于内核空间。通过 DMA 技术以及地址映射技术，用户空间与内核空间无须数据拷贝，实现了 zero copy 不使用 page cache 的 Direct I/O：读写操作直接在磁盘上进行，不使用 page cache 机制，通常结合用户空间的用户缓存使用。通过 DMA 技术直接与磁盘/网卡进行数据交互，实现了 zero copy 3.2 sendfilesnedfile 的应用场景是：用户从磁盘读取一些文件数据后不需要经过任何计算与处理就通过网络传输出去。此场景的典型应用是消息队列。 在传统 I/O 下，正如第一节所示，上述应用场景的一次数据传输需要四次 CPU 全权负责的拷贝与四次上下文切换，正如本文第一节所述。 sendfile 主要使用到了两个技术： DMA 技术； 传递文件描述符代替数据拷贝； 下面依次讲解这两个技术的作用。 1.利用 DMA 技术 sendfile 依赖于 DMA 技术，将四次 CPU 全程负责的拷贝与四次上下文切换减少到两次，如下图所示： 利用 DMA 技术减少 2 次 CPU 全程参与的拷贝 DMA 负责磁盘到内核空间中的 Page cache（read buffer）的数据拷贝以及从内核空间中的 socket buffer 到网卡的数据拷贝。 2.传递文件描述符代替数据拷贝 传递文件描述可以代替数据拷贝，这是由于两个原因： page cache 以及 socket buffer 都在内核空间中； 数据在传输中没有被更新； 利用传递文件描述符代替内核中的数据拷贝 注意事项：只有网卡支持 SG-DMA（The Scatter-Gather Direct Memory Access）技术才可以通过传递文件描述符的方式避免内核空间内的一次 CPU 拷贝。这意味着此优化取决于 Linux 系统的物理网卡是否支持（Linux 在内核 2.4 版本里引入了 DMA 的 scatter/gather – 分散/收集功能，只要确保 Linux 版本高于 2.4 即可）。 3.一次系统调用代替两次系统调用 由于 sendfile 仅仅对应一次系统调用，而传统文件操作则需要使用 read 以及 write 两个系统调用。 正因为如此，sendfile 能够将用户态与内核态之间的上下文切换从 4 次讲到 2 次。 sendfile 系统调用仅仅需要两次上下文切换 另一方面，我们需要注意 sendfile 系统调用的局限性。如果应用程序需要对从磁盘读取的数据进行写操作，例如解密或加密，那么 sendfile 系统调用就完全没法用。这是因为用户线程根本就不能够通过 sendfile 系统调用得到传输的数据。 3.3 mmapmmap 技术在[4] 中单独展开，请移步阅读。 3.4 Direct I/ODirect I/O 即直接 I/O。其名字中的”直接”二字用于区分使用 page cache 机制的缓存 I/O。 缓存文件 I/O：用户空间要读写一个文件并不直接与磁盘交互，而是中间夹了一层缓存，即 page cache； 直接文件 I/O：用户空间读取的文件直接与磁盘交互，没有中间 page cache 层； “直接”在这里还有另一层语义：其他所有技术中，数据至少需要在内核空间存储一份，但是在 Direct I/O 技术中，数据直接存储在用户空间中，绕过了内核。 Direct I/O 模式如下图所示： Direct I/O 示意图 此时用户空间直接通过 DMA 的方式与磁盘以及网卡进行数据拷贝。 Direct I/O 的读写非常有特点： Write 操作：由于其不使用 page cache，所以其进行写文件，如果返回成功，数据就真的落盘了（不考虑磁盘自带的缓存）； Read 操作：由于其不使用 page cache，每次读操作是真的从磁盘中读取，不会从文件系统的缓存中读取。 事实上，即使 Direct I/O 还是可能需要使用操作系统的 fsync 系统调用。为什么？ 这是因为虽然文件的数据本身没有使用任何缓存，但是文件的元数据仍然需要缓存，包括 VFS 中的 inode cache 和 dentry cache 等。 在部分操作系统中，在 Direct I/O 模式下进行 write 系统调用能够确保文件数据落盘，但是文件元数据不一定落盘。如果在此类操作系统上，那么还需要执行一次 fsync 系统调用确保文件元数据也落盘。否则，可能会导致文件异常、元数据确实等情况。MySQL 的 O_DIRECT 与 O_DIRECT_NO_FSYNC 配置是一个具体案例[9]。 Direct I/O 的优缺点： （1）优点 Linux 中的直接 I/O 技术省略掉缓存 I/O 技术中操作系统内核缓冲区的使用，数据直接在应用程序地址空间和磁盘之间进行传输，从而使得自缓存应用程序可以省略掉复杂的系统级别的缓存结构，而执行程序自己定义的数据读写管理，从而降低系统级别的管理对应用程序访问数据的影响。 与其他零拷贝技术一样，避免了内核空间到用户空间的数据拷贝，如果要传输的数据量很大，使用直接 I/O 的方式进行数据传输，而不需要操作系统内核地址空间拷贝数据操作的参与，这将会大大提高性能。 （2）缺点 由于设备之间的数据传输是通过 DMA 完成的，因此用户空间的数据缓冲区内存页必须进行 page pinning（页锁定），这是为了防止其物理页框地址被交换到磁盘或者被移动到新的地址而导致 DMA 去拷贝数据的时候在指定的地址找不到内存页从而引发缺页错误，而页锁定的开销并不比 CPU 拷贝小，所以为了避免频繁的页锁定系统调用，应用程序必须分配和注册一个持久的内存池，用于数据缓冲。 如果访问的数据不在应用程序缓存中，那么每次数据都会直接从磁盘进行加载，这种直接加载会非常缓慢。 在应用层引入直接 I/O 需要应用层自己管理，这带来了额外的系统复杂性； 谁会使用 Direct I/O？ IBM[5]的一篇文章指出，自缓存应用程序（ self-caching applications）可以选择使用 Direct I/O。 自缓存应用程序 对于某些应用程序来说，它会有它自己的数据缓存机制，比如，它会将数据缓存在应用程序地址空间，这类应用程序完全不需要使用操作系统内核中的高速缓冲存储器，这类应用程序就被称作是自缓存应用程序（ self-caching applications ）。 例如，应用内部维护一个缓存空间，当有读操作时，首先读取应用层的缓存数据，如果没有，那么就通过 Direct I/O 直接通过磁盘 I/O 来读取数据。缓存仍然在应用，只不过应用觉得自己实现一个缓存比操作系统的缓存更高效。 数据库管理系统是这类应用程序的一个代表。自缓存应用程序倾向于使用数据的逻辑表达方式，而非物理表达方式；当系统内存较低的时候，自缓存应用程序会让这种数据的逻辑缓存被换出，而并非是磁盘上实际的数据被换出。自缓存应用程序对要操作的数据的语义了如指掌，所以它可以采用更加高效的缓存替换算法。自缓存应用程序有可能会在多台主机之间共享一块内存，那么自缓存应用程序就需要提供一种能够有效地将用户地址空间的缓存数据置为无效的机制，从而确保应用程序地址空间缓存数据的一致性。 page cache 是 Linux 为所有应用提供的缓存机制，但是数据库应用太特殊了，page cache 影响了数据对特性的追求。 另一方面，目前 Linux 上的异步 IO 库，其依赖于文件使用 O_DIRECT 模式打开，它们通常一起配合使用。 如何使用 Direct I/O？ 用户应用需要实现用户空间内的缓存区，读/写操作应当尽量通过此缓存区提供。如果有性能上的考虑，那么尽量避免频繁地基于 Direct I/O 进行读/写操作。 4. 典型案例4.1 KakfaKafka 作为一个消息队列，涉及到磁盘 I/O 主要有两个操作： Provider 向 Kakfa 发送消息，Kakfa 负责将消息以日志的方式持久化落盘； Consumer 向 Kakfa 进行拉取消息，Kafka 负责从磁盘中读取一批日志消息，然后再通过网卡发送； Kakfa 服务端接收 Provider 的消息并持久化的场景下使用 mmap 机制[6]，能够基于顺序磁盘 I/O 提供高效的持久化能力，使用的 Java 类为 java.nio.MappedByteBuffer。 Kakfa 服务端向 Consumer 发送消息的场景下使用 sendfile 机制[7]，这种机制主要两个好处： sendfile 避免了内核空间到用户空间的 CPU 全程负责的数据移动； sendfile 基于 Page Cache 实现，因此如果有多个 Consumer 在同时消费一个主题的消息，那么由于消息一直在 page cache 中进行了缓存，因此只需一次磁盘 I/O，就可以服务于多个 Consumer； 使用 mmap 来对接收到的数据进行持久化，使用 sendfile 从持久化介质中读取数据然后对外发送是一对常用的组合。但是注意，你无法利用 sendfile 来持久化数据，利用 mmap 来实现 CPU 全程不参与数据搬运的数据拷贝。 4.2 MySQLMySQL 的具体实现比 Kakfa 复杂很多，这是因为支持 SQL 查询的数据库本身比消息队列对复杂很多。 MySQL 的零拷贝技术使用方式请移步我的另一篇文章[8]。 5. 总结DMA 技术使得内存与其他组件，例如磁盘、网卡进行数据拷贝时，CPU 仅仅需要发出控制信号，而拷贝数据的过程则由 DMAC 负责完成。 Linux 的零拷贝技术有多种实现策略，但根据策略可以分为如下几种类型： 减少甚至避免用户空间和内核空间之间的数据拷贝：在一些场景下，用户进程在数据传输过程中并不需要对数据进行访问和处理，那么数据在 Linux 的 Page Cache 和用户进程的缓冲区之间的传输就完全可以避免，让数据拷贝完全在内核里进行，甚至可以通过更巧妙的方式避免在内核里的数据拷贝。这一类实现一般是是通过增加新的系统调用来完成的，比如 Linux 中的 mmap()，sendfile() 以及 splice() 等。 绕过内核的直接 I/O：允许在用户态进程绕过内核直接和硬件进行数据传输，内核在传输过程中只负责一些管理和辅助的工作。这种方式其实和第一种有点类似，也是试图避免用户空间和内核空间之间的数据传输，只是第一种方式是把数据传输过程放在内核态完成，而这种方式则是直接绕过内核和硬件通信，效果类似但原理完全不同。 内核缓冲区和用户缓冲区之间的传输优化：这种方式侧重于在用户进程的缓冲区和操作系统的页缓存之间的 CPU 拷贝的优化。这种方法延续了以往那种传统的通信方式，但更灵活。 REFERENCE [1]CPU：一个故事看懂DMA [2]Linux I/O 原理和 Zero-copy 技术全面揭秘 [3]零复制 - 维基百科，自由的百科全书 [4][mmap](https://spongecaptain.cool/SimpleClearFileIO/3. mmap.html) [5]直接 I/O 的动机 [6]kafka.log.AbstractIndex [7]Apache Kafka DOCUMENTATION [8]MySQL 的零拷贝技术 [9]InnoDB Startup Options and System Variables","categories":[],"tags":[]},{"title":"","slug":"10. RDMA","date":"2023-02-04T02:34:09.202Z","updated":"2023-02-04T02:33:07.972Z","comments":true,"path":"2023/02/04/10. RDMA/","link":"","permalink":"https://sk-xinye.github.io/2023/02/04/10.%20RDMA/","excerpt":"","text":"RDMA[toc] 1. 什么是 RDMARDMA 即 Remote Direct Memory Access，其区别于 DMA（Direct Memory Access）。在数据中心领域，RDMA 是一种绕过远程主机操作系统内核访问其内存中数据的技术，由于不经过操作系统，不仅节省了大量 CPU 资源，同样也提高了系统吞吐量、降低了系统的网络通信延迟，尤其适合在大规模并行计算机集群中有广泛应用。在基于 NVMe over Fabric 的数据中心中，RDMA 可以配合高性能的 NVMe SSD 构建高性能、低延迟的存储网络[1]。 RDMA 技术基于 DMA 技术的硬件技术实现，此项技术特点在于在于不需要 CPU 干预而直接访问远程主机内存（应当说成 CPU 不需要负责数据搬运，仅仅需要管理），重点是为解决网络传输中服务器端数据处理的延迟。 2. RDMA 的技术总之，RDMA 技术使得两个远程主机之间避免了： CPU 负责的数据搬运，其属于零拷贝技术的一种； 数据无需进入内核空间，直接通过网卡传输； 上图清晰地说明了传统通过 Socket 传输的数据，虽然可以实现零拷贝，但是并不能够避免 TCP、IP 层的参与，需要复杂的报头处理逻辑。 但是 RDMA 由于直接通过底层的网络进行数据传输，因此避免了数据在内核空间的报文处理消耗以及数据拷贝消耗。 REFERENCE [1] 远程直接内存访问 [2] 从天猫双11成交额2684亿看RDMA网络","categories":[],"tags":[]},{"title":"1.并发使用","slug":"2-asyncio","date":"2023-01-19T02:30:53.000Z","updated":"2023-07-16T13:14:55.049Z","comments":true,"path":"2023/01/19/2-asyncio/","link":"","permalink":"https://sk-xinye.github.io/2023/01/19/2-asyncio/","excerpt":"","text":"asyncio异步编程python异步库https://github.com/aio-libs 迭代器是一个实现了迭代器协议的对象，python的一些内置数据类型（列表，数组，字符串，字典等）都可以通过for语句进行迭代，我们也可以自己创建一个容器，实现了迭代器协议，可以通过for，next方法进行迭代，在迭代的末尾，会引发stopIteration异常。实现了__iter__和__next__方法 生成器（generator）【一种特殊迭代器】 是通过yield语句快速生成迭代器，可以不用iter和next方法 yield可以使一个普通函数变成一个生成器，并且相应的next()方法返回是yield后的值。一种更直观的解释是：程序执行到yield时会返回结果并暂停，再次调用next时会从上次暂停的地方继续开始执行。 生成器自身有构成一个迭代器，每次迭代时使用一个yield返回 的值，一个生成器中可以有多个yield的值 123456789101112131415161718import sysdef fibonacci(n): # 生成器函数 - 斐波那契 a, b, counter = 0, 1, 0 while True: if (counter &gt; n): return yield a a, b = b, a + b counter += 1f = fibonacci(10) # f 是一个迭代器，由生成器返回生成while True: try: print (next(f), end=&quot; &quot;) except StopIteration: sys.exit()&quot;&quot;&quot;结果0 1 1 2 3 5 8 13 21 34 55&quot;&quot;&quot; python异步编程原理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106&quot;&quot;&quot;1.遍历url 生成对应的协程对象，并将该对象协程fetch方法，放入task中2.task 初始化future 开启协程，并将step函数（协程触发函数放到future的回调属性中），并将socker 注册对应future的set_result 该方法会设置结果，并且触发回调3.开启ioloop 当发现了事件之后，调用事件的回调函数，通过set_result 完成，向下执行程序为了节省网络IO时间，通过5种IO模型中的io多路复用，达到节省网络IO到达的时间的目的，其实根本目的是为了节省等待数据准备 (Waiting for the data to be ready)的时间，但是从内核写到用户进程的时间还是会阻塞的loop 不断轮训发生的事件，并且执行注册在selector上的回调函数 这里的回调函数一方面为Future 对象的result赋值，另一方面执行Task中的step 函数用于驱动协程的执行Future 保存将来的执行结果，配合Task ，协程函数完成回调的执行Task 触发器协程函数： 执行注册监听到selector上，并配合事件回调循环&quot;&quot;&quot;import socketfrom selectors import *selector = DefaultSelectorstopped = Falseurls_todo = &#123;&#x27;www.baidu.com&#x27;, &#x27;www.baidu.com/1&#x27;, &#x27;/2&#x27;, &#x27;/3&#x27;, &#x27;/4&#x27;, &#x27;/5&#x27;, &#x27;/6&#x27;, &#x27;/7&#x27;, &#x27;/8&#x27;, &#x27;/9&#x27;&#125;class Future: def __init__(self): self.result = None self._callbacks = [] def add_done_callback(self, fn): self._callbacks.append(fn) def set_result(self, result): self.result = result for fn in self._callbacks: fn(self)class Crawler: def __init__(self, url): self.url = url self.response = b&#x27;&#x27; def fetch(self): sock = socket.socket() sock.setblocking(False) try: print(&#x27;****&#x27;*50) sock.connect((&#x27;220.181.111.148&#x27;, 80)) except BlockingIOError: pass f = Future() print(&quot;++++&quot;*50) def on_connected(): f.set_result(None) print(sock.fileno()) selector.register(sock.fileno(), EVENT_WRITE, on_connected) yield f print(&quot;yield 之后了&quot;) selector.unregister(sock.fileno()) get = &#x27;GET &#123;0&#125; HTTP/1.0\\r\\nHost: 220.181.111.148\\r\\n\\r\\n&#x27;.format(self.url) sock.send(get.encode(&#x27;ascii&#x27;)) global stopped while True: f = Future() def on_readable(): f.set_result(sock.recv(4096)) selector.register(sock.fileno(), EVENT_READ, on_readable) chunk = yield f selector.unregister(sock.fileno()) if chunk: self.response += chunk else: urls_todo.remove(self.url) if not urls_todo: stopped = True breakclass Task: def __init__(self, coro): self.coro = coro f = Future() f.set_result(None) self.step(f) def step(self, future): try: next_future = self.coro.send(future.result) except StopIteration: print(&quot;停止task&quot;) return next_future.add_done_callback(self.step)def loop(): while not stopped: events = selector.select() for event_key, event_mask in events: callback = event_key.data callback()if __name__ == &#x27;__main__&#x27;: import time start = time.time() for url in urls_todo: crawler = Crawler(url) Task(crawler.fetch()) loop() print(time.time() - start) asyncio 使用讲解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108import asyncioimport timeimport uvloop # 这个效率更高asyncio.set_event_loop_policy(uvloop.EventLoopPolicy())async def testa(x): print(&quot;in test a&quot;) await asyncio.sleep(3) print(&quot;a 又开始了&quot;) return xasync def testb(x): print(&quot;in test b&quot;) await asyncio.sleep(1) print(&quot;b 又开始了&quot;) return xdef callbackfunc(task): print(&quot;回调函数，运行结果为&#123;&#125;&quot;.format(task.result()))async def main1(): &quot;&quot;&quot;顺序执行，串行 in test a a 又开始了 in test b b 又开始了 test a result is a test b result is b 一共耗时为：4.002900838851929 &quot;&quot;&quot; start =time.time() a_result = await testa(&quot;a&quot;) b_result = await testb(&quot;b&quot;) print(&quot;test a result is &#123;&#125;&quot;.format(a_result)) print(&quot;test b result is &#123;&#125;&quot;.format(b_result)) print(&quot;一共耗时为：&#123;&#125;&quot;.format(time.time()-start))async def main2(): &quot;&quot;&quot;并行执行 in test a in test b b 又开始了 a 又开始了 test a result is a test b result is b 一共耗时为：3.0003840923309326 &quot;&quot;&quot; start = time.time() a_result,b_result = await asyncio.gather(testa(&quot;a&quot;),testb(&quot;b&quot;)) print(&quot;test a result is &#123;&#125;&quot;.format(a_result)) print(&quot;test b result is &#123;&#125;&quot;.format(b_result)) print(&quot;一共耗时为：&#123;&#125;&quot;.format(time.time()-start))async def main3(): &quot;&quot;&quot;通过创建task来进行并行执行 &lt;Task pending name=&#x27;Task-2&#x27; coro=&lt;testa() running at C:/Users/zx-176/PycharmProjects/untitled/2020/async/asyncio_test.py:4&gt;&gt; &lt;Task pending name=&#x27;Task-3&#x27; coro=&lt;testb() running at C:/Users/zx-176/PycharmProjects/untitled/2020/async/asyncio_test.py:10&gt;&gt; False False in test a in test b b 又开始了 a 又开始了 True True a b 一共耗时为：3.000962257385254 &quot;&quot;&quot; start = time.time() taska = asyncio.ensure_future(testa(&quot;a&quot;)) # 返回的是task对象 taskb = asyncio.ensure_future(testb(&quot;b&quot;)) print(taska) print(taskb) print(taska.done(),taskb.done()) await taska await taskb print(taska.done(),taskb.done()) print(taska.result()) print(taskb.result()) print(&quot;一共耗时为：&#123;&#125;&quot;.format(time.time() - start))async def main4(): &quot;&quot;&quot;结果同3&quot;&quot;&quot; start = time.time() taska = asyncio.create_task(testa(&quot;a&quot;)) # 返回task对象 # task对象可以增加回调函数 taska.add_done_callback(callbackfunc) taskb = asyncio.create_task(testb(&quot;b&quot;)) print(taska) print(taskb) print(taska.done(),taskb.done()) await taska await taskb print(taska.done(),taskb.done()) print(taska.result()) print(taskb.result()) print(&quot;一共耗时为：&#123;&#125;&quot;.format(time.time() - start))if __name__ == &#x27;__main__&#x27;: asyncio.run(main4()) # loop = asyncio.get_event_loop() # loop.run_until_complete(main1()) wait和gather区别 首先，gather是需要所有任务都执行结束，如果某一个协程函数崩溃了，则会抛异常，都不会有结果。 wait可以定义函数返回的时机，可以是FIRST_COMPLETED(第一个结束的)，FIRST_EXCEPTION(第一个出现异常的)，ALL_COMPLETED(全部执行完，默认的) 如何使用线程池进程池对无法使用异步库的函数进行并发12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import asyncioimport multiprocessingfrom concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutorimport osimport threadingimport time#定义阻塞函数def ping(url): print(&quot;阻塞函数开始执行，当前进程ID为：&#123;&#125;，线程ID为：&#123;&#125;&quot;.format(os.getpid(),threading.current_thread())) time.sleep(2) print(&quot;模拟ping 输出:&#123;&#125;&quot;.format(url)) print(&quot;阻塞函数结束执行，当前进程ID为：&#123;&#125;，线程ID为：&#123;&#125;&quot;.format(os.getpid(),threading.current_thread()))#定义阻塞函数async def ping2(url): print(&quot;阻塞函数开始执行，当前进程ID为：&#123;&#125;，线程ID为：&#123;&#125;&quot;.format(os.getpid(),threading.current_thread())) await asyncio.sleep(2) print(&quot;模拟ping 输出:&#123;&#125;&quot;.format(url)) print(&quot;阻塞函数结束执行，当前进程ID为：&#123;&#125;，线程ID为：&#123;&#125;&quot;.format(os.getpid(),threading.current_thread()))#定义一个跑事件循环的线程函数def start_thread_loop(loop): asyncio.set_event_loop(loop) loop.run_forever()def main0(): &quot;&quot;&quot;没并发&quot;&quot;&quot; print(&quot;in main 当前进程ID为：&#123;&#125;，线程ID为：&#123;&#125;&quot;.format(os.getpid(),threading.current_thread())) loop = asyncio.get_event_loop() t = threading.Thread(target=start_thread_loop,args=(loop,)) t.start() loop.call_soon_threadsafe(ping,&quot;www.baidu.com&quot;) loop.call_soon_threadsafe(ping, &quot;www.qq.com&quot;) loop.call_soon_threadsafe(ping, &quot;www.weixin.com&quot;) print(&quot;主线程不阻塞&quot;)def main1(): &quot;&quot;&quot;均在不同线程中跑&quot;&quot;&quot; print(&quot;in main 当前进程ID为：&#123;&#125;，线程ID为：&#123;&#125;&quot;.format(os.getpid(),threading.current_thread())) loop = asyncio.get_event_loop() t = threading.Thread(target=start_thread_loop,args=(loop,)) t.start() loop.run_in_executor(None,ping,&quot;www.baidu.com&quot;) loop.run_in_executor(None, ping, &quot;www.qq.com&quot;) loop.run_in_executor(None, ping, &quot;www.weixin.com&quot;) print(&quot;主线程不阻塞&quot;)def main2(): print(&quot;in main 当前进程ID为：&#123;&#125;，线程ID为：&#123;&#125;&quot;.format(os.getpid(),threading.current_thread())) loop = asyncio.get_event_loop() t = threading.Thread(target=start_thread_loop,args=(loop,)) t.start() thread_executor = ThreadPoolExecutor(3) process_executor= ProcessPoolExecutor(3) loop.run_in_executor(process_executor,ping,&quot;www.baidu.com&quot;) loop.run_in_executor(process_executor, ping, &quot;www.qq.com&quot;) loop.run_in_executor(process_executor, ping, &quot;www.weixin.com&quot;) print(&quot;主线程不阻塞&quot;)def main3(): &quot;&quot;&quot;同进程同线程&quot;&quot;&quot; print(&quot;in main 当前进程ID为：&#123;&#125;，线程ID为：&#123;&#125;&quot;.format(os.getpid(),threading.current_thread())) loop = asyncio.get_event_loop() t = threading.Thread(target=start_thread_loop,args=(loop,)) t.start() asyncio.run_coroutine_threadsafe(ping2(&quot;www.baidu.com&quot;),loop) asyncio.run_coroutine_threadsafe(ping2(&quot;www.qq.com&quot;), loop) asyncio.run_coroutine_threadsafe(ping2(&quot;www.weixin.com&quot;), loop) print(&quot;主线程不阻塞&quot;)if __name__ == &#x27;__main__&#x27;: main3()","categories":[],"tags":[]},{"title":"5.iptables","slug":"5-iptables","date":"2022-08-16T01:58:17.000Z","updated":"2023-07-16T13:14:55.060Z","comments":true,"path":"2022/08/16/5-iptables/","link":"","permalink":"https://sk-xinye.github.io/2022/08/16/5-iptables/","excerpt":"","text":"iptables概念 我们知道，linux 系统中有 route，用来和其他网络连接。如果没有它，linux 就无法和外部通信了 iptables 是一系列的规则，用来对内核中的数据进行处理。没有它，linux 可以正常工作，有了它，linux 可以对网络中的数据的操作更加多样化，可见 iptables 是一个辅助角色，可以让 linux 的网络系统更强大。 iptables 其实不是真正的防火墙，我们可以把它理解成一个客户端代理，用户通过 iptables 这个代理，将用户的安全设定执行到对应的”安全框架”中，这个”安全框架”才是真正的防火墙，这个框架的名字叫 netfilter netfilter 才是防火墙真正的安全框架（framework），netfilter位于内核空间。 iptables 其实是一个命令行工具，位于用户空间，我们用这个工具操作真正的框架。 Netfilter是Linux操作系统核心层内部的一个数据包处理模块，它具有如下功能： 网络地址转换(Network Address Translate) 数据包内容修改 数据包过滤的防火墙功能 在 iptables 中 有两个概念：链（PREROUTING、INPUT、OUTPUT、FORWARD、POSTROUTING）和表（raw–&gt;mangle–&gt;nat–&gt;filter） 链 总结一下，数据包先经过 PREOUTING，由该链确定数据包的走向： 目的地址是本地，则发送到 INPUT，让INPUT决定是否接收下来送到用户空间，流程为①—&gt;②; 若满足 PREROUTING 的 nat 表上的转发规则，则发送给 FORWARD，然后再经过 POSTROUTING 发送出去，流程为： ①—&gt;③—&gt;④—&gt;⑥ 主机发送数据包时，流程则是⑤—&gt;⑥ 标记 ACCEPT：允许数据包通过。 DROP：直接丢弃数据包，不给任何回应信息，这时候客户端会感觉自己的请求泥牛入海了，过了超时时间才会有反应。 REJECT：拒绝数据包通过，必要时会给数据发送端一个响应的信息，客户端刚请求就会收到拒绝的信息。 SNAT：源地址转换，解决内网用户用同一个公网地址上网的问题。 MASQUERADE：是SNAT的一种特殊形式，适用于动态的、临时会变的ip上。 DNAT：目标地址转换。 REDIRECT：在本机做端口映射。 MARK：打标记。 12iptables -t nat -nvL 默认-t是filteriptables-save 表 每个表（tables）对应了一个特定的功能，有filter表（实现包过滤）、nat表（实现网络地址转换）、mangle表（实现包修改）、raw表（实现数据跟踪），不同的表中包含了不同的钩子函数，不同表的同一钩子函数具有一定的优先级：raw–&gt;mangle–&gt;nat–&gt;filter。 删除iptables中规则1234567iptables -L -n --line-number 查看记录号iptables -D INPUT 1 删除input表中第一条规则iptables -Fiptables -Ziptables -Xiptables -t nat -F 清空该表里所有","categories":[],"tags":[]},{"title":"4.画图相关","slug":"4-画图相关","date":"2022-07-13T10:56:42.000Z","updated":"2023-02-04T02:33:07.970Z","comments":true,"path":"2022/07/13/4-画图相关/","link":"","permalink":"https://sk-xinye.github.io/2022/07/13/4-%E7%94%BB%E5%9B%BE%E7%9B%B8%E5%85%B3/","excerpt":"","text":"示例创建dataframe 12345678柱形图import matplotlib.pyplot as pltdf.plot(kind=&#x27;bar&#x27;, title=title)plt.xlabel(x_label)plt.ylabel(y_label)path = os.path.join(save_path, name)plt.savefig(path,bbox_inches=&quot;tight&quot;)plt.show()","categories":[],"tags":[]},{"title":"7.常见软件安装","slug":"7-常见软件安装","date":"2022-06-13T03:07:07.000Z","updated":"2023-02-04T02:33:07.968Z","comments":true,"path":"2022/06/13/7-常见软件安装/","link":"","permalink":"https://sk-xinye.github.io/2022/06/13/7-%E5%B8%B8%E8%A7%81%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85/","excerpt":"","text":"总1https://zhangsnow.com/kernel/cgroups.html go安装123456789101112mkdir /usr/local/gocd /usr/local/gowget https://dl.google.com/go/go1.7.6.linux-amd64.tar.gzvi /etc/profileexport GOROOT=/usr/local/goexport PATH=/usr/local/go/bin:$PATHsource /etc/profile hcache安装1234567wget https://silenceshell-1255345740.cos.ap-shanghai.myqcloud.com/hcachechmod +x hcachemv hcache /usr/local/bin/hcache -top 3 查看使用缓存最多的3个文件hcache --top 3 --bname 查看使用缓存最多的3个文件（文件一列指显示文件名）hcache -pid 1397 查看指定进程的缓存使用 pc stat安装raid信息查看","categories":[],"tags":[]},{"title":"3.常规使用","slug":"3-常规使用","date":"2022-05-18T03:31:27.000Z","updated":"2023-07-16T13:14:55.059Z","comments":true,"path":"2022/05/18/3-常规使用/","link":"","permalink":"https://sk-xinye.github.io/2022/05/18/3-%E5%B8%B8%E8%A7%84%E4%BD%BF%E7%94%A8/","excerpt":"","text":"时间解析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960from dateutil.parser import parseparse(a)def time_action(self,time_data,cut_seconds):&quot;&quot;&quot;时间戳加减秒操作&quot;&quot;&quot;pre_time = datetime.datetime.fromtimestamp(1659428736.1127937)now_time = datetime.timedelta(cut_seconds)+pre_timereturn now_time.import timeimport datetime#当前时间datetimenow = datetime.datetime.now()print(&#x27;\\n当前时间datetime：\\t%s，类型：%s&#x27;%(now,type(now)))#当前时间strnow_str = str(now)print(&#x27;当前时间str：\\t\\t%s，类型：%s&#x27;%(now_str,type(now_str)))#当前时间时间戳now_mktime =time.time()print(&#x27;当前时间时间戳：\\t%s，类型：%s&#x27;%(now_mktime,type(now_mktime)))#今天、昨天、明天today = datetime.date.today()yesterday = today - datetime.timedelta(days=1)tommorrow = today + datetime.timedelta(days=1)print(&#x27;\\n今天:%s,昨天:%s,明天：%s&#x27;%(today,yesterday,tomorrow))#任意时间字符串转为datetimetoday_8 = str(today)[0:10]+&#x27; 08:00:00&#x27;today_8_time = datetime.datetime.strptime(today_8,&#x27;%Y-%m-%d %H:%M:%S&#x27;)print(&#x27;获取当天任意时间:&#x27;,today_8,&#x27;类型:&#x27;,type(today_8),today_8_time,&#x27;类型:&#x27;,type(today_8_time))#datetime转为字符串now_str = datetime.datetime.strftime(now,&#x27;%Y-%m-%d %H:%M:%S&#x27;)now_str_1 = str(now)[0:19]print(&#x27;\\ndatetime转为字符串：\\t%s，类型：%s&#x27;%(now_str,type(now_str)))#字符串转为datetimestr2datetime = datetime.datetime.strptime(now_str,&#x27;%Y-%m-%d %H:%M:%S&#x27;)now_time_1 = datetime.datetime.strptime(str(now)[0:19],&#x27;%Y-%m-%d %H:%M:%S&#x27;)print(&#x27;字符串转为datetime：\\t%s，类型：%s&#x27;%(str2datetime,type(str2datetime)))#将datetime转为时间戳datetime2mktime = time.mktime(today.timetuple())print(&#x27;\\ndatetime转为时间戳：\\t%s，类型：%s&#x27;%(datetime2mktime,type(datetime2mktime)))#将时间戳转为datetimemktime2datetime = datetime.datetime.fromtimestamp(now_mktime)print(&#x27;时间戳转为字符串：\\t%s，类型：%s&#x27;%(mktime2datetime,type(mktime2datetime)))#字符串转为时间戳str2mktime = time.mktime(time.strptime(str(today), &#x27;%Y-%m-%d&#x27;))print(&#x27;\\n字符串转为时间戳：\\t%s，类型：%s&#x27;%(str2mktime,type(str2mktime)))#时间戳转为字符串# time.localtime()将时间戳转换为struct_timemktime2str = time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, time.localtime(now_mktime))print(&#x27;时间戳转为字符串：\\t%s，类型：%s&#x27;%(mktime2str,type(mktime2str))) es查询数据1234567891011121314151617181920212223242526class ESGetData: &quot;&quot;&quot;获取es索引数据&quot;&quot;&quot; def __init__(self,hosts): self.hosts = hosts self.es_client = zElasticsearch(self.hosts) def get_data_by_index(self,index:str): print(index) &quot;&quot;&quot;通过索引名获取数据&quot;&quot;&quot; body = &#123;&quot;query&quot;:&#123;&quot;match_all&quot;:&#123;&#125;&#125;&#125; general_data = helpers.scan(client=self.es_client,query=body,scroll=&quot;100m&quot;,index=index) kind = self.get_index_kind(index) return general_data,kind def get_data_by_id(self,index,id): &quot;&quot;&quot;通过_id 获取数据&quot;&quot;&quot; print(index) try: data = self.es_client.get(index=index, id=id) bin_data = dec_obj_from_str(data[&#x27;_source&#x27;][&#x27;BIN_DATA&#x27;]) except Exception as e: print(e) print(&quot;index &#123;&#125; not has id&quot;.format(index, id)) bin_data = &#123;&#125; return bin_data 参数传递使用12345678910111213def parser_arg(namespace=None): _parse = argparse.ArgumentParser(description=&quot;check index data&quot;) _parse.add_argument(&#x27;-e&#x27;, &#x27;--hosts&#x27;, help=&#x27;es hosts list&#x27;,default=[&quot;192.168.81.192&quot;],nargs=&quot;+&quot;) _parse.add_argument(&#x27;-i&#x27;, &#x27;--index_list&#x27;, help=&#x27;es index list&#x27;,nargs=&quot;+&quot;,default=[&quot;180_binary_e_mp_day_read-13404&quot;,&quot;180_binary_e_mp_cur_curve-13404&quot;,&quot;180_binary_e_mp_power_curve-13404&quot;,&quot;180_binary_e_mp_vol_curve-13404&quot;]) _parse.add_argument(&#x27;-d&#x27;, &#x27;--doc_id&#x27;, type=str, help=&#x27;search _id data&#x27;,default=None) _parse.add_argument(&#x27;-m&#x27;, &#x27;--meter_message&#x27;,nargs=&quot;+&quot;, help=&#x27;search meter message&#x27;,default=None) _parse.add_argument(&#x27;-f&#x27;, &#x27;--flag&#x27;, nargs=&quot;+&quot;, help=&#x27;search flag&#x27;, default=None) _parse.add_argument(&#x27;-s&#x27;, &#x27;--save_path&#x27;, help=&#x27;save result path&#x27;, default=None) return _parse.parse_args(namespace=namespace)args = parser_arg()index_list = args.index_listhosts = args.hosts py-spy 使用../.nox/zs_power-opt-py-ve1-bin-python/bin/py-spy record -o /home/zshield/logs/profile.svg – ../.nox/zs_power-opt-py-ve1-bin-python/bin/python3.8 -m main.pp2_main -e 192.168.82.86:19200,192.168.80.230:19200 -B –sz_open –sz=/home/zshield/conf/sz_app/sz.conf –process_num ‘{“data”:1,”sz”:1,”comp”:1,”write”:1,”dg”:1,”fs”:1}’ –use-pro-data-mode –high_open /home/zdzhou/miniconda3/bin/py-spy record -o /home/zdzhou/profile.svg – /home/zdzhou/miniconda3/bin/python -m main.pp2_main -e zxtech:&#x5a;&#x78;&#x6f;&#100;&#49;&#49;&#50;&#95;&#x73;&#x68;&#105;&#110;&#105;&#x6e;&#x67;&#49;&#48;&#x40;&#x31;&#x39;&#x32;&#x2e;&#x31;&#54;&#x38;&#x2e;&#x38;&#51;&#x2e;&#52;&#x32;:19400 -a 219762232 –on5 13403 –sz_open –comp_topo monitor_p0_topo_v3.yml -p0 –high_open –day_v2 –use-pro-data-mode –v3_test python-rocksdb的基本使用1.打开数据库 123import rocksdbdb = rocksdb.DB(&quot;test.db&quot;,rocksdb.Options(create_if_missing=True))# create_if_missing=True 如果不存在则创建名为&#x27;test.db&#x27;的数据库 2.设置键值对和获取值 12345678910111213141516171819202122# RocksDB中存储的是byte strings。在python2中为str类型，在python3中为bytes类型。db.put(b&quot;key&quot;,b&quot;value&quot;) #设置一个键值对，key =&gt; valuedb.get(b&quot;key&quot;) #获取键为key的值，如果不在则返回为空b&#x27;value&#x27;db.delete(b&quot;key&quot;) #删除键为key为的键值对# 将几个操作合并为一个操作batch = rocksdb.WriteBatch()batch.put(b&quot;key&quot;, b&quot;v1&quot;)batch.delete(b&quot;key&quot;)batch.put(b&quot;key&quot;, b&quot;v2&quot;)batch.put(b&quot;key&quot;, b&quot;v3&quot;)db.write(batch)# 一次获取多个键值对&gt;&gt;&gt; db.put(b&quot;key1&quot;,b&quot;v1&quot;)&gt;&gt;&gt; ret = db.multi_get([b&quot;key&quot;,b&quot;key1&quot;])&gt;&gt;&gt; ret&#123;b&#x27;key&#x27;: b&#x27;v3&#x27;, b&#x27;key1&#x27;: b&#x27;v1&#x27;&#125; 3.迭代 123456789101112131415161718192021222324&gt;&gt;&gt; it = db.iterkeys() # 默认是无效的，先得调用seek方法&gt;&gt;&gt; it.seek_to_first()&gt;&gt;&gt; print(list(it))[b&#x27;a&#x27;, b&#x27;key&#x27;, b&#x27;key1&#x27;]&gt;&gt;&gt; it.seek_to_last()&gt;&gt;&gt; print(list(it))[b&#x27;key1&#x27;]&gt;&gt;&gt; it = db.itervalues() #对值进行迭代&gt;&gt;&gt; it.seek_to_first()&gt;&gt;&gt; print(list(it))[b&#x27;b&#x27;, b&#x27;v3&#x27;, b&#x27;v1&#x27;]&gt;&gt;&gt; it = db.iteritems() #对键值对进行迭代&gt;&gt;&gt; it.seek_to_first()&gt;&gt;&gt; print(list(it))[(b&#x27;a&#x27;, b&#x27;b&#x27;), (b&#x27;key&#x27;, b&#x27;v3&#x27;), (b&#x27;key1&#x27;, b&#x27;v1&#x27;)]# 反向迭代&gt;&gt;&gt; it.seek_to_last()&gt;&gt;&gt; print(list(reversed(it)))[(b&#x27;key1&#x27;, b&#x27;v1&#x27;), (b&#x27;key&#x27;, b&#x27;v3&#x27;), (b&#x27;a&#x27;, b&#x27;b&#x27;)] 4.快照 1234567&gt;&gt;&gt; snapshot = db.snapshot()&gt;&gt;&gt; it = db.iteritems(snapshot=snapshot)&gt;&gt;&gt; it&lt;rocksdb._rocksdb.ItemsIterator object at 0x7f77ae179608&gt;&gt;&gt;&gt; it.seek_to_first()&gt;&gt;&gt; print(dict(it))&#123;b&#x27;a&#x27;: b&#x27;b&#x27;, b&#x27;key&#x27;: b&#x27;v3&#x27;, b&#x27;key1&#x27;: b&#x27;v1&#x27;&#125; 5.备份和恢复 1234567# 备份backup = rocksdb.BackupEngine(&quot;test.db/backups&quot;)backup.create_backup(db, flush_before_backup=True)# 恢复backup = rocksdb.BackupEngine(&quot;test.db/backups&quot;)backup.restore_latest_backup(&quot;test.db&quot;, &quot;test.db&quot;) Python处理非标准 json 格式字符串1list = eval(expr, type(&#x27;Dummy&#x27;, (dict,), dict(__getitem__=lambda s, n: n))()) python反射简单反射1CURVE_DATA_CLASS_MAP = &#123;subclass.type: subclass for subclass in IncrementCurveData.__subclasses__()&#125; 包的反射 类加载1234567891011121314151617181920212223242526272829303132333435363738&quot;&quot;&quot;类加载模块&quot;&quot;&quot;import importlibimport inspectimport pkgutilimport sysfrom alies_to_k8s import out_resultfrom alies_to_k8s import sourcedef load_similar_class(pkg, cls): collect = &#123;&#125; if hasattr(pkg, &#x27;__path__&#x27;): for loader, module_name, is_pkg in pkgutil.walk_packages(pkg.__path__, pkg.__name__ + &#x27;.&#x27;): # print(module_name) if module_name not in sys.modules: importlib.import_module(module_name) md = sys.modules[module_name] md_collect = load_module(md, cls) collect.update(md_collect) else: md_collect = load_module(pkg, cls) collect.update(md_collect) return collectdef load_module(md, cls): collect = &#123;&#125; for i in vars(md).values(): if inspect.isclass(i) and issubclass(i, cls): if getattr(i, &#x27;inuse&#x27;, &#x27;True&#x27;): collect[i.name] = i return collectCLIENT = &#123;subclass.name: subclass for subclass in out_result.IClient.__subclasses__()&#125;OUT_HANDLER = load_similar_class(out_result, out_result.IOutput)API_SOURCE = load_similar_class(source,source.ISource) python使用概念参数传递 在 python 的世界上，所有的数据,包括函数/类全都是对象。在函数传递参数时,只是让两个变量标识相同的对象 如果传递的是不可变对象（int float string tuple）时，对于不可变对象的操作，均会产生新的标识对象，并不影响外部实参对象值 如果传递的是可变对象（dict set list）时，对于可变对象的操作，是在原对象的基础上进行的操作，会改变外部实参对象值 但如果3中对可变对象的操作是重新赋予新的标识，那就会产生新的对象，也就不会影响外部对象，例如x = x+[1] 这里可以类比c++中存在3中传参方式（值传递，引用传递，指针传递），其中值传递，会创建新的对象（形参是实参的拷贝），不会影响外部对象，而引用及指针传递，因为操作的对象和外部对象一致，所以会影响外部对象的值","categories":[],"tags":[]},{"title":"6.性能监控","slug":"6-性能监控","date":"2022-05-13T09:43:42.000Z","updated":"2023-02-04T02:33:07.968Z","comments":true,"path":"2022/05/13/6-性能监控/","link":"","permalink":"https://sk-xinye.github.io/2022/05/13/6-%E6%80%A7%E8%83%BD%E7%9B%91%E6%8E%A7/","excerpt":"","text":"监控方式Diamond（Telegraf InfluxDB官方的數據採集） + InfluxDB + Grafana 运行InfluxDB及Grafana以下說明以docker運行InfluxDB及Grafana的程序，首先在系統上建立需要的directories: 1mkdir -p /BGdata/&#123;influxdb,grafana&#125; /var/log/grafana 在power.git中已有現成的docker-compose YML文件: 12345678910111213141516171819202122version: &#x27;3&#x27;services: influxdb: image: influxdb:1.8.3 container_name: influxdb network_mode: host restart: always environment: - INFLUXDB_HTTP_LOG_ENABLED=false - INFLUXDB_DATA_QUERY_LOG_ENABLED=false volumes: - /BGdata/influxdb:/var/lib/influxdb grafana: image: grafana/grafana:7.3.1 container_name: grafana network_mode: host restart: always user: root volumes: - /BGdata/grafana:/var/lib/grafana - /var/log/grafana:/var/log/grafana 上面定義了兩個docker containers，influxdb及grafana，並且設置需要的volume mapping等等。 接下來使用docker-compose開啟這兩個containers: 12345678[root@sj-node2 monitor]# /opt/py3.6/ve1/bin/docker-compose -f mon.yml up -dCreating influxdb ... doneCreating grafana ... done[root@sj-node2 monitor]# /opt/py3.6/ve1/bin/docker-compose -f mon.yml ps Name Command State Ports-------------------------------------------------grafana /run.sh Upinfluxdb /entrypoint.sh influxd Up 開始使用之前還需要在InfluxDB中建立一個database用來儲存採集的數據，這個可以直接在運行中的influxdb container中使用”influx”工具完成: 123456789101112131415161718[root@sj-node2 monitor]# docker exec -it influxdb /bin/bashroot@sj-node2:/#root@sj-node2:/# influxConnected to http://localhost:8086 version 1.8.3InfluxDB shell version: 1.8.3&gt; show databasesname: databasesname----_internal&gt; create database grafana&gt; show databasesname: databasesname----_internalgrafana&gt; 以上步驟在InfluxDB中建立一個”grafana” database，完成後，就可以開始將監控採集數據寫入這個database中 Diamond build/install目前已經在公司內部bitbucket建立了一個Diamond的git repo: ssh://git@192.168.1.139:7999/analytics/diamond.git Production版本位於”4.0”分支上，是基於4.0.515版本(最近的一個release)，再加上了兩個小改動: 支持與較新版本InfluxDB的相容性 支持配置InfluxDB server的base URL 除了上面這兩個改動，另外也從master分支合併了關於Elasticsearch collector的幾個小改動，包括支持配置指定username/password (ES開啟authentication的場景) 注意Diamond用的是CentOS 7上default的Python 2.7，因此build環境中需要: Python 2.7: CentOS 7已經default安裝 pip: 可用”yum install python2-pip”安裝 Python的”setuptools”模塊: 可用”pip install setuptools”安裝 Python的”wheel”模塊: 可用”pip install wheel”安裝 在準備好的build環境中，如下build出一個wheel包: 123456789101112[ahuang@sj-node1 tmp]$ git clone ssh://git@192.168.1.139:7999/analytics/diamond.git...[ahuang@sj-node1 tmp]$ cd diamond/[ahuang@sj-node1 diamond]$ git checkout 4.0Branch 4.0 set up to track remote branch 4.0 from origin.Switched to a new branch &#x27;4.0&#x27;[ahuang@sj-node1 diamond]$ make bdist_wheel...[ahuang@sj-node1 diamond]$ ls -l dist/total 316-rw-rw-r--. 1 ahuang ahuang 322632 Nov 13 2020 diamond-4.0.531_zs4-py2.py3-none-any.whl[ahuang@sj-node1 diamond]$ 這個wheel包可以直接用pip安裝在目標系統上，另外目標系統上也須要Python的”influxdb”模塊，例如使用以下命令同時安裝: 1pip install influxdb dist/diamond-4.0.531_zs4-py2.py3-none-any.whl 運行Diamond及config 如 上安裝diamond後，運行config的位置為”/etc/diamond/diamond.conf”，可先將diamond config的例子copy一份，再基於此做修改，另外，diamond本身運行的日志位於”/var/log/diamond” directory，第一次運行前需要建立此directory: 12cp /usr/lib/python2.7/site-packages/etc/diamond/diamond.conf.example /etc/diamond/diamond.confmkdir /var/log/diamond 使用以下命令運行diamond 1/usr/bin/diamond 如果需要troubleshoot運行問題(例如config格式錯誤)，可以用debug模式: 1/usr/bin/diamond -f -l Diamond支持非常多的輸入數據源(collectors)及輸出目的地(handlers)，上面的config example中有一些基本的選項，詳細文件可參考:https://diamond.readthedocs.io/en/latest/ 以下以SJ office用於ES節點監控的例子，說明一些需要的config選項 輸出(handlers)選項這裡要使用的輸出目的地是InfluxDB，因此首先需要在diamond.conf中設定handlers (位於”[server]”下): 123[server]...handlers = diamond.handler.influxdbHandler.InfluxdbHandler InfluxdbHandler本身也需要一些選項，因此在”[handlers]”下加上以下這段: 123456789[handlers]...[[InfluxdbHandler]]hostname = 192.168.200.19batch_size = 100database = grafanausername = grafanapassword = grafanabase_url = /example/proxy/path 這裡的hostname是InfluxDB的位置，database、username、password根據在InfluxDB server上建立的database來設定，base_url則是InfluxDB server的base URL，可供proxy使用(如果不設定則default是””，也就是沒有)，例如，上面在InfluxDB中建立一個名為”grafana”的database，username、password、base_url都不用設定 輸入(collectors)選項Diamond中的各種collectors支持監控不同的metrics，所有collectors共用的選項中有兩個較重要: 1234[collectors][[default]]hostname = ...interval = ... 以下說明: hostname: 指定用來標記採集到數據的名字，如果不設定，自動使用系統OS設定的hostname (一般不需要設定)interval: 多久採集一次數據，default是300秒，可視需要調整。(除了這個共用的選項，每個collector也可以指定該collector本身的interval) 這裡的config例子中已經開啟了幾個基本的collectors (例如CPU、memory、disk等等)，我們再加上一個採集Elasticsearch狀況的collector: 1234[collectors]...[[ElasticSearchCollector]]enabled = True 除了主要的diamond.conf之外，每個collector也可以有自己的config file，例如這裡我們再給兩個collectors加上config files指定一些針對個別collector的選項: 12345678/etc/diamond/collectors/ElasticSearchCollector.confenabled = Trueinterval = 120cluster = True/etc/diamond/collectors/DiskUsageCollector.confenabled = Trueinterval = 120 數據寫入InfluxDB以上面描述的方式配置、運行Diamond後，假設配置正確，Diamond會開始將監控數據寫入InfluxDB中的database，這裡以上面建立的”grafana” database為例，我們可以在InfluxDB的docker container中，使用”influx”工具來確認數據是否寫入: 123456789101112131415[root@sj-node2 ~]# docker exec -it influxdb /bin/bashroot@sj-node2:/#root@sj-node2:/# influxConnected to http://localhost:8086 version 1.8.3InfluxDB shell version: 1.8.3&gt;&gt; show databasesname: databasesname----_internalgrafana&gt; use grafanaUsing database grafana&gt; 上面使用influx工具連接上InfluxDB並選擇”grafana”這個database 12345678910111213141516171819&gt; show measurements...servers.sj-node2.memory.MemAvailableservers.sj-node2.memory.MemFreeservers.sj-node2.memory.MemTotalservers.sj-node2.memory.Shmemservers.sj-node2.memory.SwapCachedservers.sj-node2.memory.SwapFreeservers.sj-node2.memory.SwapTotalservers.sj-node2.memory.VmallocChunkservers.sj-node2.memory.VmallocTotalservers.sj-node2.memory.VmallocUsedservers.sj-node2.vmstat.pgfaultservers.sj-node2.vmstat.pgmajfaultservers.sj-node2.vmstat.pgpginservers.sj-node2.vmstat.pgpgoutservers.sj-node2.vmstat.pswpinservers.sj-node2.vmstat.pswpout&gt; 這裡”show measurements”命令顯示了此database中包括的監控metrics，因為包括ES中所有indices的metrics，數量很多，這裡只顯示少數為例。 在InfluxDB中，每個measurement類似於一個傳統database中的table，InfluxDB提供一個類似於傳統SQL的”InfluxQL”語言，可用於對InfluxDB databases做查詢等等，例如: 123456789101112131415161718&gt; select * from &quot;servers.sj-node2.memory.MemAvailable&quot;name: servers.sj-node2.memory.MemAvailabletime value---- -----...1605652120000000000 338756198401605652150000000000 338736701441605652180000000000 339273441281605652210000000000 339006423041605652240000000000 338922700801605652270000000000 338804121601605652300000000000 338929704961605652330000000000 338655354881605652360000000000 338655232001605652390000000000 338710036481605652420000000000 338891407361605652450000000000 33888215040&gt; 這裡的”select …”命令顯示此measurement中目前為止已經寫入的數據，每筆數據包括一個timestamp及value 關於”influx”工具、InfluxQL語言等等的詳細文件可參見: https://docs.influxdata.com/influxdb/v1.8/ 使用Grafana上面使用docker-compose已經開啟了Grafana，使用browser連接URL “https://:3000”，首先使用以下步驟添加data source: 使用default的username/password “admin”/“admin”登入 選擇左側的”Configuration”-&gt;”Data Sources” 選擇”Add data source” Data source type選擇InfluxDB 細節配置的部分，如果根據以上的測試程序，這裡大部分可以不填，只需改動以下配置: “HTTP”-&gt;”URL”: “http://:8086” “InfluxDB Details”-&gt;”Database”: “grafana” 選擇”Save &amp; Test”，Grafana根據配置測試後顯示”Data source is working”，並且儲存data source的配置 再選擇左側的”Configuration”-&gt;”Data Sources”，可看到上面配置的data source，名為”InfluxDB”(這個是default name，如果需要可以在上面配置時改名)配置好data source後，就可以將監控數據做可視化，在power.git中已經有一個Grafana dashboard的基本例子”src/monitor/grafana-dash-diamond.json”，可以使用以下步驟import這個例子: 選擇左側的”Dashboards”→”Manage” 選擇”Import” 選擇”Upload JSON file” 選擇上面提到的”grafana-dash-diamond.json” Options可以保留defaults不需改動，選擇”Import”完成import後即可看到dashboard顯示目前有的監控數據: 注意這裡上方的”src”選項顯示的是之前建立的data source，而”node”選項則會自動列出data source中所有存在的hosts influxdb的快照和还原快照在influxdb容器所在的节点上进行以下操作： 1234567docker exec -it influxdb bashinfluxd backup -portable -database grafana -start 2022-01-17T02:00:00Z -end 2022-01-18T13:00:00Z /tmp/data/grafana （此命令中的时间为UTC时间，需要在实际时间上减8小时）cd /tmptar cvf ./data.tar ./data/*rm -rf data/exitdocker cp influxdb:/tmp/data.tar /home/zshield/data.tar 还原1234docker cp data.tar influxdb:/docker exec -it influxdb /bin/bashtar -xvf data.tarinfluxd restore -portable -db grafana -newdb anhui_527 /data/grafana (这样就将原来库grafana还原成 anhui_527)","categories":[],"tags":[]},{"title":"2.环境包","slug":"2-环境包","date":"2022-02-18T02:26:03.000Z","updated":"2023-02-04T02:33:07.969Z","comments":true,"path":"2022/02/18/2-环境包/","link":"","permalink":"https://sk-xinye.github.io/2022/02/18/2-%E7%8E%AF%E5%A2%83%E5%8C%85/","excerpt":"","text":"列出whl包依赖/opt/py/ve1/bin/pkginfo -f requires_dist redis-4.1.3-py3-none-any.whl pip freeze &gt;requirements.txt pip download -r requirements.txt pip install –no-index –find-links=”.” -r requirements.txt","categories":[],"tags":[]},{"title":"1.环境安装","slug":"1-环境安装","date":"2022-01-20T04:38:42.000Z","updated":"2023-02-04T02:33:07.966Z","comments":true,"path":"2022/01/20/1-环境安装/","link":"","permalink":"https://sk-xinye.github.io/2022/01/20/1-%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85/","excerpt":"","text":"java 安装下载解压http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html x64 Compressed Archive 源码包 tar -zxvf jdk-8u321-linux-x64.tar.gz 配置环境变量vim /etc/profile export JAVA_HOME=/root/aliyun/jdk1.8.0_321export JRE_HOME=$JAVA_HOME/jreexport PATH=$PATH:$JAVA_HOME/binexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar source /etc/profile 检查环境javajava -version 手动编译两个测试所需文件 123456789101112public class Hello&#123; public void sayHello()&#123; System.out.println(&quot;Hello&quot;); &#125;&#125;public class Demo&#123; public static void main(String[] args) &#123; Hello Hello = new Hello(); Hello.sayHello(); &#125;&#125; 同目录下123456$ lsDemo.javaHello.java$ javac Demo.java$ java Demo 引入不同目录依赖编译执行1234567$ lsDemo.javalibs/ Hello.java$ javac -classpath ./libs Demo.java$ java -classpath .:libs Demo 引入jar包将Hello.java打成jar包 123$cd libs$ avac Hello.java$ jar -cvf hello.jar Hello.class 引入jar包执行 123456789$ lsDemo.javalibs/ hello.jar$ javac -classpath ./libs/hello.jar Demo.java$ java -classpath .:./libs/hello.jar Demojavac -classpath e:/import.jar;e:/tmp/import.jar -d destationpath test.java 引用多个jar包 优化12345678910$ javac -encoding UTF-8 -classpath ./libs -d classes Demo.java# 参数说明-encoding UTF-8 定源文件使用的字符编码-classpath ./libs 指定查找用户类文件和注释处理程序的位置-d classes 指定放置生成的类文件的位置，必须存在Demo.java source files$ cd classes$ java Demo mavenhttps://blog.csdn.net/YF_Li123/article/details/79953731 helloword","categories":[],"tags":[]},{"title":"2.python常见用法","slug":"2-python常见用法","date":"2022-01-04T03:04:22.000Z","updated":"2023-02-04T02:33:07.967Z","comments":true,"path":"2022/01/04/2-python常见用法/","link":"","permalink":"https://sk-xinye.github.io/2022/01/04/2-python%E5%B8%B8%E8%A7%81%E7%94%A8%E6%B3%95/","excerpt":"","text":"时间12from dateutil import parserparser.parse(&quot;Aug 28 1999 12:00AM&quot;) # datetime.datetime(1999, 8, 28, 0, 0) py-spy1/opt/py3.6/ve1/bin/py-spy record -o /home/zshield/logs/profile.svg -- /opt/py3.6/ve1/bin/python3.8 -m main.pp2_main","categories":[],"tags":[]},{"title":"5.案例","slug":"5-案例","date":"2021-12-22T08:58:36.000Z","updated":"2023-02-04T02:33:07.931Z","comments":true,"path":"2021/12/22/5-案例/","link":"","permalink":"https://sk-xinye.github.io/2021/12/22/5-%E6%A1%88%E4%BE%8B/","excerpt":"","text":"docker 拒绝连接问题：是由于docker 内程序占用了内存被oom kill掉了导致的 丢包分析 从图中你可以看出，可能发生丢包的位置，实际上贯穿了整个网络协议栈。换句话说，全程都有丢包的可能。比如我们从下往上看： 在两台 VM 连接之间，可能会发生传输失败的错误，比如网络拥塞、线路错误等； 在网卡收包后，环形缓冲区可能会因为溢出而丢包； 在链路层，可能会因为网络帧校验失败、QoS 等而丢包； 在 IP 层，可能会因为路由失败、组包大小超过 MTU 等而丢包； 在传输层，可能会因为端口未监听、资源占用超过内核限制等而丢包； 在套接字层，可能会因为套接字缓冲区溢出而丢包； 在应用层，可能会因为应用程序异常而丢包； 此外，如果配置了 iptables 规则，这些网络包也可能因为 iptables 过滤规则而丢包。 链路层当缓冲区溢出等原因导致网卡丢包时，Linux 会在网卡收发数据的统计信息中，记录下收发错误的次数。你可以通过 ethtool 或者 netstat ，来查看网卡的丢包记录。比如，可以在容器中执行下面的命令，查看丢包情况： 12345root@nginx:/# netstat -iKernel Interface tableIface MTU RX-OK RX-ERR RX-DRP RX-OVR TX-OK TX-ERR TX-DRP TX-OVR Flgeth0 100 31 0 0 0 8 0 0 0 BMRUlo 65536 0 0 0 0 0 0 0 0 LRU RX-OK、RX-ERR、RX-DRP、RX-OVR，分别表示接收时的总包数、总错误数、进入 Ring Buffer 后因其他原因（如内存不足）导致的丢包数以及 Ring Buffer 溢出导致的丢包数。TX-OK、TX-ERR、TX-DRP、TX-OVR 也代表类似的含义，只不过是指发送时对应的各个指标。 网络层和传输层netstat -s 命令，就可以看到协议的收发汇总，以及错误信息了： 1234567891011121314151617181920212223242526272829303132333435root@nginx:/# netstat -sIp: Forwarding: 1 //开启转发 31 total packets received //总收包数 0 forwarded //转发包数 0 incoming packets discarded //接收丢包数 25 incoming packets delivered //接收的数据包数 15 requests sent out //发出的数据包数Icmp: 0 ICMP messages received //收到的ICMP包数 0 input ICMP message failed //收到ICMP失败数 ICMP input histogram: 0 ICMP messages sent //ICMP发送数 0 ICMP messages failed //ICMP失败数 ICMP output histogram:Tcp: 0 active connection openings //主动连接数 0 passive connection openings //被动连接数 11 failed connection attempts //失败连接尝试数 0 connection resets received //接收的连接重置数 0 connections established //建立连接数 25 segments received //已接收报文数 21 segments sent out //已发送报文数 4 segments retransmitted //重传报文数 0 bad segments received //错误报文数 0 resets sent //发出的连接重置数Udp: 0 packets received ...TcpExt: 11 resets received for embryonic SYN_RECV sockets //半连接重置数 0 packet headers predicted TCPTimeouts: 7 //超时数 TCPSynRetrans: 4 //SYN重传数... iptablesiptables 的原理，它基于 Netfilter 框架，通过一系列的规则，对网络数据包进行过滤（如防火墙）和修改（如 NAT） 这些 iptables 规则，统一管理在一系列的表中，包括 filter（用于过滤）、nat（用于 NAT）、mangle（用于修改分组数据） 和 raw（用于原始数据包）等。而每张表又可以包括一系列的链，用于对 iptables 规则进行分组管理。 tcpdump抓包123root@nginx:/# tcpdump -i eth0 -nn port 80tcpdump: verbose output suppressed, use -v or -vv for full protocol decodelistening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes cpu利用率太高内核线程我们知道，在 Linux 中，用户态进程的“祖先”，都是 PID 号为 1 的 init 进程。比如，现在主流的 Linux 发行版中，init 都是 systemd 进程；而其他的用户态进程，会通过 systemd 来进行管理。 实际上，Linux 在启动过程中，有三个特殊的进程，也就是 PID 号最小的三个进程。 0 号进程为 idle 进程，这也是系统创建的第一个进程，它在初始化 1 号和 2 号进程后，演变为空闲任务。当 CPU 上没有其他任务执行时，就会运行它。 1 号进程为 init 进程，通常是 systemd 进程，在用户态运行，用来管理其他用户态进程。 2 号进程为 kthreadd 进程，在内核态运行，用来管理内核线程。 火焰图安装好工具后，要生成火焰图，其实主要需要三个步骤： 执行 perf script ，将 perf record 的记录转换成可读的采样记录； 执行 stackcollapse-perf.pl 脚本，合并调用栈信息； 执行 flamegraph.pl 脚本，生成火焰图。 123生成火焰图$ perf script -i /root/perf.data | ./stackcollapse-perf.pl --all | ./flamegraph.pl &gt; ksoftirqd.svg执行成功后，使用浏览器打开 ksoftirqd.svg ，你就可以看到生成的火焰图了 动态追踪技术，通过探针机制，来采集内核或者应用程序的运行信息，从而可以不用修改内核和应用程序的代码，就获得丰富的信息，帮你分析、定位想要排查的问题 动态追踪（Dynamic Tracing）动态追踪的事件源根据事件类型的不同，动态追踪所使用的事件源，可以分为静态探针、动态探针以及硬件事件等三类。它们的关系如下图所示： 动态追踪机制Linux 也提供了一系列的动态追踪机制，比如 ftrace、perf、eBPF 等。 eBPF 则在 BPF（Berkeley Packet Filter）的基础上扩展而来，不仅支持事件跟踪机制，还可以通过自定义的 BPF 代码（使用 C 语言）来自由扩展。所以，eBPF 实际上就是常驻于内核的运行时，可以说就是 Linux 版的 DTrace。 除此之外，还有很多内核外的工具，也提供了丰富的动态追踪功能。最常见的就是前面提到的 SystemTap，我们之前多次使用过的 BCC（BPF Compiler Collection），以及常用于容器性能分析的 sysdig 等。 perfperf 可以用来分析 CPU cache、CPU 迁移、分支预测、指令周期等各种硬件事件；perf 也可以只对感兴趣的事件进行动态追踪。 eBPF 和 BCCftrace 和 perf 的功能已经比较丰富了，不过，它们有一个共同的缺陷，那就是不够灵活，没法像 DTrace 那样通过脚本自由扩展。 而 eBPF 就是 Linux 版的 DTrace，可以通过 C 语言自由扩展（这些扩展通过 LLVM 转换为 BPF 字节码后，加载到内核中执行）。下面这张图，就表示了 eBPF 追踪的工作原理： 从图中你可以看到，eBPF 的执行需要三步： 从用户跟踪程序生成 BPF 字节码；加载到内核中运行；向用户空间输出结果。 实际上，在 eBPF 执行过程中，编译、加载还有 maps 等操作，对所有的跟踪程序来说都是通用的。把这些过程通过 Python 抽象起来，也就诞生了 BCC（BPF Compiler Collection）。 系统优化CPU 优化CPU 性能优化的核心，在于排除所有不必要的工作、充分利用 CPU 缓存并减少进程调度对性能的影响。 第一种，把进程绑定到一个或者多个 CPU 上，充分利用 CPU 缓存的本地性，并减少进程间的相互影响。 第二种，为中断处理程序开启多 CPU 负载均衡，以便在发生大量中断时，可以充分利用多 CPU 的优势分摊负载。 第三种，使用 Cgroups 等方法，为进程设置资源限制，避免个别进程消耗过多的 CPU。同时，为核心应用程序设置更高的优先级，减少低优先级任务的影响。 内存优化 第一种，除非有必要，Swap 应该禁止掉。这样就可以避免 Swap 的额外 I/O ，带来内存访问变慢的问题。 第二种，使用 Cgroups 等方法，为进程设置内存限制。这样就可以避免个别进程消耗过多内存，而影响了其他进程。对于核心应用，还应该降低 oom_score，避免被 OOM 杀死。 第三种，使用大页、内存池等方法，减少内存的动态分配，从而减少缺页异常。 磁盘和文件系统 I/O 优化 第一种，也是最简单的方法，通过 SSD 替代 HDD、或者使用 RAID 等方法，提升 I/O 性能。 第二种，针对磁盘和应用程序 I/O 模式的特征，选择最适合的 I/O 调度算法。比如，SSD 和虚拟机中的磁盘，通常用的是 noop 调度算法；而数据库应用，更推荐使用 deadline 算法。 第三，优化文件系统和磁盘的缓存、缓冲区，比如优化脏页的刷新频率、脏页限额，以及内核回收目录项缓存和索引节点缓存的倾向等等。 除此之外，使用不同磁盘隔离不同应用的数据、优化文件系统的配置选项、优化磁盘预读、增大磁盘队列长度等，也都是常用的优化思路。 网络优化 首先，从内核资源和网络协议的角度来说，我们可以对内核选项进行优化，比如： 你可以增大套接字缓冲区、连接跟踪表、最大半连接数、最大文件描述符数、本地端口范围等内核资源配额； 也可以减少 TIMEOUT 超时时间、SYN+ACK 重传数、Keepalive 探测时间等异常处理参数； 还可以开启端口复用、反向地址校验，并调整 MTU 大小等降低内核的负担。 其次，从网络接口的角度来说，我们可以考虑对网络接口的功能进行优化，比如： 你可以将原来 CPU 上执行的工作，卸载到网卡中执行，即开启网卡的 GRO、GSO、RSS、VXLAN 等卸载功能； 也可以开启网络接口的多队列功能，这样，每个队列就可以用不同的中断号，调度到不同 CPU 上执行； 还可以增大网络接口的缓冲区大小以及队列长度等，提升网络传输的吞吐量。 最后，在极限性能情况（比如 C10M）下，内核的网络协议栈可能是最主要的性能瓶颈，所以，一般会考虑绕过内核协议栈。 你可以使用 DPDK 技术，跳过内核协议栈，直接由用户态进程用轮询的方式，来处理网络请求。同时，再结合大页、CPU 绑定、内存对齐、流水线并发等多种机制，优化网络包的处理效率。 你还可以使用内核自带的 XDP 技术，在网络包进入内核协议栈前，就对其进行处理。这样，也可以达到目的，获得很好的性能。 应用程序优化 第一，从 CPU 使用的角度来说，简化代码、优化算法、异步处理以及编译器优化等，都是常用的降低 CPU 使用率的方法，这样可以利用有限的 CPU 处理更多的请求。 第二，从数据访问的角度来说，使用缓存、写时复制、增加 I/O 尺寸等，都是常用的减少磁盘 I/O 的方法，这样可以获得更快的数据处理速度。 第三，从内存管理的角度来说，使用大页、内存池等方法，可以预先分配内存，减少内存的动态分配，从而更好地内存访问性能。 第四，从网络的角度来说，使用 I/O 多路复用、长连接代替短连接、DNS 缓存等方法，可以优化网络 I/O 并减少网络请求数，从而减少网络延时带来的性能问题。 第五，从进程的工作模型来说，异步处理、多线程或多进程等，可以充分利用每一个 CPU 的处理能力，从而提高应用程序的吞吐能力。 你还可以使用消息队列、CDN、负载均衡等各种方法，来优化应用程序的架构，将原来单机要承担的任务，调度到多台服务器中并行处理。这样也往往能获得更好的整体性能。 gdb调试https://fasionchan.com/posts/debug-python-with-gdb/https://meteorix.github.io/2019/02/13/gdbpython/ 准备 yum install gdb &lt;python-dbg,目前这个centos不好安装，可不装&gt; 准备代码 gdb /opt/py3.8/ve1/bin/python 执行 python—-&gt; import sys —–&gt;sys.path.insert(0,/root(这个下面存放libpython.py文件,或者append也可以)) —–&gt;import libpython —–&gt;end bt py-bt py-list 123参考文档 ：https://fasionchan.com/posts/debug-python-with-gdb/ https://meteorix.github.io/2019/02/13/gdbpython/gdb /home/zshield/data_interconnect/py/ve1/bin/python core.22614bt 常见命令1234567bt # 当前C调用栈py-bt # 当前Py调用栈py-list # 当前py代码位置info thread # 线程信息thread &lt;id&gt; # 切换到某个线程thread apply all py-list # 查看所有线程的py代码位置ctrl-c # 中断 磁盘信息收集lspci |grep -i raid smartctl –scan 查看hdd信息 lsblk/lsscsi/blkid 获取挂载盘 ./storcli64 /c0 show 确定raid信息 https://support.huawei.com/enterprise/zh/doc/EDOC1000163568/219f7d6d pvdisplay 确定物理卷 vgdisplay 确定卷组 lvdisplay 确定逻辑卷 可以通过lsblk 验证","categories":[],"tags":[]},{"title":"4.网络","slug":"4-网络","date":"2021-12-21T02:19:51.000Z","updated":"2023-02-04T02:33:07.930Z","comments":true,"path":"2021/12/21/4-网络/","link":"","permalink":"https://sk-xinye.github.io/2021/12/21/4-%E7%BD%91%E7%BB%9C/","excerpt":"","text":"网络相关概念网络模型 Linux 网络栈 传输层在应用程序数据前面增加了 TCP 头； 网络层在 TCP 数据包前增加了 IP 头； 而网络接口层，又在 IP 数据包前后分别增加了帧头和帧尾。 最上层的应用程序，需要通过系统调用，来跟套接字接口进行交互； 套接字的下面，就是我们前面提到的传输层、网络层和网络接口层； 最底层，则是网卡驱动程序以及物理网卡设备。 这里我简单说一下网卡。网卡是发送和接收网络包的基本设备。在系统启动过程中，网卡通过内核中的网卡驱动程序注册到系统中。而在网络收发过程中，内核通过中断跟网卡进行交互。 Linux 网络收发流程 当一个网络帧到达网卡后，网卡会通过 DMA 方式，把这个网络包放到收包队列中； 然后通过硬中断，告诉中断处理程序已经收到了网络包。 网卡中断处理程序会为网络帧分配内核数据结构（sk_buff），并将其拷贝到 sk_buff 缓冲区中； 然后再通过软中断，通知内核收到了新的网络帧。 在链路层检查报文的合法性，找出上层协议的类型（比如 IPv4 还是 IPv6），再去掉帧头、帧尾，然后交给网络层。 网络层取出 IP 头，判断网络包下一步的走向，比如是交给上层处理还是转发。当网络层确认这个包是要发送到本机后，就会取出上层协议的类型（比如 TCP 还是 UDP），去掉 IP 头，再交给传输层处理。 传输层取出 TCP 头或者 UDP 头后，根据 &lt; 源 IP、源端口、目的 IP、目的端口 &gt; 四元组作为标识，找出对应的 Socket，并把数据拷贝到 Socket 的接收缓存中。 应用程序就可以使用 Socket 接口，读取到新接收到的数据了 socket编程 服务端首先创建socket文件，然后bind监听，最后listen() 客户端创建socket文件，connect() 开始与服务端进行3次连接 3次握手涉及到2个队列，未完成连接队列（接收到syn 还未得到客户端确认），已连接队列（接收到了客户端的确认） 应用程序通过accept()从已连接队列中拿到连接的socket fd，然后生成新的socket fd 返回给客户端用户通信 当使用io多路复用器时（select poll epoll）,就可以同时监听多个文件描述符了,核心就是告诉我们哪些文件描述符可读，哪些文件描述符可写 select：程序把文件描述符集合交给select的系统调用，select遍历每个文件描述符后返回那些可以操作的文件描述符，然后客户端就可以进行读写了。但文件描述符集合大小有限制为1024个 poll 在文件描述符集合的限制上进行了改进 select和poll都有缺点：每次都要将文件描述符集合从用户态到内核态，并从内核态到用户态，而且内核和用户态都需要遍历文件描述符集合 epoll有点：当数据到达网卡时，会触发中断，正常情况下cpu会把相应的数据复制到内存中，和相关文件描述符进行绑定。epoll在这个基础上做了延伸，epoll首先是在内核中维护了一个红黑树，以及一些链表结构，当数据到达网卡拷贝到内存时会把相应的文件描述符从红黑树中拷贝到链表中，这样链表存储的就是已经到达数据的文件描述符，这样当程序调用epoll_wait的时候就能直接把能读的文件描述符返回给应用程序 除了epoll_wait外，epoll还有两个系统调用，分别是epoll_create 和epoll_ctl（这里需要将文件描述符拷贝到内核态）。分别用于初始化epoll和把文件描述符添加到红黑树中 java中selector就是对select、poll、epoll的封装 性能指标 带宽，表示链路的最大传输速率，单位通常为 b/s （比特 / 秒）。 吞吐量，表示单位时间内成功传输的数据量，单位通常为 b/s（比特 / 秒）或者 B/s（字节 / 秒）。吞吐量受带宽限制，而吞吐量 / 带宽，也就是该网络的使用率。 延时，表示从网络请求发出后，一直到收到远端响应，所需要的时间延迟。在不同场景中，这一指标可能会有不同含义。比如，它可以表示，建立连接需要的时间（比如 TCP 握手延时），或一个数据包往返所需的时间（比如 RTT）。 PPS，是 Packet Per Second（包 / 秒）的缩写，表示以网络包为单位的传输速率。PPS 通常用来评估网络的转发能力，比如硬件交换机，通常可以达到线性转发（即 PPS 可以达到或者接近理论最大值）。而基于 Linux 服务器的转发，则容易受网络包大小的影响。 除了以上指标还包括网络的可用性（网络能否正常通信）、并发连接数（TCP 连接数量）、丢包率（丢包百分比）、重传率（重新传输的网络包比例）等也是常用的性能指标。 对 TCP 或者 Web 服务来说，更多会用并发连接数和每秒请求数（QPS，Query per Second）等指标，它们更能反应实际应用程序的性能。 网络配置查看网络配置（ifconfig/ip）ifconfig 和 ip 分别属于软件包 net-tools 和 iproute2，iproute2 是 net-tools 的下一代。 12345678910111213141516171819202122$ ifconfig eth0eth0: flags=4163&lt;UP,BROADCAST,RUNNING,MULTICAST&gt; mtu 1500 inet 10.240.0.30 netmask 255.240.0.0 broadcast 10.255.255.255 inet6 fe80::20d:3aff:fe07:cf2a prefixlen 64 scopeid 0x20&lt;link&gt; ether 78:0d:3a:07:cf:3a txqueuelen 1000 (Ethernet) RX packets 40809142 bytes 9542369803 (9.5 GB) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 32637401 bytes 4815573306 (4.8 GB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0$ ip -s addr show dev eth02: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 78:0d:3a:07:cf:3a brd ff:ff:ff:ff:ff:ff inet 10.240.0.30/12 brd 10.255.255.255 scope global eth0 valid_lft forever preferred_lft forever inet6 fe80::20d:3aff:fe07:cf2a/64 scope link valid_lft forever preferred_lft forever RX: bytes packets errors dropped overrun mcast 9542432350 40809397 0 0 0 193 TX: bytes packets errors dropped carrier collsns 4815625265 32637658 0 0 0 0 注意： 第一，网络接口的状态标志。ifconfig 输出中的 RUNNING ，或 ip 输出中的 LOWER_UP ，都表示物理网络是连通的，即网卡已经连接到了交换机或者路由器中。如果你看不到它们，通常表示网线被拔掉了 第二，MTU 的大小。MTU 默认大小是 1500，根据网络架构的不同（比如是否使用了 VXLAN 等叠加网络），你可能需要调大或者调小 MTU 的数值。 第三，网络接口的 IP 地址、子网以及 MAC 地址。这些都是保障网络功能正常工作所必需的，你需要确保配置正确。 第四，网络收发的字节数、包数、错误数以及丢包情况，特别是 TX 和 RX 部分的 errors、dropped、overruns、carrier 以及 collisions 等指标不为 0 时，通常表示出现了网络 I/O 问题。其中： errors 表示发生错误的数据包数，比如校验错误、帧同步错误等； dropped 表示丢弃的数据包数，即数据包已经收到了 Ring Buffer，但因为内存不足等原因丢包； overruns 表示超限数据包数，即网络 I/O 速度过快，导致 Ring Buffer 中的数据包来不及处理（队列满）而导致的丢包； carrier 表示发生 carrirer 错误的数据包数，比如双工模式不匹配、物理电缆出现问题等； collisions 表示碰撞数据包数。 套接字信息你可以用 netstat 或者 ss（推荐） ，来查看套接字、网络栈、网络接口以及路由表的信息。 1234567891011121314151617# head -n 3 表示只显示前面3行# -l 表示只显示监听套接字# -n 表示显示数字地址和端口(而不是名字)# -p 表示显示进程信息$ netstat -nlp | head -n 3Active Internet connections (only servers)Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program nametcp 0 0 127.0.0.53:53 0.0.0.0:* LISTEN 840/systemd-resolve# -l 表示只显示监听套接字# -t 表示只显示 TCP 套接字# -n 表示显示数字地址和端口(而不是名字)# -p 表示显示进程信息$ ss -ltnp | head -n 3State Recv-Q Send-Q Local Address:Port Peer Address:PortLISTEN 0 128 127.0.0.53%lo:53 0.0.0.0:* users:((&quot;systemd-resolve&quot;,pid=840,fd=13))LISTEN 0 128 0.0.0.0:22 0.0.0.0:* users:((&quot;sshd&quot;,pid=1459,fd=3)) 其中，接收队列（Recv-Q）和发送队列（Send-Q）需要你特别关注，它们通常应该是 0。当你发现它们不是 0 时，说明有网络包的堆积发生。 当套接字处于连接状态（Established）时，Recv-Q 表示套接字缓冲还没有被应用程序取走的字节数（即接收队列长度）。而 Send-Q 表示还没有被远端主机确认的字节数（即发送队列长度）。 当套接字处于监听状态（Listening）时，Recv-Q 表示全连接队列的长度。而 Send-Q 表示全连接队列的最大长度。 协议栈统计信息使用 netstat 或 ss ，也可以查看协议栈的信息： ss 只显示已经连接、关闭、孤儿套接字等简要统计 而 netstat 则提供的是更详细的网络协议栈信息。比如，上面 netstat 的输出示例，就展示了 TCP 协议的主动连接、被动连接、失败重试、发送和接收的分段数量等各种信息。 网络吞吐和 PPSsar 增加 -n 参数就可以查看网络的统计信息，比如网络接口（DEV）、网络接口错误（EDEV）、TCP、UDP、ICMP 等等。执行下面的命令，你就可以得到网络接口统计信息： 12345678# 数字1表示每隔1秒输出一组数据$ sar -n DEV 1Linux 4.15.0-1035 (ubuntu) 01/06/19 _x86_64_ (2 CPU)13:21:40 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil13:21:41 eth0 18.00 20.00 5.79 4.25 0.00 0.00 0.00 0.0013:21:41 docker0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0013:21:41 lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 rxpck/s 和 txpck/s 分别是接收和发送的 PPS，单位为包 / 秒。 rxkB/s 和 txkB/s 分别是接收和发送的吞吐量，单位是 KB/ 秒。 rxcmp/s 和 txcmp/s 分别是接收和发送的压缩数据包数，单位是包 / 秒。 %ifutil 是网络接口的使用率，即半双工模式下为 (rxkB/s+txkB/s)/Bandwidth，而全双工模式下为 max(rxkB/s, txkB/s)/Bandwidth。 Bandwidth 可以用 ethtool 来查询，它的单位通常是 Gb/s 或者 Mb/s，不过注意这里小写字母 b ，表示比特而不是字节。我们通常提到的千兆网卡、万兆网卡等，单位也都是比特。如下你可以看到，我的 eth0 网卡就是一个千兆网卡： 12$ ethtool eth0 | grep Speed Speed: 1000Mb/s 连通性和延时我们通常使用 ping ，来测试远程主机的连通性和延时，而这基于 ICMP 协议。比如，执行下面的命令，你就可以测试本机到 114.114.114.114 这个 IP 地址的连通性和延时： CK10I/O 模型优化 水平触发（Level Trigger，LT）：只要文件描述符可以非阻塞地执行 I/O ，就会触发通知。也就是说，应用程序可以随时检查文件描述符的状态，然后再根据状态，进行 I/O 操作。 边缘触发（Edge Trigger，ET）：只有在文件描述符的状态发生改变（也就是 I/O 请求达到）时，才发送一次通知。这时候，应用程序需要尽可能多地执行 I/O，直到无法继续读写，才可以停止。如果 I/O 没执行完，或者因为某种原因没来得及处理，那么这次通知也就丢失了。 区别： 对于水平触发模式，一个事件只要有，就会一直触发； 对于边缘触发模式，只有一个事件从无到有才会触发。 这两个词汇来自电学术语，你可以将 fd 上有数据认为是高电平，没有数据认为是低电平，将 fd 可写认为是高电平，fd 不可写认为是低电平。那么水平模式的触发条件是状态处于高电平，而边缘模式的触发条件是新来一次电信号将当前状态变为高电平，即： 水平模式的触发条件：1. 低电平 =&gt; 高电平 2. 处于高电平状态 边缘模式的触发条件：1. 低电平 =&gt; 高电平 说的有点抽象，以 socket 的读事件为例，对于水平模式，只要 socket 上有未读完的数据，就会一直产生 EPOLLIN 事件；而对于边缘模式，socket 上每新来一次数据就会触发一次，如果上一次触发后，未将 socket 上的数据读完，也不会再触发，除非再新来一次数据。对于 socket 写事件，如果 socket 的 TCP 窗口一直不饱和，会一直触发 EPOLLOUT 事件；而对于边缘模式，只会触发一次，除非 TCP 窗口由不饱和变成饱和再一次变成不饱和，才会再次触发 EPOLLOUT 事件。 socket 可读事件水平模式触发条件：1. socket上无数据 =&gt; socket上有数据 2. socket处于有数据状态 socket 可读事件边缘模式触发条件：1. socket上无数据 =&gt; socket上有数据 2. socket又新来一次数据 socket 可写事件水平模式触发条件：1. socket可写 =&gt; socket可写 2. socket不可写 =&gt; socket可写 socket 可写事件边缘模式触发条件：1. socket不可写 =&gt; socket可写 I/O 多路复用的方法。这里其实有很多实现方法，我带你来逐个分析一下。 第一种，使用非阻塞 I/O 和水平触发通知，比如使用 select 或者 poll。 第二种，使用非阻塞 I/O 和边缘触发通知，比如 epoll。 epoll 使用红黑树，在内核中管理文件描述符的集合，这样，就不需要应用程序在每次操作时都传入、传出这个集合。 epoll 使用事件驱动的机制，只关注有 I/O 事件发生的文件描述符，不需要轮询扫描整个集合。 第三种，使用异步 I/O（Asynchronous I/O，简称为 AIO）。 异步 I/O 允许应用程序同时发起很多 I/O 操作，而不用等待这些操作完成。而在 I/O 完成后，系统会用事件通知（比如信号或者回调函数）的方式，告诉应用程序。这时，应用程序才会去查询 I/O 操作的结果。 工作模型优化第一种，主进程 + 多个 worker 子进程，这也是最常用的一种模型。 主进程执行 bind() + listen() 后，创建多个子进程； 然后，在每个子进程中，都通过 accept() 或 epoll_wait() ，来处理相同的套接字。 最常用的反向代理服务器 Nginx 就是这么工作的。它也是由主进程和多个 worker 进程组成。主进程主要用来初始化套接字，并管理子进程的生命周期；而 worker 进程，则负责实际的请求处理。我画了一张图来表示这个关系。 为了避免惊群问题， Nginx 在每个 worker 进程中，都增加一个了全局锁（accept_mutex）。这些 worker 进程需要首先竞争到锁，只有竞争到锁的进程，才会加入到 epoll 中，这样就确保只有一个 worker 子进程被唤醒。 第二种，监听到相同端口的多进程模型。 在这种方式下，所有的进程都监听相同的接口，并且开启 SO_REUSEPORT 选项，由内核负责将请求负载均衡到这些监听进程中去。这一过程如下图所示。 C1000K从 C10K 到 C100K ，可能只需要增加系统的物理资源就可以满足；但从 C100K 到 C1000K ，就不仅仅是增加物理资源就能解决的问题了。这时，就需要多方面的优化工作了，从硬件的中断处理和网络功能卸载、到网络协议栈的文件描述符数量、连接状态跟踪、缓存队列等内核的优化，再到应用程序的工作模型优化，都是考虑的重点。 C10M要实现 C10M ，就不只是增加物理资源，或者优化内核和应用程序可以解决的问题了。这时候，就需要用 XDP 的方式，在内核协议栈之前处理网络包；或者用 DPDK 直接跳过网络协议栈，在用户空间通过轮询的方式直接处理网络包。 测试网络基准测试根据前面学过的 TCP/IP 协议栈的原理，这个问题应该不难回答。比如： 基于 HTTP 或者 HTTPS 的 Web 应用程序，显然属于应用层，需要我们测试 HTTP/HTTPS 的性能； 而对大多数游戏服务器来说，为了支持更大的同时在线人数，通常会基于 TCP 或 UDP ，与客户端进行交互，这时就需要我们测试 TCP/UDP 的性能； 当然，还有一些场景，是把 Linux 作为一个软交换机或者路由器来用的。这种情况下，你更关注网络包的处理能力（即 PPS），重点关注网络层的转发性能。 转发性能36 hping3 TCP/UDP 性能iperf 或者 netperf。iperf 和 netperf 都是最常用的网络性能测试工具，测试 TCP 和 UDP 的吞吐量。它们都以客户端和服务器通信的方式，测试一段时间内的平均吞吐量。 12345678910111213141516在目标机器上启动 iperf 服务端：# -s表示启动服务端，-i表示汇报间隔，-p表示监听端口$ iperf3 -s -i 1 -p 10000在另一台机器上运行 iperf 客户端，运行测试：# -c表示启动客户端，192.168.0.30为目标服务器的IP# -b表示目标带宽(单位是bits/s)# -t表示测试时间# -P表示并发数，-p表示目标服务器监听端口$ iperf3 -c 192.168.0.30 -b 1G -t 15 -P 2 -p 10000稍等一会儿（15 秒）测试结束后，回到目标服务器，查看 iperf 的报告：[ ID] Interval Transfer Bandwidth...[SUM] 0.00-15.04 sec 0.00 Bytes 0.00 bits/sec sender[SUM] 0.00-15.04 sec 1.51 GBytes 860 Mbits/sec receiver HTTP 性能要测试 HTTP 的性能，也有大量的工具可以使用，比如 ab、webbench 等，都是常用的 HTTP 压力测试工具。其中，ab 是 Apache 自带的 HTTP 压测工具，主要测试 HTTP 服务的每秒请求数、请求延迟、吞吐量以及请求延迟的分布情况等。 123456789101112131415161718192021222324252627282930313233343536# Ubuntu$ apt-get install -y apache2-utils# CentOS$ yum install -y httpd-tools# -c表示并发请求数为1000，-n表示总的请求数为10000$ ab -c 1000 -n 10000 http://192.168.0.30/...Server Software: nginx/1.15.8Server Hostname: 192.168.0.30Server Port: 80...Requests per second: 1078.54 [#/sec] (mean)Time per request: 927.183 [ms] (mean)Time per request: 0.927 [ms] (mean, across all concurrent requests)Transfer rate: 890.00 [Kbytes/sec] receivedConnection Times (ms) min mean[+/-sd] median maxConnect: 0 27 152.1 1 1038Processing: 9 207 843.0 22 9242Waiting: 8 207 843.0 22 9242Total: 15 233 857.7 23 9268Percentage of the requests served within a certain time (ms)50% 2366% 2475% 2480% 2690% 27495% 119598% 233599% 4663100% 9268 (longest request) 应用负载性能在应用层，我们关注的是应用程序的并发连接数、每秒请求数、处理延迟、错误数等，可以使用 wrk、JMeter 等工具，模拟用户的负载，得到想要的测试结果。 tcpdump 和 Wiresharktcpdump12# CentOSyum install -y tcpdump wireshark 123456789$ tcpdump -nn udp port 53 or host 35.190.27.188$ tcpdump -nn udp port 53 or host 35.190.27.188 -w ping.pcap-nn ，表示不解析抓包中的域名（即不反向解析）、协议以及端口号。udp port 53 ，表示只显示 UDP 协议的端口号（包括源端口和目的端口）为 53 的包。host 35.190.27.188 ，表示只显示 IP 地址（包括源地址和目的地址）为 35.190.27.188 的包。这两个过滤条件中间的“ or ”，表示或的关系，也就是说，只要满足上面两个条件中的任一个，就可以展示出来。sudo tcpdump -Z zshield -X -nn &#x27;net 10.240.241.0/24 and port 18081&#x27; -w packet tcpdump 输出格式： 时间戳 协议 源地址.源端口 &gt; 目的地址.目的端口 网络包详细信息 wireshark 点击 Statistics -&gt; Flow Graph，然后，在弹出的界面中的 Flow type 选择 TCP Flows，你可以更清晰的看到，整个过程中 TCP 流的执行过程 1234567891011121314151617# --tcp表示使用TCP协议，-p表示端口号，-n表示不对结果中的IP地址执行反向域名解析$ traceroute --tcp -p 80 -n baidu.comtraceroute to baidu.com (123.125.115.110), 30 hops max, 60 byte packets 1 * * * 2 * * * 3 * * * 4 * * * 5 * * * 6 * * * 7 * * * 8 * * * 9 * * *10 * * *11 * * *12 * * *13 * * *14 123.125.115.110 20.684 ms * 20.798 ms traceroute 会在路由的每一跳发送三个包，并在收到响应后，输出往返延时。如果无响应或者响应超时 123456# 记录一会（比如30s）后按Ctrl+C结束$ perf record -a -g -- sleep 30# 输出报告$ perf report -g graph,0 工具根据指标找工具 根据工具查指标","categories":[],"tags":[]},{"title":"3.IO","slug":"3-IO","date":"2021-12-19T12:52:59.000Z","updated":"2021-12-21T12:45:11.267Z","comments":true,"path":"2021/12/19/3-IO/","link":"","permalink":"https://sk-xinye.github.io/2021/12/19/3-IO/","excerpt":"","text":"文件系统文件系统，本身是对存储设备上的文件，进行组织管理的机制。组织方式不同，就会形成不同的文件系统。 索引节点（index node）和目录项（directory entry）为了方便管理，Linux 文件系统为每个文件都分配两个数据结构，索引节点（index node）和目录项（directory entry）。它们主要用来记录文件的元信息和目录结构。 索引节点，简称为 inode，用来记录文件的元数据，比如 inode 编号、文件大小、访问权限、修改日期、数据的位置等。索引节点和文件一一对应，它跟文件内容一样，都会被持久化存储到磁盘中。所以记住，索引节点同样占用磁盘空间。 目录项，简称为 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的关联关系。多个关联的目录项，就构成了文件系统的目录结构。不过，不同于索引节点，目录项是由内核维护的一个内存数据结构，所以通常也被叫做目录项缓存。 换句话说，索引节点是每个文件的唯一标志，而目录项维护的正是文件系统的树状结构。目录项和索引节点的关系是多对一，你可以简单理解为，一个文件可以有多个别名。通过硬链接为文件创建的别名，就会对应不同的目录项，不过这些目录项本质上还是链接同一个文件，所以，它们的索引节点相同。 磁盘文件读写最小单位扇区 –为了增加效率将连续的8个扇区组成逻辑块4KB–&gt;逻辑快，操作的就是逻辑块 注意： 第一，目录项本身就是一个内存缓存，而索引节点则是存储在磁盘中的数据。在前面的 Buffer 和 Cache 原理中，我曾经提到过，为了协调慢速磁盘与快速 CPU 的性能差异，文件内容会缓存到页缓存 Cache 中。 第二，磁盘在执行文件系统格式化时，会被分成三个存储区域，超级块、索引节点区和数据块区。其中， 超级块，存储整个文件系统的状态。 索引节点区，用来存储索引节点。 数据块区，则用来存储文件数据 虚拟文件系统目录项、索引节点、逻辑块以及超级块，构成了 Linux 文件系统的四大基本要素。不过，为了支持各种不同的文件系统，Linux 内核在用户进程和文件系统的中间，又引入了一个抽象层，也就是虚拟文件系统 VFS（Virtual File System）。 分为3类： 第一类是基于磁盘的文件系统，也就是把数据直接存储在计算机本地挂载的磁盘中。常见的 Ext4、XFS、OverlayFS 等，都是这类文件系统。 第二类是基于内存的文件系统，也就是我们常说的虚拟文件系统。这类文件-系统，不需要任何磁盘分配存储空间，但会占用内存。我们经常用到的 /proc 文件系统，其实就是一种最常见的虚拟文件系统。此外，/sys 文件系统也属于这一类，主要向用户空间导出层次化的内核对象。 第三类是网络文件系统，也就是用来访问其他计算机数据的文件系统，比如 NFS、SMB、iSCSI 等。 这些文件系统，要先挂载到 VFS 目录树中的某个子目录（称为挂载点），然后才能访问其中的文件。拿第一类，也就是基于磁盘的文件系统为例，在安装系统时，要先挂载一个根目录（/），在根目录下再把其他文件系统（比如其他的磁盘分区、/proc 文件系统、/sys 文件系统、NFS 等）挂载进来。 文件系统 I/O文件读写方式的各种差异，导致 I/O 的分类多种多样。包括缓冲与非缓冲 I/O、直接与非直接 I/O、阻塞与非阻塞 I/O、同步与异步 I/O 等 缓冲与非缓冲 I/O（，根据是否利用标准库缓存，可以把文件 I/O 分为缓冲 I/O 与非缓冲 I/O。） 缓冲 I/O，是指利用标准库缓存来加速文件的访问，而标准库内部再通过系统调度访问文件。 非缓冲 I/O，是指直接通过系统调用来访问文件，不再经过标准库缓存。 直接与非直接 I/O（根据是否利用操作系统的页缓存，可以把文件 I/O 分为直接 I/O 与非直接 I/O） 直接 I/O，是指跳过操作系统的页缓存，直接跟文件系统交互来访问文件。 非直接 I/O 正好相反，文件读写时，先要经过系统的页缓存，然后再由内核或额外的系统调用，真正写入磁盘。 阻塞与非阻塞 I/O（根据应用程序是否阻塞自身运行，可以把文件 I/O 分为阻塞 I/O 和非阻塞 I/O） 所谓阻塞 I/O，是指应用程序执行 I/O 操作后，如果没有获得响应，就会阻塞当前线程，自然就不能执行其他任务。 所谓非阻塞 I/O，是指应用程序执行 I/O 操作后，不会阻塞当前的线程，可以继续执行其他的任务，随后再通过轮询或者事件通知的形式，获取调用的结果。 同步与异步 I/O (根据是否等待响应结果，可以把文件 I/O 分为同步和异步 I/O) 所谓同步 I/O，是指应用程序执行 I/O 操作后，要一直等到整个 I/O 完成后，才能获得 I/O 响应。 所谓异步 I/O，是指应用程序执行 I/O 操作后，不用等待完成和完成后的响应，而是继续执行就可以。等到这次 I/O 完成后，响应会用事件通知的方式，告诉应用程序。 容量12345678$ df -h /dev/sda1Filesystem Size Used Avail Use% Mounted on/dev/sda1 29G 3.1G 26G 11% /索引节点$ df -i /dev/sda1Filesystem Inodes IUsed IFree IUse% Mounted on/dev/sda1 3870720 157460 3713260 5% / 缓存1234567891011121314151617181920$ cat /proc/meminfo | grep -E &quot;SReclaimable|Cached&quot;Cached: 748316 kBSwapCached: 0 kBSReclaimable: 179508 kB在实际性能分析中，我们更常使用 slabtop ，来找到占用内存最多的缓存类型。# 按下c按照缓存大小排序，按下a按照活跃对象数排序$ slabtopActive / Total Objects (% used) : 277970 / 358914 (77.4%)Active / Total Slabs (% used) : 12414 / 12414 (100.0%)Active / Total Caches (% used) : 83 / 135 (61.5%)Active / Total Size (% used) : 57816.88K / 73307.70K (78.9%)Minimum / Average / Maximum Object : 0.01K / 0.20K / 22.88KOBJS ACTIVE USE OBJ SIZE SLABS OBJ/SLAB CACHE SIZE NAME69804 23094 0% 0.19K 3324 21 13296K dentry16380 15854 0% 0.59K 1260 13 10080K inode_cache58260 55397 0% 0.13K 1942 30 7768K kernfs_node_cache485 413 0% 5.69K 97 5 3104K task_struct1472 1397 0% 2.00K 92 16 2944K kmalloc-2048 磁盘由文件系统层、通用块层和设备层构成的 Linux 存储系统 I/O 栈。其中，通用块层是 Linux 磁盘 I/O 的核心。向上，它为文件系统和应用程序，提供访问了块设备的标准接口；向下，把各种异构的磁盘设备，抽象为统一的块设备，并会对文件系统和应用程序发来的 I/O 请求进行重新排序、请求合并等，提高了磁盘访问的效率。 磁盘性能指标说到磁盘性能的衡量标准，必须要提到五个常见指标，也就是我们经常用到的，使用率、饱和度、IOPS、吞吐量以及响应时间等。这五个指标，是衡量磁盘性能的基本指标。 使用率，是指磁盘处理 I/O 的时间百分比。过高的使用率（比如超过 80%），通常意味着磁盘 I/O 存在性能瓶颈。 饱和度，是指磁盘处理 I/O 的繁忙程度。过高的饱和度，意味着磁盘存在严重的性能瓶颈。当饱和度为 100% 时，磁盘无法接受新的 I/O 请求。 IOPS（Input/Output Per Second），是指每秒的 I/O 请求数。 吞吐量，是指每秒的 I/O 请求大小。 响应时间，是指 I/O 请求从发出到收到响应的间隔时间。 这里要注意的是，使用率只考虑有没有 I/O，而不考虑 I/O 的大小。换句话说，当使用率是 100% 的时候，磁盘依然有可能接受新的 I/O 请求。 在数据库、大量小文件等这类随机读写比较多的场景中，IOPS 更能反映系统的整体性能；而在多媒体等顺序读写较多的场景中，吞吐量才更能反映系统的整体性能。 磁盘 I/O 观测iostat 是最常用的磁盘 I/O 性能观测工具，它提供了每个磁盘的使用率、IOPS、吞吐量等各种常见的性能指标，当然，这些指标实际上来自 /proc/diskstats。 1234567# -d表示显示I/O性能指标，-x表示显示扩展统计（即所有I/O指标）$ iostat -d -x 1Device r/s w/s rkB/s wkB/s rrqm/s wrqm/s %rrqm %wrqm r_await w_await aqu-sz rareq-sz wareq-sz svctm %utilloop0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00loop1 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sda 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00sdb 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 %util ，就是我们前面提到的磁盘 I/O 使用率； r/s+ w/s ，就是 IOPS； rkB/s+wkB/s ，就是吞吐量； r_await+w_await ，就是响应时间. 进程 I/O 观测pidstat iptop 123$ pidstat -d 113:39:51 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command13:39:52 102 916 0.00 4.00 0.00 0 rsyslogd 用户 ID（UID）和进程 ID（PID） 。 每秒读取的数据大小（kB_rd/s） ，单位是 KB。 每秒发出的写请求数据大小（kB_wr/s） ，单位是 KB。 每秒取消的写请求数据大小（kB_ccwr/s） ，单位是 KB。 块 I/O 延迟（iodelay），包括等待同步块 I/O 和换入块 I/O 结束的时间，单位是时钟周期。 12345$ iotopTotal DISK READ : 0.00 B/s | Total DISK WRITE : 7.85 K/sActual DISK READ: 0.00 B/s | Actual DISK WRITE: 0.00 B/sTID PRIO USER DISK READ DISK WRITE SWAPIN IO&gt; COMMAND15055 be/3 root 0.00 B/s 7.85 K/s 0.00 % 0.00 % systemd-journald 从这个输出，你可以看到，前两行分别表示，进程的磁盘读写大小总数和磁盘真实的读写大小总数。因为缓存、缓冲区、I/O 合并等因素的影响，它们可能并不相等。 剩下的部分，则是从各个角度来分别表示进程的 I/O 情况，包括线程 ID、I/O 优先级、每秒读磁盘的大小、每秒写磁盘的大小、换入和等待 I/O 的时钟百分比等。 1234567891011121314分析系统调用时用的工具$ strace -p 18940strace: Process 18940 attached...mmap(NULL, 314576896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f0f7aee9000mmap(NULL, 314576896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f0f682e8000write(3, &quot;2018-12-05 15:23:01,709 - __main&quot;..., 314572844) = 314572844munmap(0x7f0f682e8000, 314576896) = 0write(3, &quot;\\n&quot;, 1) = 1munmap(0x7f0f7aee9000, 314576896) = 0close(3) = 0stat(&quot;/tmp/logtest.txt.1&quot;, &#123;st_mode=S_IFREG|0644, st_size=943718535, ...&#125;) = 0 123456789lsof。它专门用来查看进程打开文件列表，不过，这里的“文件”不只有普通文件，还包括了目录、块设备、动态库、网络套接字等。$ lsof -p 18940COMMAND PID USER FD TYPE DEVICE SIZE/OFF NODE NAMEpython 18940 root cwd DIR 0,50 4096 1549389 /python 18940 root rtd DIR 0,50 4096 1549389 /…python 18940 root 2u CHR 136,0 0t0 3 /dev/pts/0python 18940 root 3w REG 8,1 117944320 303 /tmp/logtest.txt FD 表示文件描述符号 TYPE 表示文件类型 NAME 表示文件路径 12345678案例27filetop。它是 bcc 软件包的一部分，基于 Linux 内核的 eBPF（extended Berkeley Packet Filters）机制，主要跟踪内核中文件的读写情况，并输出线程 ID（TID）、读写大小、读写类型以及文件名称。pensnoop 。它同属于 bcc 软件包，可以动态跟踪内核中的 open 系统调用。这样，我们就可以找出这些 txt 文件的路径。$ opensnoop12280 python 6 0 /tmp/9046db9e-fe25-11e8-b13f-0242ac110002/650.txt12280 python 6 0 /tmp/9046db9e-fe25-11e8-b13f-0242ac110002/651.txt12280 python 6 0 /tmp/9046db9e-fe25-11e8-b13f-0242ac110002/652.txt 用法性能指标 首先，最容易想到的是存储空间的使用情况，包括容量、使用量以及剩余空间等 容易忽略的是索引节点的使用情况，它也包括容量、使用量以及剩余量等三个指标。 其次，你应该想到的是前面多次提到过的缓存使用情况，包括页缓存、目录项缓存、索引节点缓存以及各个具体文件系统（如 ext4、XFS 等）的缓存。 根据指标找工具 根据工具查指标 配置 先用 iostat 发现磁盘 I/O 性能瓶颈； 再借助 pidstat ，定位出导致瓶颈的进程； 随后分析进程的 I/O 行为； 最后，结合应用程序的原理，分析这些 I/O 的来源。 所以，为了缩小排查范围，我通常会先运行那几个支持指标较多的工具，如 iostat、vmstat、pidstat 等。","categories":[],"tags":[]},{"title":"2.内存","slug":"2-内存","date":"2021-12-19T11:16:11.000Z","updated":"2023-02-04T02:33:07.929Z","comments":true,"path":"2021/12/19/2-内存/","link":"","permalink":"https://sk-xinye.github.io/2021/12/19/2-%E5%86%85%E5%AD%98/","excerpt":"","text":"内存内存映射只有内核可以访问物理内存，进程只能访问虚拟内存。 内存映射，其实就是将虚拟内存地址映射到物理内存地址。为了完成内存映射，内核为每个进程都维护了一张页表，记录虚拟地址与物理地址的映射关系，如下图所示： 页表实际上存储在 CPU 的内存管理单元 MMU 中，这样，正常情况下，处理器就可以直接通过硬件，找出要访问的内存。 而当进程访问的虚拟地址在页表中查不到时，系统会产生一个缺页异常，进入内核空间分配物理内存、更新进程页表，最后再返回用户空间，恢复进程的运行。 虚拟内存还通过多级页表和大页（HugePage）解决页表项过多的问题 虚拟内存空间分布 通过这张图你可以看到，用户空间内存，从低到高分别是五种不同的内存段。 只读段，包括代码和常量等。 数据段，包括全局变量等。 堆，包括动态分配的内存，从低地址开始向上增长。 文件映射段，包括动态库、共享内存等，从高地址开始向下增长。 栈，包括局部变量和函数调用的上下文等。栈的大小是固定的，一般是 8 MB。 在这五个内存段中，堆和文件映射段的内存是动态分配的。比如说，使用 C 标准库的 malloc() 或者 mmap() ，就可以分别在堆和文件映射段动态分配内存。 内存分配与回收malloc() 是 C 标准库提供的内存分配函数，对应到系统调用上，有两种实现方式，即 brk() 和 mmap()。当进程通过 malloc() 申请内存后，内存并不会立即分配，而是在首次访问时，才通过缺页异常陷入内核中分配内存。 对小块内存（小于 128K），C 标准库使用 brk() 来分配，也就是通过移动堆顶的位置来分配内存。这些内存释放后并不会立刻归还系统，而是被缓存起来，这样就可以重复使用。而大块内存（大于 128K），则直接使用内存映射 mmap() 来分配，也就是在文件映射段找一块空闲内存分配出去。 在发现内存紧张时，系统就会通过一系列机制来回收内存，比如下面这三种方式： 回收缓存，比如使用 LRU（Least Recently Used）算法，回收最近使用最少的内存页面； 回收不常访问的内存，把不常用的内存通过交换分区直接写到磁盘中； 杀死进程，内存紧张时系统还会通过 OOM（Out of Memory），直接杀掉占用大量内存的进程。 一个进程消耗的内存越大，oom_score 就越大； 一个进程运行占用的 CPU 越多，oom_score 就越小。 echo -16 &gt; /proc/$(pidof sshd)/oom_adj 如何查看内存使用情况free: 123456789101112131415# 注意不同版本的free输出可能会有所不同$ free total used free shared buff/cache availableMem: 8169348 263524 6875352 668 1030472 7611064Swap: 0 0 0第一列，total 是总内存大小；第二列，used 是已使用内存的大小，包含了共享内存；第三列，free 是未使用内存的大小；第四列，shared 是共享内存的大小；第五列，buff/cache 是缓存和缓冲区的大小；最后一列，available 是新进程可用内存的大小。pidstat -r -p 123123查看进程内存占用情况 包括rss 及vfs top: 123456789101112131415161718192021# 按下M切换到内存排序$ top...KiB Mem : 8169348 total, 6871440 free, 267096 used, 1030812 buff/cacheKiB Swap: 0 total, 0 free, 0 used. 7607492 avail MemPID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND430 root 19 -1 122360 35588 23748 S 0.0 0.4 0:32.17 systemd-journal1075 root 20 0 771860 22744 11368 S 0.0 0.3 0:38.89 snapd1048 root 20 0 170904 17292 9488 S 0.0 0.2 0:00.24 networkd-dispat 1 root 20 0 78020 9156 6644 S 0.0 0.1 0:22.92 systemd12376 azure 20 0 76632 7456 6420 S 0.0 0.1 0:00.01 systemd12374 root 20 0 107984 7312 6304 S 0.0 0.1 0:00.00 sshd...VIRT 是进程虚拟内存的大小，只要是进程申请过的内存，即便还没有真正分配物理内存，也会计算在内。RES 是常驻内存的大小，也就是进程实际使用的物理内存大小，但不包括 Swap 和共享内存。SHR 是共享内存的大小，比如与其他进程共同使用的共享内存、加载的动态链接库以及程序的代码段等。%MEM 是进程使用物理内存占系统总内存的百分比。 buffer 与cache Buffers 是对原始磁盘块的临时存储，也就是用来缓存磁盘的数据，通常不会特别大（20MB 左右）。这样，内核就可以把分散的写集中起来，统一优化磁盘的写入，比如可以把多次小的写合并成单次大的写等等。 Cached 是从磁盘读取文件的页缓存，也就是用来缓存从文件读取的数据。这样，下次访问这些文件数据时，就可以直接从内存中快速获取，而不需要再次访问缓慢的磁盘。 SReclaimable 是 Slab 的一部分。Slab 包括两部分，其中的可回收部分，用 SReclaimable 记录；而不可回收部分，用 SUnreclaim 记录。 slabtop 12345678# 清理文件页、目录项、Inodes等各种缓存$ echo 3 &gt; /proc/sys/vm/drop_caches# 首先清理缓存$ echo 3 &gt; /proc/sys/vm/drop_caches# 然后运行dd命令向磁盘分区/dev/sdb1写入2G数据$ dd if=/dev/urandom of=/dev/sdb1 bs=1M count=2048 1234567891011# 每隔1秒输出1组数据$ vmstat 1procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----r b swpd free buff cache si so bi bo in cs us sy id wa st0 0 0 7743608 1112 92168 0 0 0 0 52 152 0 1 100 0 00 0 0 7743608 1112 92168 0 0 0 0 36 92 0 0 100 0 0buff 和 cache 就是我们前面看到的 Buffers 和 Cache，单位是 KB。bi 和 bo 则分别表示块设备读取和写入的大小，单位为块 / 秒。因为 Linux 中块的大小是 1KB，所以这个单位也就等价于 KB/s。写文件时会用到 Cache 缓存数据，而写磁盘则会用到 Buffer 来缓存数据。所以，回到刚刚的问题，虽然文档上只提到，Cache 是文件读的缓存，但实际上，Cache 也会缓存写文件时的数据。 Buffer 是对磁盘数据的缓存，而 Cache 是文件数据的缓存，它们既会用在读请求中，也会用在写请求中。 工具cachestat 和 cachetop ，它们正是查看系统缓存命中情况的工具。这两个工具都是 bcc 软件包的一部分，它们基于 Linux 内核的 eBPF（extended Berkeley Packet Filters）机制，来跟踪内核中管理的缓存，并输出缓存的使用和命中情况。在17中 内存泄露分析工具（memleak 是 bcc 软件包中的一个工具）18 1memleak -a -p &lt;pid&gt; kmemoryleak 用法 根据指标找工具 根据工具查指标 分析流程 优化 最好禁止 Swap（swapoff -a）。如果必须开启 Swap，降低 swappiness 的值，减少内存回收时 Swap 的使用倾向。 减少内存的动态分配。比如，可以使用内存池、大页（HugePage）等。 尽量使用缓存和缓冲区来访问数据。比如，可以使用堆栈明确声明内存空间，来存储需要缓存的数据；或者用 Redis 这类的外部缓存组件，优化数据的访问。 使用 cgroups 等方式限制进程的内存使用情况。这样，可以确保系统内存不会被异常进程耗尽。 通过 /proc/pid/oom_adj ，调整核心应用的 oom_score。这样，可以保证即使内存紧张，核心应用也不会被 OOM 杀死。","categories":[],"tags":[]},{"title":"1.cpu性能","slug":"1-cpu性能","date":"2021-12-16T09:00:46.000Z","updated":"2023-02-04T02:33:07.928Z","comments":true,"path":"2021/12/16/1-cpu性能/","link":"","permalink":"https://sk-xinye.github.io/2021/12/16/1-cpu%E6%80%A7%E8%83%BD/","excerpt":"","text":"性能工具uptime123$ uptime02:34:03 up 2 days, 20:14, 1 user, load average: 0.63, 0.83, 0.88当前时间 系统运行时间 正在登陆用户 1分钟 5分钟 15分钟平均负载（load average） stress mpstat pidstat12345678910111213141516171819202122232425262728cpu 密集型排查$ stress --cpu 1 --timeout 600stress 是一个 Linux 系统压力测试工具，这里我们用作异常进程模拟平均负载升高的场景。sysstat 包含了常用的 Linux 性能工具，用来监控和分析系统的性能。我们的案例会用到这个包的两个命令 mpstat 和 pidstat。mpstat 是一个常用的多核 CPU 性能分析工具，用来实时查看每个 CPU 的性能指标，以及所有 CPU 的平均指标。pidstat 是一个常用的进程性能分析工具，用来实时查看进程的 CPU、内存、I/O 以及上下文切换等性能指标。# -d 参数表示高亮显示变化的区域$ watch -d uptime..., load average: 1.00, 0.75, 0.39# -P ALL 表示监控所有CPU，后面数字5表示间隔5秒后输出一组数据$ mpstat -P ALL 5Linux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU)13:30:06 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle13:30:11 all 50.05 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 49.9513:30:11 0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.0013:30:11 1 100.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00# 间隔5秒后输出一组数据$ pidstat -u 5 113:37:07 UID PID %usr %system %guest %wait %CPU CPU Command13:37:12 0 2962 100.00 0.00 0.00 0.00 100.00 1 stress从这里可以明显看到，stress 进程的 CPU 使用率为 100% 1234567891011121314151617181920212223I/O密集型排查$ stress -i 1 --timeout 600$ watch -d uptime..., load average: 1.06, 0.58, 0.37# 显示所有CPU的指标，并在间隔5秒输出一组数据$ mpstat -P ALL 5 1Linux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU)13:41:28 CPU %usr %nice %sys %iowait %irq %soft %steal %guest %gnice %idle13:41:33 all 0.21 0.00 12.07 32.67 0.00 0.21 0.00 0.00 0.00 54.8413:41:33 0 0.43 0.00 23.87 67.53 0.00 0.43 0.00 0.00 0.00 7.7413:41:33 1 0.00 0.00 0.81 0.20 0.00 0.00 0.00 0.00 0.00 98.99# 间隔5秒后输出一组数据，-u表示CPU指标$ pidstat -u 5 1 -dLinux 4.15.0 (ubuntu) 09/22/18 _x86_64_ (2 CPU)13:42:08 UID PID %usr %system %guest %wait %CPU CPU Command13:42:13 0 104 0.00 3.39 0.00 0.00 3.39 1 kworker/1:1H13:42:13 0 109 0.00 0.40 0.00 0.00 0.40 0 kworker/0:1H13:42:13 0 2997 2.00 35.53 0.00 3.99 37.52 1 stress13:42:13 0 3057 0.00 0.40 0.00 0.00 0.40 0 pidstat vmstat12345678910111213141516171819vmstat 是一个常用的系统性能分析工具，主要用来分析系统的内存使用情况，也常用来分析 CPU 上下文切换和中断的次数。# 每隔5秒输出1组数据$ vmstat 5procs -----------memory---------- ---swap-- -----io---- -system-- ------cpu-----r b swpd free buff cache si so bi bo in cs us sy id wa st0 0 0 7005360 91564 818900 0 0 0 0 25 33 0 0 100 0 0cs（context switch）是每秒上下文切换的次数。in（interrupt）则是每秒中断的次数。r（Running or Runnable）是就绪队列的长度，也就是正在运行和等待 CPU 的进程数。b（Blocked）则是处于不可中断睡眠状态的进程数。# 每隔5秒输出1组数据 加 -t显示线程$ pidstat -w 5Linux 4.15.0 (ubuntu) 09/23/18 _x86_64_ (2 CPU)08:18:26 UID PID cswch/s nvcswch/s Command08:18:31 0 1 0.20 0.00 systemd08:18:31 0 8 5.40 0.00 rcu_sched ab测试工具123456789# 并发100个请求测试Nginx性能，总共测试1000个请求$ ab -c 100 -n 1000 http://192.168.0.10:10000/This is ApacheBench, Version 2.3 &lt;$Revision: 1706008 $&gt;Copyright 1996 Adam Twiss, Zeus Technology Ltd,...Requests per second: 87.86 [#/sec] (mean)Time per request: 1138.229 [ms] (mean)... perf 工具12345678910111213141516171819202122232425perf 工具使用 见5 6# -g 开启调用关系，-p指定进程号pref top -g -p 1# 查看使用最多的函数perf top# 记录性能事件，等待大约15秒后按 Ctrl+C 退出$ perf record -g# 查看报告$ perf report短时进程# 按 Ctrl+C 结束$ execsnoopPCOMM PID PPID RET ARGSsh 30394 30393 0stress 30396 30394 0 /usr/local/bin/stress -t 1 -d 1sh 30398 30393 0stress 30399 30398 0 /usr/local/bin/stress -t 1 -d 1sh 30402 30400 0stress 30403 30402 0 /usr/local/bin/stress -t 1 -d 1sh 30405 30393 0stress 30407 30405 0 /usr/local/bin/stress -t 1 -d 1... pidstat1234567IO wait 较高时# -d 展示 I/O 统计数据，-p 指定进程号，间隔 1 秒输出 3 组数据$ pidstat -d -p 4344 1 306:38:50 UID PID kB_rd/s kB_wr/s kB_ccwr/s iodelay Command06:38:51 0 4344 0.00 0.00 0.00 0 app06:38:52 0 4344 0.00 0.00 0.00 0 app06:38:53 0 4344 0.00 0.00 0.00 0 app dstatdstat ，它的好处是，可以同时查看 CPU 和 I/O 这两种资源的使用情况，便于对比分析。 123456789101112131415161718# 间隔1秒输出10组数据$ dstat 1 10You did not select any stats, using -cdngy by default.--total-cpu-usage-- -dsk/total- -net/total- ---paging-- ---system--usr sys idl wai stl| read writ| recv send| in out | int csw 0 0 96 4 0|1219k 408k| 0 0 | 0 0 | 42 885 0 0 2 98 0| 34M 0 | 198B 790B| 0 0 | 42 138 0 0 0 100 0| 34M 0 | 66B 342B| 0 0 | 42 135 0 0 84 16 0|5633k 0 | 66B 342B| 0 0 | 52 177 0 3 39 58 0| 22M 0 | 66B 342B| 0 0 | 43 144 0 0 0 100 0| 34M 0 | 200B 450B| 0 0 | 46 147 0 0 2 98 0| 34M 0 | 66B 342B| 0 0 | 45 134 0 0 0 100 0| 34M 0 | 66B 342B| 0 0 | 39 131 0 0 83 17 0|5633k 0 | 66B 342B| 0 0 | 46 168 0 3 39 59 0| 22M 0 | 66B 342B| 0 0 | 37 134 可以看到每当 iowait 升高（wai）时，磁盘的读请求（read）都会很大。这说明 iowait 的升高跟磁盘的读请求有关，很可能就是磁盘读导致的。 sar tcpdump1234567891011121314sar 可以用来查看系统的网络收发情况，还有一个好处是，不仅可以观察网络收发的吞吐量（BPS，每秒收发的字节数），还可以观察网络收发的 PPS，即每秒收发的网络帧数。# -n DEV 表示显示网络收发的报告，间隔1秒输出一组数据$ sar -n DEV 115:03:46 IFACE rxpck/s txpck/s rxkB/s txkB/s rxcmp/s txcmp/s rxmcst/s %ifutil15:03:47 eth0 12607.00 6304.00 664.86 358.11 0.00 0.00 0.00 0.0115:03:47 docker0 6302.00 12604.00 270.79 664.66 0.00 0.00 0.00 0.0015:03:47 lo 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.0015:03:47 veth9f6bbcd 6302.00 12604.00 356.95 664.66 0.00 0.00 0.00 0.05、# -i eth0 只抓取eth0网卡，-n不解析协议名和主机名# tcp port 80表示只抓取tcp协议并且端口号为80的网络帧$ tcpdump -i eth0 -n tcp port 8015:11:32.678966 IP 192.168.0.2.18238 &gt; 192.168.0.30.80: Flags [S], seq 458303614, win 512, length 0... 名词解释平均负载：是指单位时间内，系统处于可运行状态和不可中断状态的平均进程数，也就是平均活跃进程数，它和 CPU 使用率并没有直接关系。可运行状态进程：是指正在使用 CPU 或者正在等待 CPU 的进程，也就是我们常用 ps 命令看到的，处于 R 状态（Running 或 Runnable）的进程。不可中断状态进程：是指正处于内核态关键流程中的进程，并且这些流程是不可打断的，比如最常见的是等待硬件设备的 I/O 响应，也就是我们在 ps 命令中看到的 D 状态（Uninterruptible Sleep，也称为 Disk Sleep）的进程。cpu上下文：CPU 寄存器，是 CPU 内置的容量小、但速度极快的内存。而程序计数器，则是用来存储 CPU 正在执行的指令位置、或者即将执行的下一条指令位置。它们都是 CPU 在运行任何任务前，必须的依赖环境CPU 上下文切换：就是先把前一个任务的 CPU 上下文（也就是 CPU 寄存器和程序计数器）保存起来，然后加载新任务的上下文到这些寄存器和程序计数器，最后再跳转到程序计数器所指的新位置，运行新任务。包括进程上下文切换、线程上下文切换以及中断上下文切换。一次系统调用的过程，其实是发生了两次 CPU 上下文切换，用户态到内核态，内核态到用户态 一次系统调用的过程，其实是发生了两次 CPU 上下文切换 指标平均负载对于应用负载最理想的，就是每个 CPU 上都刚好运行着一个进程 即对于2c的系统，load average 最好为2 首先你要知道系统有几个 CPU，这可以通过 top 命令或者从文件 /proc/cpuinfo 中读取，如下： 123# 关于grep和wc的用法请查询它们的手册或者网络搜索$ grep &#x27;model name&#x27; /proc/cpuinfo | wc -l2 当平均负载高于 CPU 数量 70% 的时候，你就应该分析排查负载高的问题了。一旦负载过高，就可能导致进程响应变慢，进而影响服务的正常功能。 平均负载与 CPU 使用率平均负载：不仅包括了正在使用 CPU 的进程，还包括等待 CPU 和等待 I/O 的进程。CPU 使用率: 是单位时间内 CPU 繁忙情况的统计，跟平均负载并不一定完全对应。比如: - CPU 密集型进程，使用大量 CPU 会导致平均负载升高，此时这两者是一致的； - I/O 密集型进程，等待 I/O 也会导致平均负载升高，但 CPU 使用率不一定很高； - 大量等待 CPU 的进程调度也会导致平均负载升高，此时的 CPU 使用率也会比较高。 进程上下文切换无法获取资源而导致的自愿上下文切换；被系统强制调度导致的非自愿上下文切换。 CPU 缓存的命中率 用法根据指标找工具 根据工具查指标 分析流程 CPU 优化应用程序优化编译器优化：很多编译器都会提供优化选项，适当开启它们，在编译阶段你就可以获得编译器的帮助，来提升性能。比如， gcc 就提供了优化选项 -O2，开启后会自动对应用程序的代码进行优化。算法优化：使用复杂度更低的算法，可以显著加快处理速度。比如，在数据比较大的情况下，可以用 O(nlogn) 的排序算法（如快排、归并排序等），代替 O(n^2) 的排序算法（如冒泡、插入排序等）。异步处理：使用异步处理，可以避免程序因为等待某个资源而一直阻塞，从而提升程序的并发处理能力。比如，把轮询替换为事件通知，就可以避免轮询耗费 CPU 的问题。多线程代替多进程：前面讲过，相对于进程的上下文切换，线程的上下文切换并不切换进程地址空间，因此可以降低上下文切换的成本。善用缓存：经常访问的数据或者计算过程中的步骤，可以放到内存中缓存起来，这样在下次用时就能直接从内存中获取，加快程序的处理速度。 系统优化CPU 绑定：把进程绑定到一个或者多个 CPU 上，可以提高 CPU 缓存的命中率，减少跨 CPU 调度带来的上下文切换问题。CPU 独占：跟 CPU 绑定类似，进一步将 CPU 分组，并通过 CPU 亲和性机制为其分配进程。这样，这些 CPU 就由指定的进程独占，换句话说，不允许其他进程再来使用这些 CPU。优先级调整：使用 nice 调整进程的优先级，正值调低优先级，负值调高优先级。优先级的数值含义前面我们提到过，忘了的话及时复习一下。在这里，适当降低非核心应用的优先级，增高核心应用的优先级，可以确保核心应用得到优先处理。为进程设置资源限制：使用 Linux cgroups 来设置进程的 CPU 使用上限，可以防止由于某个应用自身的问题，而耗尽系统资源。NUMA（Non-Uniform Memory Access）优化：支持 NUMA 的处理器会被划分为多个 node，每个 node 都有自己的本地内存空间。NUMA 优化，其实就是让 CPU 尽可能只访问本地内存。中断负载均衡：无论是软中断还是硬中断，它们的中断处理程序都可能会耗费大量的 CPU。开启 irqbalance 服务或者配置 smp_affinity，就可以把中断处理过程自动负载均衡到多个 CPU 上。 补充判定 自愿上下文切换变多了，说明进程都在等待资源，有可能发生了 I/O 等其他问题； 非自愿上下文切换变多了，说明进程都在被强制调度，也就是都在争抢 CPU，说明 CPU 的确成了瓶颈； 中断次数变多了，说明 CPU 被中断处理程序占用，还需要通过查看 /proc/interrupts 文件来分析具体的中断类型。 mpstat pidstat vmstat 联合判断是什么的性能瓶颈","categories":[],"tags":[]},{"title":"0.性能概览","slug":"0-性能概览","date":"2021-12-16T08:42:27.000Z","updated":"2021-12-19T07:12:26.863Z","comments":true,"path":"2021/12/16/0-性能概览/","link":"","permalink":"https://sk-xinye.github.io/2021/12/16/0-%E6%80%A7%E8%83%BD%E6%A6%82%E8%A7%88/","excerpt":"","text":"性能指标分析 选择指标评估应用程序和系统的性能 为应用程序和系统设置性能目标 进行性能基准测试 性能分析定位瓶颈 优化系统和应用程序 性能监控和告警 linux 性能工具谱图 linux 性能优化脑图","categories":[],"tags":[]},{"title":"1.初始设计","slug":"1-初始设计","date":"2021-11-16T05:51:48.000Z","updated":"2021-12-19T07:12:26.884Z","comments":true,"path":"2021/11/16/1-初始设计/","link":"","permalink":"https://sk-xinye.github.io/2021/11/16/1-%E5%88%9D%E5%A7%8B%E8%AE%BE%E8%AE%A1/","excerpt":"","text":"设计本次重构要解决问题功能性问题： 特定节点上下线指定服务 集群状态分发接口（目前第一次接入原有集群需要手动填写集群状态文件） 滚动升级 代码问题： 硬编码问题（提取到配置文件中） 各服务操作耦合问题（需重新定义接口，减少耦合，增加复用性） 模块边界问题（需重新定义模块） 日志问题 保存的元数据信息 当前集群状态{“kafka”:[ip1,ip2],”es”:[ip1,ip2,ip3],”current_ip”:”ip1”} 任务{“add_node”:{“kafka”:[“ip1”]},”status”:”finash”} 接口 增加节点 接收到命令{“kafka”:[ip1,ip2,ip3],”add_node”:{“kafka”:[ip3]},”current”：”ip”}1.1 通过判断不是增加的节点，修改：服务配置文件，记录的两个文件1.2 如果是增加节点，修改：修改上述3种文件并重启 减少节点 接收到命令{“kafka”:[ip1,ip2],”remove_node”:{“kafka”:[ip3]},”current_ip”:”ip”} 2.1比较参数，如果不是减少节点，修改：服务配置文件，记录的两个文件 2.2如果是减少节点，修改上述3种文件并重启 模块设计 日志模块：用已有的 通信模块（api）：用已有的 manager模块：重新设计 配置文件模块：重新设计 做实际处理模块：修改 打包部署模块：重新设计 manager 模块设计负责让不同节点启动不同服务 接收到的请求：{“cluster”:{“kafka”:[ip2],”zookeeper”:[ip1,ip2],”es”:[ip1,ip2]},”add_ip”:{“kafka”:[“ip2”]}} all_ip = [] 数据目录\\192.168.100.100\\pbu\\失准测试部\\发布版本\\新init包\\192.168.100.100\\pbu\\失准测试部\\发布版本\\A-7-1 失准项目（项目经理：帅金荣）\\1-失准二期版本\\V2.0.1版本\\v2.0.1全量包（包含性能优化）","categories":[],"tags":[]},{"title":"1.并发使用","slug":"1-并发使用","date":"2021-11-16T02:30:53.000Z","updated":"2021-12-19T07:12:26.879Z","comments":true,"path":"2021/11/16/1-并发使用/","link":"","permalink":"https://sk-xinye.github.io/2021/11/16/1-%E5%B9%B6%E5%8F%91%E4%BD%BF%E7%94%A8/","excerpt":"","text":"多线程需要解决问题 自动调度线程 主线程可以获取某一个线程（或者任务的）的状态，以及返回值。 当一个线程完成的时候，主线程能够立即知道。 让多线程和多进程的编码接口一致 简单使用1234567891011121314151617181920212223242526272829from concurrent.futures import ThreadPoolExecutorimport time# 参数times用来模拟网络请求的时间def get_html(times): time.sleep(times) print(&quot;get page &#123;&#125;s finished&quot;.format(times)) return timesexecutor = ThreadPoolExecutor(max_workers=2)# 通过submit函数提交执行的函数到线程池中，submit函数立即返回，不阻塞task1 = executor.submit(get_html, (3))task2 = executor.submit(get_html, (2))# done方法用于判定某个任务是否完成print(task1.done())# cancel方法用于取消某个任务,该任务没有放入线程池中才能取消成功print(task2.cancel())time.sleep(4)print(task1.done())# result方法可以获取task的执行结果print(task1.result())# 执行结果# False # 表明task1未执行完成# False # 表明task2取消失败，因为已经放入了线程池中# get page 2s finished# get page 3s finished# True # 由于在get page 3s finished之后才打印，所以此时task1必然完成了# 3 # 得到task1的任务返回值 ThreadPoolExecutor构造实例的时候，传入max_workers参数来设置线程池中最多能同时运行的线程数目。 使用submit函数来提交线程需要执行的任务（函数名和参数）到线程池中，并返回该任务的句柄（类似于文件、画图），注意submit()不是阻塞的，而是立即返回。 通过submit函数返回的任务句柄，能够使用done()方法判断该任务是否结束。上面的例子可以看出，由于任务有2s的延时，在task1提交后立刻判断，task1还未完成，- 而在延时4s之后判断，task1就完成了。 使用cancel()方法可以取消提交的任务，如果任务已经在线程池中运行了，就取消不了。这个例子中，线程池的大小设置为2，任务已经在运行了，所以取消失败。如果- 改变线程池的大小为1，那么先提交的是task1，task2还在排队等候，这是时候就可以成功取消。 使用result()方法可以获取任务的返回值。查看内部代码，发现这个方法是阻塞的。 as_completed上面虽然提供了判断任务是否结束的方法，但是不能在主线程中一直判断啊。有时候我们是得知某个任务结束了，就去获取结果，而不是一直判断每个任务有没有结束。这是就可以使用as_completed方法一次取出所有任务的结果。 123456789101112131415161718192021222324from concurrent.futures import ThreadPoolExecutor, as_completed, all_complete waitimport time# 参数times用来模拟网络请求的时间def get_html(times): time.sleep(times) print(&quot;get page &#123;&#125;s finished&quot;.format(times)) return timesexecutor = ThreadPoolExecutor(max_workers=2)urls = [3, 2, 4] # 并不是真的urlall_task = [executor.submit(get_html, (url)) for url in urls]for future in as_completed(all_task): data = future.result() print(&quot;in main: get page &#123;&#125;s success&quot;.format(data))# 执行结果# get page 2s finished# in main: get page 2s success# get page 3s finished# in main: get page 3s success# get page 4s finished# in main: get page 4s success as_completed()方法是一个生成器，在没有任务完成的时候，会阻塞，在有某个任务完成的时候，会yield这个任务，就能执行for循环下面的语句，然后继续阻塞住，循环到所有的任务结束。从结果也可以看出，先完成的任务会先通知主线程。 map除了上面的as_completed方法，还可以使用executor.map方法，但是有一点不同。 123456789101112131415161718192021from concurrent.futures import ThreadPoolExecutorimport time# 参数times用来模拟网络请求的时间def get_html(times): time.sleep(times) print(&quot;get page &#123;&#125;s finished&quot;.format(times)) return timesexecutor = ThreadPoolExecutor(max_workers=2)urls = [3, 2, 4] # 并不是真的urlfor data in executor.map(get_html, urls): print(&quot;in main: get page &#123;&#125;s success&quot;.format(data))# 执行结果# get page 2s finished# get page 3s finished# in main: get page 3s success# in main: get page 2s success# get page 4s finished# in main: get page 4s success 使用map方法，无需提前使用submit方法，map方法与python标准库中的map含义相同，都是将序列中的每个元素都执行同一个函数。上面的代码就是对urls的每个元素都执行get_html函数，并分配各线程池。可以看到执行结果与上面的as_completed方法的结果不同，输出顺序和urls列表的顺序相同，就算2s的任务先执行完成，也会先打印出3s的任务先完成，再打印2s的任务完成 waitwait方法可以让主线程阻塞，直到满足设定的要求。 12345678910111213141516171819from concurrent.futures import ThreadPoolExecutor, wait, ALL_COMPLETED, FIRST_COMPLETEDimport time# 参数times用来模拟网络请求的时间def get_html(times): time.sleep(times) print(&quot;get page &#123;&#125;s finished&quot;.format(times)) return timesexecutor = ThreadPoolExecutor(max_workers=2)urls = [3, 2, 4] # 并不是真的urlall_task = [executor.submit(get_html, (url)) for url in urls]wait(all_task, return_when=ALL_COMPLETED)print(&quot;main&quot;)# 执行结果# get page 2s finished# get page 3s finished# get page 4s finished# main wait方法接收3个参数，等待的任务序列、超时时间以及等待条件。等待条件return_when默认为ALL_COMPLETED，表明要等待所有的任务都结束。可以看到运行结果中，确实是所有任务都完成了，主线程才打印出main。等待条件还可以设置为FIRST_COMPLETED，表示第一个任务完成就停止等待。 多进程普通使用12345678910111213141516171819import multiprocessingimport timedef get_html(n): time.sleep(n) print(&quot;sub_progress success&quot;) return nif __name__ == &quot;__main__&quot;: #注意，在windows下这句话必须加！！！ progress = multiprocessing.Process(target=get_html, args=(2,)) print(progress.pid) progress.start() # 启动子进程 print(progress.pid) # 打印子进程的PID progress.join() # 等待子进程结束 print(&quot;main progress end&quot;)# 执行结果# None# 5444# sub_progress success# main progress end multiprocessing.Process()传入执行入口函数和参数。 start()方法启动子进程。 progress.pid属性可以获取子进程的ID号，在未启动前，未None，启动后，得到系统分配的ID号。 join()方法等待子进程结束。 进程池1multiprocessing模块中提供Pool类来支持进程池编程。 1234567891011121314151617import multiprocessingimport timedef get_html(n): time.sleep(n) print(&quot;sub_progress success&quot;) return nif __name__ == &quot;__main__&quot;: #注意，在windows下这句话必须加！！！ pool = multiprocessing.Pool(multiprocessing.cpu_count()) result = pool.apply_async(get_html, args=(3,)) pool.close() # 在join前必须运行close() pool.join() print(result.get()) # 得到子进程的返回值# 执行结果 # sub_progress success# 3 使用multiprocessing.Pool()来创建进程池，可以设置进程池的大小，默认为CPU的核心个数，一般进程也不会超过CPU的核心个数，超过了效率也不会高。 apply_async方法传入执行入口函数和参数，传入之后，就进入了子进程等待队列中，等待执行。返回值是ApplyResult对象，存储子进程的结果。 join()方法等待所有子进程的结束，调用前必须使用close()方法。 ApplyResult对象的get()方法，获取子进程的返回值。 multiprocessing模块同样提供了类似于map的方法： 12345678910111213141516171819import multiprocessingimport timedef get_html(n): time.sleep(n) print(&quot;&#123;&#125;s sub_progress success&quot;.format(n)) return nif __name__ == &quot;__main__&quot;: #注意，在windows下这句话必须加！！！ pool = multiprocessing.Pool(multiprocessing.cpu_count()) # for result in pool.imap_unordered(get_html, [1, 3, 2]): for result in pool.imap(get_html, [1, 3, 2]): print(&quot;&#123;&#125;s sleep success&quot;.format(result))# 执行结果# 1s sub_progress success# 1s sleep success# 2s sub_progress success# 3s sub_progress success# 3s sleep success# 2s sleep success imap方法，结果输出顺序与传入的可迭代对象的顺序相同。 imap_unordered方法，结果输出顺序与子进程结束的时间顺序相同。 进程池2进程池的更好实现虽然multiprocessing模块中提供Pool类，但是使用进程池的时候，使用concurrent.futures模块中的ProcessPoolExecutor类更加方便。在介绍线程池的时候，介绍了ThreadPoolExecutor的使用，而ProcessPoolExecutor的接口实现和ThreadPoolExecutor完全一样，只用多创建多线程改完创建多进程即可。值得注意的是：在windows下进行多进程编程的时候，主程序一定要加if name == “main“:。 在CPU密集型操作时，使用多进程可以加快速度。多进程API与多线程API基本相同，在使用线程池的时候，可以使用multiprocessing.Pool，也可以使用Future模块下的ProcessPoolExecutor，推荐后者。","categories":[],"tags":[]},{"title":"100.red问题排查","slug":"100-red问题排查","date":"2021-11-08T02:59:16.000Z","updated":"2021-12-19T07:12:26.857Z","comments":true,"path":"2021/11/08/100-red问题排查/","link":"","permalink":"https://sk-xinye.github.io/2021/11/08/100-red%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/","excerpt":"","text":"red 问题排查思路集群状态排查curl -uelastic:pwd -XGET “http://localhost:9200/_cluster/health?pretty&quot; active_shards 是涵盖了所有索引的所有分片的汇总值，其中包括副本分片。 relocating_shards 显示当前正在从一个节点迁往其他节点的分片的数量。通常来说应该是 0，不过在 Elasticsearch 发现集群不太均衡时，该值会上涨。比如说：添加了一个新节点，或者下线了一个节点。 initializing_shards 显示的是刚刚创建的分片的个数。比如，当你刚创建第一个索引，分片都会短暂的处于 initializing 状态，分片不应该长期停留在 initializing 状态。你还可能在节点刚重启的时候看到initializing 分片：当分片从磁盘上加载后，它们会从 initializing 状态开始。所以这一般是临时状态。 unassigned_shards 是已经在集群状态中存在的分片，但是实际在集群里又找不着。最常见的体现在副本上。比如，我有两个es节点，索引设置分片数量为 10， 3 副本，那么在集群上，由于灾备原则，主分片和其对应副本不能同时在一个节点上,es无法找到其他节点来存放第三个副本的分片，所以就会有 10 个未分配副本分片。如果你的集群是 red 状态，也会长期保有未分配分片（因为缺少主分片）。 上面说了一种造成 unassigned_shards的原因，就是副本太多，节点太少，es无法完成分片。 举一反三！由于索引的副本是可以动态修改的，那么，如果在修改时分配的副本数大于节点数目，那么肯定会有分片是这个状态。 目前集群爆红，但是所有节点都还在，有点诡异，从集群状态看，一共是两个分片有问题，一个正在初始化，一个是unassigned。确定了故障范围后，我们再来从索引层面、分片层面深入的分析具体原因把。 索引层面分析curl -uelastic:pwd -XGET “http://localhost:9200/_cluster/health?pretty&amp;level=indices&quot; 分片层面分析curl -uelastic:pwd -XGET “http://localhost:9200/_cluster/health?pretty&amp;level=shards&quot; 诊断分片未分配原因12345curl -uelastic:pwd -XGET &quot;http://localhost:9200/_cluster/allocation/explain&quot; -H&quot;Content-Type:application/json&quot; -d &#x27;&#123; &quot;index&quot;: &quot;B_2020-01-05&quot;, &quot;shard&quot;: 0, &quot;primary&quot;: true&#125;&#x27; 尝试集群reroute命令 curl -XPOST “http://ip:9200/_cluster/reroute?retry_failed=true&quot; 总结遇到集群Red时，我们可以从如下方法排查 集群层面：/_cluster/health。 索引层面：/_cluster/health?pretty&amp;level=indices。 分片层面：/_cluster/health?pretty&amp;level=shards。 看恢复情况：/_recovery?pretty 有unassigned分片的排查思路 _cluster/allocation/explain，先诊断。 /_cluster/reroute尝试重新分配 数据重放（最终解决方案） 先新建备份索引 12345678curl -XPUT &#x27;http://xxxx:9200/a_index_copy/&#x27; -d &#x27;&#123;“settings”:&#123; “index”:&#123; “number_of_shards”:3, “number_of_replicas”:2 &#125; &#125;&#125;&#x27; 通过reindex，将目前可用的数据导入 12345678910POST _reindex&#123;&quot;source&quot;: &#123; &quot;index&quot;: &quot;a_index&quot; &#125;, &quot;dest&quot;: &#123; &quot;index&quot;: &quot;a_index_copy&quot;, &quot;op_type&quot;: &quot;create&quot; &#125;&#125; 删除a_index索引，这个必须要先做，否则别名无法添加. curl -XDELETE ‘http://xxxx:9200/a_index&#39; 给a_index_copy添加别名a_index ```shellcurl -XPOST ‘http://xxxx:9200/_aliases&#39; -d ‘ { &quot;actions&quot;: [ &#123;&quot;add&quot;: &#123;&quot;index&quot;: &quot;a_index_copy&quot;, &quot;alias&quot;: &quot;a_index&quot;&#125;&#125; ] }’","categories":[],"tags":[]},{"title":"","slug":"index-1","date":"2021-09-20T02:09:33.224Z","updated":"2021-08-08T13:43:29.804Z","comments":true,"path":"2021/09/20/index-1/","link":"","permalink":"https://sk-xinye.github.io/2021/09/20/index-1/","excerpt":"","text":"镜像制作 docker基本操作 docker iamge build -t dr.z/test:aliun . docker tag IMAGEID(镜像id) REPOSITORY:TAG（仓库：标签） 修改标签，仓库名 docker rmi IMAGEID(镜像id) 删除镜像 docker rm container(容器id) 删除容器 docker save -o es.tar dr.z/elasticsearch/elasticsearch-oss:6.3.2 保存镜像 docker load &lt; /root/docker_result/images/es.tar 加载镜像 docker load -i zs_theft.xz 加载镜像 docker cp container(容器id):/opt/py ./ 拷贝容器内容到本地 docker cp /opt/test/file.txt container(容器id):/opt/py 拷贝本地到容器内容 docker commit container(容器id) centos-vim 保存容器为镜像 docker run -it IMAGEID(镜像id) /bin/bash 运行并进入镜像 保存镜像 docker save -o /home/dyufei/tensorflow.tar tensorflow/tensorflow 或者如下 docker save tensorflow/tensorflow &gt; /home/dyufei/tensorflow.tar 加载本地镜像 docker load -i tensorflow.tar export/import与 save/load区别 A ：export/import 是根据容器来导出镜像（因此没有镜像的历史记录）而 save/load 操作的对象是镜像 B ：export/import 镜像的历史记录再导后无法进行回滚操作，而save/load镜像有完整的历史记录可以回滚 docker export tensorboard &gt; /home/dyufei/tensorflow_tensorboard.tar 或者如下 docker export -o /home/dyufei/tensorflow_tensorboard.tar tensorboard 导入容器的镜像 sudo docker import - /home/dyufei/tensorflow_tensorboard.tar 将 pip 源修改为阿里云源 pip config list 查看当前 pip 的配置 接着修改配置文件 shell pip config set global.index-url http://mirrors.aliyun.com/pypi/simple/ pip config set install.trusted-host mirrors.aliyun.com 删除镜像失败 docker rmi $(docker images --filter \"dangling=true\" -q --no-trunc) docker rmi images linux 操作 rm -rf ./test_chk_ln 删除软连接 ln -s 源文件 软链接","categories":[],"tags":[]},{"title":"Page_Cache_零拷贝_顺序读写_堆外内存","slug":"Page-Cache-零拷贝-顺序读写-堆外内存","date":"2021-09-15T07:17:03.000Z","updated":"2021-09-20T02:09:06.531Z","comments":true,"path":"2021/09/15/Page-Cache-零拷贝-顺序读写-堆外内存/","link":"","permalink":"https://sk-xinye.github.io/2021/09/15/Page-Cache-%E9%9B%B6%E6%8B%B7%E8%B4%9D-%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98/","excerpt":"","text":"在学习零拷贝等NIO技术之前，我们需要先知道什么是DMA。DMA(Direct Memory Access,直接存储器访问)。在ＤＭＡ出现之前，CPU与外设之间的数据传送方式有程序传送方式、中断传送方式。CPU是通过系统总线与其他部件连接并进行数据传输。不管何种传送方式，都要消耗CPU，间接影响了其他任务的执行。 DMA原理DMA的出现就是为了解决批量数据的输入/输出问题。DMA是指外部设备不通过CPU而直接与系统内存交换数据的接口技术。类比显卡，也是从CPU中剥离出来的功能。将这些特殊的模块进行剥离，使得CPU可以更加专注于计算工作。通常系统总线是由CPU管理的，在DMA方式时，就希望CPU把这些总线让出来而由DMA控制器接管，控制传送的字节数，判断DMA是否结束，以及发出DMA结束信号。因此DMA控制器必须有以下功能: 能向CPU发出系统保持(HOLD)信号，提出总线接管请求； 当CPU发出允许接管信号后，对总线的控制由DMA接管; 能对存储器寻址及能修改地址指针，实现对内存的读写； 能决定本次DMA传送的字节数，判断DMA传送是否借宿。 发出DMA结束信号，使CPU恢复正常工作状态。 pagecache文件从应用程序的角度看，操作系统提供了一个统一的虚拟机，在该虚拟机中没有各种机器的具体细节，只有进程、文件、地址空间以及进程间通信等逻辑概念。这种抽象虚拟机使得应用程序的开发变得相对容易。对于存储设备上的数据，操作系统向应用程序提供的逻辑概念就是”文件”。应用程序要存储或访问数据时，只需读或者写”文件”的一维地址空间即可，而这个地址空间与存储设备上存储块之间的对应关系则由操作系统维护。说白了，文件就是基于内核态Page Cache的一层抽象，下文有详细介绍。 Page Cache的作用 中描述了 Linux 操作系统中文件 Cache 管理与内存管理以及文件系统的关系示意图。从图中可以看到，在 Linux 中，具体文件系统，如 ext2/ext3、jfs、ntfs 等，负责在文件 Cache和存储设备之间交换数据，位于具体文件系统之上的虚拟文件系统VFS负责在应用程序和文件 Cache 之间通过 read/write 等接口交换数据，而内存管理系统负责文件 Cache 的分配和回收，同时虚拟内存管理系统(VMM)则允许应用程序和文件 Cache 之间通过 memory map的方式交换数据。可见，在 Linux 系统中，文件 Cache 是内存管理系统、文件系统以及应用程序之间的一个联系枢纽。 Page Cache相关的数据结构每一个 Page Cache 包含若干 Buffer Cache。 内存管理系统与Page Cache交互，负责维护每项 Page Cache 的分配和回收，同时在使用 memory map 方式访问时负责建立映射； VFS 与Page Cache交互，负责 Page Cache 与用户空间的数据交换，即文件读写； 具体文件系统则一般只与 Buffer Cache 交互，它们负责在外围存储设备和 Buffer Cache 之间交换数据。 假定了 Page 的大小是 4K，则文件的每个4K的数据块最多只能对应一个 Page Cache 项，它通过一个是 radix tree来管理文件块和page cache的映射关系，Radix tree 是一种搜索树，Linux 内核利用这个数据结构来通过文件内偏移快速定位 Cache 项。 零拷贝Linux内核中与Page Cache操作相关的API有很多，按其使用方式可以分成两类： 类是以拷贝方式操作的相关接口， 如read/write/sendfile等； 另一类是以地址映射方式操作的相关接口，如mmap。 其中sendfile和mmap都是零拷贝的实现方案。 我们经常听说Kafka和RocketMQ等消息中间件有利用零拷贝技术来加速数据处理，提高吞吐量。所谓零拷贝，就是用户态与内核态的数据拷贝的次数为零 常规文件读写我们先看下正常文件读写所经历的阶段，即FileChannel#read，FileChannel#write，共涉及四次上下文切换（内核态和用户态的切换，包括read调用，read返回，write调用，write返回）和四次数据拷贝 mmapmmap 把文件映射到用户空间里的虚拟地址空间，实现文件和进程虚拟地址空间中一段虚拟地址的一一对映关系。 省去了从内核缓冲区复制到用户空间的过程，进程就可以采用指针的方式读写操作这一段内存（文件 / page cache），而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作。相反，内核空间对这段区域的修改也直接反映到用户空间，从而可以实现用户态和内核态对此内存区域的共享。 但在真正使用到这些数据前却不会消耗物理内存，也不会有读写磁盘的操作，只有真正使用这些数据时，虚拟内存管理系统 VMS 才根据缺页加载的机制从磁盘加载对应的数据块到内核态的Page Cache。这样的文件读写文件方式少了数据从内核缓存到用户空间的拷贝，效率很高。 概括而言，mmap有以下特点： 文件（page cache）直接映射到用户虚拟地址空间，内核态和用户态共享一片page cache，避免了一次数据拷贝 建立mmap之后，并不会立马加载数据到内存，只有真正使用数据时，才会引发缺页异常并加载数据到内存 memory map具体步骤如下： 首先，应用程序调用mmap（图中1） 陷入到内核中后调用do_mmap_pgoff（图中2）。该函数从应用程序的地址空间中分配一段区域作为映射的内存地址，并使用一个VMA（vm_area_struct）结构代表该区域，之后就返回到应用程序（图中3）。 当应用程序访问mmap所返回的地址指针时（图中4），由于虚实映射尚未建立，会触发缺页中断（图中5）。 之后系统会调用缺页中断处理函数（图中6），在缺页中断处理函数中，内核通过相应区域的VMA结构判断出该区域属于文件映射，于是调用具体文件系统的接口读入相应的Page Cache项（图中7、8、9），并填写相应的虚实映射表。 经过这些步骤之后，应用程序就可以正常访问相应的内存区域了。 sendfile从Linux 2.1版内核开始，Linux引入了sendfile，也能减少一次拷贝 这种方式避免了与用户空间进行交互，将四次拷贝减少到三次，内核态与用户态的切换从四次减少到两次。 在 Linux 内核 2.4 及后期版本中，针对套接字缓冲区描述符做了相应调整，DMA自带了收集功能，对于用户方面，用法还是一样。内部只把包含数据位置和长度信息的描述符追加到套接字缓冲区，DMA 引擎直接把数据从内核缓冲区传到协议引擎，从而消除了最后一次 CPU参与的拷贝动作。 顺序读写我们时常听到顺序读写比随机读写更高效的论断，那么什么是顺序读写？要想搞清楚顺序读写，我们首先要掌握文件的预读机制，它是一种将磁盘块预读到page cache的机制。 Linux内核中文件预读算法的具体过程是这样的： 对于每个文件的第一个读请求，系统读入所请求的页面并读入紧随其后的少数几个页面(不少于一个页面，通常是三个页面)，这时的预读称为同步预读。 对于第二次读请求，如果所读页面不在Cache中，即不在前次预读的group中，则表明文件访问不是顺序访问，系统继续采用同步预读； 如果所读页面在Cache中，则表明前次预读命中，操作系统把预读group扩大一倍，并让底层文件系统读入group中剩下尚不在Cache中的文件数据块，这时的预读称为异步预读。 无论第二次读请求是否命中，系统都要更新当前预读group的大小。此外，系统中定义了一个window，它包括前一次预读的group和本次预读的group。 任何接下来的读请求都会处于两种情况之一： 第一种情况是所请求的页面处于预读window中，这时继续进行异步预读并更新相应的window和group； 第二种情况是所请求的页面处于预读window之外，这时系统就要进行同步预读并重置相应的window和group。如下是Linux内核预读机制的一个示意图，其中a是某次读操作之前的情况，b是读操作所请求页面不在window中的情况，而c是读操作所请求页面在window中的情况。 图中group指一次读入page cached的集合；window包括前一次预读的group和本次预读的group；浅灰色代表要用户想要查找的page cache，深灰色代表命中的page。 以顺序读为例，当用户发起一个 fileChannel.read(4kb) 之后，实际发生了两件事 操作系统从磁盘加载了 16kb 进入 PageCache，这被称为预读 操作通从 PageCache 拷贝 4kb 进入用户内存 最终我们在用户内存访问到了 4kb，为什么顺序读快？很容量想到，当用户继续访问接下来的 [4kb,16kb] 的磁盘内容时，便是直接从 PageCache 去访问了。试想一下，当需要访问 16kb 的磁盘内容时，是发生 4 次磁盘 IO 快，还是发生 1 次磁盘 IO+4 次内存 IO 快呢？答案是显而易见的，这一切都是 PageCache 带来的优化。","categories":[],"tags":[]},{"title":"性能测试","slug":"性能测试","date":"2021-09-12T01:25:02.000Z","updated":"2021-09-13T00:29:56.563Z","comments":true,"path":"2021/09/12/性能测试/","link":"","permalink":"https://sk-xinye.github.io/2021/09/12/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/","excerpt":"","text":"elasticsearch esrally的使用esrally 是Elasticsearch 官方出的集群基础测试框架，使用python编写的。它的工作原理是：先下载需要测试数据集，然后在本地执行测试。由于网络原因每次下载都很慢，而且每次测试都会重新下载，我们采用离线测试的方式，先将数据下载下来。同时，以为它本身是支持Docker使用的，为了方便使用，我使用docker-compose的方式，并且制作了镜像。 数据和镜像准备镜像准备拉取官方镜像docker pull elastic/rally:latest 数据准备 拉取rally-tracks项目 12git clone https://github.com/elastic/rally-tracks.gitcd rally-tracks 下载测试数据，通过download下载或者直接找到数据直接下载 1234# 下载测试数据./download.sh geonames# 查看rally-tracks/geonames/tack.json,&quot;base-url&quot;+&quot;source-file&quot;就是数据地址http://benchmarks.elasticsearch.org.s3.amazonaws.com/corpora/geonames/documents-2.json.b2cd track.json docker-compose文件编写 在官方的Dockerfile中，使用的默认执行用户的1000，这里可能存在一些权限问题（创建uuid为1000的用户，并且把所有要映射的文件都改为该用户所属）。同时，官方建议是把/rally/.rally文件夹在本地进行映射，因为这些配置，以及数据集都是在该文件夹下的，如果不进行本地映射的话，不便于结果的保存及数据集的使用。而在映射了/rally/.rally文件夹后，有需要手动进行esrally configure,所以，直接调整了entrypoint.sh文件： 1234#!/usr/bin/env bashset -Eeo pipefailesrally configureexec &quot;&amp;@&quot; rally.yml 文件内容如下： –track=geonames,表示使用geonames数据集进行测试； –offline，表示离线使用，不去下载数据集； –target-hosts=:9200,表示需要测试的ES集群地址，端口为http端口 123456789version: &quot;3&quot;services: esrally: container_name: esrally image: dr.z/elastic/rally:latest volumns: - /opt/analytic_rally:/rally/.rally - /opt/entrypoint.sh:/entrypoint.sh command: &quot;esrally race --track=geonames --challenge=append-no-conflicts --pipeline=benchmark-only --target-hosts=192.168.13.132:9200&quot; 数据映射手动创建/opt/analytic_rally文件夹，启动镜像，使之初始化docker-compose -f rally.yml up,查看/opt/analytic_rally已经初始化了一些配置和文件。然后会报各种错误，一一解决这些错误就可以正常使用了： Expected a git repository at [/root/.rally/benchmarks/tracks/default] but the directory does not exist 这个错误很明显，我们只需要手动创建对应的文件夹就好了。 [/rally/.rally/benchmarks/tracks/default] must be a git repository.\\n\\nPlease run:\\ngit -C /rally/.rally/benchmarks/tracks/default init 这个错误是因为需要git目录，也已经给出了解决方案，不同的是我们是在myrally文件夹进行操作： 12345cd /opt/analytic_rally/benchmarks/tracks/defaultgit inittouch .gitignoregit add .git commit -m &quot;init default&quot; Could not load ‘/rally/.rally/benchmarks/tracks/default/geonames/track.json’.The complete track has been written to ‘tmp/tmpyadqlaqi.json’ for diagnos is .”,’(could not load track from &#39;track.json&#39;)’这个错误就需要我们拉取下来的rally-tracks 项目了 cp rally-tracks/geonames/ myrally/benchmarks/tracks/default/ -r cannot find /rally/.rally/benchmarks/data/geonames/documents-2.json.bz2. Please disable offilne mode and retry again. 这个错误也比较明显，这是，我们就可以直接手动下载数据集了 12mkdir /opt/analytic_rally/benchmarks/data/geonames/ -Pcp geonames/documents-2.jaon.bz2 myrally/benchmarks/data/geonames/ 到这里，就可以测试了 使用 冷热分离 es roll over单机单节点和单机多节点es 集群一般情况下为多无服务器的单ES节点组成的集群，或为单台服务器的多ES 节点组成的集群。本文探讨多台服务器的单es节点组成的集群与多台服务器多ES节点组成的集群进行性能对比，通过探针查询规则以及es堆内存进行比较，搭建monitoring with Diamond+influxDB+Grafana对CPU、RAM、load average、GC及es堆内存监控，也可通过Elasticsearch kopf插件对ES进行监控 基础工作： 配置：3200单机（cpu:40,内存：256，硬盘：4块HHD组成的raid5），每个节点es的heap都设置为31G 三台3200服务器各启动一个es节点组成三节点集群，所有节点均作为数据节点和master节点 三台3200服务器各启动3各es节点组成九节点集群，所有节点均作为数据节点和master节点（组成集群后需对数据进行负载均衡） 结果： 对于该场景，计算较多，适合使用单机多节点，无论是查询速度，es堆内存使用等，都占有优势","categories":[],"tags":[]},{"title":"性能优化","slug":"性能优化","date":"2021-09-09T06:35:09.000Z","updated":"2021-09-13T00:29:56.564Z","comments":true,"path":"2021/09/09/性能优化/","link":"","permalink":"https://sk-xinye.github.io/2021/09/09/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","excerpt":"","text":"写入速度优化在es的默认设置下，是综合考虑数据可靠性、搜索实时性、写入速度等因素的。当离开默认配置、追求极致的写入速度时，很多是以牺牲可靠性和搜索实时性为代价的。有时候，业务上对数据可靠性和搜索实时性要求并不高，反而对写入速度要求很高，此时可以调整一些策略，最大化写入速度。接下来的优化基于集群正常运行的前提下，如果是集群首次批量导入数据，则可以将副本数设置为0，导入完毕在将副本数调整回去，这样副分片只需要复制，节省了构建索引过程。综合来说，提升写入速度从以下几方面入手： 加大translog flush间隔，目的是降低iops、writeblock 加大index refresh间隔，除了降低I/O，更重要的是降低了segment merge频率 调整bulk请求 优化磁盘间的任务均匀情况，将shard 尽量均匀分布到物理主机的各个磁盘 优化节点间的任务分布，将任务尽量均匀地分发各节点 优化lucence层建立索引的过程，目的是降低CPU占用率及I/O，例如，禁用_all字段 translog flush间隔调整这是影响ES写入速度的最大因素。但是只有这样，写操作才有可能是可靠的。如果系统可以接受一定概率的数据丢失（例如，数据写入主分片成功，尚未复制到副本分片时，主机断电。由于数据既没有刷到Lucene,translog 也没有刷盘，恢复时translog中没有这个数据，数据丢失），则调整translog持久化策略为周期性和一定大小的时候“flush”,例如： index.translog.durability:async 设置为async标识translog的刷盘策略按sync_interval配置指定时间周期进行 index.translog.sync_interval:120s 加大translog刷盘间隔时间。默认为5s，不可低于100ms index.translog.flush_threshold_size:1024mb 超过这个大小会导致refresh操作，产生新的lucene分段。默认为512Mb 索引刷新间隔refresh interval默认情况下索引的refresh_interval为1秒，这意味着数据写1秒后就可以被搜索到，每次索引的refresh会产生一个新的lucene段，这会导致频繁的segment merge行为，如果不需要这么高的搜索实时性，应该降低索引refresh周期，例如：index.refresh_interval:120s 段合并 segmnet merge 操作对系统I/O和内存占用都比较高，merge行为由lucene控制，位置为：index.merge.scheduler.max_thread_count。 最大线程数max_thread_count的默认值如下：Math.min(3, Runtime.getRuntime().availableProcessors() / 2) 以上是一个比较理想值，如果只有一块硬盘并且非SSD,则应该把它设置为1，因为在旋转存储介质算上并发写，由于寻址的原因，只会降低写入速度。 indexing buffer indexing buffer 在为doc建立索引时使用，当缓冲满时会刷入磁盘，生成一个新的segmnet,这是除refresh_interval刷新索引外，另一个生成新segment的机会。 每个shard有自己的indexing buffer,下面的这个buffer大小的配置需要除以这个节点上所有shard的数量：indices.memory.index_buffer_size。默认为整个堆空间的10%。 如果有了以上设置的话，那indices.memory.min_index_buffer_size默认为48MB，indices.memory.max_index_buffer_siz默认值为无限。 在执行大量的索引操作时，indices.memory.index_buffer_size的默认设置可能不够，这和可用堆内存、单节点上的shard数量有关，可以考虑适当增大该值 使用bulk批量写比一个索引请求只写单个文档的效率高得多，但是要注意bulk请求的整体字节数不要太大，太大的请求可能会给集群带来内存压力，因此每个请求最好避免超过几十兆字节，即使较大的请求看上去执行得更好 bulk线程池和队列建立索引的过程属于计算密集型任务，应该使用固定大小的线程池配置，来不及处理的任务放入队列。线程池最大线程数量应配置为CPU核心数+1，这也是bulk线程池的默认设置，可以避免过多的上下文切换。队列大小可以适当增加，但一定要严格控制大小，过大的队列导致较高的GC压力，并可能导致Full GC 频繁发生 并发执行bulk请求bulk写请求是个长任务，为了给系统增加足够的写入压力，写入过程应该多个客户端、多线程地并发执行，如果要验证系统的极限写入能力，那么目标就是把CPU压满。磁盘util、内存等一般都不是瓶颈。如果CPU没有压满，则应该提高写入端的并发数量。但是要注意bulk线程池队列的reject情况，出现reject代表ES的bulk队列已满，客户端请求被拒绝，此时客户端会收到429错误（TOO_MANAY_REQUESTS）,客户端对此的处理策略应该是延迟重试。不应该忽略这个异常，否则写入系统的数据会少于预期。即使客户端正确处理了429错误，我们仍然应该尽量避免产生reject。因此，在评估极限的写入并发量应该控制在不reject前提下的最大值为宜。 磁盘间的任务均衡如果部署方案是为path.data配置多个路径来使用多块磁盘，则ES在分配shard时，落到各磁盘上的shard可能并不均匀，这种不均匀可能会导致某些磁盘繁忙，利用率在较长时间持续达到100%。这种不均匀达到一定程度会对写入性能产生负面影响。 ES在处理多路径时，有限将shard分配到可用空间占比最多的磁盘上，因此短时间创建的shard 可能被击中分配到各个磁盘上，即使可用空间是99%和98%的差别。后来es在2.x版本开始解决这个问题：预估一下shard会使用的空间，从磁盘可用空间中减去这部分，直到现在6.x版也是这种处理方式。但是实现也存在一些问题： 从可用空间减去预估大小这种机制只存在于一次索引创建的过程中，下一次的索引创建，磁盘可用空间并不是上次做减法后的结果。这也可以理解，毕竟预估是不准的，一直减下去空间很快就减没了。 但是最终的效果是，这种机制并没有根本上解决问题，即使没有完美的解决方案，这种机制的效果也不够好。如果单一的机制不能解决所有的场景，那么至少应该为不同场景准备多种选择。为此，ES增加了两种策略。 简单轮询：在系统初始阶段，简单轮询的效果是最均匀的 基于可用空间的动态加权轮询：以可用空间作为权重，在磁盘之间加权轮询 节点间的任务均衡为了节点间的任务林亮均衡，数据写入客户端应该把bulk请求轮询发送各个节点。当适用JAVA API 或 REST API的bulk接口发送数据时，客户端将会轮询发送到集群节点，节点列表取决于： 使用java API,当设置client.transport.sniff为true(默认为false)时，列表为所有数据节点，否则节点列表为构建客户端对象时传入的节点列表 使用REST API时，列表为构建对象时添加进去的节点 java API的TransportClient和REST API的RestClient都是线程安全的，如果写入程序自己创建线程池控制并发，则应该使用同一个Client对象。再次建议使用REST API，java API会在未来的版本中废弃，REST API有良好的版本兼容性。理论上，java api在序列化上有性能优势，但是只有在吞吐量非常大时才值得考虑序列化的开销带来的影响，通常搜索并不是高吞吐量的业务 要观察bulk请求在不同节点间的均衡性，可以通过cat接口观察bulk线程池和队列情况：_cat/thread_pool 索引过程调整和优化自动生成doc ID通过ES写入流程可以看出，写入doc时如果外部指定了id,则ES会先尝试读取doc的版本号，以判断是否需要更新。这会涉及一次读取磁盘的操作，通过自动生成doc ID可以避免这个环节。 调整字段Mappings 减少字段数量，对于不需要建立索引的字段，不写入ES 将不需要建立索引的字段index属性设置为not_analyzed或no。对字段部分次，或者不索引，可以减少很多运算操作，降低CPU占用。尤其是binary类型，默认情况下占用CPU非常高，而这种类型进行分词通常没有什么意义。 减少字段内容长度，如果原始数据的大段内容无需全部建立索引，则可以尽量减少不必要的内容 使用不同的分词器（analyzer）,不同的分析器在索引过程中运算复杂度也有较大的差异 调整_source字段_source字段用于存储doc原始数据，对于部分不需要存储的字段，可以通过includes excludes 过滤，或者将_source禁用，一般用于索引和数据分离。这样可以降低I/O的压力，不过实际场景中大多不会禁用_source,而即使过滤掉某些字段，对于写入速度的提升作用也不大，满负荷情况下，基本是CPU先跑满了，瓶颈在与CPU。 禁用_all字段从ES6.0开始，_all字段默认为不启用，而在此前的版本中，_all字段默认是开启的，_all字段中包含所有字段分词后的关键词，作用是可以在搜索的时候不指定特定字段，从所有字段中检索。ES6.0默认禁用_all字段主要有以下几点原因： 由于需要从其他的全部字段复制所有字段值，导致_all字段占用非常大的空间。 _all字段有自己的分词器，在进行某些查询时（例如，同义词），结果不符合预期，因为没有匹配同一个分词器 由于数据重复引起的额外建立索引的开销 想要调试时，其内容不容易检查 有些用户甚至不知道存在这个字段，导致查询混乱 有更好的替代方式 关于此问题，可以参考 https:github.com/elastic/elasticsearch/issues/19784 。在ES6.0之前的版本中，可以在mapping中将enabled设置为false 来禁用_all字段: 1234567curl -X PUT &quot;localhost:9200/my_index&quot; -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123; &quot;mapping&quot;:&#123; &quot;type_1&quot;:&#123; &quot;_all&quot;:&#123;&quot;enabled&quot;:false&#125;, &quot;properties&quot;:&#123;....&#125;&#125;&#125;&#125;&#x27; 禁用_all字段可以明显降低对CPU和I/O的压力 对Analyzed的字段禁用NormsNorms用于在搜索时计算doc的评分，如果不需要评分，则可以将其禁用：“title”:{“type”:”string”,”norms”:{“enabled”:false}} index option设置index_options用于控制在建立倒排索引过程中，哪些内容会被添加到倒排索引，例如，doc数量、词频、postions、offsets等信息，优化这些配置可以一定程度降低索引过程中的运算任务，节省CPU占用率。不过在实际场景中，通常很难确定业务将来会不会用到这个信息，除非一开始方案就明确这样设计的。 要用raid 0 或者10ES 搜索速度优化为文件系统cache预留足够的内存空间在一般情况下，应用程序的读写都是被操作系统”cache”(除了direct方式)，cache保存在系统物理内存中（线上应该禁用swap）,命中cache可以降低对磁盘的直接访问频率。搜索很依赖对系统cache的命中，如果某个请求需要从磁盘读取数据，则一定会产生相对较高的延迟。应该至少为系统cache预留一般的可用物理内存，更大的内存有更高的cache命中率。 使用更快的硬件写入性能对CPU的性能更敏感，而搜索性能在一般情况下更多的是在于I/O能力，使用ssd会比旋转类存储介质好得多。尽量避免使用NFS等远程文件系统，如果NFS比本地存储慢3倍，则在搜索场景下响应速度可能会慢10倍左右。这可能是因为搜索请求有更多的随机访问。 如果搜索类型属于计算比较多，则可以考虑使用更快CPU。 文档类型为了让搜索时的成本更低，文档应该合理建模。特别是应该避免join操作，嵌套（nested）会使查询慢几倍，父子（parent-child）关系可能使查询慢数百倍，因此，如果可以通过非规范化（denormalizing）文档来回答相同的问题，则可以显著地提高搜索速度。 预索引数据还可以针对某些查询的模式来优化数据的索引方式。例如，如果所有文档都有一个price字段，并且大多数查询在一个固定的范围上运行range聚合，那么可以通过将范围“pre-indexing”到索引中并使用聚合来加快聚合速度。 字段映射有些字段的内容是数值，但并不意味着其总是应该被映射为数值类型，例如，一些标识符，将他们映射为keyword可能会比integer或long更好。 避免使用脚本一般来说，应该避免使用脚本。如果一定要用，则应该优先考虑painless和expressions. 优化日期搜索在使用日期范围检索时，使用now的查询通常不能缓存，因为匹配到的范围一直在变化。但是，从用户体验角度来看，切换到完整的日期通常是可以接受的，这样可以更好地利用查询缓存。 为只读索引执行force-merge为不再更新的只读索引执行force merge,将lucene索引合并为单个分段，可以提升查询速度。当一个lucene索引存在多个分段时，每个分段会单独执行搜索再将结果合并，将只读索引强制合并为一个lucene分段不仅可以优化搜索过程，对索引恢复速度也有好处。 基于日期进行轮询的索引的旧数据一般都不会再更新。此前的章节中说过，应该避免持续地写一个固定的索引，直到它巨大无比，而应该按照一定的策略，例如，每天生成一个新的索引，然后用别名关联，或者使用索引通配符。这样，可以每天选一个时间点对昨天的索引执行force-merge、Shrink等操作。 globle ordinals全局序号是一种数据结构，用于在keyword字段上运行terms聚合。他用一个数值来代表字段中的字符串值，然后为每一数值分配一个bucket.这需要一个对global ordinals和bucket的构建过程。默认情况下，他们被延迟构建，因为ES不知道哪些字段将用于terms聚合，哪些字段不会。参考：https://www.elastic.co/guide/en/elasticsearch/guide/current/preload-fielddata.html https://blog.csdn.net/zwgdft/artical/details/83215977 execution hintterms 聚合有两种不同机制： 通过直接使用字段值来聚合每个桶的数据（map） 通过使用字段的全局序号并为每个全局序号分配一个bucket(global_ordinals). ES 使用global_ordinals作为keyword字段的默认选项，它使用全局序号动态地分配bucket,因此内存使用与聚合结果中的字段数量是线性关系。在大部分情况下，这种方式的速度很快。当查询只会匹配少量文档时，可以考虑使用map.默认情况下，map只在脚本上运行聚合时使用，因为他们没有序数。 预热文件系统cache如果ES主机重启，则文件系统缓存将为空，此时搜索会比较慢。可以使用index.store.preload设置，通过指定文件扩展名，显式地告诉操作系统应该将哪些文件加载到内存中，例如，配置到elasticsearch.yml文件中：index.store.preload:[“nvd”,”dvd”] 或者在索引创建时设置： 123456PUT /my_index&#123; &quot;settings&quot;:&#123; &quot;index.store.preload&quot;:[&quot;nvd&quot;,&quot;dvd&quot;] &#125;&#125; 如果文件缓存不够大，则无法保存所有数据，那么为太多文件预加载数据到文件系统缓存中会使搜索速度变慢，应谨慎使用。 转换查询表达式在组合查询中通过bool过滤器进行and、or和not的多个逻辑组合检索，这种组合查询中的表达式在下面的情况下可以做等价转换：（A|B）&amp;(C|D)==&gt;(A&amp;B)|（A&amp;D）|(B&amp;C)|(B&amp;D) 使用近似聚合近似聚合以牺牲少量的精度为代价，大幅度提高执行效率，降低了内存使用。近似聚合的方式可以参考官方手册：Percentiles Aggregation(https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-precentile-aggregation.html) Cardinality Aggregation(https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html) 深度优先还是广度优先ES 有两种不同的聚合方式：深度优先和广度优先。深度优先是默认，先构建完整的树，然后修剪无用节点。大多数情况下深度聚合都能正常工作，但是在特殊场合适合广度优先，先执行第一次聚合，再继续下一层之前修剪，官方例子：https://www.elastic.co/guide/cn/elasticsearch/guide/curretn/_preventing_combinatorial_explosions.html 限制搜索请求分片数一个搜索请求涉及的分片数量越多，协调节点的CPU和内存压力越大。默认情况下，ES会拒绝超过1000个分片的搜索请求。我们应该更好的组织数据，让搜索请求的分片数更少。如果想调节这个值，则可以通过actions.search.shard_count配置项进行修改 虽然限制搜索的分片数并不能直接提升单个索引请求的速度，但协调节点的压力会间接影响搜索速度，例如，占用更多内存会产生更多GC压力，可能导致更多的stop-the-world时间等，因此间接影响了协调节点的性能，所以我们仍把它列作本章的一部分 利用自适应副本选择（ARS）提升ES响应速度为了充分利用计算资源和负载均衡，协调节点将搜索请求轮询转发到分片的每个副本，轮询策略是负载均衡中最简答的策略，任何一个负载均衡器都具备这种基础的策略，缺点是不考虑后端实际系统压力和健康水平。","categories":[],"tags":[]},{"title":"6.性能优化","slug":"6-性能优化","date":"2021-09-08T11:24:46.000Z","updated":"2021-09-10T13:07:40.771Z","comments":true,"path":"2021/09/08/6-性能优化/","link":"","permalink":"https://sk-xinye.github.io/2021/09/08/6-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/","excerpt":"","text":"开发调优 避免创建重复的RDD 开发过程中忘记之前已经创建过了，导致了重复创建，浪费资源 尽可能复用同一个RDD 我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数 对多次使用的RDD进行持久化 默认memory 尽量避免使用shuffle类算子 使用map-side预聚合的shuffle操作 在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。 使用高性能的算子 使用reduceByKey/aggregateByKey替代groupByKey 使用mapPartitions替代普通map 使用filter之后进行coalesce操作 使用repartitionAndSortWithinPartitions替代repartition与sort类操作 广播大变量 广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。 使用Kryo优化序列化性能 参数调优 num-executors 这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的 每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。 executor-memory 每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少 阿里云上设置的是10G executor-cores Executor的CPU core数量设置为2~4个较为合适。 driver-memory Driver的内存通常来说不设置，或者设置1G左右应该就够了。 spark.default.parallelism Spark作业的默认task数量为5001000个较为合适。设置该参数为num-executors * executor-cores的23倍较为合适 Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。 spark.storage.memoryFraction 默认是0.6。也就是说，默认Executor 60%的内存，如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。 spark.shuffle.memoryFraction 默认是0.2 如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用 数据倾斜如何定位导致数据倾斜的代码 数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。 通过web_ui确定哪个stage 最慢，然后进行对应的调优工作 某个task执行特别慢的情况 首先要看的，就是数据倾斜发生在第几个stage中。 如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage； 如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage 某个task莫名其妙内存溢出的情况 我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。 一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。 导致数据倾斜的key的数据分布情况 如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。 如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。 数据倾斜的解决方案使用Hive ETL预处理数据方案适用场景：导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。 方案实现思路：此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。 方案实现原理：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。 方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。 方案缺点：治标不治本，Hive ETL中还是会发生数据倾斜。 方案实践经验：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。 项目实践经验：在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。 过滤少数导致倾斜的key方案适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。 方案实现思路：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。 方案实现原理：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。 方案优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。 方案缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。 方案实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。 提高shuffle操作的并行度方案适用场景：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。 方案实现思路：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。 方案实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。 方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。 方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。 方案实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。 两阶段聚合（局部聚合+全局聚合）方案适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。 方案实现思路：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。 方案实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。 方案优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。 方案缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。 将reduce join转为map join方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。 方案实现思路：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。 方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。 方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。 方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。 采样倾斜key并分拆join操作使用随机前缀和扩容RDD进行join多种方案组合使用","categories":[],"tags":[]},{"title":"5.shuffle与内存管理","slug":"5-shuffle与内存管理","date":"2021-09-08T09:23:46.000Z","updated":"2021-09-10T13:07:40.766Z","comments":true,"path":"2021/09/08/5-shuffle与内存管理/","link":"","permalink":"https://sk-xinye.github.io/2021/09/08/5-shuffle%E4%B8%8E%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","excerpt":"","text":"内存管理堆内和堆外内存规划作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。堆内内存受到 JVM 统一管理，堆外内存是直接向操作系统进行内存的申请和释放。 堆内内存动态占用机制，存储区域、计算区域、其他区域 shuffleShuffleManager发展概述 在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。 在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。 因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。 HashShuffleManager运行原理 SortShuffleManager运行原理SortShuffleManager的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。 普通运行机制内存数据结构–&gt;排序，分批溢写磁盘，清空内存数据结构–&gt;临时磁盘文件都进行合并，这就是merge过程 bypass运行机制而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。 shuffle相关参数调优spark.shuffle.file.buffer 默认值：32k 参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.reducer.maxSizeInFlight 默认值：48m 参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。 调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。 spark.shuffle.io.maxRetries 默认值：3 参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。 调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。 spark.shuffle.io.retryWait 默认值：5s 参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。 调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。 spark.shuffle.memoryFraction 默认值：0.2 参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。 调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。 spark.shuffle.manager 默认值：sort 参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。 调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。 spark.shuffle.sort.bypassMergeThreshold 默认值：200 参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。 调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。 spark.shuffle.consolidateFiles 默认值：false 参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。 调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%","categories":[],"tags":[]},{"title":"4.sparkSql","slug":"4-sparkSql","date":"2021-09-08T06:08:52.000Z","updated":"2021-09-10T13:07:40.765Z","comments":true,"path":"2021/09/08/4-sparkSql/","link":"","permalink":"https://sk-xinye.github.io/2021/09/08/4-sparkSql/","excerpt":"","text":"定义Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块 Hive and SparkSQL 数据兼容方面 SparkSQL 不但兼容 Hive，还可以从 RDD、parquet 文件、JSON 文件中获取数据，未来版本甚至支持获取 RDBMS 数据以及 cassandra 等 NOSQL 数据； 性能优化方面 除了采取 In-Memory Columnar Storage、byte-code generation 等优化技术外、将会引进 Cost Model 对查询进行动态评估、获取最佳物理计划等等； 组件扩展方面 无论是 SQL 的语法解析器、分析器还是优化器都可以重新定义，进行扩展","categories":[],"tags":[]},{"title":"3.spark核心","slug":"3-spark核心","date":"2021-09-07T03:49:13.000Z","updated":"2021-09-10T13:07:40.762Z","comments":true,"path":"2021/09/07/3-spark核心/","link":"","permalink":"https://sk-xinye.github.io/2021/09/07/3-spark%E6%A0%B8%E5%BF%83/","excerpt":"","text":"核心数据结构Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是： RDD : 弹性分布式数据集 累加器：分布式共享只写变量 广播变量：分布式共享只读变量 RDD定义RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合 弹性 存储的弹性：内存与磁盘的自动切换； 容错的弹性：数据丢失可以自动恢复； 计算的弹性：计算出错重试机制； 分片的弹性：可根据需要重新分片。 分布式：数据存储在大数据集群不同节点上 数据集：RDD 封装了计算逻辑，并不保存数据 数据抽象：RDD 是一个抽象类，需要子类具体实现 不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装计算逻辑 可分区、并行计算 核心属性 分区列表RDD 数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。 分区计算函数Spark 在计算时，是使用分区函数对每一个分区进行计算 RDD 之间的依赖关系RDD 是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个 RDD 建立依赖关系 分区器（可选）当数据为 KV 类型数据时，可以通过设定分区器自定义数据的分区 首选位置（可选）计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算 执行原理 从计算的角度来讲，数据处理过程中需要计算资源（内存 &amp; CPU）和计算模型（逻辑）。执行时，需要将计算资源和计算模型进行协调和整合。 Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果 RDD 是 Spark 框架中用于数据处理的核心模型，接下来我们看看，在 Yarn 环境中，RDD的工作原理: 启动 Yarn 集群环境。（包括ResourceManager,NodeManager） Spark 通过申请资源创建调度节点和计算节点。(启动ApplycationMaster,driver,excutor) Spark 框架根据需求将计算逻辑根据分区划分成不同的任务 调度节点将任务根据计算节点状态发送到对应的计算节点进行计算 从以上流程可以看出 RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给Executor 节点执行计算，接下来我们就一起看看 Spark 框架中 RDD 是具体是如何进行数据处理的。 基础编程创建RDD12345678val sparkConf = new SparkConf().setMaster(&quot;local[*]&quot;).setAppName(&quot;spark&quot;)val sparkContext = new SparkContext(sparkConf)val rdd1 = sparkContext.parallelize(List(1,2,3,4))val rdd2 = sparkContext.makeRDD(List(1,2,3,4))rdd1.collect().foreach(println)rdd2.collect().foreach(println)sparkContext.stop()从底层代码实现来讲，makeRDD 方法其实就是 parallelize 方法 map123def map[U: ClassTag](f: T =&gt; U): RDD[U]val dataRDD: RDD[Int] = sparkContext.makeRDD(List(1,2,3,4))val dataRDD1: RDD[Int] = dataRDD.map(num =&gt; &#123;num * 2&#125;) mapPartitions将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。 map与mapPartitions 区别 数据处理角度 Map 算子是分区内一个数据一个数据的执行，类似于串行操作。而 mapPartitions 算子是以分区为单位进行批处理操作。 功能的角度 Map 算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。MapPartitions 算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据 性能的角度 Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处理，所以性能较高。但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用 map 操作 mapPartitionsWithIndex将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。 1val dataRDD1 = dataRDD.mapPartitionsWithIndex((index, datas) =&gt; &#123;datas.map(index, _)&#125;) flatMap将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射 1val dataRDD = sparkContext.makeRDD(List(List(1,2),List(3,4) ),1) val dataRDD1 = dataRDD.flatMap(list =&gt; list) glom将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变 groupBy将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为 shuffle。极限情况下，数据可能被分在同一个分区中 一个组的数据在一个分区中，但是并不是说一个分区中只有一个组 filter将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。 当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现数据倾斜 sample根据指定的规则从数据集中抽取数据 distinct将数据集中重复的数据去重 coalesce根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率 当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少分区的个数，减小任务调度成本 repartition该操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true。无论是将分区数多的RDD 转换为分区数少的 RDD，还是将分区数少的 RDD 转换为分区数多的 RDD，repartition操作都可以完成，因为无论如何都会经 shuffle 过程。 12val dataRDD = sparkContext.makeRDD(List(1,2,3,4,1,2 ),2)val dataRDD1 = dataRDD.repartition(4) sortBy该操作用于排序数据。在排序之前，可以将数据通过 f 函数进行处理，之后按照 f 函数处理的结果进行排序，默认为升序排列。排序后新产生的 RDD 的分区数与原 RDD 的分区数一致。中间存在 shuffle 的过程 12val dataRDD = sparkContext.makeRDD(List(1,2,3,4,1,2 ),2)val dataRDD1 = dataRDD.sortBy(num=&gt;num, false, 4) intersection对源 RDD 和参数 RDD 求交集后返回一个新的 RDD 123val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4))val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6))val dataRDD = dataRDD1.intersection(dataRDD2) union对源 RDD 和参数 RDD 求并集后返回一个新的 RDD 123val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4))val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6))val dataRDD = dataRDD1.union(dataRDD2) subtract以一个 RDD 元素为主，去除两个 RDD 中重复元素，将其他元素保留下来。求差集 123val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4))val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6))val dataRDD = dataRDD1.subtract(dataRDD2) zip将两个 RDD 中的元素，以键值对的形式进行合并。其中，键值对中的 Key 为第 1 个 RDD中的元素，Value 为第 2 个 RDD 中的相同位置的元素。 123val dataRDD1 = sparkContext.makeRDD(List(1,2,3,4))val dataRDD2 = sparkContext.makeRDD(List(3,4,5,6))val dataRDD = dataRDD1.zip(dataRDD2) partitionBy将数据按照指定 Partitioner 重新进行分区。Spark 默认的分区器是 HashPartitioner 123val rdd: RDD[(Int, String)] = sc.makeRDD(Array((1,&quot;aaa&quot;),(2,&quot;bbb&quot;),(3,&quot;ccc&quot;)),3)import org.apache.spark.HashPartitionerval rdd2: RDD[(Int, String)] = rdd.partitionBy(new HashPartitioner(2)) reduceByKey可以将数据按照相同的 Key 对 Value 进行聚合 123val dataRDD1 = sparkContext.makeRDD(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3)))val dataRDD2 = dataRDD1.reduceByKey(_+_)val dataRDD3 = dataRDD1.reduceByKey(_+_, 2) groupByKey将数据源的数据根据 key 对 value 进行分组 1234val dataRDD1 = sparkContext.makeRDD(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3)))val dataRDD2 = dataRDD1.groupByKey()val dataRDD3 = dataRDD1.groupByKey(2)val dataRDD4 = dataRDD1.groupByKey(new HashPartitioner(2)) reduceByKey与groupByKey区别从 shuffle 的角度：reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较高。 从功能的角度：reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用 groupByKey aggregateByKey将数据根据不同的规则进行分区内计算和分区间计算 foldByKey当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey 12val dataRDD1 = sparkContext.makeRDD(List((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,3)))val dataRDD2 = dataRDD1.foldByKey(0)(_+_) combineByKey最通用的对 key-value 型 rdd 进行聚集操作的聚集函数（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致 reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别 reduceByKey: 相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同 FoldByKey: 相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同 AggregateByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同 CombineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同。 sortByKey在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口(特质)，返回一个按照 key 进行排序的 join在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素连接在一起的(K,(V,W))的 RDD 123val rdd: RDD[(Int, String)] = sc.makeRDD(Array((1, &quot;a&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;)))val rdd1: RDD[(Int, Int)] = sc.makeRDD(Array((1, 4), (2, 5), (3, 6)))rdd.join(rdd1).collect().foreach(println) leftOuterJoin类似于 SQL 语句的左外连接 cogroup在类型为(K,V)和(K,W)的 RDD 上调用，返回一个(K,(Iterable&lt; V &gt;,Iterable&lt; W &gt;))类型的 RDD 123val dataRDD1 = sparkContext.makeRDD(List((&quot;a&quot;,1),(&quot;a&quot;,2),(&quot;c&quot;,3)))val dataRDD2 = sparkContext.makeRDD(List((&quot;a&quot;,1),(&quot;c&quot;,2),(&quot;c&quot;,3)))val value: RDD[(String, (Iterable[Int], Iterable[Int]))] = dataRDD1.cogroup(dataRDD2) ———————RDD 行动算子———————————- reduce聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据 123val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))// 聚合数据val reduceResult: Int = rdd.reduce(_+_) collect在驱动程序中，以数组 Array 的形式返回数据集的所有元素 123val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))// 收集数据到Driver rdd.collect().foreach(println) count返回 RDD 中元素的个数 123val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))// 返回 RDD 中元素的个数val countResult: Long = rdd.count() first返回 RDD 中的第一个元素 123val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))// 返回 RDD 中元素的个数val firstResult: Int = rdd.first() println(firstResult) take takeOrdered返回一个由 RDD 的前 n 个元素组成的数组 返回该 RDD 排序后的前 n 个元素组成的数组 save 相关算子123// 保存成 Text 文件 rdd.saveAsTextFile(&quot;output&quot;)// 序列化成对象保存到文件 rdd.saveAsObjectFile(&quot;output1&quot;)// 保存成 Sequencefile 文件 rdd.map((_,1)).saveAsSequenceFile(&quot;output2&quot;) foreach分布式遍历 RDD 中的每一个元素，调用指定函数 123456val rdd: RDD[Int] = sc.makeRDD(List(1,2,3,4))// 收集后打印rdd.map(num=&gt;num).collect().foreach(println)println(&quot;****************&quot;)// 分布式打印rdd.foreach(println) 闭包检测 从计算的角度, 算子以外的代码都是在 Driver 端执行, 算子里面的代码都是在 Executor端执行。 那么在 scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果 如果使用的算子外的数据无法序列化，就意味着无法传值给 Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12 版本后闭包编译方式发生了改变 序列化通过继承Serializable 使用kryo序列化 RDD 依赖RDD 血缘关系lineage血统，以便恢复丢失分区 RDD 依赖关系这里所谓的依赖关系，其实就是两个相邻 RDD 之间的关系 RDD 窄依赖窄依赖表示每一个父(上游)RDD 的 Partition 最多被子（下游）RDD 的一个 Partition 使用，窄依赖我们形象的比喻为独生子女 class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependencyT RDD 宽依赖宽依赖表示同一个父（上游）RDD 的 Partition 被多个子（下游）RDD 的 Partition 依赖，会引起 Shuffle，总结：宽依赖我们形象的比喻为多生。 12345678class ShuffleDependency[K: ClassTag, V: ClassTag, C: ClassTag]( @transient private val _rdd: RDD[_ &lt;: Product2[K, V]], val partitioner: Partitioner, val serializer: Serializer = SparkEnv.get.serializer, val keyOrdering: Option[Ordering[K]] = None, val aggregator: Option[Aggregator[K, V, C]] = None, val mapSideCombine: Boolean = false)extends Dependency[Product2[K, V]] RDD 阶段划分DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。例如，DAG 记录了 RDD 的转换过程和任务的阶段。 RDD 任务划分RDD 任务切分中间分为：Application、Job、Stage 和 Task Application：初始化一个 SparkContext 即生成一个 Application； Job：一个 Action 算子就会生成一个 Job； Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1； Task：一个 Stage 阶段中，最后一个 RDD 的分区个数就是 Task 的个数。 RDD 持久化cache persist mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2) 检查点123456// 设置检查点路径 sc.setCheckpointDir(&quot;./checkpoint1&quot;)// 创建一个 RDD，读取指定位置文件:hello atguigu atguigu val lineRdd: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)// 业务逻辑 val wordRdd: RDD[String] = lineRdd.flatMap(line =&gt; line.split(&quot; &quot;))val wordToOneRdd: RDD[(String, Long)] = wordRdd.map &#123; word =&gt; &#123; (word, System.currentTimeMillis()) &#125; &#125;// 增加缓存,避免再重新跑一个 job 做 checkpoint wordToOneRdd.cache() // 数据检查点：针对 wordToOneRdd 做检查点计算 wordToOneRdd.checkpoint()// 触发执行逻辑 wordToOneRdd.collect().foreach(println) 缓存与检查点区别 Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在 HDFS 等容错、高可用的文件系统，可靠性高 建议对 checkpoint()的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Ca### RDD 分区器che 缓存中读取数据即可，否则需要再从头计算一次 RDD RDD 分区器Spark 目前支持 Hash 分区和 Range 分区，和用户自定义分区。Hash 分区为当前的默认分区。分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区，进而决定了 Reduce 的个数。 只有 Key-Value 类型的 RDD 才有分区器，非 Key-Value 类型的 RDD 分区的值是 None 每个 RDD 的分区 ID 范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的 Hash 分区：对于给定的 key，计算其 hashCode,并除以分区个数取余 Range 分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序 累加器实现原理累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行 merge。 广播变量广播变量实现原理广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送","categories":[],"tags":[]},{"title":"2.spark架构","slug":"2-spark架构","date":"2021-09-07T02:45:18.000Z","updated":"2023-02-04T02:33:07.939Z","comments":true,"path":"2021/09/07/2-spark架构/","link":"","permalink":"https://sk-xinye.github.io/2021/09/07/2-spark%E6%9E%B6%E6%9E%84/","excerpt":"","text":"运行架构Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。 如下图所示，它展示了一个 Spark 执行时的基本结构。图形中的 Driver 表示 master，负责管理整个集群中的作业任务调度。图形中的 Executor 则是 slave，负责实际执行任务 核心组件DriverSpark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作 Driver 在 Spark 作业执行时主要负责： 将用户程序转化为作业（job） 在 Executor 之间调度任务(task) 跟踪 Executor 的执行情况 通过 UI 展示查询运行情 实际上，我们无法准确地描述 Driver 的定义，因为在整个的编程过程中没有看到任何有关Driver 的字眼。所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为Driver 类 ExecutorSpark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立。Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行 Executor 有两个核心功能： 负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程 它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。 Master &amp; WorkerSpark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master 和 Worker，这里的 Master 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于 Yarn 环境中的 RM, 而Worker 呢，也是进程，一个 Worker 运行在集群中的一台服务器上，由 Master 分配资源对数据进行并行的处理和计算，类似于 Yarn环境中 NM。 ApplicationMasterHadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包含 ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。 说的简单点就是，ResourceManager（资源）和 Driver（计算）之间的解耦合靠的就是ApplicationMaster。 核心概念Executor 与 CoreSpark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量。 并行度（Parallelism）与分区在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们将整个集群并行执行任务的数量称之为并行度。那么一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。 默认情况下，Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建 RDD 时指定。记住，这里的并行执行的任务数量，并不是指的切分任务的数量，不要混淆了。 提交流程所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又有细微的区别，我们这里不进行详细的比较，但是因为国内工作中，将 Spark 引用部署到Yarn 环境中会更多一些，所以本课程中的提交流程是基于 Yarn 环境的。 Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式：Client和 Cluster。两种模式主要区别在于：Driver 程序的运行节点位置。 Yarn Client 模式Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试 Driver 在任务提交的本地机器上运行 Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，负责向 ResourceManager 申请 Executor 内存 ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程 Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行 Yarn Cluster 模式Cluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于实际生产环境。 在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动ApplicationMaster 随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver。 Driver 启动后向 ResourceManager 申请 Executor 内存，ResourceManager 接到ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动Executor 进程 Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数 之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。","categories":[],"tags":[]},{"title":"1.初始spark","slug":"1-初始spark","date":"2021-09-06T10:54:28.000Z","updated":"2021-09-10T13:07:40.748Z","comments":true,"path":"2021/09/06/1-初始spark/","link":"","permalink":"https://sk-xinye.github.io/2021/09/06/1-%E5%88%9D%E5%A7%8Bspark/","excerpt":"","text":"spark or hadoopSpark和Hadoop的根本差异是多个作业之间的数据通信问题 : Spark多个作业之间数据通信是基于内存，而 Hadoop 是基于磁盘。 Spark 只有在 shuffle 的时候将数据写入磁盘，而 Hadoop 中多个 MR 作业之间的数据交互都要依赖于磁盘交互 但是 Spark 是基于内存的，所以在实际的生产环境中，由于内存的限制，可能会由于内存资源不够导致 Job 执行失败，此时，MapReduce 其实是一个更好的选择，所以 Spark并不能完全替代 MR。 spark 核心模块 Spark Core Spark Core 中提供了 Spark 最基础与最核心的功能，Spark 其他的功能如：Spark SQL，Spark Streaming，GraphX, MLlib 都是在 Spark Core 的基础上进行扩展的 Spark SQL Spark SQL 是 Spark 用来操作结构化数据的组件。通过 Spark SQL，用户可以使用 SQL或者 Apache Hive 版本的 SQL 方言（HQL）来查询数据。 Spark Streaming Spark Streaming 是 Spark 平台上针对实时数据进行流式计算的组件，提供了丰富的处理数据流的 API。 Spark MLlib MLlib 是 Spark 提供的一个机器学习算法库。MLlib 不仅提供了模型评估、数据导入等额外的功能，还提供了一些更底层的机器学习原语。 Spark GraphX GraphX 是 Spark 面向图计算提供的框架与算法库。 spark 运行环境Local 模式所谓的 Local 模式，就是不需要其他任何节点资源就可以在本地执行 Spark 代码的环境，一般用于教学，调试，演示等。例如bin/spark-shell 提交应用1bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[2] ./examples/jars/spark-examples_2.12-3.0.0.jar 10 –class 表示要执行程序的主类，此处可以更换为咱们自己写的应用程序 –master local[2] 部署模式，默认为本地模式，数字表示分配的虚拟 CPU 核数量 spark-examples_2.12-3.0.0.jar 运行的应用类所在的 jar 包，实际使用时，可以设定为咱们自己打的 jar 包 数字 10 表示程序的入口参数，用于设定当前应用的任务数量 Standalone 模式local 本地模式毕竟只是用来进行练习演示的，真实工作中还是要将应用提交到对应的集群中去执行，这里我们来看看只使用 Spark 自身节点运行的集群模式，也就是我们所谓的独立部署（Standalone）模式。Spark 的 Standalone 模式体现了经典的 master-slave 模式。 Linux1 Linux2 Linux3 Spark Worker Master Worker Worker 1bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://linux1:7077 ./examples/jars/spark-examples_2.12-3.0.0.jar 10 –class 表示要执行程序的主类 –master spark://linux1:7077 独立部署模式，连接到 Spark 集群 spark-examples_2.12-3.0.0.jar 运行类所在的 jar 包 数字 10 表示程序的入口参数，用于设定当前应用的任务数量 配置高可用（HA)通过zookeeper 保证数据一致性 Yarn 模式独立部署（Standalone）模式由 Spark 自身提供计算资源，无需其他框架提供资源。这种方式降低了和其他第三方资源框架的耦合性，独立性非常强。但是你也要记住，Spark 主要是计算框架，而不是资源调度框架，所以本身提供的资源调度并不是它的强项，所以还是和其他专业的资源调度框架集成会更靠谱一些。所以接下来我们来学习在强大的 Yarn 环境下 Spark 是如何工作的（其实是因为在国内工作中，Yarn 使用的非常多）。 K8S &amp; Mesos 模式容器化部署是目前业界很流行的一项技术，基于 Docker 镜像运行能够让用户更加方便地对应用进行管理和运维。容器管理工具中最为流行的就是 Kubernetes（k8s），而 Spark也在最近的版本中支持了 k8s 部署模式。这里我们也不做过多的讲解。https://spark.apache.org/docs/latest/running-on-kubernetes.html 部署模式对比 模式 Spark 安装机器数 需启动的进程 所属者 应用场景 Local 1 无 Spark 测试 Standalone 3 Master 及 Worker Spark 单独部署 Yarn 1 Yarn 及 HDFS Hadoop 混合部署 端口号 Spark 查看当前 Spark-shell 运行任务情况端口号：4040（计算） Spark Master 内部通信服务端口号：7077 Standalone 模式下，Spark Master Web 端口号：8080（资源） Spark 历史服务器端口号：18080 Hadoop YARN 任务运行情况查看端口号：8088","categories":[],"tags":[]},{"title":"0.mapping设置","slug":"0-mapping设置及常见命令","date":"2021-09-01T09:44:27.000Z","updated":"2023-02-04T02:33:07.913Z","comments":true,"path":"2021/09/01/0-mapping设置及常见命令/","link":"","permalink":"https://sk-xinye.github.io/2021/09/01/0-mapping%E8%AE%BE%E7%BD%AE%E5%8F%8A%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4/","excerpt":"","text":"动态模板设置 命令reindexcurl -X POST “localhost:9200/_reindex?pretty” -H ‘Content-Type: application/json’ -d’{ “source”:{“index”:”yi_simulation_power_zhang_filtered”}, “dest”:{“index”:”simulation_power_zhang_filtered”}}’ delete_by_querycurl -X POST “localhost:9200/tg_meter/_search?pretty” -H ‘Content-Type: application/json’ -d’{ “query”:{ “term”:{ “SCENE.raw”:”hf1-1” } }}’ 删除索引curl -XDELETE “localhost:9200/power_status_appraise_idx_2021” 模板操作GET /_template/power_satus_appraise_tmplDELETE /_template/power_satus_appraise_tmpl 查询curl power_index/_search?q=ORG_NO:”123213”+q=TIME:”2020-02-02” 导出数据curl -XPOST http://localhost:19200/_opendistro/_sql?format=csv -u ‘zshield:Zx123456_shining10’ -H ‘Content-Type: application/json’ -d ‘{“query”: “SELECT * FROM 索引”}’","categories":[],"tags":[]},{"title":"1.初识ES","slug":"1-初识ES","date":"2021-08-25T02:07:50.000Z","updated":"2021-09-10T13:07:40.740Z","comments":true,"path":"2021/08/25/1-初识ES/","link":"","permalink":"https://sk-xinye.github.io/2021/08/25/1-%E5%88%9D%E8%AF%86ES/","excerpt":"","text":"基本概念","categories":[],"tags":[]},{"title":"ES合并方案","slug":"ES数据迁移","date":"2021-08-25T02:07:50.000Z","updated":"2023-02-04T02:33:07.914Z","comments":true,"path":"2021/08/25/ES数据迁移/","link":"","permalink":"https://sk-xinye.github.io/2021/08/25/ES%E6%95%B0%E6%8D%AE%E8%BF%81%E7%A7%BB/","excerpt":"","text":"ES合并方案由于吉林是将两个集群合并成一个集群，数据量比较大所有我们选择使用snapshot的方式来进行数据迁移，并且采取将二期集群数据迁移到一期并将二期集群合并到一期集群的方式。具体步骤如下： 分别在一期和二期集群上创建快照仓库（参考数据迁移方案中的1-i、1-ii、1-iii这三步） 对二期集群和一期集群上相同的index做快照，命名为快照snapshot_2（快照名字可以自己决定），（参考数据迁移方案中的1-iv-b这步） 删除二期集群中刚才已经备份的索引 对二期集群中剩下的索引进行快照，命名快照snapshot_1（参考数据迁移方案中的1-iv-a这步） 将二期集群中快照对应的目录文件夹（数据迁移方案中的1-ii中配置的映射路径）拷贝到一期集群中相同的目录下。 在一期集群中恢复二期独有的index（参考数据迁移方案中的2-v-a这步），快照名为步骤4中snapshot_1。然后等待完成 在一期集群中恢复和而相同的，快照名为步骤4中snapshot_2。参考下面命令，记得将ES地址改成你的服务器地址。然后等待完成 12345678curl -XPOST &quot;http://127.0.0.1:9200/_snapshot/my_backup/snapshot_2/_restore?wait_for_completion=true&quot; -H &quot;Content-Type: application/json&quot; -d &#x27;&#123; &quot;ignore_unavailable&quot;: true, &quot;include_global_state&quot;: false, &quot;rename_pattern&quot;: &quot;(.+)&quot;, &quot;rename_replacement&quot;: &quot;v2_$1&quot;, &quot;include_aliases&quot;: false&#125;&#x27; 恢复成功后将二期集群是集群处理后当新集群加入一期集群。 Elasticsearch 数据迁移Snapshot迁移速度快，适用于数据量大的场景(不同的版本或项目可能es地址不相同，请自行根据自己现场的情况对es地址（http://127.0.0.1:9200）进行修改) 源端ES集群 要在 ES compose(/home/zshield/docker/compose/elasticsearch.yml) 配置文件 elasticsearch.yml 中找到environment并在下面添加如下一行（设置仓库路径）。 1- path.repo=/usr/share/elasticsearch/backups 在 ES compose(/home/zshield/docker/compose/elasticsearch.yml) 配置文件 elasticsearch.yml中将步骤1-i中配置的地址映射出来。在volumes下添加如下一行（记住映射出的地址必须和启动ES是同一个用户组）。然后重启ES 1- &quot;/home/zshield/BIGdata/backups:/usr/share/elasticsearch/backups&quot; 然后调用 snapshot api 创建仓库（repository），具体如下： 123456789###my_backup 为仓库名可自行更换curl -XPUT &quot;http://127.0.0.1:9200/_snapshot/my_backup&quot; -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123; &quot;type&quot;: &quot;fs&quot;, &quot;settings&quot;: &#123; &quot;location&quot;: &quot;/usr/share/elasticsearch/backups&quot; , &quot;compress&quot;: true &#125;&#125;&#x27; 创建快照（snapshot）执行命令后等待返回，如果数据大可能比较慢。最后看到有success字样代表成功。 备份所有索引。将 ES 集群内所有索引备份到my_backup仓库下，并命名为snapshot_1(可自行更换)，这个命令会立刻返回，并在后台异步执行直到结束。如果希望创建快照命令阻塞执行，可以添加wait_for_completion参数。命令执行的时间与索引大小相关。 1curl -XPUT &quot;http://127.0.0.1:9200/_snapshot/my_backup/snapshot_1?wait_for_completion=true&quot; 备份指定索引。可以在创建快照的时候指定要备份的索引。参数 indices 的值为多个索引的时候，需要用，隔开且不能有空格。 123456curl -XPUT &quot;http://127.0.0.1:9200/__snapshot/my_backup/snapshot_1?wait_for_completion=true&quot; -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123; &quot;indices&quot;:&quot;index_1,index_2,logstash*&quot;, &quot;ignore_unavailable&quot;:true, &quot;include_global_state&quot;:false&#125;&#x27; 最后备份的数据被存入1-ii中映射的目录，将改目录所有数据压缩并且拷贝到目的集群。 目的ES集群 要在 ES compose(/home/zshield/docker/compose/elasticsearch.yml) 配置文件 elasticsearch.yml 中找到environment并在下面添加如下一行（设置仓库路径）。 1- path.repo=/usr/share/elasticsearch/backups 在 ES compose(/home/zshield/docker/compose/elasticsearch.yml) 配置文件 elasticsearch.yml中将步骤1-i中配置的地址映射出来。在volumes下添加如下一行（记住映射出的地址必须和启动ES是同一个用户组）。然后重启ES 1- &quot;/home/zshield/BIGdata/backups:/usr/share/elasticsearch/backups&quot; 然后调用 snapshot api 创建仓库（repository），具体如下： 123456789###my_backup 为仓库名可自行更换curl -XPUT &quot;http://127.0.0.1:9200/_snapshot/my_backup&quot; -H &#x27;Content-Type: application/json&#x27; -d &#x27;&#123; &quot;type&quot;: &quot;fs&quot;, &quot;settings&quot;: &#123; &quot;location&quot;: &quot;/usr/share/elasticsearch/backups&quot; , &quot;compress&quot;: true &#125;&#125;&#x27; 将源ES机器拷贝过来的数据解压缩到2-ii步骤中映射的目录中 恢复数据 执行快照恢复命令会把把这个快照里的备份的所有索引都恢复到ES集群中。如果 snapshot_1 包括五个索引，这五个都会被恢复到我们集群里。 1curl -XPOST &quot;http://127.0.0.1:9200/_snapshot/my_backup/snapshot_1/_restore&quot; 可以用附加的选项用来重命名索引。这个选项允许通过模式匹配索引名称，然后通过恢复进程提供一个新名称。 123456789curl -XPOST &quot;http://127.0.0.1:9200/_snapshot/my_backup/snapshot_2/_restore?pretty&quot; -H &quot;Content-Type: application/json&quot; -d &#x27;&#123; &quot;indices&quot;: &quot;index_1,index_2&quot;, &quot;ignore_unavailable&quot;: true, &quot;include_global_state&quot;: false, &quot;rename_pattern&quot;: &quot;(.+)&quot;, &quot;rename_replacement&quot;: &quot;v2_$1&quot;, &quot;include_aliases&quot;: false&#125;&#x27; ES 移除节点使用说明 获取ES节点信息 1curl zshield:Zx123456_shining10@localhost:19200/_cat/nodes?pretty 在任意非移除节点执行remove_es_node.py文件 1/opt/py3.6/ve1/bin/python3.6 remove_es_node.py remove-es-node -m zshield:Zx123456_shining10@localhost -r 192.168.83.63 参数说明： 可通过/opt/py3.6/ve1/bin/python3.6 remove_es_node.py remove-es-node –help查看 其中-m 后面接的是通信节点，帮助执行exclude 命令 -r 为要移除节点 结果说明 当出现移除数据成功后，可下线对应的节点 恢复被禁用的节点（让禁用的节点可以重新加入集群） 1curl -XPUT http://zshield:Zx123456_shining10@192.168.83.65:19200/_cluster/settings?pretty -H &quot;Content-Type: application/json&quot; -d &#x27;&#123;&quot;transient&quot;:&#123;&quot;cluster.routing.allocation.exclude._ip&quot;: null&#125;&#125;&#x27; reindex: POST _reindex{“source”: {“remote”: {“host”: “http://192.168.83.119:19200&quot;,&quot;username&quot;: “zshield”,”password”: “Zx123456_shining10”}, “index”: “binary_e_mp_day_read-13401”, “size”: 100, “query”: {“match_all”: {}}}, “dest”: {“index”:”binary_e_mp_day_read-13401”}, “script”: {“source”: “if (ctx._id.length() &gt; 512) {ctx._id = ‘null’}”, “lang”: “painless”}}","categories":[],"tags":[]},{"title":"9.yaml相关","slug":"9-yaml相关","date":"2021-07-30T07:50:51.000Z","updated":"2021-08-08T13:43:29.700Z","comments":true,"path":"2021/07/30/9-yaml相关/","link":"","permalink":"https://sk-xinye.github.io/2021/07/30/9-yaml%E7%9B%B8%E5%85%B3/","excerpt":"","text":"整体 apiVersion：此处值是v1，这个版本号需要根据安装的Kubernetes版本和资源类型进行变化，记住不是写死的。 kind：此处创建的是Pod，根据实际情况，此处资源类型可以是Deployment、Job、Ingress、Service、CronJob等。 metadata:包含Pod的一些meta信息，比如名称、namespace、标签等信息。 spec:包括一些container，storage，volume以及其他Kubernetes需要的参数，以及诸如是否在容器失败时重新启动容器的属性。可在特定Kubernetes API找到完整的Kubernetes Pod的属性。 CronJobsjobTemplatespec.jobTemplate是任务的模版，它是必须的。它和 Job的语法完全一样， 除了它是嵌套的没有 apiVersion 和 kind。 编写任务的 .spec 并发行规则spec.concurrencyPolicy 也是可选的。它声明了 CronJob 创建的任务执行时发生重叠如何处理。 spec 仅能声明下列规则中的一种： Allow (默认)：CronJob 允许并发任务执行。 Forbid： CronJob 不允许并发任务执行；如果新任务的执行时间到了而老任务没有执行完，CronJob 会忽略新任务的执行。 Replace：如果新任务的执行时间到了而老任务没有执行完，CronJob 会用新任务替换当前正在运行的任务。 任务历史限制spec.successfulJobsHistoryLimit 和 .spec.failedJobsHistoryLimit是可选的。 这两个字段指定应保留多少已完成和失败的任务。 默认设置为3和1。限制设置为0代表相应类型的任务完成后不会保留。 开始的最后期限spec.startingDeadlineSeconds 域是可选的。 它表示任务如果由于某种原因错过了调度时间，开始该任务的截止时间的秒数。过了截止时间，CronJob 就不会开始任务。 不满足这种最后期限的任务会被统计为失败任务。如果该域没有声明，那任务就没有最后期限。 schedule┌───────────── 分钟 (0 - 59)│ ┌───────────── 小时 (0 - 23)│ │ ┌───────────── 月的某天 (1 - 31)│ │ │ ┌───────────── 月份 (1 - 12)│ │ │ │ ┌───────────── 周的某天 (0 - 6) （周日到周一；在某些系统上，7 也是星期日）│ │ │ │ ││ │ │ │ ││ │ │ │ │ 输入 描述 相当于 @yearly (or @annually) 每年 1 月 1 日的午夜运行一次 0 0 1 1 * @monthly 每月第一天的午夜运行一次 0 0 1 * * @weekly 每周的周日午夜运行一次 0 0 * * 0 @daily (or @midnight) 每天午夜运行一次 0 0 * * * @hourly 每小时的开始一次 0 * * * * cron 表达式 顺序 秒 分钟 小时 日期 月份 星期 年(可选) 取值 0-59 0-59 0-23 1-30(31) 1月12日 1月7日 允许特殊字符 , - * / , - * / , - * / , - * / ? L W C , - * / , - * / L # C 1970-2099 , - * / 字段含义 *：代表所有可能的值 -：指定范围 ,：列出枚举 例如在分钟里，”5,15”表示5分钟和20分钟触发 /：指定增量 例如在分钟里，”3/15”表示从3分钟开始，没隔15分钟执行一次 ?：表示没有具体的值，使用?要注意冲突 L：表示last，例如星期中表示7或SAT，月份中表示最后一天31或30，6L表示这个月倒数第6天，FRIL- 表示这个月的最后一个星期五 W：只能用在月份中，表示最接近指定天的工作日 #：只能用在星期中，表示这个月的第几个周几，例如6#3表示这个月的第3个周五 示例 0 * * * * ? 每1分钟触发一次 0 0 * * * ? 每天每1小时触发一次 0 0 10 * * ? 每天10点触发一次 0 * 14 * * ? 在每天下午2点到下午2:59期间的每1分钟触发 0 30 9 1 * ? 每月1号上午9点半 0 15 10 15 * ? 每月15日上午10:15触发 */5 * * * * ? 每隔5秒执行一次 0 */1 * * * ? 每隔1分钟执行一次 0 0 5-15 * * ? 每天5-15点整点触发 0 0/3 * * * ? 每三分钟触发一次 0 0 0 1 * ? 每月1号凌晨执行一次","categories":[],"tags":[]},{"title":"8.网络","slug":"8-网络","date":"2021-07-03T08:52:50.000Z","updated":"2023-02-04T02:33:07.825Z","comments":true,"path":"2021/07/03/8-网络/","link":"","permalink":"https://sk-xinye.github.io/2021/07/03/8-%E7%BD%91%E7%BB%9C/","excerpt":"","text":"容器网络网络栈”，实际上是被隔离在它自己的 Network Namespace 当中的。 而所谓“网络栈”，就包括了： 网卡（Network Interface） 回环设备（Loopback Device） 路由表（Routing Table） iptables 规则。 对于一个进程来说，这些要素，其实就构成了它发起和响应网络请求的基本环境。 需要指出的是，作为一个容器，它可以声明直接使用宿主机的网络栈（–net=host），即：不开启 Network Namespace，比如： 1$docker run –d –net=host --name nginx-host nginx 在这种情况下，这个容器启动后，直接监听的就是宿主机的 80 端口。 问题： 像这样直接使用宿主机网络栈的方式，虽然可以为容器提供良好的网络性能，但也会不可避免地引入共享网络资源的问题，比如端口冲突。所以，在大多数情况下，我们都希望容器进程能使用自己 Network Namespace 里的网络栈，即：拥有属于自己的 IP 地址和端口。 这个被隔离的容器进程，该如何跟其他 Network Namespace 里的容器进程进行交互呢？ 网桥而为了实现上述目的，Docker 项目会默认在宿主机上创建一个名叫 docker0 的网桥，凡是连接在 docker0 网桥上的容器，就可以通过它来进行通信。 我们就需要使用一种名叫Veth Pair的虚拟设备了。 Veth Pair 设备的特点是：它被创建出来后，总是以两张虚拟网卡（Veth Peer）的形式成对出现的。并且，从其中一个“网卡”发出的数据包，可以直接出现在与它对应的另一张“网卡”上，哪怕这两个“网卡”在不同的 Network Namespace 里。 这就使得 Veth Pair 常常被用作连接不同 Network Namespace 的“网线”。 原理 当进入容器后，查看容器ifconfig网络，发现eth0的网卡，它正是一个 Veth Pair 设备在容器里的这一端。 通过 route 命令查看 nginx-1 容器的路由表，我们可以看到，这个 eth0 网卡是这个容器里的默认路由设备；所有对 172.17.0.0/16 网段的请求，也会被交给 eth0 来处理（第二条 172.17.0.0 路由规则）。 而这个 Veth Pair 设备的另一端，则在宿主机上。你可以通过查看宿主机的网络设备看到它， 通过 ifconfig 命令的输出，你可以看到，nginx-1 容器对应的 Veth Pair 设备，在宿主机上是一张虚拟网卡。它的名字叫作 veth9c02e56。 并且，通过 brctl show 的输出，你可以看到这张网卡被“插”在了 docker0 上。 如果我们再在这台宿主机上启动另一个 Docker 容器，比如 nginx-2：你就会发现一个新的、名叫 vethb4963f3 的虚拟网卡，也被“插”在了 docker0 网桥上。 这其中的原理其实非常简单： 当你在 nginx-1 容器里访问 nginx-2 容器的 IP 地址（比如 ping 172.17.0.3）的时候，这个目的 IP 地址会匹配到 nginx-1 容器里的第二条路由规则。可以看到，这条路由规则的网关（Gateway）是 0.0.0.0，这就意味着这是一条直连规则，即：凡是匹配到这条规则的 IP 包，应该经过本机的 eth0 网卡，通过二层网络直接发往目的主机。 而要通过二层网络到达 nginx-2 容器，就需要有 172.17.0.3 这个 IP 地址对应的 MAC 地址。所以 nginx-1 容器的网络协议栈，就需要通过 eth0 网卡发送一个 ARP 广播，来通过 IP 地址查找对应的 MAC 地址。 ARP（Address Resolution Protocol），是通过三层的 IP 地址找到对应的二层 MAC 地址的协议。 这个 eth0 网卡，是一个 Veth Pair，它的一端在这个 nginx-1 容器的 Network Namespace 里，而另一端则位于宿主机上（Host Namespace），并且被“插”在了宿主机的 docker0 网桥上。 一旦一张虚拟网卡被“插”在网桥上，它就会变成该网桥的“从设备”。从设备会被“剥夺”调用网络协议栈处理数据包的资格，从而“降级”成为网桥上的一个端口。 而这个端口唯一的作用，就是接收流入的数据包，然后把这些数据包的“生杀大权”（比如转发或者丢弃），全部交给对应的网桥。 所以，在收到这些 ARP 请求之后，docker0 网桥就会扮演二层交换机的角色，把 ARP 广播转发到其他被“插”在 docker0 上的虚拟网卡上。 这样，同样连接在 docker0 上的 nginx-2 容器的网络协议栈就会收到这个 ARP 请求，从而将 172.17.0.3 所对应的 MAC 地址回复给 nginx-1 容器。 有了这个目的 MAC 地址，nginx-1 容器的 eth0 网卡就可以将数据包发出去。 而根据 Veth Pair 设备的原理，这个数据包会立刻出现在宿主机上的 veth9c02e56 虚拟网卡上。不过，此时这个 veth9c02e56 网卡的网络协议栈的资格已经被“剥夺”，所以这个数据包就直接流入到了 docker0 网桥里。 docker0 处理转发的过程，则继续扮演二层交换机的角色。此时，docker0 网桥根据数据包的目的 MAC 地址（也就是 nginx-2 容器的 MAC 地址），在它的 CAM 表（即交换机通过 MAC 地址学习维护的端口和 MAC 地址的对应表）里查到对应的端口（Port）为：vethb4963f3，然后把数据包发往这个端口。 而这个端口，正是 nginx-2 容器“插”在 docker0 网桥上的另一块虚拟网卡，当然，它也是一个 Veth Pair 设备。这样，数据包就进入到了 nginx-2 容器的 Network Namespace 里。 所以，nginx-2 容器看到的情况是，它自己的 eth0 网卡上出现了流入的数据包。这样，nginx-2 的网络协议栈就会对请求进行处理，最后将响应（Pong）返回到 nginx-1。 熟悉了 docker0 网桥的工作方式，你就可以理解，在默认情况下，被限制在 Network Namespace 里的容器进程，实际上是通过 Veth Pair 设备 + 宿主机网桥的方式，实现了跟同其他容器的数据交换。 与之类似地，当你在一台宿主机上，访问该宿主机上的容器的 IP 地址时，这个请求的数据包，也是先根据路由规则到达 docker0 网桥，然后被转发到对应的 Veth Pair 设备，最后出现在容器里。这个过程的示意图， 同样地，当一个容器试图连接到另外一个宿主机时，比如：ping 10.168.0.3，它发出的请求数据包，首先经过 docker0 网桥出现在宿主机上。然后根据宿主机的路由表里的直连路由规则（10.168.0.0/24 via eth0)），对 10.168.0.3 的访问请求就会交给宿主机的 eth0 处理。 所以接下来，这个数据包就会经宿主机的 eth0 网卡转发到宿主机网络上，最终到达 10.168.0.3 对应的宿主机上。 当你遇到容器连不通“外网”的时候，你都应该先试试 docker0 网桥能不能 ping 通，然后查看一下跟 docker0 和 Veth Pair 设备相关的 iptables 规则是不是有异常，往往就能够找到问题的答案了。 跨主机通信Overlay Network（覆盖网络）。 深入解析容器跨主机网络 Flannel项目说起Flannel 项目是 CoreOS 公司主推的容器网络方案。事实上，Flannel 项目本身只是一个框架，真正为我们提供容器网络功能的，是 Flannel 的后端实现。目前，Flannel 支持三种后端实现，分别是： VXLAN； host-gw； UDP。 UDP 模式开始，来为你讲解容器“跨主网络”的实现原理例子：我有两台宿主机。 宿主机 Node 1 上有一个容器 container-1，它的 IP 地址是 100.96.1.2，对应的 docker0 网桥的地址是：100.96.1.1/24。 宿主机 Node 2 上有一个容器 container-2，它的 IP 地址是 100.96.2.3，对应的 docker0 网桥的地址是：100.96.2.1/24。 我们现在的任务，就是让 container-1 访问 container-2。 container-1 容器里的进程发起的 IP 包，其源地址就是 100.96.1.2，目的地址就是 100.96.2.3。 由于目的地址 100.96.2.3 并不在 Node 1 的 docker0 网桥的网段里，所以这个 IP 包会被交给默认路由规则，通过容器的网关进入 docker0 网桥（如果是同一台宿主机上的容器间通信，走的是直连规则），从而出现在宿主机上。 这时候，这个 IP 包的下一个目的地，就取决于宿主机上的路由规则了。此时，Flannel 已经在宿主机上创建出了一系列的路由规则，以 Node 1 为例，如下所示： 123456# 在 Node 1 上$ ip routedefault via 10.168.0.1 dev eth0100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.1.0100.96.1.0/24 dev docker0 proto kernel scope link src 100.96.1.110.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.2 可以看到，由于我们的 IP 包的目的地址是 100.96.2.3，它匹配不到本机 docker0 网桥对应的 100.96.1.0/24 网段，只能匹配到第二条、也就是 100.96.0.0/16 对应的这条路由规则，从而进入到一个叫作 flannel0 的设备中。 而这个 flannel0 设备的类型就比较有意思了：它是一个 TUN 设备（Tunnel 设备）。 在 Linux 中，TUN 设备是一种工作在三层（Network Layer）的虚拟网络设备。TUN 设备的功能非常简单，即：在操作系统内核和用户应用程序之间传递 IP 包。 像上面提到的情况，当操作系统将一个 IP 包发送给 flannel0 设备之后，flannel0 就会把这个 IP 包，交给创建这个设备的应用程序，也就是 Flannel 进程。这是一个从内核态（Linux 操作系统）向用户态（Flannel 进程）的流动方向。 反之，如果 Flannel 进程向 flannel0 设备发送了一个 IP 包，那么这个 IP 包就会出现在宿主机网络栈中，然后根据宿主机的路由表进行下一步处理。这是一个从用户态向内核态的流动方向。 所以，当 IP 包从容器经过 docker0 出现在宿主机，然后又根据路由表进入 flannel0 设备后，宿主机上的 flanneld 进程（Flannel 项目在每个宿主机上的主进程），就会收到这个 IP 包。然后，flanneld 看到了这个 IP 包的目的地址，是 100.96.2.3，就把它发送给了 Node 2 宿主机。 flanneld 通过子网Subnet来知道IP 地址对应的容器，是运行在 Node 2 上 在由 Flannel 管理的容器网络里，一台宿主机上的所有容器，都属于该宿主机被分配的一个“子网”。 在我们的例子中，Node 1 的子网是 100.96.1.0/24，container-1 的 IP 地址是 100.96.1.2。 Node 2 的子网是 100.96.2.0/24，container-2 的 IP 地址是 100.96.2.3。 而这些子网与宿主机的对应关系，正是保存在 Etcd 当中，如下所示： 1234$ etcdctl ls /coreos.com/network/subnets/coreos.com/network/subnets/100.96.1.0-24/coreos.com/network/subnets/100.96.2.0-24/coreos.com/network/subnets/100.96.3.0-24 所以，flanneld 进程在处理由 flannel0 传入的 IP 包时，就可以根据目的 IP 的地址（比如 100.96.2.3），匹配到对应的子网（比如 100.96.2.0/24），从 Etcd 中找到这个子网对应的宿主机的 IP 地址是 10.168.0.3，如下所示： 12$ etcdctl get /coreos.com/network/subnets/100.96.2.0-24&#123;&quot;PublicIP&quot;:&quot;10.168.0.3&quot;&#125; 所以说，flanneld 在收到 container-1 发给 container-2 的 IP 包之后，就会把这个 IP 包直接封装在一个 UDP 包里，然后发送给 Node 2。 不难理解，这个 UDP 包的源地址，就是 flanneld 所在的 Node 1 的地址，而目的地址，则是 container-2 所在的宿主机 Node 2 的地址。 当然，这个请求得以完成的原因是，每台宿主机上的 flanneld，都监听着一个 8285 端口，所以 flanneld 只要把 UDP 包发往 Node 2 的 8285 端口即可。 Node 2 上监听 8285 端口的进程也是 flanneld，所以这时候，flanneld 就可以从这个 UDP 包里解析出封装在里面的、container-1 发来的原 IP 包。 而接下来 flanneld 的工作就非常简单了：flanneld 会直接把这个 IP 包发送给它所管理的 TUN 设备，即 flannel0 设备。 Linux 内核网络栈就会负责处理这个 IP 包，具体的处理方法，就是通过本机的路由表来寻找这个 IP 包的下一步流向。 而 Node 2 上的路由表，跟 Node 1 非常类似，如下所示： 123456# 在 Node 2 上$ ip routedefault via 10.168.0.1 dev eth0100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.2.0100.96.2.0/24 dev docker0 proto kernel scope link src 100.96.2.110.168.0.0/24 dev eth0 proto kernel scope link src 10.168.0.3 由于这个 IP 包的目的地址是 100.96.2.3，它跟第三条、也就是 100.96.2.0/24 网段对应的路由规则匹配更加精确。所以，Linux 内核就会按照这条路由规则，把这个 IP 包转发给 docker0 网桥。 docker0 网桥会扮演二层交换机的角色，将数据包发送给正确的端口，进而通过 Veth Pair 设备进入到 container-2 的 Network Namespace 里。 需要注意的是，上述流程要正确工作还有一个重要的前提，那就是 docker0 网桥的地址范围必须是 Flannel 为宿主机分配的子网。这个很容易实现，以 Node 1 为例，你只需要给它上面的 Docker Daemon 启动时配置如下所示的 bip 参数即可： 12$FLANNEL_SUBNET=100.96.1.1/24$ dockerd --bip=$FLANNEL_SUBNET ... UDP被弃用原因： 实际上，相比于两台宿主机之间的直接通信，基于 Flannel UDP 模式的容器通信多了一个额外的步骤，即 flanneld 的处理过程。而这个过程，由于使用到了 flannel0 这个 TUN 设备，仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝，如下所示： 第一次：用户态的容器进程发出的 IP 包经过 docker0 网桥进入内核态； 第二次：IP 包根据路由表进入 TUN（flannel0）设备，从而回到用户态的 flanneld 进程； 第三次：flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去。 此外，我们还可以看到，Flannel 进行 UDP 封装（Encapsulation）和解封装（Decapsulation）的过程，也都是在用户态完成的。在 Linux 操作系统中，上述这些上下文切换和用户态操作的代价其实是比较高的，这也正是造成 Flannel UDP 模式性能不好的主要原因 我们在进行系统级编程的时候，有一个非常重要的优化原则，就是要减少用户态到内核态的切换次数，并且把核心的处理逻辑都放在内核态进行 VXLANVXLAN，即 Virtual Extensible LAN（虚拟可扩展局域网），是 Linux 内核本身就支持的一种网络虚似化技术。所以说，VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network）。 VXLAN 的覆盖网络的设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核 VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信。当然，实际上，这些“主机”可能分布在不同的宿主机上，甚至是分布在不同的物理机房里。 而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端。这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点）。 而 VTEP 设备的作用，其实跟前面的 flanneld 进程非常相似。只不过，它进行封装和解封装的对象，是二层数据帧（Ethernet frame）；而且这个工作的执行流程，全部是在内核里完成的（因为 VXLAN 本身就是 Linux 内核中的一个模块）。 上述基于 VTEP 设备进行“隧道”通信的流程，我也为你总结成了一幅图，如下所示： 可以看到，图中每台宿主机上名叫 flannel.1 的设备，就是 VXLAN 所需的 VTEP 设备，它既有 IP 地址，也有 MAC 地址。 现在，我们的 container-1 的 IP 地址是 10.1.15.2，要访问的 container-2 的 IP 地址是 10.1.16.3。 那么，与前面 UDP 模式的流程类似，当 container-1 发出请求之后，这个目的地址是 10.1.16.3 的 IP 包，会先出现在 docker0 网桥，然后被路由到本机 flannel.1 设备进行处理。 为了能够将“原始 IP 包”封装并且发送到正确的宿主机，VXLAN 就需要找到这条“隧道”的出口，即：目的宿主机的 VTEP 设备。 而这个设备的信息，正是每台宿主机上的 flanneld 进程负责维护的。 比如，当 Node 2 启动并加入 Flannel 网络之后，在 Node 1（以及所有其他节点）上，flanneld 就会添加一条如下所示的路由规则： 12345$ route -nKernel IP routing tableDestination Gateway Genmask Flags Metric Ref Use Iface...10.1.16.0 10.1.16.0 255.255.255.0 UG 0 0 0 flannel.1 这条规则的意思是：凡是发往 10.1.16.0/24 网段的 IP 包，都需要经过 flannel.1 设备发出，并且，它最后被发往的网关地址是：10.1.16.0。 10.1.16.0 正是 Node 2 上的 VTEP 设备（也就是 flannel.1 设备）的 IP 地址。 “源 VTEP 设备”收到“原始 IP 包”后，就要想办法把“原始 IP 包”加上一个目的 MAC 地址，封装成一个二层数据帧，然后发送给“目的 VTEP 设备”（当然，这么做还是因为这个 IP 包的目的地址不是本机）。 这里需要解决的问题就是：“目的 VTEP 设备”的 MAC 地址是什么？ 此时，根据前面的路由记录，我们已经知道了“目的 VTEP 设备”的 IP 地址。而要根据三层 IP 地址查询对应的二层 MAC 地址，这正是 ARP（Address Resolution Protocol ）表的功能。 而这里要用到的 ARP 记录，也是 flanneld 进程在 Node 2 节点启动时，自动添加在 Node 1 上的。我们可以通过 ip 命令看到它，如下所示： 123# 在 Node 1 上$ ip neigh show dev flannel.110.1.16.0 lladdr 5e:f8:4f:00:e3:37 PERMANENT 有了这个“目的 VTEP 设备”的 MAC 地址，Linux 内核就可以开始二层封包工作了。这个二层帧的格式，如下所示： 然后，Linux 内核会把这个数据帧封装进一个 UDP 包里发出去。 … 接下来，Node 1 上的 flannel.1 设备就可以把这个数据帧从 Node 1 的 eth0 网卡发出去。显然，这个帧会经过宿主机网络来到 Node 2 的 eth0 网卡。 这时候，Node 2 的内核网络栈会发现这个数据帧里有 VXLAN Header，并且 VNI=1。所以 Linux 内核会对它进行拆包，拿到里面的内部数据帧，然后根据 VNI 的值，把它交给 Node 2 上的 flannel.1 设备。 而 flannel.1 设备则会进一步拆包，取出“原始 IP 包”。接下来就回到了我在上一篇文章中分享的单机容器网络的处理流程。最终，IP 包就进入到了 container-2 容器的 Network Namespace 里。 不难看到，上面的例子有一个共性，那就是用户的容器都连接在 docker0 网桥上。而网络插件则在宿主机上创建了一个特殊的设备（UDP 模式创建的是 TUN 设备，VXLAN 模式创建的则是 VTEP 设备），docker0 与这个设备之间，通过 IP 转发（路由表）进行协作。然后，网络插件真正要做的事情，则是通过某种方法，把不同宿主机上的特殊设备连通，从而达到容器跨主机通信的目的。 实际上，上面这个流程，也正是 Kubernetes 对容器网络的主要处理方法。只不过，Kubernetes 是通过一个叫作 CNI 的接口，维护了一个单独的网桥来代替 docker0。这个网桥的名字就叫作：CNI 网桥，它在宿主机上的设备名称默认是：cni0。 https://blog.opskumu.com/kubernetes-ext-service.htmlKubernetes 外部服务映射Table of Contents 外部域名映射到内部 Service -&gt; ExternalName 外部 IP 映射到内部 Service 2.1. IP 映射 2.2. IP + 端口映射 小结集群内的应用有时候需要调用外部的服务，我们知道集群内部服务调用都是通过 Service 互相访问，那么针对外部的服务是否也可以保持统一使用 Service 呢？答案是肯定的，通过 Service 访问外部服务，除了方式统一以外，还能带来其他好处。如配置统一，不同环境（空间）相同应用访问外部不同环境的数据库，可以通过 Service 映射保持两边配置统一，达到不同空间应用通过相同 Service Name 访问不同的外部数据库。如图 test-1 和 test-2 两个空间为两个不同的业务环境，通过服务映射，不同空间相同的 Service 访问到对应外部不同环境的数据库： external-service.png 另外，还可以保证最小化变更，如果外部数据库 IP 之类的变动，只需要修改 Service 对应映射即可，服务本身配置无需变动。具体场景如下： 1 外部域名映射到内部 Service -&gt; ExternalNameapiVersion: v1kind: Servicemetadata: name: mysqlspec: externalName: mysql.example.com type: ExternalName创建之后，同一空间 Pod 就可以通过 mysql:3306 访问外部的 MySQL 服务。 需要注意的是，虽然 externalName 也支持填写 IP，但是并不会被 Ingress 和 CoreDNS 解析（KubeDNS 支持）。如果有 IP 相关的需求，则可以使用 Headless Service -&gt; Type ExternalName 。另外一个需要注意的是，因为 CNAME 的缘故，如果外部的服务又经过一层代理转发，如 Nginx，除非配置对应的 server_name ，否则映射无效。 2 外部 IP 映射到内部 Service2.1 IP 映射前文提过，虽然 externalName 字段可以配置为 IP 地址，但是 Ingress 和 CoreDNS 并不会解析，如果外部服务为 IP 提供，那么可以使用 Headless Service 实现映射。 apiVersion: v1kind: Servicemetadata: name: mysqlspec: clusterIP: None type: ClusterIP apiVersion: v1kind: Endpointsmetadata: name: mysqlsubsets: addresses: ip: 192.168.1.10service 不指定 selector，手动维护创建 endpoint，创建之后就可以通过 mysql:3306 访问 192.168.1.10:3306 服务的目的。Headless Service 不能修改端口相关，如果要修改访问端口，则需要进一步操作。 2.2 IP + 端口映射如果外部的端口不是标准的端口，想通过 Service 访问时候使用标准端口，如外部 MySQL 提供端口为 3307，内部想通过 Service 3306 访问，这个时候则可以通过如下方式实现： apiVersion: v1kind: Servicemetadata: name: mysqlspec: type: ClusterIP ports: port: 3306targetPort: 3307 apiVersion: v1kind: Endpointsmetadata: name: mysqlsubsets: addresses: ip: 192.168.1.10ports: port: 3307service 不指定 selector，手动维护创建 endpoint，创建之后就可以通过 mysql:3306 达到访问外部 192.168.1.10:3307 服务的目的。 参考 Kubernetes best practices: mapping external services3 小结我们可以看出以上外部服务映射，externalName 和 Headless Service 方式映射外部服务是没有经过中间层代理的，都是通过 DNS 劫持实现。而有端口变更需求的时候，则要经过内部 kube-proxy 层转发。正常情况下，能尽可能少的引入中间层就少引用，特别是数据库类的应用，因为引入中间层虽然带来了便利，但也意味着可能会带来性能损耗，特别是那些对延迟比较敏感的服务。","categories":[],"tags":[]},{"title":"7.pod","slug":"7-pod","date":"2021-07-02T01:53:13.000Z","updated":"2021-08-08T13:43:29.699Z","comments":true,"path":"2021/07/02/7-pod/","link":"","permalink":"https://sk-xinye.github.io/2021/07/02/7-pod/","excerpt":"","text":"Pod，是 Kubernetes 项目中最小的 API 对象。如果换一个更专业的说法，我们可以这样描述：Pod，是 Kubernetes 项目的原子调度单位。 为什么我们会需要 Pod 容器的本质到底是什么？ 容器的本质是进程。 容器，就是未来云计算系统中的进程；容器镜像就是这个系统里的“.exe”安装包。那么 Kubernetes 呢？ 你应该也能立刻回答上来：Kubernetes 就是操作系统！ 部署的应用，往往都存在着类似于“进程和进程组”的关系。更具体地说，就是这些应用之间有着密切的协作关系，使得它们必须部署在同一台机器上。 当选择用pod做部署时，会选择资源合适的机器启动该pod。也就是说pod是基本单位 pod 实现原理 首先，关于 Pod 最重要的一个事实是：它只是一个逻辑概念。 也就是说，Kubernetes 真正处理的，还是宿主机操作系统上 Linux 容器的 Namespace 和 Cgroups，而并不存在一个所谓的 Pod 的边界或者隔离环境。 Pod，其实是一组共享了某些资源的容器。 Pod 里的所有容器，共享的是同一个 Network Namespace，并且可以声明共享同一个 Volume。 在 Kubernetes 项目里，Pod 的实现需要使用一个中间容器，这个容器叫作 Infra 容器。 在这个 Pod 中，Infra 容器永远都是第一个被创建的容器，而其他用户定义的容器，则通过 Join Network Namespace 的方式，与 Infra 容器关联在一起。 Infra 容器一定要占用极少的资源，所以它使用的是一个非常特殊的镜像，叫作：k8s.gcr.io/pause。这个镜像是一个用汇编语言编写的、永远处于“暂停”状态的容器，解压后的大小也只有 100~200 KB 左右。 这也就意味着，对于 Pod 里的容器 A 和容器 B 来说： 它们可以直接使用 localhost 进行通信； 它们看到的网络设备跟 Infra 容器看到的完全一样； 一个 Pod 只有一个 IP 地址，也就是这个 Pod 的 Network Namespace 对应的 IP 地址； 当然，其他的所有网络资源，都是一个 Pod 一份，并且被该 Pod 中的所有容器共享； Pod 的生命周期只跟 Infra 容器一致，而与容器 A 和 B 无关。 而对于同一个 Pod 里面的所有用户容器来说，它们的进出流量，也可以认为都是通过 Infra 容器完成的。这一点很重要，因为将来如果你要为 Kubernetes 开发一个网络插件时，应该重点考虑的是如何配置这个 Pod 的 Network Namespace，而不是每一个用户容器如何使用你的网络配置，这是没有意义的。 有了这个设计之后，共享 Volume 就简单多了：Kubernetes 项目只要把所有 Volume 的定义都设计在 Pod 层级即可。 这样，一个 Volume 对应的宿主机目录对于 Pod 来说就只有一个，Pod 里的容器只要声明挂载这个 Volume，就一定可以共享这个 Volume 对应的宿主机目录。 1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata:name: two-containersspec:restartPolicy: Nevervolumes:- name: shared-data hostPath: path: /datacontainers:- name: nginx-container image: nginx volumeMounts: - name: shared-data mountPath: /usr/share/nginx/html- name: debian-container image: debian volumeMounts: - name: shared-data mountPath: /pod-data command: [&quot;/bin/sh&quot;] args: [&quot;-c&quot;, &quot;echo Hello from the debian container &gt; /pod-data/index.html&quot;] 在这个例子中，debian-container 和 nginx-container 都声明挂载了 shared-data 这个 Volume。而 shared-data 是 hostPath 类型。所以，它对应在宿主机上的目录就是：/data。而这个目录，其实就被同时绑定挂载进了上述两个容器当中。 容器设计模式Pod 这种“超亲密关系”容器的设计思想，实际上就是希望，当用户想在一个容器里跑多个功能并不相关的应用时，应该优先考虑它们是不是更应该被描述成一个 Pod 里的多个容器。 通过initContainers 挂载目录 sidecar 实际上，有了 Pod 之后，这样的问题就很容易解决了。我们可以把 WAR 包和 Tomcat 分别做成镜像，然后把它们作为一个 Pod 里的两个容器“组合”在一起。这个 Pod 的配置文件如下所示： 12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata:name: javaweb-2spec:initContainers:- image: geektime/sample:v2 name: war command: [&quot;cp&quot;, &quot;/sample.war&quot;, &quot;/app&quot;] volumeMounts: - mountPath: /app name: app-volumecontainers:- image: geektime/tomcat:7.0 name: tomcat command: [&quot;sh&quot;,&quot;-c&quot;,&quot;/root/apache-tomcat-7.0.42-v2/bin/start.sh&quot;] volumeMounts: - mountPath: /root/apache-tomcat-7.0.42-v2/webapps name: app-volume ports: - containerPort: 8080 hostPort: 8001volumes:- name: app-volume emptyDir: &#123;&#125; 在 Pod 中，所有 Init Container 定义的容器，都会比 spec.containers 定义的用户容器先启动。并且，Init Container 容器会按顺序逐一启动，而直到它们都启动并且退出了，用户容器才会启动。 所以，这个 Init Container 类型的 WAR 包容器启动后，我执行了一句”cp /sample.war /app”，把应用的 WAR 包拷贝到 /app 目录下，然后退出。 而后这个 /app 目录，就挂载了一个名叫 app-volume 的 Volume。 接下来就很关键了。Tomcat 容器，同样声明了挂载 app-volume 到自己的 webapps 目录下。 等 Tomcat 容器启动时，它的 webapps 目录下就一定会存在 sample.war 文件：这个文件正是 WAR 包容器启动时拷贝到这个 Volume 里面的，而这个 Volume 是被这两个容器共享的。 这个所谓的“组合”操作，正是容器设计模式里最常用的一种模式，它的名字叫：sidecar。 所以，我们用 Init Container 的方式优先运行 WAR 包容器，扮演了一个 sidecar 的角色。 日志收集，通过sidecar 比如，我现在有一个应用，需要不断地把日志文件输出到容器的 /var/log 目录中。 我就可以把一个 Pod 里的 Volume 挂载到应用容器的 /var/log 目录上。 然后，我在这个 Pod 里同时运行一个 sidecar 容器，它也声明挂载同一个 Volume 到自己的 /var/log 目录上。 接下来 sidecar 容器就只需要做一件事儿，那就是不断地从自己的 /var/log 目录里读取日志文件，转发到 MongoDB 或者 Elasticsearch 中存储起来。这样，一个最基本的日志收集工作就完成了 Pod，实际上是在扮演传统基础设施里“虚拟机”的角色；而容器，则是这个虚拟机里运行的用户程序。 你就可以把整个虚拟机想象成为一个 Pod，把这些进程分别做成容器镜像，把有顺序关系的容器，定义为 Init Container。这才是更加合理的、松耦合的容器编排诀窍，也是从传统应用架构，到“微服务架构”最自然的过渡方式。 注意：Pod 这个概念，提供的是一种编排思想，而不是具体的技术方案。所以，如果愿意的话，你完全可以使用虚拟机来作为 Pod 的实现，然后把用户容器都运行在这个虚拟机里。比如，Mirantis 公司的virtlet 项目就在干这个事情。甚至，你可以去实现一个带有 Init 进程的容器项目，来模拟传统应用的运行方式。这些工作，在 Kubernetes 中都是非常轻松的，也是我们后面讲解 CRI 时会提到的内容。相反的，如果强行把整个应用塞到一个容器里，甚至不惜使用 Docker In Docker 这种在生产环境中后患无穷的解决方案，恐怕最后往往会得不偿失。 pod基本概念 Pod，而不是容器，才是 Kubernetes 项目中的最小编排单位。 将这个设计落实到 API 对象上，容器（Container）就成了 Pod 属性里的一个普通的字段。 那么，一个很自然的问题就是：到底哪些属性属于 Pod 对象，而又有哪些属性属于 Container 呢？ 凡是调度、网络、存储，以及安全相关的属性，基本上是 Pod 级别的。 这些属性的共同特征是，它们描述的是“机器”这个整体，而不是里面运行的“程序”。 Pod 中几个重要字段的含义和用法 NodeSelector：是一个供用户将 Pod 与 Node 进行绑定的字段，用法如下所示： 123456apiVersion: v1kind: Pod...spec: nodeSelector: disktype: ssd 这样的一个配置，意味着这个 Pod 永远只能运行在携带了“disktype: ssd”标签（Label）的节点上；否则，它将调度失败。 NodeName： 一旦 Pod 的这个字段被赋值，Kubernetes 项目就会被认为这个 Pod 已经经过了调度，调度的结果就是赋值的节点名字。所以，这个字段一般由调度器负责设置，但用户也可以设置它来“骗过”调度器，当然这个做法一般是在测试或者调试的时候才会用到。 HostAliases：定义了 Pod 的 hosts 文件（比如 /etc/hosts）里的内容，用法如下： 12345678910apiVersion: v1kind: Pod...spec:hostAliases:- ip: &quot;10.1.2.3&quot; hostnames: - &quot;foo.remote&quot; - &quot;bar.remote&quot;... 在这个 Pod 的 YAML 文件中，我设置了一组 IP 和 hostname 的数据。修改的就是/etc/hosts中的内容 凡是跟容器的 Linux Namespace 相关的属性，也一定是 Pod 级别的。 原因也很容易理解：Pod 的设计，就是要让它里面的容器尽可能多地共享 Linux Namespace，仅保留必要的隔离和限制能力。这样，Pod 模拟出的效果，就跟虚拟机里程序间的关系非常类似了。 在下面这个 Pod 的 YAML 文件中，我定义了 shareProcessNamespace=true： 12345678910111213apiVersion: v1kind: Podmetadata:name: nginxspec:shareProcessNamespace: truecontainers:- name: nginx image: nginx- name: shell image: busybox stdin: true tty: true 这就意味着这个 Pod 里的容器要共享 PID Namespace。 tty 和 stdin 在 Pod 的 YAML 文件里声明开启它们俩，其实等同于设置了 docker run 里的 -it（-i 即 stdin，-t 即 tty）参数。 如果你还是不太理解它们俩的作用的话，可以直接认为 tty 就是 Linux 给用户提供的一个常驻小程序，用于接收用户的标准输入，返回操作系统的标准输出。当然，为了能够在 tty 中输入信息，你还需要同时开启 stdin（标准输入流）。 1$kubectl create -f nginx.yaml 接下来，我们使用 kubectl attach 命令，连接到 shell 容器的 tty 上 1$kubectl attach -it nginx -c shell 这样，我们就可以在 shell 容器里执行 ps 指令，查看所有正在运行的进程： 12345678$ kubectl attach -it nginx -c shell/ # ps axPID USER TIME COMMAND 1 root 0:00 /pause 8 root 0:00 nginx: master process nginx -g daemon off;14 101 0:00 nginx: worker process15 root 0:00 sh21 root 0:00 ps ax 可以看到，在这个容器里，我们不仅可以看到它本身的 ps ax 指令，还可以看到 nginx 容器的进程，以及 Infra 容器的 /pause 进程。这就意味着，整个 Pod 里的每个容器的进程，对于所有容器来说都是可见的：它们共享了同一个 PID Namespace。 凡是 Pod 中的容器要共享宿主机的 Namespace，也一定是 Pod 级别的定义，比如： 123456789101112131415apiVersion: v1kind: Podmetadata:name: nginxspec:hostNetwork: truehostIPC: truehostPID: truecontainers:- name: nginx image: nginx- name: shell image: busybox stdin: true tty: true 在这个 Pod 中，我定义了共享宿主机的 Network、IPC 和 PID Namespace。这就意味着，这个 Pod 里的所有容器，会直接使用宿主机的网络、直接与宿主机进行 IPC 通信、看到宿主机里正在运行的所有进程。 ContainersInit Containers 的生命周期，会先于所有的 Containers，并且严格按照定义的顺序执行,其他一样。 Container 的定义，和 Docker 相比并没有什么太大区别。我在前面的容器技术概念入门系列文章中，和你分享的 Image（镜像）、Command（启动命令）、workingDir（容器的工作目录）、Ports（容器要开发的端口），以及 volumeMounts（容器要挂载的 Volume）都是构成 Kubernetes 项目中 Container 的主要字段。不过在这里，还有这么几个属性值得你额外关注。 首先，是 ImagePullPolicy 字段。 它定义了镜像拉取的策略。而它之所以是一个 Container 级别的属性，是因为容器镜像本来就是 Container 定义中的一部分。 ImagePullPolicy 的值默认是 Always，即每次创建 Pod 都重新拉取一次镜像。另外，当容器的镜像是类似于 nginx 或者 nginx:latest 这样的名字时，ImagePullPolicy 也会被认为 Always。 如果它的值被定义为 Never 或者 IfNotPresent，则意味着 Pod 永远不会主动拉取这个镜像，或者只在宿主机上不存在这个镜像时才拉取。 其次，是 Lifecycle 字段 Container Lifecycle Hooks 顾名思义，Container Lifecycle Hooks 的作用，是在容器状态发生变化时触发一系列“钩子”。我们来看这样一个例子： 123456789101112131415apiVersion: v1kind: Podmetadata: name: lifecycle-demospec:containers:- name: lifecycle-demo-container image: nginx lifecycle: postStart: exec: command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo Hello from the postStart handler &gt; /usr/share/message&quot;] preStop: exec: command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;] 先说 postStart 吧。它指的是，在容器启动后，立刻执行一个指定的操作。需要明确的是，postStart 定义的操作，虽然是在 Docker 容器 ENTRYPOINT 执行之后，但它并不严格保证顺序。也就是说，在 postStart 启动时，ENTRYPOINT 有可能还没有结束。 而类似地，preStop 发生的时机，则是容器被杀死之前（比如，收到了 SIGKILL 信号）。而需要明确的是，preStop 操作的执行，是同步的。所以，它会阻塞当前的容器杀死流程，直到这个 Hook 定义操作完成之后，才允许容器被杀死，这跟 postStart 不一样。 Pod 对象在 Kubernetes 中的生命周期Pod 生命周期的变化，主要体现在 Pod API 对象的Status 部分，这是它除了 Metadata 和 Spec 之外的第三个重要字段。其中，pod.status.phase，就是 Pod 的当前状态，它有如下几种可能的情况： Pending。这个状态意味着，Pod 的 YAML 文件已经提交给了 Kubernetes，API 对象已经被创建并保存在 Etcd 当中。但是，这个 Pod 里有些容器因为某种原因而不能被顺利创建。比如，调度不成功。 Running。这个状态下，Pod 已经调度成功，跟一个具体的节点绑定。它包含的容器都已经创建成功，并且至少有一个正在运行中。 Succeeded。这个状态意味着，Pod 里的所有容器都正常运行完毕，并且已经退出了。这种情况在运行一次性任务时最为常见。 Failed。这个状态下，Pod 里至少有一个容器以不正常的状态（非 0 的返回码）退出。这个状态的出现，意味着你得想办法 Debug 这个容器的应用，比如查看 Pod 的 Events 和日志。 Unknown。这是一个异常状态，意味着 Pod 的状态不能持续地被 kubelet 汇报给 kube-apiserver，这很有可能是主从节点（Master 和 Kubelet）间的通信出现了问题。 更进一步地，Pod 对象的 Status 字段，还可以再细分出一组 Conditions。这些细分状态的值包括：PodScheduled、Ready、Initialized，以及 Unschedulable。它们主要用于描述造成当前 Status 的具体原因是什么。 比如，Pod 当前的 Status 是 Pending，对应的 Condition 是 Unschedulable，这就意味着它的调度出现了问题。 而其中，Ready 这个细分状态非常值得我们关注：它意味着 Pod 不仅已经正常启动（Running 状态），而且已经可以对外提供服务了。这两者之间（Running 和 Ready）是有区别的，你不妨仔细思考一下。 更多信息可以看 $GOPATH/src/k8s.io/kubernetes/vendor/k8s.io/api/core/v1/types.go 里，type Pod struct ，尤其是 PodSpec 部分的内容。 特殊的 Volume，叫作 Projected Volume，你可以把它翻译为“投射数据卷”Kubernetes 支持的 Projected Volume 一共有四种： Secret； ConfigMap； Downward API； ServiceAccountToken。 Secret它的作用，是帮你把 Pod 想要访问的加密数据，存放到 Etcd 中。然后，你就可以通过在 Pod 的容器里挂载 Volume 的方式，访问到这些 Secret 里保存的信息了。 Secret 最典型的使用场景，莫过于存放数据库的 Credential 信息，比如下面这个例子： 1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: test-projected-volume spec: containers: - name: test-secret-volume image: busybox args: - sleep - &quot;86400&quot; volumeMounts: - name: mysql-cred mountPath: &quot;/projected-volume&quot; readOnly: true volumes: - name: mysql-cred projected: sources: - secret: name: user - secret: name: pass 在这个 Pod 中，我定义了一个简单的容器。它声明挂载的 Volume，并不是常见的 emptyDir 或者 hostPath 类型，而是 projected 类型。 而这个 Volume 的数据来源（sources），则是名为 user 和 pass 的 Secret 对象，分别对应的是数据库的用户名和密码。 这里用到的数据库的用户名、密码，正是以 Secret 对象的方式交给 Kubernetes 保存的。完成这个操作的指令，如下所示： 1234567891011121314151617$ cat ./username.txtadmin$ cat ./password.txtc1oudc0w!$ kubectl create secret generic user --from-file=./username.txt$ kubectl create secret generic pass --from-file=./password.txt$ kubectl get secretsNAME TYPE DATA AGEuser Opaque 1 51spass Opaque 1 51s#要求base64转码，避免明文$ echo -n &#x27;admin&#x27; | base64YWRtaW4=$ echo -n &#x27;1f2d1e2e67df&#x27; | base64MWYyZDFlMmU2N2Rm 还可以通过yml方式 12345678apiVersion: v1kind: Secretmetadata: name: mysecrettype: Opaquedata: user: YWRtaW4= pass: MWYyZDFlMmU2N2Rm 启动pod,验证 12345678910$kubectl create -f test-projected-volume.yaml$ kubectl exec -it test-projected-volume -- /bin/sh$ ls /projected-volume/userpass$ cat /projected-volume/userroot$ cat /projected-volume/pass1f2d1e2e67df ConfigMap它与 Secret 的区别在于，ConfigMap 保存的是不需要加密的、应用所需的配置信息。 而 ConfigMap 的用法几乎与 Secret 完全相同：你可以使用 kubectl create configmap 从文件或者目录创建 ConfigMap，也可以直接编写 ConfigMap 对象的 YAML 文件。 比如，一个 Java 应用所需的配置文件（.properties 文件），就可以通过下面这样的方式保存在 ConfigMap 里： 12345678910111213141516171819202122# .properties 文件的内容$ cat example/ui.propertiescolor.good=purplecolor.bad=yellowallow.textmode=truehow.nice.to.look=fairlyNice# 从.properties 文件创建 ConfigMap$ kubectl create configmap ui-config --from-file=example/ui.properties# 查看这个 ConfigMap 里保存的信息 (data)$ kubectl get configmaps ui-config -o yamlapiVersion: v1data:ui.properties: | color.good=purple color.bad=yellow allow.textmode=true how.nice.to.look=fairlyNicekind: ConfigMapmetadata:name: ui-config... 备注：kubectl get -o yaml 这样的参数，会将指定的 Pod API 对象以 YAML 的方式展示出来。 Downward API它的作用是：让 Pod 里的容器能够直接获取到这个 Pod API 对象本身的信息。 1234567891011121314151617181920212223242526272829303132apiVersion: v1kind: Podmetadata:name: test-downwardapi-volumelabels: zone: us-est-coast cluster: test-cluster1 rack: rack-22spec:containers: - name: client-container image: k8s.gcr.io/busybox command: [&quot;sh&quot;, &quot;-c&quot;] args: - while true; do if [[ -e /etc/podinfo/labels ]]; then echo -en &#x27;\\n\\n&#x27;; cat /etc/podinfo/labels; fi; sleep 5; done; volumeMounts: - name: podinfo mountPath: /etc/podinfo readOnly: falsevolumes: - name: podinfo projected: sources: - downwardAPI: items: - path: &quot;labels&quot; fieldRef: fieldPath: metadata.labels 在这个 Pod 的 YAML 文件中，我定义了一个简单的容器，声明了一个 projected 类型的 Volume。只不过这次 Volume 的数据来源，变成了 Downward API。而这个 Downward API Volume，则声明了要暴露 Pod 的 metadata.labels 信息给容器。 通过这样的声明方式，当前 Pod 的 Labels 字段的值，就会被 Kubernetes 自动挂载成为容器里的 /etc/podinfo/labels 文件。 而这个容器的启动命令，则是不断打印出 /etc/podinfo/labels 里的内容。所以，当我创建了这个 Pod 之后，就可以通过 kubectl logs 指令，查看到这些 Labels 字段被打印出来，如下所示： 12345$ kubectl create -f dapi-volume.yaml$ kubectl logs test-downwardapi-volumecluster=&quot;test-cluster1&quot;rack=&quot;rack-22&quot;zone=&quot;us-est-coast&quot; 目前，Downward API 支持的字段已经非常丰富了，比如： 12345678910111213141516171. 使用 fieldRef 可以声明使用:spec.nodeName - 宿主机名字status.hostIP - 宿主机 IPmetadata.name - Pod 的名字metadata.namespace - Pod 的 Namespacestatus.podIP - Pod 的 IPspec.serviceAccountName - Pod 的 Service Account 的名字metadata.uid - Pod 的 UIDmetadata.labels[&#x27;&lt;KEY&gt;&#x27;] - 指定 &lt;KEY&gt; 的 Label 值metadata.annotations[&#x27;&lt;KEY&gt;&#x27;] - 指定 &lt;KEY&gt; 的 Annotation 值metadata.labels - Pod 的所有 Labelmetadata.annotations - Pod 的所有 Annotation2. 使用 resourceFieldRef 可以声明使用:容器的 CPU limit容器的 CPU request容器的 memory limit容器的 memory request Service Account (ServiceAccountToken)我现在有了一个 Pod，我能不能在这个 Pod 里安装一个 Kubernetes 的 Client，这样就可以从容器里直接访问并且操作这个 Kubernetes 的 API 了呢？ 你首先要解决 API Server 的授权问题。 Service Account 对象的作用，就是 Kubernetes 系统内置的一种“服务账户”，它是 Kubernetes 进行权限分配的对象。 比如，Service Account A，可以只被允许对 Kubernetes API 进行 GET 操作， 而 Service Account B，则可以有 Kubernetes API 的所有操作的权限。 再来看 Pod 的另一个重要的配置：容器健康检查和恢复机制我们一起来看一个 Kubernetes 文档中的例子 123456789101112131415161718192021apiVersion: v1kind: Podmetadata:labels: test: livenessname: test-liveness-execspec:containers:- name: liveness image: busybox args: - /bin/sh - -c - touch /tmp/healthy; sleep 30; rm -rf /tmp/healthy; sleep 600 livenessProbe: exec: command: - cat - /tmp/healthy initialDelaySeconds: 5 periodSeconds: 5 在这个 Pod 中，我们定义了一个有趣的容器。它在启动之后做的第一件事，就是在 /tmp 目录下创建了一个 healthy 文件，以此作为自己已经正常运行的标志。而 30 s 过后，它会把这个文件删除掉。 与此同时，我们定义了一个这样的 livenessProbe（健康检查）。它的类型是 exec，这意味着，它会在容器启动后，在容器里面执行一句我们指定的命令，比如：“cat /tmp/healthy”。这时，如果这个文件存在，这条命令的返回值就是 0，Pod 就会认为这个容器不仅已经启动，而且是健康的。这个健康检查，在容器启动 5 s 后开始执行（initialDelaySeconds: 5），每 5 s 执行一次（periodSeconds: 5） 你还可以通过设置 restartPolicy，改变 Pod 的恢复策略。除了 Always，它还有 OnFailure 和 Never 两种情况： Always：在任何情况下，只要容器不在运行状态，就自动重启容器； OnFailure: 只在容器 异常时才自动重启容器； Never: 从来不重启容器。 只要 Pod 的 restartPolicy 指定的策略允许重启异常的容器（比如：Always），那么这个 Pod 就会保持 Running 状态，并进行容器重启。否则，Pod 就会进入 Failed 状态 。 对于包含多个容器的 Pod，只有它里面所有的容器都进入异常状态后，Pod 才会进入 Failed 状态。 PodPreset 里定义的内容，只会在 Pod API 对象被创建之前追加在这个对象本身上，而不会影响任何 Pod 的控制器的定义。","categories":[],"tags":[]},{"title":"6.搭建一个完整的Kubernetes集群","slug":"6-搭建一个完整的Kubernetes集群","date":"2021-07-01T12:56:08.000Z","updated":"2021-07-03T02:13:29.821Z","comments":true,"path":"2021/07/01/6-搭建一个完整的Kubernetes集群/","link":"","permalink":"https://sk-xinye.github.io/2021/07/01/6-%E6%90%AD%E5%BB%BA%E4%B8%80%E4%B8%AA%E5%AE%8C%E6%95%B4%E7%9A%84Kubernetes%E9%9B%86%E7%BE%A4/","excerpt":"","text":"目标 在所有节点上安装 Docker 和 kubeadm； 部署 Kubernetes Master； 部署容器网络插件； 部署 Kubernetes Worker； 部署 Dashboard 可视化插件； 部署容器存储插件。 安装 kubeadm 和 Docker123456$ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -$ cat &lt;&lt;EOF &gt; /etc/apt/sources.list.d/kubernetes.listdeb http://apt.kubernetes.io/ kubernetes-xenial mainEOF$ apt-get update$ apt-get install -y docker.io kubeadm 部署 Kubernetes 的 Master 节点这里我编写了一个给 kubeadm 用的 YAML 文件（名叫：kubeadm.yaml）： 123456789apiVersion: kubeadm.k8s.io/v1alpha1kind: MasterConfigurationcontrollerManagerExtraArgs: horizontal-pod-autoscaler-use-rest-clients: &quot;true&quot; horizontal-pod-autoscaler-sync-period: &quot;10s&quot; node-monitor-grace-period: &quot;10s&quot;apiServerExtraArgs: runtime-config: &quot;api/all=true&quot;kubernetesVersion: &quot;stable-1.11&quot; 然后，我们只需要执行一句指令： 1$kubeadm init --config kubeadm.yaml 部署完成后，kubeadm 会生成一行指令： 1kubeadm join 10.168.0.2:6443 --token 00bwbx.uvnaa2ewjflwu1ry --discovery-token-ca-cert-hash sha256:00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711 这个 kubeadm join 命令，就是用来给这个 Master 节点添加更多工作节点（Worker）的命令。我们在后面部署 Worker 节点的时候马上会用到它，所以找一个地方把这条命令记录下来。 现在，我们就可以使用 kubectl get 命令来查看当前唯一一个节点的状态了： 123$ kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster NotReady master 1d v1.11.1 kubectl describe node master对象的详细信息、状态和事件（Event） 部署网络插件1$kubectl apply -f https:&#x2F;&#x2F;git.io&#x2F;weave-kube-1.6 部署完成后，我们可以通过 kubectl get 重新检查 Pod 的状态： 1234567891011$ kubectl get pods -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-78fcdf6894-j9s52 1/1 Running 0 1dcoredns-78fcdf6894-jm4wf 1/1 Running 0 1detcd-master 1/1 Running 0 9skube-apiserver-master 1/1 Running 0 9skube-controller-manager-master 1/1 Running 0 9skube-proxy-xbd47 1/1 Running 0 1dkube-scheduler-master 1/1 Running 0 9sweave-net-cmk27 2/2 Running 0 19s 部署 Kubernetes 的 Worker 节点Kubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 Pod。 所以，相比之下，部署 Worker 节点反而是最简单的，只需要两步即可完成。 第一步，在所有 Worker 节点上执行“安装 kubeadm 和 Docker”一节的所有步骤。 第二步，执行部署 Master 节点时生成的 kubeadm join 指令：1$kubeadm join 10.168.0.2:6443 --token 00bwbx.uvnaa2ewjflwu1ry --discovery-token-ca-cert-hash sha256:00eb62a2a6020f94132e3fe1ab721349bbcd3e9b94da9654cfe15f2985ebd711 通过 Taint/Toleration 调整 Master 执行 Pod 的策略 我在前面提到过，默认情况下 Master 节点是不允许运行用户 Pod 的。而 Kubernetes 做到这一点，依靠的是 Kubernetes 的 Taint/Toleration 机制。 它的原理非常简单：一旦某个节点被加上了一个 Taint，即被“打上了污点”，那么所有 Pod 就都不能在这个节点上运行，因为 Kubernetes 的 Pod 都有“洁癖” 12#为节点打上“污点”（Taint）的命令$kubectl taint nodes node1 foo=bar:NoSchedule 除非，有个别的 Pod 声明自己能“容忍”这个“污点”，即声明了 Toleration，它才可以在这个节点上运行。 我们只要在 Pod 的.yaml 文件中的 spec 部分，加入 tolerations 字段即可： 123456789apiVersion: v1kind: Pod...spec:tolerations:- key: &quot;foo&quot; operator: &quot;Equal&quot; value: &quot;bar&quot; effect: &quot;NoSchedule&quot; 部署 Dashboard 可视化插件在 Kubernetes 社区中，有一个很受欢迎的 Dashboard 项目，它可以给用户提供一个可视化的 Web 界面来查看当前集群的各种信息。 1$kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml 部署完成之后，我们就可以查看 Dashboard 对应的 Pod 的状态了： 部署容器存储插件用两条指令，Rook 就可以把复杂的 Ceph 存储后端部署起来： 12345678910111213$kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/operator.yaml$kubectl apply -f https://raw.githubusercontent.com/rook/rook/master/cluster/examples/kubernetes/ceph/cluster.yaml$ kubectl get pods -n rook-ceph-systemNAME READY STATUS RESTARTS AGErook-ceph-agent-7cv62 1/1 Running 0 15srook-ceph-operator-78d498c68c-7fj72 1/1 Running 0 44srook-discover-2ctcv 1/1 Running 0 15s$ kubectl get pods -n rook-cephNAME READY STATUS RESTARTS AGErook-ceph-mon0-kxnzh 1/1 Running 0 13srook-ceph-mon1-7dn2t 1/1 Running 0 2s 我的第一个容器化应用通过编写yml文件通过指令（$ kubectl create -f 我的配置文件）运行他们 12345678910111213141516171819apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deploymentspec: selector: matchLabels: app: nginx replicas: 2 template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 所谓 Deployment，是一个定义多副本应用（即多个副本 Pod）的对象 此外，Deployment 还负责在 Pod 定义发生变化时，对每个副本进行滚动更新（Rolling Update）。 我给它定义的 Pod 副本个数 (spec.replicas) 是：2。 Pod 具体的又长什么样子,我定义了一个 Pod 模版（spec.template），这个模版描述了我想要创建的 Pod 的细节。 这个 Pod 里只有一个容器，这个容器的镜像（spec.containers.image）是 nginx:1.7.9，这个容器监听端口（containerPort）是 80。 Pod 就是 Kubernetes 世界里的“应用”；而一个应用，可以由多个容器组成。 像这样使用一种 API 对象（Deployment）管理另一种 API 对象（Pod）的方法，在 Kubernetes 中，叫作“控制器”模式（controller pattern）。 每一个 API 对象都有一个叫作 Metadata 的字段，这个字段就是 API 对象的“标识”，即元数据，它也是我们从 Kubernetes 里找到这个对象的主要依据。这其中最主要使用到的字段是 Labels。 而像 Deployment 这样的控制器对象，就可以通过这个 Labels 字段从 Kubernetes 中过滤出它所关心的被控制对象。 比如，在上面这个 YAML 文件中，Deployment 会把所有正在运行的、携带“app: nginx”标签的 Pod 识别为被管理的对象，并确保这些 Pod 的总数严格等于两个。 而这个过滤规则的定义，是在 Deployment 的“spec.selector.matchLabels”字段。我们一般称之为：Label Selector。 另外，在 Metadata 中，还有一个与 Labels 格式、层级完全相同的字段叫 Annotations，它专门用来携带 key-value 格式的内部信息 所谓内部信息，指的是对这些信息感兴趣的，是 Kubernetes 组件本身，而不是用户。所以大多数 Annotations，都是在 Kubernetes 运行过程中，被自动加在这个 API 对象上。 一个 Kubernetes 的 API 对象的定义，大多可以分为 Metadata 和 Spec 两个部分。前者存放的是这个对象的元数据，对所有 API 对象来说，这一部分的字段和格式基本上是一样的；而后者存放的，则是属于这个对象独有的定义，用来描述它所要表达的功能。 运行 查看yml运行状态 查看API对象细节 12345678$kubectl create -f nginx-deployment.yaml$ kubectl get pods -l app=nginxNAME READY STATUS RESTARTS AGEnginx-deployment-67594d6bf6-9gdvr 1/1 Running 0 10mnginx-deployment-67594d6bf6-v6j7w 1/1 Running 0 10m$ kubectl describe pod nginx-deployment-67594d6bf6-9gdvr kubectl get 指令的作用，就是从 Kubernetes 里面获取（GET）指定的 API 对象。可以看到，在这里我还加上了一个 -l 参数，即获取所有匹配 app: nginx 标签的 Pod。需要注意的是，在命令行中，所有 key-value 格式的参数，都使用“=”而非“:”表示。 describe很重要，如果有异常发生，你一定要第一时间查看这些其中的Events，往往可以看到非常详细的错误信息。 升级修改yml即可 12345678910111213141516171819... spec: containers: - name: nginx image: nginx:1.8 # 这里被从 1.7.9 修改为 1.8 ports: - containerPort: 80 volumeMounts: - mountPath: &quot;/usr/share/nginx/html&quot; name: nginx-vol volumes: - name: nginx-vol emptyDir: &#123;&#125; # 或者： volumes: - name: nginx-vol hostPath: path: /var/data 我们可以使用 kubectl replace 或者apply指令来完成这个更新： 12345$kubectl replace -f nginx-deployment.yaml$ kubectl apply -f nginx-deployment.yaml# 修改 nginx-deployment.yaml 的内容$ kubectl apply -f nginx-deployment.yaml 而 Pod 中的容器，使用的是 volumeMounts 字段来声明自己要挂载哪个 Volume，并通过 mountPath 字段来定义容器内的 Volume 目录，比如：/usr/share/nginx/html。","categories":[],"tags":[]},{"title":"5.Kubernetes一键部署利器kubeadm","slug":"5-Kubernetes一键部署利器kubeadm","date":"2021-07-01T07:25:57.000Z","updated":"2021-07-02T00:09:54.227Z","comments":true,"path":"2021/07/01/5-Kubernetes一键部署利器kubeadm/","link":"","permalink":"https://sk-xinye.github.io/2021/07/01/5-Kubernetes%E4%B8%80%E9%94%AE%E9%83%A8%E7%BD%B2%E5%88%A9%E5%99%A8kubeadm/","excerpt":"","text":"要真正发挥容器技术的实力，你就不能仅仅局限于对 Linux 容器本身的钻研和使用。 更深入的学习容器技术的关键在于，如何使用这些技术来“容器化”你的应用。 我们的应用既可能是 Java Web 和 MySQL 这样的组合 也可能是 Cassandra 这样的分布式系统。哪些 Cassandra 容器是主，哪些是从？主从容器如何区分？它们之间又如何进行自动发现和通信？Cassandra 容器的持久化数据又如何保持，等等。 这也是为什么我们要反复强调 Kubernetes 项目的主要原因：这个项目体现出来的容器化“表达能力”，具有独有的先进性和完备性。这就使得它不仅能运行 Java Web 与 MySQL 这样的常规组合，还能够处理 Cassandra 容器集群等复杂编排问题。所以，对这种编排能力的剖析、解读和最佳实践，将是本专栏最重要的一部分内容。 Kubernetes 项目简单的部署方法 kubeadm部署方式这个项目的目的，就是要让用户能够通过这样两条指令完成一个 Kubernetes 集群的部署： # 创建一个 Master 节点 $ kubeadm init # 将一个 Node 节点加入到当前集群中 $ kubeadm join &lt;Master 节点的 IP 和端口 &gt; kubeadm 的工作原理Kubernetes 部署问题 到目前为止，在容器里运行 kubelet，依然没有很好的解决办法，我也不推荐你用容器去部署 Kubernetes 项目。 把 kubelet 直接运行在宿主机上，然后使用容器部署其他的 Kubernetes 组件。 你使用 kubeadm 的第一步，是在机器上手动安装 kubeadm、kubelet 和 kubectl 这三个二进制文件。当然，kubeadm 的作者已经为各个发行版的 Linux 准备好了安装包，所以你只需要执行： $ apt-get install kubeadm 使用“kubeadm init”部署 Master 节点了。 kubeadm init 的工作流程 kubeadm 首先要做的，是一系列的检查工作，以确定这台机器可以用来部署 Kubernetes。这一步检查，我们称为“Preflight Checks”，它可以为你省掉很多后续的麻烦。 Preflight Checks 包括了很多方面，比如： Linux 内核的版本必须是否是 3.10 以上？ Linux Cgroups 模块是否可用？ 机器的 hostname 是否标准？在 Kubernetes 项目里，机器的名字以及一切存储在 Etcd 中的 API 对象，都必须使用标准的 DNS 命名（RFC 1123）。 用户安装的 kubeadm 和 kubelet 的版本是否匹配？ 机器上是不是已经安装了 Kubernetes 的二进制文件？ Kubernetes 的工作端口 10250/10251/10252 端口是不是已经被占用？ ip、mount 等 Linux 指令是否存在？ Docker 是否已经安装？ 在通过了 Preflight Checks 之后，kubeadm 要为你做的，是生成 Kubernetes 对外提供服务所需的各种证书和对应的目录。 Kubernetes 对外提供服务时，除非专门开启“不安全模式”，否则都要通过 HTTPS 才能访问 kube-apiserver。这就需要为 Kubernetes 集群配置好证书文件。 kubeadm 为 Kubernetes 项目生成的证书文件都放在 Master 节点的 /etc/kubernetes/pki 目录下。在这个目录下，最主要的证书文件是 ca.crt 和对应的私钥 ca.key。 用户使用 kubectl 获取容器日志等 streaming 操作时，需要通过 kube-apiserver 向 kubelet 发起请求，这个连接也必须是安全的。 kubeadm 为这一步生成的是 apiserver-kubelet-client.crt 文件，对应的私钥是 apiserver-kubelet-client.key。 证书生成后，kubeadm 接下来会为其他组件生成访问 kube-apiserver 所需的配置文件。这些文件的路径是：/etc/kubernetes/xxx.conf： ls /etc/kubernetes/ admin.conf controller-manager.conf kubelet.conf scheduler.conf 这些文件里面记录的是，当前这个 Master 节点的服务器地址、监听端口、证书目录等信息。这样，对应的客户端（比如 scheduler，kubelet 等），可以直接加载相应的文件，使用里面的信息与 kube-apiserver 建立安全连接。 接下来，kubeadm 会为 Master 组件生成 Pod 配置文件。 Kubernetes 有三个 Master 组件 kube-apiserver、kube-controller-manager、kube-scheduler，而它们都会被使用 Pod 的方式部署起来。 在 Kubernetes 中，有一种特殊的容器启动方法叫做“Static Pod”。它允许你把要部署的 Pod 的 YAML 文件放在一个指定的目录里。这样，当这台机器上的 kubelet 启动时，它会自动检查这个目录，加载所有的 Pod YAML 文件，然后在这台机器上启动它们。 kubelet 在 Kubernetes 项目中的地位非常高，在设计上它就是一个完全独立的组件，而其他 Master 组件，则更像是辅助性的系统容器。 在 kubeadm 中，Master 组件的 YAML 文件会被生成在 /etc/kubernetes/manifests 路径下。 123456789101112131415161718192021222324252627282930313233343536373839404142apiVersion: v1kind: Podmetadata:annotations: scheduler.alpha.kubernetes.io/critical-pod: &quot;&quot;creationTimestamp: nulllabels: component: kube-apiserver tier: control-planename: kube-apiservernamespace: kube-systemspec:containers:- command: - kube-apiserver - --authorization-mode=Node,RBAC - --runtime-config=api/all=true - --advertise-address=10.168.0.2 ... - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key image: k8s.gcr.io/kube-apiserver-amd64:v1.11.1 imagePullPolicy: IfNotPresent livenessProbe: ... name: kube-apiserver resources: requests: cpu: 250m volumeMounts: - mountPath: /usr/share/ca-certificates name: usr-share-ca-certificates readOnly: true ...hostNetwork: truepriorityClassName: system-cluster-criticalvolumes:- hostPath: path: /etc/ca-certificates type: DirectoryOrCreate name: etc-ca-certificates... 这个 Pod 里只定义了一个容器，它使用的镜像是：k8s.gcr.io/kube-apiserver-amd64:v1.11.1 。这个镜像是 Kubernetes 官方维护的一个组件镜像。 这个容器的启动命令（commands）是 kube-apiserver –authorization-mode=Node,RBAC …，这样一句非常长的命令。其实，它就是容器里 kube-apiserver 这个二进制文件再加上指定的配置参数而已。 如果你要修改一个已有集群的 kube-apiserver 的配置，需要修改这个 YAML 文件。 这些组件的参数也可以在部署时指定。 在这一步完成后，kubeadm 还会再生成一个 Etcd 的 Pod YAML 文件，用来通过同样的 Static Pod 的方式启动 Etcd。所以，最后 Master 组件的 Pod YAML 文件如下所示 $ ls /etc/kubernetes/manifests/ etcd.yaml kube-apiserver.yaml kube-controller-manager.yaml kube-scheduler.yaml 一旦这些 YAML 文件出现在被 kubelet 监视的 /etc/kubernetes/manifests 目录下，kubelet 就会自动创建这些 YAML 文件中定义的 Pod，即 Master 组件的容器。 Master 容器启动后，kubeadm 会通过检查 localhost:6443/healthz 这个 Master 组件的健康检查 URL，等待 Master 组件完全运行起来。 然后，kubeadm 就会为集群生成一个 bootstrap token。在后面，只要持有这个 token，任何一个安装了 kubelet 和 kubadm 的节点，都可以通过 kubeadm join 加入到这个集群当中。 这个 token 的值和使用方法会，会在 kubeadm init 结束后被打印出来。 在 token 生成之后，kubeadm 会将 ca.crt 等 Master 节点的重要信息，通过 ConfigMap 的方式保存在 Etcd 当中，供后续部署 Node 节点使用。 这个 ConfigMap 的名字是 cluster-info。 kubeadm init 的最后一步，就是安装默认插件。Kubernetes 默认 kube-proxy 和 DNS 这两个插件是必须安装的。它们分别用来提供整个集群的服务发现和 DNS 功能。 其实，这两个插件也只是两个容器镜像而已，所以 kubeadm 只要用 Kubernetes 客户端创建两个 Pod 就可以了。 kubeadm join 的工作流程这个流程其实非常简单，kubeadm init 生成 bootstrap token 之后，你就可以在任意一台安装了 kubelet 和 kubeadm 的机器上执行 kubeadm join 了。 cluster-info 里的 kube-apiserver 的地址、端口、证书，kubelet 就可以以“安全模式”连接到 apiserver 上，这样一个新的节点就部署完成了。 配置 kubeadm 的部署参数kubeadm 确实简单易用，可是我又该如何定制我的集群组件参数呢？ 在这里，我强烈推荐你在使用 kubeadm init 部署 Master 节点时，使用下面这条指令： 1$ kubeadm init --config kubeadm.yaml 这时，你就可以给 kubeadm 提供一个 YAML 文件（比如，kubeadm.yaml），它的内容如下所示（我仅列举了主要部分） 123456789101112131415161718192021222324252627apiVersion: kubeadm.k8s.io/v1alpha2kind: MasterConfigurationkubernetesVersion: v1.11.0api:advertiseAddress: 192.168.0.102bindPort: 6443...etcd:local: dataDir: /var/lib/etcd image: &quot;&quot;imageRepository: k8s.gcr.iokubeProxy:config: bindAddress: 0.0.0.0 ...kubeletConfiguration:baseConfig: address: 0.0.0.0 ...networking:dnsDomain: cluster.localpodSubnet: &quot;&quot;serviceSubnet: 10.96.0.0/12nodeRegistration:criSocket: /var/run/dockershim.sock... 然后，kubeadm 就会使用上面这些信息替换 /etc/kubernetes/manifests/kube-apiserver.yaml 里的 command 字段里的参数了。 而这个 YAML 文件提供的可配置项远不止这些。比如，你还可以修改 kubelet 和 kube-proxy 的配置，修改 Kubernetes 使用的基础镜像的 URL（默认的k8s.gcr.io/xxx镜像 URL 在国内访问是有困难的），指定自己的证书文件，指定特殊的容器运行时等等。这些配置项，就留给你在后续实践中探索了。 总结 kubeadm 能够用于生产环境吗？ 到目前为止（2018 年 9 月），这个问题的答案是：不能。 如果你有部署规模化生产环境的需求，我推荐使用kops或者 SaltStack 这样更复杂的部署工具 一方面，作为 Kubernetes 项目的原生部署工具，kubeadm 对 Kubernetes 项目特性的使用和集成，确实要比其他项目“技高一筹”，非常值得我们学习和借鉴； 另一方面，kubeadm 的部署方法，不会涉及到太多的运维工作，也不需要我们额外学习复杂的部署工具。而它部署的 Kubernetes 集群，跟一个完全使用二进制文件搭建起来的集群几乎没有任何区别。","categories":[],"tags":[]},{"title":"4.初识k8s","slug":"4-初识k8s","date":"2021-07-01T01:12:11.000Z","updated":"2021-07-01T12:45:07.206Z","comments":true,"path":"2021/07/01/4-初识k8s/","link":"","permalink":"https://sk-xinye.github.io/2021/07/01/4-%E5%88%9D%E8%AF%86k8s/","excerpt":"","text":"回顾docker 一个“容器”，实际上是一个由 Linux Namespace、Linux Cgroups 和 rootfs 三种技术构建出来的进程的隔离环境。 一个正在运行的 Linux 容器，其实可以被“一分为二”地看待： 一组联合挂载在 /var/lib/docker/aufs/mnt 上的 rootfs，这一部分我们称为“容器镜像”（Container Image），是容器的静态视图； 一个由 Namespace+Cgroups 构成的隔离环境，这一部分我们称为“容器运行时”（Container Runtime），是容器的动态视图。 初识k8s容器就从一个开发者手里的小工具，一跃成为了云计算领域的绝对主角；而能够定义容器组织和管理规范的“容器编排”技术，则当仁不让地坐上了容器技术领域的“头把交椅”。 最具代表性的容器编排工具，当属 Docker 公司的 Compose+Swarm 组合，以及 Google 与 RedHat 公司共同主导的 Kubernetes 项目。 Kubernetes 项目 前身是google 的Borg 系统， Kubernetes 项目要解决的问题是什么 在不同的发展阶段，Kubernetes 需要着重解决的问题是不同的。 但是，对于大多数用户来说，他们希望 Kubernetes 项目带来的体验是确定的：现在我有了应用的容器镜像，请帮我在一个给定的集群上把这个应用运行起来。 更进一步地说，我还希望 Kubernetes 能给我提供路由网关、水平扩展、监控、备份、灾难恢复等一系列运维能力。 运行在大规模集群中的各种任务之间，实际上存在着各种各样的关系。这些关系的处理，才是作业编排和管理系统最困难的地方。 基础架构 跟它的原型项目 Borg 非常类似，都由 Master 和 Node 两种节点组成，而这两种角色分别对应着控制节点和计算节点。 控制节点，即 Master 节点，由三个紧密协作的独立组件组合而成，它们分别是负责 API 服务的 kube-apiserver、负责调度的 kube-scheduler，以及负责容器编排的 kube-controller-manager。 整个集群的持久化数据，则由 kube-apiserver 处理后保存在 Etcd 中。 在 Kubernetes 项目中，kubelet 主要负责同容器运行时（比如 Docker 项目）打交道。 而这个交互所依赖的，是一个称作 CRI（Container Runtime Interface）的远程调用接口，这个接口定义了容器运行时的各项核心操作，比如：启动一个容器需要的所有参数。 而具体的容器运行时，比如 Docker 项目，则一般通过 OCI 这个容器运行时规范同底层的 Linux 操作系统进行交互，即：把 CRI 请求翻译成对 Linux 操作系统的调用（操作 Linux Namespace 和 Cgroups 等）。 kubelet 还通过 gRPC 协议同一个叫作 Device Plugin 的插件进行交互。 这个插件，是 Kubernetes 项目用来管理 GPU 等宿主机物理设备的主要组件，也是基于 Kubernetes 项目进行机器学习训练、高性能作业支持等工作必须关注的功能。 kubelet 的另一个重要功能，则是调用网络插件和存储插件为容器配置网络和持久化存储。 这两个插件与 kubelet 进行交互的接口，分别是 CNI（Container Networking Interface）和 CSI（Container Storage Interface）。 Borg 对于 Kubernetes 项目的指导作用又体现在哪里呢答案是，Master 节点。即：如何编排、管理、调度用户提交的作业？ Kubernetes 项目最主要的设计思想是，从更宏观的角度，以统一的方式来定义任务之间的各种关系，并且为将来支持更多种类的关系留有余地。 pod Kubernetes 项目对容器间的“访问”进行了分类，首先总结出了一类非常常见的“紧密交互”的关系，即：这些应用之间需要非常频繁的交互和访问；又或者，它们会直接通过本地文件进行信息交换。 在常规环境下，这些应用往往会被直接部署在同一台机器上，通过 Localhost 通信，通过本地磁盘目录交换文件。而在 Kubernetes 项目中，这些容器则会被划分为一个“Pod”，Pod 里的容器共享同一个 Network Namespace、同一组数据卷，从而达到高效率交换信息的目的。 而对于另外一种更为常见的需求，比如 Web 应用与数据库之间的访问关系，Kubernetes 项目则提供了一种叫作“Service”的服务。像这样的两个应用，往往故意不部署在同一台机器上，这样即使 Web 应用所在的机器宕机了，数据库也完全不受影响。可是，我们知道，对于一个容器来说，它的 IP 地址等信息不是固定的，那么 Web 应用又怎么找到数据库容器的 Pod 呢？ 所以，Kubernetes 项目的做法是给 Pod 绑定一个 Service 服务，而 Service 服务声明的 IP 地址等信息是“终生不变”的。这个Service 服务的主要作用，就是作为 Pod 的代理入口（Portal），从而代替 Pod 对外暴露一个固定的网络地址。 按照这幅图的线索，我们从容器这个最基础的概念出发，首先遇到了容器间“紧密协作”关系的难题，于是就扩展到了 Pod； 有了 Pod 之后，我们希望能一次启动多个应用的实例，这样就需要 Deployment 这个 Pod 的多实例管理器； 而有了这样一组相同的 Pod 后，我们又需要通过一个固定的 IP 地址和端口以负载均衡的方式访问它，于是就有了 Service。 如果现在两个不同 Pod 之间不仅有“访问关系”，还要求在发起时加上授权信息。Kubernetes 项目提供了一种叫作 Secret 的对象，它其实是一个保存在 Etcd 里的键值对数据。 运行形态也是关键因素，比如 Job，用来描述一次性运行的 Pod（比如，大数据任务）；再比如 DaemonSet，用来描述每个宿主机上必须且只能运行一个副本的守护进程服务；又比如 CronJob，则用于描述定时任务等等。 可以看到，Kubernetes 项目并没有像其他项目那样，为每一个管理功能创建一个指令，然后在项目中实现其中的逻辑。这种做法，的确可以解决当前的问题，但是在更多的问题来临之后，往往会力不从心。 相比之下，在 Kubernetes 项目中，我们所推崇的使用方法是： 首先，通过一个“编排对象”，比如 Pod、Job、CronJob 等，来描述你试图管理的应用； 然后，再为它定义一些“服务对象”，比如 Service、Secret、Horizontal Pod Autoscaler（自动水平扩展器）等。这些对象，会负责具体的平台级功能。 声明式 API123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment labels: app: nginxspec: replicas: 2 selector: matchLabels: app: nginx template: metadata: labels: app: nginx spec: containers: - name: nginx image: nginx:1.7.9 ports: - containerPort: 80 在上面这个 YAML 文件中，我们定义了一个 Deployment 对象，它的主体部分（spec.template 部分）是一个使用 Nginx 镜像的 Pod，而这个 Pod 的副本数是 2（replicas=2）。 执行 $ kubectl create -f nginx-deployment.yaml","categories":[],"tags":[]},{"title":"3.深入理解容器镜像","slug":"3-深入理解容器镜像","date":"2021-06-30T01:44:21.000Z","updated":"2021-07-01T12:45:07.204Z","comments":true,"path":"2021/06/30/3-深入理解容器镜像/","link":"","permalink":"https://sk-xinye.github.io/2021/06/30/3-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F/","excerpt":"","text":"Namespace 的作用是“隔离”，它让应用进程只能看到该 Namespace 内的“世界”；而 Cgroups 的作用是“限制”，它给这个“世界”围上了一圈看不见的墙。 容器里的进程看到的文件系统又是什么样子的呢实现 Mount Namespace 修改的，是容器进程对文件系统“挂载点”的认知。 这也就意味着，只有在“挂载”这个操作发生之后，进程的视图才会被改变。 而在此之前，新创建的容器会直接继承宿主机的各个挂载点。 这就是 Mount Namespace 跟其他 Namespace 的使用略有不同的地方：它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效。 挂载“/”目录，使容器内的文件系统单一 在 Linux 操作系统里，有一个名为 chroot 的命令change root file system 即改变进程的根目录到你指定的位置。 假设，我们现在有一个 $HOME/test 目录，想要把它作为一个 /bin/bash 进程的根目录。 首先，创建一个 test 目录和几个 lib 文件夹： 123$ mkdir -p $HOME/test$ mkdir -p $HOME/test/&#123;bin,lib64,lib&#125;$ cd $T 然后，把 bash 命令拷贝到 test 目录对应的 bin 路径下： 1$ cp -v /bin/&#123;bash,ls&#125; $HOME/test/bin 接下来，把 bash 命令需要的所有 so 文件，也拷贝到 test 目录对应的 lib 路径下。找到 so 文件可以用 ldd 命令： 123$ T=$HOME/test$ list=&quot;$(ldd /bin/ls | egrep -o &#x27;/lib.*\\.[0-9]&#x27;)&quot;$ for i in $list; do cp -v &quot;$i&quot; &quot;$&#123;T&#125;$&#123;i&#125;&quot;; done 最后，执行 chroot 命令，告诉操作系统，我们将使用 $HOME/test 目录作为 /bin/bash 进程的根目录： 1$ chroot $HOME/test /bin/bash 这时，你如果执行 “ls /“，就会看到，它返回的都是 $HOME/test 目录下面的内容，而不是宿主机的内容。 实际上，Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统里的第一个 Namespace。 我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如 Ubuntu16.04 的 ISO。这样，在容器启动之后，我们在容器里通过执行 “ls /“ 查看根目录下的内容，就是 Ubuntu 16.04 的所有目录和文件。 而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。 对 Docker 项目来说，它最核心的原理实际上就是为待创建的用户进程： 启用 Linux Namespace 配置； 设置指定的 Cgroups 参数； 切换进程的根目录（Change Root）。 不过，Docker 项目在最后一步的切换上会优先使用 pivot_root 系统调用，如果系统不支持，才会使用 chroot。 需要明确的是，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。 /usr/src/一般用于存放内核源代码 /boot一般用于存放可引导内核 /usr/lib/modules/kernel/存放内核内置的已编译好的驱动程序 所以说，rootfs 只包括了操作系统的“躯壳”，并没有包括操作系统的“灵魂”。 缺点容器相比于虚拟机的主要缺陷之一：毕竟后者不仅有模拟出来的硬件机器充当沙盒，而且每个沙盒里还运行着一个完整的 Guest OS 给应用随便折腾。 由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。 对一个应用来说，操作系统本身才是它运行所需要的最完整的“依赖库”。 联合文件系统（Union File System） 123456789101112131415$ tree.├── A│ ├── a│ └── x└── B├── b└── x$ mkdir C$ mount -t aufs -o dirs=./A:./B none ./C$ tree ./C./C├── a├── b└── x 层的概念就是在联合操作系统中 镜像的层都放置在 /var/lib/docker/image/overlay2 目录下，然后被联合挂载在 /var/lib/docker/aufs/mnt 里面。 在/sys/fs/aufs 有个si=972c6d361e6b32ba。 然后使用这个 ID，你就可以在 /sys/fs/aufs 下查看被联合挂载在一起的各个层的信息： 分层 第一部分，只读层。 它是这个容器的 rootfs 最下面的五层，对应的正是 ubuntu:latest 镜像的五层。可以看到，它们的挂载方式都是只读的（ro+wh，即 readonly+whiteout，至于什么是 whiteout，我下面马上会讲到）。 为了实现这样的删除操作，AuFS 会在可读写层创建一个 whiteout 文件，把只读层里的文件“遮挡”起来。 第二部分，可读写层。 它是这个容器的 rootfs 最上面的一层（6e3be5d2ecccae7cc），它的挂载方式为：rw，即 read write。在没有写入文件之前，这个目录是空的。而一旦在容器里做了写操作，你修改产生的内容就会以增量的方式出现在这个层中。 第三部分，Init 层。 它是一个以“-init”结尾的层，夹在只读层和读写层之间。Init 层是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。 总结 rootfs。它只是一个操作系统的所有文件和目录，并不包含内核，最多也就几百兆。而相比之下，传统虚拟机的镜像大多是一个磁盘的“快照”，磁盘有多大，镜像就至少有多大。 通过结合使用 Mount Namespace 和 rootfs，容器就能够为进程构建出一个完善的文件系统隔离环境。当然，这个功能的实现还必须感谢 chroot 和 pivot_root 这两个系统调用切换进程根目录的能力。 而在 rootfs 的基础上，Docker 公司创新性地提出了使用多个增量 rootfs 联合挂载一个完整 rootfs 的方案，这就是容器镜像中“层”的概念。 通过“分层镜像”的设计，以 Docker 镜像为核心，来自不同公司、不同团队的技术人员被紧密地联系在了一起。而且，由于容器镜像的操作是增量式的，这样每次镜像拉取、推送的内容，比原本多个完整的操作系统的大小要小得多；而共享层的存在，可以使得所有这些容器镜像需要的总空间，也比每个镜像的总和要小。这样就使得基于容器镜像的团队协作，要比基于动则几个 GB 的虚拟机磁盘镜像的协作要敏捷得多。 更重要的是，一旦这个镜像被发布，那么你在全世界的任何一个地方下载这个镜像，得到的内容都完全一致，可以完全复现这个镜像制作者当初的完整环境。这，就是容器技术“强一致性”的重要体现。 制作 安装docker 1curl -sSL https://get.daocloud.io/docker | sh 用 Docker 部署一个用 Python 编写的 Web 应用 app.py 1234567891011121314from flask import Flaskimport socketimport osapp = Flask(__name__)@app.route(&#x27;/&#x27;)def hello():html = &quot;&lt;h3&gt;Hello &#123;name&#125;!&lt;/h3&gt;&quot; \\ &quot;&lt;b&gt;Hostname:&lt;/b&gt; &#123;hostname&#125;&lt;br/&gt;&quot;return html.format(name=os.getenv(&quot;NAME&quot;, &quot;world&quot;), hostname=socket.gethostname())if __name__ == &quot;__main__&quot;:app.run(host=&#x27;0.0.0.0&#x27;, port=80) requirements 1$ cat requirements.txt Flask 制作容器镜像 1234567891011121314# 使用官方提供的 Python 开发镜像作为基础镜像FROM python:2.7-slim# 将工作目录切换为 /appWORKDIR /app# 将当前目录下的所有内容复制到 /app 下ADD . /app# 使用 pip 命令安装这个应用所需要的依赖RUN pip install --trusted-host pypi.python.org -r requirements.txt# 允许外界访问容器的 80 端口EXPOSE 80# 设置环境变量ENV NAME World# 设置容器进程为：python app.py，即：这个 Python 应用的启动命令CMD [&quot;python&quot;, &quot;app.py&quot;] 这里，app.py 的实际路径是 /app/app.py。所以，CMD [“python”, “app.py”] 等价于 “docker run python app.py”。 另外，在使用 Dockerfile 时，你可能还会看到一个叫作 ENTRYPOINT 的原语。实际上，它和 CMD 都是 Docker 容器进程启动所必需的参数，完整执行格式是：“ENTRYPOINT CMD”。 默认情况下，Docker 会为你提供一个隐含的 ENTRYPOINT，即：/bin/sh -c。所以，在不指定 ENTRYPOINT 时，比如在我们这个例子里，实际上运行在容器里的完整进程是：/bin/sh -c “python app.py”，即 CMD 的内容就是 ENTRYPOINT 的参数。 Dockerfile 里的原语并不都是指对容器内部的操作。就比如 ADD，它指的是把当前目录（即 Dockerfile 所在的目录）里的文件，复制到指定容器内的目录当中。 读懂这个 Dockerfile 之后，我再把上述内容，保存到当前目录里一个名叫“Dockerfile”的文件中：$ ls Dockerfile app.py requirements.txt 接下来，我就可以让 Docker 制作这个镜像了，在当前目录执行：$ docker build -t helloworld . 其中，-t 的作用是给这个镜像加一个 Tag，即：起一个好听的名字。docker build 会自动加载当前目录下的 Dockerfile 文件，然后按照顺序，执行文件中的原语。 需要注意的是，Dockerfile 中的每个原语执行后，都会生成一个对应的镜像层。即使原语本身并没有明显地修改文件的操作（比如，ENV 原语），它对应的层也会存在。只不过在外界看来，这个层是空的。 接下来，我使用这个镜像，通过 docker run 命令启动容器：$ docker run -p 4000:80 helloworld 在这一句命令中，镜像名 helloworld 后面，我什么都不用写，因为在 Dockerfile 中已经指定了 CMD。 我已经通过 -p 4000:80 告诉了 Docker，请把容器内的 80 端口映射在宿主机的 4000 端口上。 用 docker tag 命令给容器镜像起一个完整的名字：$ docker tag helloworld geektime/helloworld:v1 然后，我执行 docker push：$ docker push geektime/helloworld:v1 我还可以使用 docker commit 指令，把一个正在运行的容器，直接提交为一个镜像。 $ docker push geektime/helloworld:v2 123456$ docker exec -it 4ddf4638572d /bin/sh# 在容器内部新建了一个文件root@4ddf4638572d:/app# touch test.txtroot@4ddf4638572d:/app# exit# 将这个新建的文件提交到镜像中保存$ docker commit 4ddf4638572d geektime/helloworld:v2 docker exec 是怎么做到进入容器里的呢 实际上，Linux Namespace 创建的隔离空间虽然看不见摸不着，但一个进程的 Namespace 信息在宿主机上是确确实实存在的，并且是以一个文件的方式存在。 通过如下指令，你可以看到当前正在运行的 Docker 容器的进程号（PID）是 25686： 1$ docker inspect --format &#x27;&#123;&#123; .State.Pid &#125;&#125;&#x27; 4ddf4638572d 这时，你可以通过查看宿主机的 proc 文件，看到这个 25686 进程的所有 Namespace 对应的文件 $ ls -l /proc/25686/ns 这也就意味着：一个进程，可以选择加入到某个进程已有的 Namespace 当中，从而达到“进入”这个进程所在容器的目的，这正是 docker exec 的实现原理。 而这个操作所依赖的，乃是一个名叫 setns() 的 Linux 系统调用。 $ docker run -it –net container:4ddf4638572d busybox ifconfig 共享network namespace 而如果我指定–net=host，就意味着这个容器不会为进程启用 Network Namespace。共享主机网络 docker commit docker commit，实际上就是在容器运行起来后，把最上层的“可读写层”，加上原先容器镜像的只读层，打包组成了一个新的镜像。当然，下面这些只读层在宿主机上是共享的，不会占用额外的空间。 而由于使用了联合文件系统，你在容器里对镜像 rootfs 所做的任何修改，都会被操作系统先复制到这个可读写层，然后再修改。这就是所谓的：Copy-on-Write。 而正如前所说，Init 层的存在，就是为了避免你执行 docker commit 时，把 Docker 自己对 /etc/hosts 等文件做的修改，也一起提交掉。 Volume（数据卷） Volume 机制，允许你将宿主机上指定的目录或者文件，挂载到容器里面进行读取和修改操作。 宿主机文件映射到容器中 $ docker run -v /test … $ docker run -v /home:/test … 在第一种情况下，由于你并没有显示声明宿主机目录，那么 Docker 就会默认在宿主机上创建一个临时目录 /var/lib/docker/volumes/[VOLUME_ID]/_data，然后把它挂载到容器的 /test 目录上。 而在第二种情况下，Docker 就直接把宿主机的 /home 目录挂载到容器的 /test 目录上。 总结2","categories":[],"tags":[]},{"title":"2.限制与隔离","slug":"2-限制与隔离","date":"2021-06-29T13:42:37.000Z","updated":"2021-06-30T13:08:13.464Z","comments":true,"path":"2021/06/29/2-限制与隔离/","link":"","permalink":"https://sk-xinye.github.io/2021/06/29/2-%E9%99%90%E5%88%B6%E4%B8%8E%E9%9A%94%E7%A6%BB/","excerpt":"","text":"进程一旦“程序”被执行起来，它就从磁盘上的二进制文件，变成了计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合。 容器Cgroups 技术是用来制造约束的主要手段，而Namespace 技术则是用来修改进程视图的主要方法。 隔离 通过int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL) 系统调用，创建进程号为“1”的独立隔离进程空间，达到隔离的作用 除了PID Namespace，Linux 操作系统还提供了 Mount、UTS、IPC、Network 和 User 这些 Namespace，用来对各种不同的进程上下文进行“障眼法”操作。 Mount Namespace，用于让被隔离进程只看到当前 Namespace 里的挂载点信息；Network Namespace，用于让被隔离进程看到当前 Namespace 里的网络设备和配置。 以上就是 Linux 容器最基本的实现原理了。 Docker 容器这个听起来玄而又玄的概念，实际上是在创建容器进程时，指定了这个进程所需要启用的一组 Namespace 参数。这样，容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。而对于宿主机以及其他不相关的程序，它就完全看不到了。 所以说，容器，其实是一种特殊的进程而已。 使用虚拟化技术作为应用沙盒，就必须要由 Hypervisor 来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的 Guest OS 才能执行用户的应用进程。这就不可避免地带来了额外的资源消耗和占用。 隔离得不彻底 首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。 尽管你可以在容器里通过 Mount Namespace 单独挂载其他不同版本的操作系统文件，比如 CentOS 或者 Ubuntu，但这并不能改变共享宿主机内核的事实。 这意味着，如果你要在 Windows 宿主机上运行 Linux 容器，或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器，都是行不通的。 其次，在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。 如果你的容器中的程序使用 settimeofday(2) 系统调用修改了时间，整个宿主机的时间都会被随之修改，这显然不符合用户的预期。 限制 Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。 Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。 Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。 在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下 /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。 123$ ls /sys/fs/cgroup/cpucgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_releasecgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 可以用来限制进程在长度为 cfs_period 的一段时间内，只能被分配到总量为 cfs_quota 的 CPU 时间。 使用： 1234root@ubuntu:/sys/fs/cgroup/cpu$ mkdir containerroot@ubuntu:/sys/fs/cgroup/cpu$ ls container/cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us cpu.shares notify_on_releasecgroup.procs cpu.cfs_quota_us cpu.rt_runtime_us cpu.stat tasks 这个目录就称为一个“控制组”。你会发现，操作系统会在你新创建的 container 目录下，自动生成该子系统对应的资源限制文件。 $ echo 20000 &gt; /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us 其他： blkio，为​​​块​​​设​​​备​​​设​​​定​​​I/O 限​​​制，一般用于磁盘等设备； cpuset，为进程分配单独的 CPU 核和对应的内存节点； memory，为进程设定内存使用的限制。 docker run -it –cpu-period=100000 –cpu-quota=20000 ubuntu /bin/bash 就是对资源使用的限制 Cgroups 对资源的限制能力也有很多不完善的地方，被提及最多的自然是 /proc 文件系统的问题。 Linux 下的 /proc 目录存储的是记录当前内核运行状态的一系列特殊文件，用户可以通过访问这些文件，查看系统以及当前正在运行的进程的信息，比如 CPU 使用情况、内存占用率等，这些文件也是 top 指令查看系统信息的主要数据来源。 但是，你如果在容器里执行 top 指令，就会发现，它显示的信息居然是宿主机的 CPU 和内存数据，而不是当前容器的数据。 造成这个问题的原因就是，/proc 文件系统并不知道用户通过 Cgroups 给这个容器做了什么样的资源限制，即：/proc 文件系统不了解 Cgroups 限制的存在。 top 是从 /prof/stats 目录下获取数据，所以道理上来讲，容器不挂载宿主机的该目录就可以了。lxcfs就是来实现这个功能的，做法是把宿主机的 /var/lib/lxcfs/proc/memoinfo 文件挂载到Docker容器的/proc/meminfo位置后。容器中进程读取相应文件内容时，LXCFS的FUSE实现会从容器对应的Cgroup中读取正确的内存限制。","categories":[],"tags":[]},{"title":"1.发展史","slug":"1-发展史","date":"2021-06-29T13:33:16.000Z","updated":"2021-06-30T00:10:35.054Z","comments":true,"path":"2021/06/29/1-发展史/","link":"","permalink":"https://sk-xinye.github.io/2021/06/29/1-%E5%8F%91%E5%B1%95%E5%8F%B2/","excerpt":"","text":"2013~2014 年，以 Cloud Foundry 为代表的 PaaS 项目，逐渐完成了教育用户和开拓市场的艰巨任务，也正是在这个将概念逐渐落地的过程中，应用“打包”困难这个问题，成了整个后端技术圈子的一块心病。 2013年Docker 项目的出现，则为这个根本性的问题提供了一个近乎完美的解决方案。dotCloud 公司则在 2013 年底大胆改名为 Docker 公司。 Fig 项目之所以受欢迎，在于它在开发者面前第一次提出了“容器编排”（Container Orchestration）的概念，后Fig 项目被收购后改名为 Compose。 谷歌开源Kubernetes，成为受欢迎项目 容器技术的兴起源于 PaaS 技术的普及； Docker 公司发布的 Docker 项目具有里程碑式的意义； Docker 项目通过“容器镜像”，解决了应用打包这个根本性难题。 容器本身没有价值，有价值的是“容器编排” 最终以 Kubernetes 项目和 CNCF 社区的胜利而告终","categories":[],"tags":[]},{"title":"常见问题","slug":"常见问题","date":"2021-06-29T08:00:43.000Z","updated":"2023-02-04T02:33:07.926Z","comments":true,"path":"2021/06/29/常见问题/","link":"","permalink":"https://sk-xinye.github.io/2021/06/29/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/","excerpt":"","text":"Kafka 如何做到高吞吐、低延迟的呢？ 这里提下 Kafka 写数据的大致方式：先写操作系统的页缓存（Page Cache）,然后由操作系统自行决定何时刷到磁盘。 因此 Kafka 达到高吞吐、低延迟的原因主要有以下 4 点： 页缓存是在内存中分配的，所以消息写入的速度很快。 Kafka 不必和底层的文件系统进行交互，所有繁琐的 I/O 操作都由操作系统来处理。 Kafka 采用追加写的方式，避免了磁盘随机写操作。 使用以 sendfile 为代表的零拷贝技术提高了读取数据的效率。 PS: 使用页缓存而非堆内存还有一个好处，就是当 Kafka broker 的进程崩溃时，堆内存的数据会丢失，但是页缓存的数据依然存在，重启 Kafka broker 后可以继续提供服务。 Kafka 的 producer 工作流程？ 封装为 ProducerRecord 实例 序列化 由 partitioner 确定具体分区 发送到内存缓冲区 由 producer 的一个专属 I/O 线程去取消息，并将其封装到一个批次 ，发送给对应分区的 kafka broker leader 将消息写入本地 log followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK Kafka 的 consumer 工作流程？ 连接 ZK 集群，拿到对应 topic 的 partition 信息和 partition 的 leader 的相关信息 连接到对应 leader 对应的 broker consumer 将自己保存的 offset 发送给 leader leader 根据 offset 等信息定位到 segment（索引文件和日志文件） 根据索引文件中的内容，定位到日志文件中该偏移量对应的开始位置读取相应长度的数据并返回给 consumer 重要参数有哪些？ acks acks = 0 : 不接收发送结果 acks = all 或者 -1: 表示发送消息时，不仅要写入本地日志，还要等待所有副本写入成功。 acks = 1: 写入本地日志即可，是上述二者的折衷方案，也是默认值。 retries 默认为 0，即不重试，立即失败。 一个大于 0 的值，表示重试次数。 buffer.memory 指定 producer 端用于缓存消息的缓冲区的大小，默认 32M； 适当提升该参数值，可以增加一定的吞吐量。 batch.size producer 会将发送分区的多条数据封装在一个 batch 中进行发送，这里的参数指的就是 batch 的大小。 该参数值过小的话，会降低吞吐量，过大的话，会带来较大的内存压力。 默认为 16K，建议合理增加该值。 丢失数据的场景？ consumer 端：不是严格意义的丢失，其实只是漏消费了。 设置了 auto.commit.enable=true ，当 consumer fetch 了一些数据但还没有完全处理掉的时候，刚好到 commit interval 触发了提交 offset 操作，接着 consumer 挂掉。这时已经fetch的数据还没有处理完成但已经被commit掉，因此没有机会再次被处理，数据丢失。 producer 端： I/O 线程发送消息之前，producer 崩溃， 则 producer 的内存缓冲区的数据将丢失。 producer 端丢失数据如何解决？ 同步发送，性能差，不推荐。 仍然异步发送，通过“无消息丢失配置”（来自胡夕的《Apache Kafka 实战》）极大降低丢失的可能性： block.on.buffer.full = true 尽管该参数在0.9.0.0已经被标记为“deprecated”，但鉴于它的含义非常直观，所以这里还是显式设置它为true，使得producer将一直等待缓冲区直至其变为可用。否则如果producer生产速度过快耗尽了缓冲区，producer将抛出异常 acks=all 很好理解，所有follower都响应了才认为消息提交成功，即”committed” retries = MAX 无限重试，直到你意识到出现了问题:) max.in.flight.requests.per.connection = 1 限制客户端在单个连接上能够发送的未响应请求的个数。设置此值是1表示kafka broker在响应请求之前client不能再向同一个broker发送请求。注意：设置此参数是为了避免消息乱序 使用KafkaProducer.send(record, callback)而不是send(record)方法 自定义回调逻辑处理消息发送失败 callback逻辑中最好显式关闭producer：close(0) 注意：设置此参数是为了避免消息乱序 unclean.leader.election.enable=false 关闭unclean leader选举，即不允许非ISR中的副本被选举为leader，以避免数据丢失 replication.factor &gt;= 3 这个完全是个人建议了，参考了Hadoop及业界通用的三备份原则 min.insync.replicas &gt; 1 消息至少要被写入到这么多副本才算成功，也是提升数据持久性的一个参数。与acks配合使用 保证replication.factor &gt; min.insync.replicas 如果两者相等，当一个副本挂掉了分区也就没法正常工作了。通常设置replication.factor = min.insync.replicas + 1即可 consumer 端丢失数据如何解决？ enable.auto.commit=false 关闭自动提交位移，在消息被完整处理之后再手动提交位移 重复数据的场景？ 网络抖动导致 producer 误以为发送错误，导致重试，从而产生重复数据，可以通过幂等性配置避免。 分区策略（即生产消息时如何选择哪个具体的分区）？ 指定了 key ，相同的 key 会被发送到相同的分区，通过key计算哈希值，（采用MurmurHash2算法，具备高运算性能及低碰撞率）； 没有指定 key，通过轮询保证各个分区上的均匀分配。 乱序的场景？ 消息重试发送 乱序如何解决？ 参数配置 max.in.flight.requests.per.connection = 1 ，但同时会限制 producer 未响应请求的数量，即造成在 broker 响应之前，producer 无法再向该 broker 发送数据。 如何选择 Partiton 的数量？ 在创建 Topic 的时候可以指定 Partiton 数量，也可以在创建完后手动修改。但 Partiton 数量只能增加不能减少。中途增加 Partiton 会导致各个 Partiton 之间数据量的不平等。 Partition 的数量直接决定了该 Topic 的并发处理能力。但也并不是越多越好。Partition 的数量对消息延迟性会产生影响。 一般建议选择 Broker Num * Consumer Num ，这样平均每个 Consumer 会同时读取 Broker 数目个 Partition ， 这些 Partition 压力可以平摊到每台 Broker 上。 可重试的异常情况有哪些？ 分区的 leader 副本不可用，一般发生再 leader 换届选举时。 controller 当前不可用，一般是 controller 在经历新一轮的选举。 网络瞬时故障。 controller 的职责有哪些？ 在 kafka 集群中，某个 broker 会被选举承担特殊的角色，即控制器（controller），用于管理和协调 kafka 集群，具体职责如下： 管理副本和分区的状态 更新集群元数据信息 创建、删除 topic 分区重分配 leader 副本选举 topic 分区扩展 broker 加入、退出集群 受控关闭 controller leader 选举 leader 挂了会怎样？（leader failover） 当 leader 挂了之后，controller 默认会从 ISR 中选择一个 replica 作为 leader 继续工作，条件是新 leader 必须有挂掉 leader 的所有数据。 如果为了系统的可用性，而容忍降低数据的一致性的话，可以将 unclean.leader.election.enable = true ，开启 kafka 的”脏 leader 选举”。当 ISR 中没有 replica，则会从 OSR 中选择一个 replica 作为 leader 继续响应请求，如此操作提高了 Kafka 的分区容忍度，但是数据一致性降低了。 broker 挂了会怎样？（broker failover） broker上面有很多 partition 和多个 leader 。因此至少需要处理如下内容： 更新该 broker 上所有 follower 的状态 从新给 leader 在该 broker 上的 partition 选举 leader 选举完成后，要更新 partition 的状态，比如谁是 leader 等kafka 集群启动后，所有的 broker 都会被 controller 监控，一旦有 broker 宕机，ZK 的监听机制会通知到 controller， controller 拿到挂掉 broker 中所有的 partition，以及它上面的存在的 leader，然后从 partition的 ISR 中选择一个 follower 作为 leader，更改 partition 的 follower 和 leader 状态。 controller 挂了会怎样？（controller failover） 由于每个 broker 都会在 zookeeper 的 “/controller” 节点注册 watcher，当 controller 宕机时 zookeeper 中的临时节点消失 所有存活的 broker 收到 fire 的通知，每个 broker 都尝试创建新的 controller path，只有一个竞选成功并当选为 controller。 Zookeeper 为 Kafka 做了什么？ 管理 broker 与 consumer 的动态加入与离开。（Producer 不需要管理，随便一台计算机都可以作为Producer 向 Kakfa Broker 发消息） 触发负载均衡，当 broker 或 consumer 加入或离开时会触发负载均衡算法，使得一个 consumer group 内的多个 consumer 的消费负载平衡。（因为一个 comsumer 消费一个或多个partition，一个 partition 只能被一个 consumer 消费） 维护消费关系及每个 partition 的消费信息。 Page Cache 带来的好处。 Linux 总会把系统中还没被应用使用的内存挪来给 Page Cache，在命令行输入free，或者 cat /proc/meminfo ，“Cached”的部分就是 Page Cache。 Page Cache 中每个文件是一棵 Radix 树（又称 PAT 位树, 一种多叉搜索树），节点由 4k 大小的 Page 组成，可以通过文件的偏移量（如 0x1110001）快速定位到某个Page。 当写操作发生时，它只是将数据写入 Page Cache 中，并将该页置上 dirty 标志。 当读操作发生时，它会首先在 Page Cache 中查找，如果有就直接返回，没有的话就会从磁盘读取文件写入 Page Cache 再读取。 可见，只要生产者与消费者的速度相差不大，消费者会直接读取之前生产者写入Page Cache的数据，大家在内存里完成接力，根本没有磁盘访问。 而比起在内存中维护一份消息数据的传统做法，这既不会重复浪费一倍的内存，Page Cache 又不需要 GC （可以放心使用60G内存了），而且即使 Kafka 重启了，Page Cache 还依然在。 我感觉kafka和es的原理其实差不多，并不能控制文件系统顺序写入，都是尽量只能保证对一个文件追加写入，由文件系统去优化这个部分。可能磁盘里面不是很干净的话（包含很多其他文件），顺序写入的效果就会差很多 你理解的没问题，不过es的file都比较小，所以在访问多个数据项的时候更像是在随机读写 而一般kafka的segment files都比较大 还有一点是kafka用了zero copy，这一点也比es快的多。 es读取索引 用的数据还挺多的，比如.fdt .fdx .cfs还有很多类似_lock的东西 所以一般不认为es是sequential disk access kafka reassign 方案 通过客户端获取kafka集群元数据信息。通过kafka-python SimpleClient对象获取元数据信息 拿到所有需要重新分配的分区信息。通过元数据信息拿到要被删除节点的node_id，最终的node_ids并筛选出元数据分区信息中副本包含该node_id的分区，即要被reassign的分区。数据格式为：{“str_replicas”:[“partition_num”,”replicas”,”topic_name”]}即{“[1,2]”:[0,[1,2],”test_topic”]} 对上述结果进行遍历，将str_replicas 类中的每一个分区中副本中轮询添加node_ids中的一个node，保证数据均匀 客户端管理工具Offset Explorer ：http://www.ibloger.net/article/3497.html kafka连接zookeeper maxbuffer 问题，可以在KAFKA_OPTS中添加 “-Djute.maxbuffer=10485760” zookeeper通过java.env 文件 export JVMFLAGS=”-Xms512m -Xmx2048m -Djute.maxbuffer=10485760” 映射到/conf/java.env中即可 kafka leader为None deleteall /brokers/topics/__consumer_offsets 停止kafka执行删除逻辑 或者删除 rmr /controller","categories":[],"tags":[]},{"title":"8.可靠性探究","slug":"8-可靠性探究","date":"2021-06-27T11:08:21.000Z","updated":"2021-06-29T13:22:13.157Z","comments":true,"path":"2021/06/27/8-可靠性探究/","link":"","permalink":"https://sk-xinye.github.io/2021/06/27/8-%E5%8F%AF%E9%9D%A0%E6%80%A7%E6%8E%A2%E7%A9%B6/","excerpt":"","text":"副本剖析回忆概念 副本是相对于分区而言的，即副本是特定分区的副本。 一个分区中包含一个或多个副本，其中一个为leader副本，其余为follower副本，各个副本位于不同的broker节点中。只有leader副本对外提供服务，follower副本只负责数据同步。 分区中的所有副本统称为 AR，而ISR 是指与leader 副本保持同步状态的副本集合，当然leader副本本身也是这个集合中的一员。 LEO标识每个分区中最后一条消息的下一个位置，分区的每个副本都有自己的LEO，ISR中最小的LEO即为HW，俗称高水位，消费者只能拉取到HW之前的消息。 从生产者发出的一条消息首先会被写入分区的leader副本，不过还需要等待ISR集合中的所有 follower 副本都同步完之后才能被认为已经提交，之后才会更新分区的 HW，进而消费者可以消费到这条消息。 失效副本replica.lag.time.max.ms来抉择，当ISR集合中的一个follower副本滞后leader副本的时间超过此参数指定的值时则判定为同步失败，需要将此follower副本剔除出ISR集合，replica.lag.time.max.ms参数的默认值为10000。 follower副本进程卡住，在一段时间内根本没有向leader副本发起同步请求，比如频繁的Full GC。 follower副本进程同步过慢，在一段时间内都无法追赶上leader副本，比如I/O开销过大。 ISR的伸缩LEO与HW对于副本而言，还有两个概念：本地副本（Local Replica）和远程副本（Remote Replica），本地副本是指对应的Log分配在当前的broker节点上，远程副本是指对应的Log分配在其他的broker节点上。 整个消息追加的过程可以概括如下： 生产者客户端发送消息至leader副本（副本1）中。 消息被追加到leader副本的本地日志，并且会更新日志的偏移量。 follower副本（副本2和副本3）向leader副本请求同步数据。 leader副本所在的服务器读取本地日志，并更新对应拉取的follower副本的信息。 leader副本所在的服务器将拉取结果返回给follower副本。 follower副本收到leader副本返回的拉取结果，将消息追加到本地日志中，并更新日志的偏移量信息。 为什么不支持读写分离可以实现，但是主写从读也有2个很明显的缺点： 数据一致性问题。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间窗口会导致主从节点之间的数据不一致。某一时刻，在主节点和从节点中A数据的值都为X，之后将主节点中A的值修改为Y，那么在这个变更通知到从节点之前，应用读取从节点中的A数据的值并不为最新的Y，由此便产生了数据不一致的问题。 延时问题。类似Redis这种组件，数据从写入主节点到同步至从节点中的过程需要经历网络→主节点内存→网络→从节点内存这几个阶段，整个过程会耗费一定的时间。而在Kafka中，主从同步会比 Redis 更加耗时，它需要经历网络→主节点内存→主节点磁盘→网络→从节点内存→从节点磁盘这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。","categories":[],"tags":[]},{"title":"7.深入客户端","slug":"7-深入客户端","date":"2021-06-26T01:38:04.000Z","updated":"2021-06-27T13:44:56.478Z","comments":true,"path":"2021/06/26/7-深入客户端/","link":"","permalink":"https://sk-xinye.github.io/2021/06/26/7-%E6%B7%B1%E5%85%A5%E5%AE%A2%E6%88%B7%E7%AB%AF/","excerpt":"","text":"分区分配策略 Kafka提供了消费者客户端参数partition.assignment.strategy来设置消费者与订阅主题之间的分区分配策略。 默认情况下，此参数的值为 org.apache.kafka.clients.consumer.RangeAssignor，即采用RangeAssignor分配策略。 除此之外，Kafka还提供了另外两种分配策略：RoundRobinAssignor 和 StickyAssignor。 消费者客户端参数 partition.assignment.strategy可以配置多个分配策略，彼此之间以逗号分隔。 RangeAssignor分配策略假设n=分区数/消费者数量，m=分区数%消费者数量，那么前m个消费者每个分配n+1个分区，后面的（消费者数量-m）个消费者每个分配n个分区。 假设消费组内有2个消费者C0和C1，都订阅了主题t0和t1，并且每个主题都有4个分区，那么订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t0p3、t1p0、t1p1、t1p2、t1p3。最终的分配结果为： 消费者c0:t0p0、t0p1、t1p0、t1p1 消费者c1:t0p2、t0p3、t1p2、t1p3 假设上面例子中2个主题都只有3个分区，那么订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为： 消费者c0:t0p0、t0p1、t1p0、t1p1 消费者c1:t0p2、t1p2 可以明显地看到这样的分配并不均匀，如果将类似的情形扩大，则有可能出现部分消费者过载的情况。 RoundRobinAssignor分配策略 RoundRobinAssignor分配策略的原理是将消费组内所有消费者及消费者订阅的所有主题的分区按照字典序排序，然后通过轮询方式逐个将分区依次分配给每个消费者。 RoundRobinAssignor分配策略对应的 partition.assignment.strategy 参数值为 org.apache.kafka.clients.consumer.RoundRobinAssignor。 假设消费组中有2个消费者C0和C1，都订阅了主题t0和t1，并且每个主题都有3个分区，那么订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为： 消费者c0:t0p0、t0p2、t1p1 消费者c1:t0p1、t1p0、t1p2 当也会有缺点：假设消费组内有3个消费者（C0、C1和C2），它们共订阅了3个主题（t0、t1、t2），这3个主题分别有1、2、3个分区，即整个消费组订阅了t0p0、t1p0、t1p1、t2p0、t2p1、t2p2这6个分区。具体而言，消费者C0订阅的是主题t0，消费者C1订阅的是主题t0和t1，消费者C2订阅的是主题t0、t1和t2 消费者c0:t0p0 消费者c1:t1p0 消费者c0:t1p1,t2p0,t2p1,t2p2 可以看到RoundRobinAssignor策略也不是十分完美，这样分配其实并不是最优解，因为完全可以将分区t1p1分配给消费者C1。 StickyAssignor分配策略“sticky”这个单词可以翻译为“黏性的”，Kafka从0.11.x版本开始引入这种分配策略，它主要有两个目的 分区的分配要尽可能均匀。 分区的分配尽可能与上次分配的保持相同。 当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目标，StickyAssignor分配策略的具体实现要比RangeAssignor和RoundRobinAssignor这两种分配策略要复杂得多。我们举例来看一下StickyAssignor分配策略的实际效果。 假设消费组内有3个消费者（C0、C1和C2），它们都订阅了4个主题（t0、t1、t2、t3），并且每个主题有2个分区。也就是说，整个消费组订阅了t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1这8个分区。最终的分配结果如下： 消费者c0:t0p0,t1p1,t3p0 消费者c1:t0p1,t2p0,t3p1 消费者c2:t1p0,t2p1 这样初看上去似乎与采用RoundRobinAssignor分配策略所分配的结果相同，但事实是否真的如此呢？再假设此时消费者 C1 脱离了消费组，那么消费组就会执行再均衡操作，进而消费分区会重新分配。如果采用RoundRobinAssignor分配策略，那么此时的分配结果如下: 消费者c0:t0p0,t1p0,t2p0,t3p0 消费者c2:t0p1,t1p1,t2p1,t3p1 如分配结果所示，RoundRobinAssignor分配策略会按照消费者C0和C2进行重新轮询分配。如果此时使用的是StickyAssignor分配策略，那么分配结果为： 消费者c0:t0p0,t1p1,t3p0,t3p0 消费者c2:t1p0,t2p1,t0p1,t3p1 使用StickyAssignor分配策略的一个优点就是可以使分区重分配具备“黏性”，减少不必要的分区移动（即一个分区剥离之前的消费者，转而分配给另一个新的消费者）。 自定义分区分配策略消费者协调器和组协调器多个消费者之间的分区分配是需要协同的，那么这个协同的过程又是怎样的呢？这一切都是交由消费者协调器（ConsumerCoordinator）和组协调器（GroupCoordinator）来完成的，它们之间使用一套组协调协议进行交互。 再均衡的原理新版的消费者客户端对此进行了重新设计，将全部消费组分成多个子集，每个消费组的子集在服务端对应一个GroupCoordinator对其进行管理，GroupCoordinator是Kafka服务端中用于管理消费组的组件。而消费者客户端中的ConsumerCoordinator组件负责与GroupCoordinator进行交互。 ConsumerCoordinator与GroupCoordinator之间最重要的职责就是负责执行消费者再均衡的操作，包括前面提及的分区分配的工作也是在再均衡期间完成的。就目前而言，一共有如下几种情形会触发再均衡的操作 有新的消费者加入消费组 有消费者宕机下线。消费者并不一定需要真正下线，例如遇到长时间的 GC、网络延迟导致消费者长时间未向GroupCoordinator发送心跳等情况时，GroupCoordinator会认为消费者已经下线。 有消费者主动退出消费组（发送 LeaveGroupRequest 请求）。比如客户端调用了unsubscrible（）方法取消对某些主题的订阅。 消费组所对应的GroupCoorinator节点发生了变更。 消费组内所订阅的任一主题或者主题的分区数量发生变化。 当有消费者加入消费组时，消费者、消费组及组协调器之间会经历一下几个阶段。 第一阶段（FIND_COORDINATOR） 消费者需要确定它所属的消费组对应的GroupCoordinator所在的broker，并创建与该broker相互通信的网络连接。 如果消费者已经保存了与消费组对应的 GroupCoordinator 节点的信息，并且与它之间的网络连接是正常的，那么就可以进入第二阶段。 否则，就需要向集群中的某个节点发送FindCoordinatorRequest请求来查找对应的GroupCoordinator，这里的“某个节点”并非是集群中的任意节点，而是负载最小的节点，即2.2.2节中的leastLoadedNode。 Kafka 在收到 FindCoordinatorRequest 请求之后，会根据 coordinator_key（也就是groupId）查找对应的GroupCoordinator节点，如果找到对应的GroupCoordinator则会返回其相对应的node_id、host和port信息。 具体查找GroupCoordinator的方式是先根据消费组groupId的哈希值计算__consumer_offsets中的分区编号 中 groupId.hashCode 就是使用 Java 中 String 类的 hashCode（）方法获得的，groupMetadataTopicPartitionCount 为主题__consumer_offsets 的分区个数，这个可以通过broker端参数offsets.topic.num.partitions来配置，默认值为50。 找到对应的__consumer_offsets中的分区之后，再寻找此分区leader副本所在的broker节点，该broker节点即为这个groupId所对应的GroupCoordinator节点。 消费者groupId最终的分区分配方案及组内消费者所提交的消费位移信息都会发送给此分区leader副本所在的broker节点，让此broker节点既扮演GroupCoordinator的角色，又扮演保存分区分配方案和组内消费者位移的角色，这样可以省去很多不必要的中间轮转所带来的开销。 第二阶段（JOIN_GROUP） 在成功找到消费组所对应的 GroupCoordinator 之后就进入加入消费组的阶段，在此阶段的消费者会向GroupCoordinator发送JoinGroupRequest请求，并处理响应。 如果是原有的消费者重新加入消费组，那么在真正发送JoinGroupRequest 请求之前还要执行一些准备工作 如果消费端参数enable.auto.commit设置为true（默认值也为true），即开启自动提交位移功能，那么在请求加入消费组之前需要向 GroupCoordinator 提交消费位移。这个过程是阻塞执行的，要么成功提交消费位移，要么超时。 如果消费者添加了自定义的再均衡监听器（ConsumerRebalanceListener），那么此时会调用onPartitionsRevoked（）方法在重新加入消费组之前实施自定义的规则逻辑，比如清除一些状态，或者提交消费位移等。 因为是重新加入消费组，之前与GroupCoordinator节点之间的心跳检测也就不需要了，所以在成功地重新加入消费组之前需要禁止心跳检测的运作。 选举消费组的leader GroupCoordinator需要为消费组内的消费者选举出一个消费组的leader，这个选举的算法也很简单，分两种情况分析。如果消费组内还没有 leader，那么第一个加入消费组的消费者即为消费组的 leader。很随机 选举分区分配策略 收集各个消费者支持的所有分配策略，组成候选集candidates。 每个消费者从候选集candidates中找出第一个自身支持的策略，为这个策略投上一票。 计算候选集中各个策略的选票数，选票数最多的策略即为当前消费组的分配策略。 在此之后，Kafka服务端就要发送JoinGroupResponse响应给各个消费者，leader消费者和其他普通消费者收到的响应内容并不相同， 第三阶段（SYNC_GROUP） leader 消费者根据在第二阶段中选举出来的分区分配策略来实施具体的分区分配，在此之后需要将分配的方案同步给各个消费者，此时leader消费者并不是直接和其余的普通消费者同步分配方案，而是通过 GroupCoordinator 这个“中间人”来负责转发同步分配方案的。 服务端在收到消费者发送的SyncGroupRequest请求之后会交由GroupCoordinator来负责具体的逻辑处理。 GroupCoordinator同样会先对SyncGroupRequest请求做合法性校验，在此之后会将从 leader 消费者发送过来的分配方案提取出来，连同整个消费组的元数据信息一起存入Kafka的__consumer_offsets主题中，最后发送响应给各个消费者以提供给各个消费者各自所属的分配方案。 当消费者收到所属的分配方案之后会调用PartitionAssignor中的onAssignment（）方法。随后再调用ConsumerRebalanceListener中的OnPartitionAssigned（）方法。之后开启心跳任务，消费者定期向服务端的GroupCoordinator发送HeartbeatRequest来确定彼此在线。 消费组元数据信息 第四阶段（HEARTBEAT） 进入这个阶段之后，消费组中的所有消费者就会处于正常工作状态。 由参数heartbeat.interval.ms指定，默认值为3000，即3秒， __consumer_offsets剖析事务At Least Once + 幂等性 = Exactly Once，但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨分区跨会话的 Exactly Once。事务 事务可以保证 Kafka 在 Exactly Once 语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。 为了实现跨分区跨会话的事务，需要引入一个全局唯一的 Transaction ID，并将 Producer获得的PID和Transaction ID绑定。这样当Producer重启后就可以通过正在进行的TransactionID 获得原来的 PID。 为了管理 Transaction，Kafka 引入了一个新的组件 Transaction Coordinator。Producer 就是通过和 Transaction Coordinator 交互获得 Transaction ID 对应的任务状态。TransactionCoordinator 还负责将事务所有写入 Kafka 的一个内部 Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。 消息传输保障一般而言，消息中间件的消息传输保障有3个层级，分别如下。 at most once：至多一次。消息可能会丢失，但绝对不会重复传输。 at least once：最少一次。消息绝不会丢失，但可能会重复传输。 exactly once：恰好一次。每条消息肯定会被传输一次且仅传输一次。 生产者提供的消息保证是最少一次，对消费者而言，消费者处理消息和提交消费位移的顺序在很大程度上决定了消费者提供哪一种消息传输保障。 如果消费者在拉取完消息之后，应用逻辑先处理消息后提交消费位移，那么在消息处理之后且在位移提交之前消费者宕机了，待它重新上线之后会从上一次位移提交的位置拉取，这样就出现了重复消费， 如果消费者在拉完消息之后，应用逻辑先提交消费位移后进行消息处理，那么在位移提交之后且在消息处理完成之前消费者宕机了，待它重新上线之后，会从已经提交的位移处开始重新消费，但之前尚有部分消息未进行消费，如此就会发生消息丢失，此时就对应at most once。 Kafka从0.11.0.0版本开始引入了幂等和事务这两个特性，以此来实现EOS（exactly once semantics，精确一次处理语义）。 幂等 所谓的幂等，简单地说就是对接口的多次调用所产生的结果和调用一次是一致的。 开启幂等性功能的方式很简单，只需要显式地将生产者客户端参数enable.idempotence设置为true即可 不过如果要确保幂等性功能正常，还需要确保生产者客户端的 retries、acks、max.in.flight.requests.per.connection这几个参数不被配置错。实际上在使用幂等性功能的时候，用户完全可以不用配置（也不建议配置）这几个参数。 如果用户显式地指定了 retries 参数，那么这个参数的值必须大于 0，否则会报出ConfigException： 如果用户没有显式地指定 retries 参数，那么 KafkaProducer 会将它置为 Integer.MAX_VALUE。 同时还需要保证max.in.flight.requests.per.connection参数的值不能大于5（这个参数的值默认为5，在2.2.1节中有相关的介绍），否则也会报出ConfigException 如果用户还显式地指定了 acks 参数，那么还需要保证这个参数的值为-1（all），如果不为-1（这个参数的值默认为1，2.3节中有相关的介绍），那么也会报出ConfigException： 为了实现生产者的幂等性，Kafka为此引入了producer id（以下简称PID）和序列号（sequence number）这两个概念 对于每个PID，消息发送到的每一个分区都有对应的序列号，这些序列号从0开始单调递增。生产者每发送一条消息就会将＜PID，分区＞对应的序列号的值加1 broker端会在内存中为每一对＜PID，分区＞维护一个序列号。对于收到的每一条消息，只有当它的序列号的值（SN_new）比broker端中维护的对应的序列号的值（SN_old）大1（即SN_new=SN_old+1）时，broker才会接收它。如果SN_new＜SN_old+1，那么说明消息被重复写入，broker可以直接将其丢弃。如果SN_new＞SN_old+1，那么说明中间有数据尚未写入，出现了乱序，暗示可能有消息丢失，对应的生产者会抛出OutOfOrderSequenceException，这个异常是一个严重的异常，后续的诸如 send（）、beginTransaction（）、commitTransaction（）等方法的调用都会抛出IllegalStateException的异常。 引入序列号来实现幂等也只是针对每一对＜PID，分区＞而言的，也就是说，Kafka的幂等只能保证单个生产者会话（session）中单分区的幂等。 事务——幂等性并不能跨多个分区运作，而事务[1]可以弥补这个缺陷。事务可以保证对多个分区写入操作的原子性。 对流式应用（Stream Processing Applications）而言，一个典型的应用模式为“consume-transform-produce”。在这种模式下消费和生产并存， Kafka 中的事务可以使应用程序将消费消息、生产消息、提交消费位移当作原子操作来处理，同时成功或失败，即使该生产或消费会跨多个分区。 为了实现事务，应用程序必须提供唯一的 transactionalId，这个 transactionalId通过客户端参数transactional.id来显式设置 事务要求生产者开启幂等特性，因此通过将transactional.id参数设置为非空从而开启事务特性的同时需要将 enable.idempotence 设置为 true transactionalId与PID一一对应，两者之间所不同的是transactionalId由用户显式设置，而PID是由Kafka内部分配的。 为了保证新的生产者启动后具有相同transactionalId的旧生产者能够立即失效，每个生产者通过transactionalId获取PID的同时，还会获取一个单调递增的producer epoch 如果使用同一个transactionalId开启两个生产者，那么前一个开启的生产者会报出如下的错误 从生产者的角度分析，通过事务，Kafka 可以保证跨生产者会话的消息幂等发送，以及跨生产者会话的事务恢复。 前者表示具有相同 transactionalId 的新生产者实例被创建且工作的时候，旧的且拥有相同transactionalId的生产者实例将不再工作。 后者指当某个生产者实例宕机后，新的生产者实例可以保证任何未完成的旧事务要么被提交（Commit），要么被中止（Abort），如此可以使新的生产者实例从一个正常的状态开始工作。 而从消费者的角度分析，事务能保证的语义相对偏弱。出于以下原因，Kafka 并不能保证已提交的事务中的所有消息都能够被消费： 对采用日志压缩策略的主题而言，事务中的某些消息有可能被清理（相同key的消息，后写入的消息会覆盖前面写入的消息） 事务中消息可能分布在同一个分区的多个日志分段（LogSegment）中，当老的日志分段被删除时，对应的消息可能会丢失。 消费者可以通过seek（）方法访问任意offset的消息，从而可能遗漏事务中的部分消息。 消费者在消费时可能没有分配到事务内的所有分区，如此它也就不能读取事务中的所有消息。 initTransactions（）方法用来初始化事务，这个方法能够执行的前提是配置了transactionalId beginTransaction（）方法用来开启事务； sendOffsetsToTransaction（）方法为消费者提供在事务内的位移提交的操作； commitTransaction（）方法用来提交事务； abortTransaction（）方法用来中止事务，类似于事务回滚。 在消费端有一个参数isolation.level，与事务有着莫大的关联，这个参数的默认值为“read_uncommitted” 设置为“read_committed”的消费端应用是消费不到这些消息的，不过在KafkaConsumer内部会缓存这些消息，直到生产者执行 commitTransaction（）方法之后它才能将这些消息推送给消费端应用。反之，如果生产者执行了 abortTransaction（）方法，那么 KafkaConsumer 会将这些缓存的消息丢弃而不推送给消费端应用。 日志文件中除了普通的消息，还有一种消息专门用来标志一个事务的结束，它就是控制消息（ControlBatch）。 控制消息一共有两种类型：COMMIT和ABORT，分别用来表征事务已经成功提交或已经被成功中止。 KafkaConsumer 可以通过这个控制消息来判断对应的事务是被提交了还是被中止了，然后结合参数isolation.level配置的隔离级别来决定是否将相应的消息返回给消费端应用， 查找TransactionCoordinator TransactionCoordinator负责分配PID和管理事务，因此生产者要做的第一件事情就是找出对应的TransactionCoordinator所在的broker节点。 与查找GroupCoordinator节点一样，也是通过FindCoordinatorRequest请求来实现的，只不过FindCoordinatorRequest中的coordinator_type就由原来的0变成了1，由此来表示与事务相关联 Kafka 在收到 FindCoorinatorRequest 请求之后，会根据 coordinator_key （也就是transactionalId）查找对应的TransactionCoordinator节点。 如果找到，则会返回其相对应的node_id、host和port信息。 具体查找TransactionCoordinator的方式是根据transactionalId的哈希值计算主题__transaction_state中的分区编号， 找到对应的分区之后，再寻找此分区leader副本所在的broker节点，该broker节点即为这个transactionalId对应的TransactionCoordinator节点。 获取PID 在找到TransactionCoordinator节点之后，就需要为当前生产者分配一个PID了。凡是开启了幂等性功能的生产者都必须执行这个操作，不需要考虑该生产者是否还开启了事务。 生产者获取PID的操作是通过InitProducerIdRequest请求来实现的 生产者的InitProducerIdRequest请求会被发送给TransactionCoordinator。 如果未开启事务特性而只开启幂等特性，那么 InitProducerIdRequest 请求可以发送给任意的 broker。 当TransactionCoordinator第一次收到包含该transactionalId的InitProducerIdRequest请求时，它会把transactionalId和对应的PID以消息（我们习惯性地把这类消息称为“事务日志消息”）的形式保存到主题__transaction_state中 与InitProducerIdRequest对应的InitProducerIdResponse响应体结构如图7-24所示，除了返回PID，InitProducerIdRequest还会触发执行以下任务 增加该 PID 对应的 producer_epoch。具有相同 PID 但 producer_epoch 小于该producer_epoch的其他生产者新开启的事务将被拒绝。 恢复（Commit）或中止（Abort）之前的生产者未完成的事务 开启事务 通过KafkaProducer的beginTransaction（）方法可以开启一个事务，调用该方法后，生产者本地会标记已经开启了一个新的事务，只有在生产者发送第一条消息之后 TransactionCoordinator才会认为该事务已经开启。 Consume-Transform-Produce AddPartitionsToTxnRequest 当生产者给一个新的分区（TopicPartition）发送数据前，它需要先向TransactionCoordinator发送AddPartitionsToTxnRequest请求（AddPartitionsToTxnRequest请求体结构如图7-25所示），这个请求会让 TransactionCoordinator 将＜transactionId，TopicPartition＞的对应关系存储在主题__transaction_state中 ProduceRequest 这一步骤很容易理解，生产者通过ProduceRequest 请求发送消息（ProducerBatch）到用户自定义主题中，这一点和发送普通消息时相同， AddOffsetsToTxnRequest 通过KafkaProducer的sendOffsetsToTransaction（）方法可以在一个事务批次里处理消息的消费和发送，方法中包含2个参数：Map＜TopicPartition，OffsetAndMetadata＞ offsets和groupId。 TxnOffsetCommitRequest 这个请求也是sendOffsetsToTransaction（）方法中的一部分，在处理完AddOffsetsToTxnRequest之后，生产者还会发送 TxnOffsetCommitRequest 请求给 GroupCoordinator，从而将本次事务中包含的消费位移信息offsets存储到主题__consumer_offsets中 提交或者中止事务 一旦数据被写入成功，我们就可以调用 KafkaProducer 的 commitTransaction（）方法或abortTransaction（）方法来结束当前的事务。","categories":[],"tags":[]},{"title":"6.深入服务端","slug":"6-深入服务端","date":"2021-06-25T05:57:01.000Z","updated":"2021-08-08T13:43:29.807Z","comments":true,"path":"2021/06/25/6-深入服务端/","link":"","permalink":"https://sk-xinye.github.io/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/","excerpt":"","text":"本章涉及协议设计、时间轮、延迟操作、控制器及参数解密，尤其是协议设计和控制器的介绍，这些是深入了解Kafka的必备知识点。 协议设计Kafka自定义了一组基于TCP的二进制协议，只要遵守这组协议的格式，就可以向Kafka发送消息，也可以从Kafka中拉取消息，或者做一些其他的事情，比如提交消费位移等。 一共包含了 43 种协议类型 每种类型的Request都包含相同结构的协议请求头（RequestHeader）和不同结构的协议请求体（RequestBody） 每种类型的Response也包含相同结构的协议响应头（ResponseHeader）和不同结构的响应体（ResponseBody） 消息累加器 RecordAccumulator 中的消息是以＜分区，Deque＜ProducerBatch＞＞的形式进行缓存的，之后由Sender线程转变成＜Node，List＜ProducerBatch＞＞的形式，针对每个Node，Sender线程在发送消息前会将对应的List＜ProducerBatch＞形式的内容转变成 ProduceRequest 的具体结构。List＜ProducerBatch＞中的内容首先会按照主题名称进行分类（对应ProduceRequest中的域topic），然后按照分区编号进行分类（对应ProduceRequest中的域partition），分类之后的ProducerBatch集合就对应ProduceRequest中的域record_set。 时间轮Kafka中存在大量的延时操作，比如延时生产、延时拉取和延时删除等。Kafka并没有使用JDK自带的Timer或DelayQueue（O(nlogn)）来实现延时的功能，而是基于时间轮的概念自定义实现了一个用于延时功能的定时器,而基于时间轮可以将插入和删除操作的时间复杂度都降为O（1）（SystemTimer） Kafka中的时间轮（TimingWheel）是一个存储定时任务的环形队列，底层采用数组实现，数组中的每个元素可以存放一个定时任务列表（TimerTaskList） TimerTaskList是一个环形的双向链表，链表中的每一项表示的都是定时任务项（TimerTaskEntry），其中封装了真正的定时任务（TimerTask）。 时间轮由多个时间格组成，每个时间格代表当前时间轮的基本时间跨度（tickMs）。时间轮的时间格个数是固定的，可用wheelSize来表示， 那么整个时间轮的总体时间跨度（interval）可以通过公式 tickMs×wheelSize计算得出。 时间轮还有一个表盘指针（currentTime），用来表示时间轮当前所处的时间，currentTime是tickMs的整数倍 currentTime可以将整个时间轮划分为到期部分和未到期部分，currentTime当前指向的时间格也属于到期部分，表示刚好到期，需要处理此时间格所对应的TimerTaskList中的所有任务。 工作流程： 若时间轮的tickMs为1ms且wheelSize等于20，那么可以计算得出总体时间跨度interval为20ms。 初始情况下表盘指针currentTime指向时间格0，此时有一个定时为2ms的任务插进来会存放到时间格为2的TimerTaskList中。 随着时间的不断推移，指针currentTime不断向前推进，过了2ms之后，当到达时间格2时，就需要将时间格2对应的TimeTaskList中的任务进行相应的到期操作。 此时若又有一个定时为 8ms 的任务插进来，则会存放到时间格 10 中，currentTime再过8ms后会指向时间格10。 如果同时有一个定时为19ms的任务插进来怎么办？新来的TimerTaskEntry会复用原来的TimerTaskList，所以它会插入原本已经到期的时间格1。 总之，整个时间轮的总体跨度是不变的，随着指针currentTime的不断推进，当前时间轮所能处理的时间段也在不断后移，总体时间范围在currentTime和currentTime+interval之间。 Kafka中不乏几万甚至几十万毫秒的定时任务，这个wheelSize的扩充没有底线，就算将所有的定时任务的到期时间都设定一个上限，比如100万毫秒，那么这个wheelSize为100万毫秒的时间轮不仅占用很大的内存空间，而且也会拉低效率。 Kafka 为此引入了层级时间轮的概念，当任务的到期时间超过了当前时间轮所表示的时间范围时，就会尝试添加到上层时间轮中。 第一层的时间轮tickMs=1ms、wheelSize=20、interval=20ms。 第二层的时间轮的tickMs为第一层时间轮的interval，即20ms。每一层时间轮的wheelSize是固定的，都是20，那么第二层的时间轮的总体时间跨度interval为400ms。 以此类推，这个400ms也是第三层的tickMs的大小，第三层的时间轮的总体时间跨度为8000ms。 同理，也会有降级操作 TimingWheel时还有一些小细节 TimingWheel中的每个双向环形链表TimerTaskList都会有一个哨兵节点（sentinel），引入哨兵节点可以简化边界条件。哨兵节点也称为哑元节点（dummy node），它是一个附加的链表节点，该节点作为第一个节点， Kafka 中的定时器只需持有 TimingWheel 的第一层时间轮的引用，并不会直接持有其他高层的时间轮，但每一层时间轮都会有一个引用（overflowWheel）指向更高一层的应用，以此层级调用可以实现定时器间接持有各个层级时间轮的引用。 并且会配合DelayQueue 完成工作，其中用TimingWheel做最擅长的任务添加和删除操作，而用DelayQueue做最擅长的时间推进工作 会有线程专门拿取DelayQueue中的到期的任务列表，推进时间轮，降级时间轮，处理操作 延时操作如果在使用生产者客户端发送消息的时候将 acks 参数设置为-1，那么就意味着需要等待ISR集合中的所有副本都确认收到消息之后才能正确地收到响应的结果，或者捕获超时异常。 在Kafka中有多种延时操作，比如前面提及的延时生产，还有延时拉取（DelayedFetch）、延时数据删除（DelayedDeleteRecords）等。 延时操作创建之后会被加入延时操作管理器（DelayedOperationPurgatory）来做专门的处理。延时操作有可能会超时，每个延时操作管理器都会配备一个定时器（SystemTimer）来做超时管理，定时器的底层就是采用时间轮（TimingWheel）实现的 时间轮的轮转是靠“收割机”线程ExpiredOperationReaper来驱动的，这里的“收割机”线程就是由延时操作管理器启动的。 定时器、“收割机”线程和延时操作管理器都是一一对应的。 延时操作需要支持外部事件的触发，所以还要配备一个监听池来负责监听每个分区的外部事件—查看是否有分区的HW发生了增长。另外需要补充的是，ExpiredOperationReaper不仅可以推进时间轮，还会定期清理监听池中已完成的延时操作。 延时拉取 Kafka在处理拉取请求时，会先读取一次日志文件，如果收集不到足够多（fetchMinBytes，由参数fetch.min.bytes配置，默认值为1）的消息，那么就会创建一个延时拉取操作（DelayedFetch）以等待拉取到足够数量的消息。当延时拉取操作执行时，会再读取一次日志文件，然后将拉取结果返回给 follower 副本。延时拉取操作也会有一个专门的延时操作管理器负责管理，大体的脉络与延时生产操作相同，不再赘述。如果拉取进度一直没有追赶上leader副本，那么在拉取leader副本的消息时一般拉取的消息大小都会不小于fetchMinBytes，这样Kafka也就不会创建相应的延时拉取操作，而是立即返回拉取结果。如果是follower副本的延时拉取，它的外部事件就是消息追加到了leader副本的本地日志文件中；如果是消费者客户端的延时拉取，它的外部事件可以简单地理解为HW的增长。 目前版本的Kafka还引入了事务的概念，对于消费者或follower副本而言，其默认的事务隔离级别为“read_uncommitted”。不过消费者可以通过客户端参数isolation.level将事务隔离级别设置为“read_committed”（注意：follower副本不可以将事务隔离级别修改为这个值），这样消费者拉取不到生产者已经写入却尚未提交的消息。对应的消费者的延时拉取，它的外部事件实际上会切换为由LSO（LastStableOffset）的增长来触发。LSO是HW之前除去未提交的事务消息的最大偏移量，LSO≤HW，有关事务和LSO的内容可以分别参考7.4节和10.2节。 控制器 在 Kafka 集群中会有一个或多个 broker，其中有一个 broker 会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态。 当某个分区的leader副本出现故障时，由控制器负责为该分区选举新的leader副本。 当检测到某个分区的ISR集合发生变化时，由控制器负责通知所有broker更新其元数据信息。 当使用kafka-topics.sh脚本为某个topic增加分区数量时，同样还是由控制器负责分区的重新分配。 控制器的选举及异常恢复Kafka中的控制器选举工作依赖于ZooKeeper，成功竞选为控制器的broker会在ZooKeeper中创建/controller这个临时（EPHEMERAL）节点，此临时节点的内容参考如下： 在任意时刻，集群中有且仅有一个控制器。每个 broker 启动的时候会去尝试读取/controller节点的brokerid的值，如果读取到brokerid的值不为-1，则表示已经有其他 broker 节点成功竞选为控制器，所以当前 broker 就会放弃竞选 如果 ZooKeeper 中不存在/controller节点，或者这个节点中的数据异常，那么就会尝试去创建/controller节点。 当前broker去创建节点的时候，也有可能其他broker同时去尝试创建这个节点，只有创建成功的那个broker才会成为控制器，而创建失败的broker竞选失败 每个broker都会在内存中保存当前控制器的brokerid值，这个值可以标识为activeControllerId。 ZooKeeper 中还有一个与控制器有关的/controller_epoch 节点，这个节点是持久（PERSISTENT）节点，节点中存放的是一个整型的controller_epoch值。controller_epoch用于记录控制器发生变更的次数，即记录当前的控制器是第几代控制器，我们也可以称之为“控制器的纪元”。 由此可见，Kafka 通过 controller_epoch 来保证控制器的唯一性，进而保证相关操作的一致性。 具备控制器身份的broker需要比其他普通的broker多一份职责，具体细节如下 监听分区相关的变化。为ZooKeeper中的/admin/reassign_partitions 节点注册 PartitionReassignmentHandler，用来处理分区重分配的动作。为 ZooKeeper 中的/isr_change_notification节点注册IsrChangeNotificetionHandler，用来处理ISR集合变更的动作。为ZooKeeper中的/admin/preferred-replica-election节点添加PreferredReplicaElectionHandler，用来处理优先副本的选举动作。 监听主题相关的变化。为 ZooKeeper 中的/brokers/topics 节点添加TopicChangeHandler，用来处理主题增减的变化；为 ZooKeeper 中的/admin/delete_topics节点添加TopicDeletionHandler，用来处理删除主题的动作。 监听broker相关的变化。为ZooKeeper中的/brokers/ids节点添加BrokerChangeHandler，用来处理broker增减的变化。 从ZooKeeper中读取获取当前所有与主题、分区及broker有关的信息并进行相应的管理。对所有主题对应的 ZooKeeper 中的/brokers/topics/＜topic＞节点添加PartitionModificationsHandler，用来监听主题中的分区分配变化。 启动并管理分区状态机和副本状态机。 更新集群的元数据信息。 如果参数 auto.leader.rebalance.enable 设置为 true，则还会开启一个名为“auto-leader-rebalance-task”的定时任务来负责维护分区的优先副本的均衡。 在目前的新版本的设计中，只有Kafka Controller在ZooKeeper上注册相应的监听器，其他的broker极少需要再监听ZooKeeper中的数据变化，这样省去了很多不必要的麻烦。不过每个broker还是会对/controller节点添加监听器，以此来监听此节点的数据变化（ControllerChangeHandler）。 当/controller 节点的数据发生变化时，每个 broker 都会更新自身内存中保存的activeControllerId。如果broker 在数据变更前是控制器，在数据变更后自身的 brokerid 值与新的 activeControllerId 值不一致，那么就需要“退位”，关闭相应的资源，比如关闭状态机、注销相应的监听器等。 当/controller节点被删除时，每个broker都会进行选举，如果broker在节点被删除前是控制器，那么在选举前还需要有一个“退位”的动作。如果有特殊需要，则可以手动删除/controller 节点来触发新一轮的选举。当然关闭控制器所对应的 broker，以及手动向/controller节点写入新的brokerid的所对应的数据，同样可以触发新一轮的选举。 优雅关闭 获取Kafka的服务进程号PIDS。可以使用Java中的jps命令或使用Linux系统中的ps命令来查看。 使用 kill-s TERM $PIDS 或 kill-15 $PIDS 的方式来关闭进程，注意千万不要使用kill-9的方式。 分区leader的选举分区leader副本的选举由控制器负责具体实施。当创建分区（创建主题或增加分区都有创建分区的动作）或分区上线（比如分区中原先的leader副本下线，此时分区需要选举一个新的leader 上线来对外提供服务）的时候都需要执行 leader 的选举动作，对应的选举策略为OfflinePartitionLeaderElectionStrategy。 这种策略的基本思路是按照 AR 集合中副本的顺序查找第一个存活的副本，并且这个副本在ISR集合中。 一个分区的AR集合在分配的时候就被指定，并且只要不发生重分配的情况，集合内部副本的顺序是保持不变的，而分区的ISR集合中副本的顺序可能会改变。 注意这里是根据AR的顺序而不是ISR的顺序进行选举的。 如果ISR集合中没有可用的副本，那么此时还要再检查一下所配置的unclean.leader.election.enable参数（默认值为false）。如果这个参数配置为true，那么表示允许从非ISR列表中的选举leader，从AR列表中找到第一个存活的副本即为leader。 当分区进行重分配（可以先回顾一下4.3.2节的内容）的时候也需要执行leader的选举动作，对应的选举策略为 ReassignPartitionLeaderElectionStrategy。 从重分配的AR列表中找到第一个存活的副本，且这个副本在目前的ISR列表中。 当发生优先副本（可以先回顾一下4.3.1节的内容）的选举时，直接将优先副本设置为leader即可，AR集合中的第一个副本即为优先副本（PreferredReplicaPartitionLeaderElectionStrategy）。 还有一种情况会发生 leader 的选举，当某节点被优雅地关闭（也就是执行ControlledShutdown）时，位于这个节点上的leader副本都会下线，所以与此对应的分区需要执行leader的选举。 从AR列表中找到第一个存活的副本，且这个副本在目前的ISR列表中，与此同时还要确保这个副本不处于正在被关闭的节点上。 参数解密broker.id broker.id是broker在启动之前必须设定的参数之一，在Kafka集群中，每个broker都有唯一的 id（也可以记作 brokerId）值用来区分彼此。 broker 在启动时会在 ZooKeeper 中的/brokers/ids路径下创建一个以当前brokerId为名称的虚节点，broker的健康状态检查就依赖于此虚节点。 当 broker 下线时，该虚节点会自动删除，其他 broker 节点或客户端通过判断/brokers/ids路径下是否有此broker的brokerId节点来确定该broker的健康状态。 config/server.properties 里的 broker.id 参数来配置brokerId，默认情况下broker.id值为-1。 在Kafka中，brokerId值必须大于等于0才有可能正常启动，但这里并不是只能通过配置文件config/server.properties来设定这个值，还可以通过meta.properties文件或自动生成功能来实现。 meta.properties文件与broker.id的关联如下 如果 log.dir 或 log.dirs 中配置了多个日志根目录，这些日志根目录中的meta.properties文件所配置的broker.id不一致则会抛出InconsistentBrokerIdException的异常。 如果config/server.properties配置文件里配置的broker.id的值和meta.properties文件里的broker.id值不一致，那么同样会抛出InconsistentBrokerIdException的异常。 如果 config/server.properties 配置文件中并未配置 broker.id 的值，那么就以meta.properties文件中的broker.id值为准。 如果没有meta.properties文件，那么在获取合适的broker.id值之后会创建一个新的meta.properties文件并将broker.id值存入其中。 如果两个文件中都没有broker.id，Kafka 还提供了另外两个 broker 端参数：broker.id.generation.enable 和reserved.broker.max.id来配合生成新的brokerId。 broker.id.generation.enable参数用来配置是否开启自动生成 brokerId 的功能，默认情况下为 true，即开启此功能。 自动生成的 brokerId 有一个基准值，即自动生成的 brokerId 必须超过这个基准值，这个基准值通过reserverd.broker.max.id参数配置，默认值为1000。也就是说，默认情况下自动生成的brokerId从1001开始。 自动生成的brokerId的原理是先往ZooKeeper中的/brokers/seqid节点中写入一个空字符 串，然 后 获 取 返 回 的 Stat 信 息 中 的 version 值，进 而 将 version 的 值 和reserved.broker.max.id参数配置的值相加。先往节点中写入数据再获取Stat信息，这样可以确保返回的 version 值大于 0，进而就可以确保生成的 brokerId 值大于reserved.broker.max.id 参数配置的值，符合非自动生成的 broker.id 的值在[0，reserved.broker.max.id]区间设定。 bootstrap.servers 我们一般可以简单地认为 bootstrap.servers 这个参数所要指定的就是将要连接的Kafka集群的broker地址列表。 不过从深层次的意义上来讲，这个参数配置的是用来发现Kafka集群元数据信息的服务地址。 客户端KafkaProducer1与Kafka Cluster直连，这是客户端给我们的既定印象，而事实上客户端连接Kafka集群要经历以下3个过程 客户端KafkaProducer2与bootstrap.servers参数所指定的Server连接，并发送MetadataRequest请求来获取集群的元数据信息。 Server在收到MetadataRequest请求之后，返回MetadataResponse给KafkaProducer2，在MetadataResponse中包含了集群的元数据信息。 客户端KafkaProducer2收到的MetadataResponse之后解析出其中包含的集群元数据信息，然后与集群中的各个节点建立连接，之后就可以发送消息了。 在绝大多数情况下，Kafka 本身就扮演着第一步和第二步中的 Server 角色，我们完全可以将这个Server的角色从Kafka中剥离出来。我们可以在这个Server的角色上大做文章，比如添加一些路由的功能、负载均衡的功能。 bootstrap.servers、metadata.broker.list、zookeeper.connect 参数往往不是很清楚。这一现象还存在Kafka所提供的诸多脚本之中，在这些脚本中连接Kafka采用的选项参数有–bootstrap-server、–broker-list和–zookeeper bootstrap-server是broker-list 的替代品，但是kafka-console-producer.sh 还在使用 zookeeper 命令是kafka-topics.sh脚本实际上操纵的就是ZooKeeper中的节点，而不是Kafka本身，它并没有被替代的必要。 服务端参数列表 元数据更新broker是有状态的服务：每台broker在内存中都维护了集群上所有节点和topic分区的状态信息——Kafka称这部分状态信息为元数据缓存(metadata cache)。本文就将讨论一下这个metadata cache的设计与实现。 cache里面存了什么首先，我们来看下cache里面都存了什么，我们以Kafka 1.0.0版本作为分析对象。Metadata cache中保存的信息十分丰富，几乎囊括了Kafka集群的各个方面，它包含了： controller所在的broker ID，即保存了当前集群中controller是哪台broker 集群中所有broker的信息：比如每台broker的ID、机架信息以及配置的若干组连接信息(比如配置了PLAINTEXT和SASL监听器就有两套连接信息，分别使用不同的安全协议和端口，甚至主机名都可能不同) 集群中所有节点的信息：严格来说，它和上一个有些重复，不过此项是按照broker ID和监听器类型进行分组的。对于超大集群来说，使用这一项缓存可以快速地定位和查找给定节点信息，而无需遍历上一项中的内容，算是一个优化吧 集群中所有分区的信息：所谓分区信息指的是分区的leader、ISR和AR信息以及当前处于offline状态的副本集合。这部分数据按照topic和分区ID进行分组，可以快速地查找到每个分区的当前状态。（注：AR表示assigned replicas，即创建topic时为该分区分配的副本集合） 每台broker都保存相同的cache吗 是的，至少Kafka在设计时的确是这样的愿景：每台Kafka broker都要维护相同的缓存，这样客户端程序(clients)随意地给任何一个broker发送请求都能够获取相同的数据，这也是为什么任何一个broker都能处理clients发来的Metadata请求的原因：因为每个broker上都有这些数据！要知道目前Kafka共有38种请求类型，能做到这一点的可谓少之又少。每个broker都能处理的能力可以缩短请求被处理的延时从而提高整体clients端的吞吐，因此用空间去换一些时间的做法是值得的。 cache是怎么更新的 如前所述，用空间去换时间，好处是降低了延时，提升了吞吐，但劣势就在于你需要处理cache的更新并且维护一致性。目前Kafka是怎么更新cache的？简单来说，就是通过发送异步更新请求(UpdateMetadata request)来维护一致性的。既然是异步的，那么在某一个时间点集群上所有broker的cache信息就未必是严格相同的。只不过在实际使用场景中，这种弱一致性似乎并没有太大的问题。原因如下： clients并不是时刻都需要去请求元数据的，且会缓存到本地； 即使获取的元数据无效或者过期了，clients通常都有重试机制，可以去其他broker上再次获取元数据; cache更新是很轻量级的，仅仅是更新一些内存中的数据结构，不会有太大的成本。因此我们还是可以安全地认为每台broker上都有相同的cache信息。 具体的更新操作实际上是由controller来完成的。controller会在一定场景下向特定broker发送UpdateMetadata请求令这些broker去更新它们各自的cache，这些broker一旦接收到请求便开始全量更新——即清空当前所有cache信息，使用UpdateMetadata请求中的数据来重新填充cache。 cache什么时候更新 实际上这个问题等同于：controller何时向特定broker发送UpdateMetadata请求？ 如果从源码开始分析，那么涉及到的场景太多了，比如controller启动时、新broker启动时、更新broker时、副本重分配时等等。我们只需要记住：只要集群中有broker或分区数据发生了变更就需要更新这些cache 举个经常有人问的例子：集群中新增加的broker是如何获取这些cache，并且其他broker是如何知晓它的？当有新broker启动时，它会在Zookeeper中进行注册，此时监听Zookeeper的controller就会立即感知这台新broker的加入，此时controller会更新它自己的缓存（注意：这是controller自己的缓存，不是本文讨论的metadata cache）把这台broker加入到当前broker列表中，之后它会发送UpdateMetadata请求给集群中所有的broker(也包括那台新加入的broker)让它们去更新metadata cache。一旦这些broker更新cache完成，它们就知道了这台新broker的存在，同时由于新broker也更新了cache，故现在它也有了集群所有的状态信息。 目前的问题 前面说过了，现在更新cache完全由controller来驱动，故controller所在broker的负载会极大地影响这部分操作（实际上，它会影响所有的controller操作）。根据目前的设计，controller所在broker依然作为一个普通broker执行其他的clients请求处理逻辑，所以如果controller broker一旦忙于各种clients请求(比如生产消息或消费消息)，那么这种更新操作的请求就会积压起来(backlog)，造成了更新操作的延缓甚至是被取消。究其根本原因在于当前controller对待数据类请求和控制类请求并无任何优先级化处理——controller一视同仁地对待这些请求，而实际上我们更希望controller能否赋予控制类请求更高的优先级。社区目前已经开始着手改造当前的设计，相信在未来的版本中此问题可以得到解决。","categories":[],"tags":[]},{"title":"5.日志存储","slug":"5-日志存储","date":"2021-06-25T02:04:59.000Z","updated":"2021-06-25T12:42:44.674Z","comments":true,"path":"2021/06/25/5-日志存储/","link":"","permalink":"https://sk-xinye.github.io/2021/06/25/5-%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/","excerpt":"","text":"文件目录布局 主题，分区是逻辑上的概念，物理上没有对应的东西 log或者说副本在物理存储上对应了目录，例如topic-log-0 ，其是＜topic＞-＜partition＞的文件夹 每个log中有多个logsegment 概念，多个分段，其实就是数据的拆分 每个分段又包含了.log .index .timeindex 等文件，还可能包含“.deleted”“.cleaned”“.swap”等临时文件，以及可能的“.snapshot”“.txnindex”“leader-epoch-checkpoint”等文件。 压缩其实和redis 类似，减少了一些元数据信息,并增加了其他可变数据类型 消息格式v2版本 v2版本中消息集称为Record Batch，而不是先前的Message Set，其内部也包含了一条或多条消息， 生产者客户端中的ProducerBatch对应这里的RecordBatch，而ProducerRecord对应这里的Record。 详细见深入理解kafka 日志索引 Kafka 中的索引文件以稀疏索引（sparse index）的方式构造消息的索引，它并不保证每个消息在索引文件中都有对应的索引项。 稀疏索引通过MappedByteBuffer将索引文件映射到内存中，以加快索引的查询速度。 使用二分查找法来快速定位偏移量的位置，如果指定的偏移量不在索引文件中，则会返回小于指定偏移量的最大偏移量。时间戳索引文件中的时间戳也保持严格的单调递增，查询指定时间戳时，也根据二分查找法来查找不大于该时间戳的最大偏移量，至于要找到对应的物理文件位置还需要根据偏移量索引文件来进行再次定位。 日志切分 当前日志分段文件的大小超过了 broker 端参数 log.segment.bytes 配置的值。log.segment.bytes参数的默认值为1073741824，即1GB。 当前日志分段中消息的最大时间戳与当前系统的时间戳的差值大于 log.roll.ms或log.roll.hours参数配置的值。如果同时配置了log.roll.ms和log.roll.hours参数，那么log.roll.ms的优先级高。默认情况下，只配置了log.roll.hours参数，其值为168，即7天。 偏移量索引文件或时间戳索引文件的大小达到broker端参数log.index.size.max.bytes配置的值。log.index.size.max.bytes的默认值为10485760，即10MB。 追加的消息的偏移量与当前日志分段的偏移量之间的差值大于Integer.MAX_VALUE，即要追加的消息的偏移量不能转变为相对偏移量（offset-baseOffset＞Integer.MAX_VALUE）。 位置索引和时间索引都是为了加速查找 日志清理 日志删除（Log Retention）：按照一定的保留策略直接删除不符合条件的日志分段。、 基于时间，log.retention.hours参数，其值为168，故默认情况下日志分段文件的保留时间为7天。 基于日志大小。retentionSize可以通过broker端参数log.retention.bytes来配置，默认值为-1，表示无穷大。 基于日志起始偏移量。 日志压缩（Log Compaction）：针对每个消息的key进行整合，对于有相同key的不同value值，只保留最后一个版本。 磁盘存储Kafka 在设计时采用了文件追加的方式来写入消息，即只能在日志文件的尾部追加新的消息，并且也不允许修改已写入的消息，这种方式属于典型的顺序写盘的操作，所以就算 Kafka使用磁盘作为存储介质，它所能承载的吞吐量也不容小觑。 页缓存 当一个进程准备读取磁盘上的文件内容时，操作系统会先查看待读取的数据所在的页（page）是否在页缓存（pagecache）中，如果存在（命中）则直接返回数据，从而避免了对物理磁盘的 I/O 操作； 如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存，之后再将数据返回给进程。 如果一个进程需要将数据写入磁盘，那么操作系统也会检测数据对应的页是否在页缓存中，如果不存在，则会先在页缓存中添加相应的页，最后将数据写入对应的页。被修改过后的页也就变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保持数据的一致性。 Linux操作系统中的vm.dirty_background_ratio参数用来指定当脏页数量达到系统内存的百分之多少之后就会触发 pdflush/flush/kdmflush 等后台回写进程的运行来处理脏页，一般设置为小于10的值即可 在Kafka中同样提供了同步刷盘及间断性强制刷盘（fsync）的功能，这些功能可以通过 log.flush.interval.messages、log.flush.interval.ms 等参数来控制。 最好关闭swap vm.swappiness 建议将这个参数的值设置为 1（1-100），这样保留了swap的机制而又最大限度地限制了它对Kafka性能的影响 磁盘I/O流程从编程角度而言，一般磁盘I/O的场景有以下四种 用户调用标准C库进行I/O操作，数据流为：应用程序buffer→C库标准IObuffer→文件系统页缓存→通过具体文件系统到磁盘。 用户调用文件 I/O，数据流为：应用程序buffer→文件系统页缓存→通过具体文件系统到磁盘。 用户打开文件时使用O_DIRECT，绕过页缓存直接读写磁盘。 用户使用类似dd工具，并使用direct参数，绕过系统cache与文件系统直接写磁盘。 写操作： 用户调用fwrite把数据写入C库标准IObuffer后就返回，即写操作通常是异步操作； 数据写入C库标准IObuffer后，不会立即刷新到磁盘，会将多次小数据量相邻写操作先缓存起来合并，最终调用write函数一次性写入（或者将大块数据分解多次write 调用）页缓存； 数据到达页缓存后也不会立即刷新到磁盘，内核有 pdflush 线程在不停地检测脏页，判断是否要写回到磁盘，如果是则发起磁盘I/O请求。 读操作： 用户调用fread到C库标准IObuffer中读取数据，如果成功则返回，否则继续； 到页缓存中读取数据，如果成功则返回，否则继续； 发起 I/O 请求，读取数据后缓存buffer和C库标准IObuffer并返回。可以看出，读操作是同步请求。 I/O请求处理： 通用块层根据I/O请求构造一个或多个bio结构并提交给调度层； 调度器将 bio 结构进行排序和合并组织成队列且确保读写操作尽可能理想：将一个或多个进程的读操作合并到一起读，将一个或多个进程的写操作合并到一起写，尽可能变随机为顺序（因为随机读写比顺序读写要慢），读必须优先满足，而写也不能等太久。 针对不同的应用场景，I/O调度策略也会影响I/O的读写性能，目前Linux系统中的I/O调度策略有4种，分别为NOOP、CFQ、DEADLINE和ANTICIPATORY，默认为CFQ。 NOOP NOOP算法的全写为No Operation。该算法实现了最简单的FIFO队列，所有I/O请求大致按照先来后到的顺序进行操作。之所以说“大致”，原因是NOOP在FIFO的基础上还做了相邻I/O请求的合并，并不是完全按照先进先出的规则满足I/O请求。 CFQ CFQ算法的全写为Completely Fair Queuing。该算法的特点是按照I/O请求的地址进行排序，而不是按照先来后到的顺序进行响应。相比于NOOP的缺点是，先来的I/O请求并不一定能被满足，可能会出现“饿死”的情况。 DEADLINE DEADLINE在CFQ的基础上，解决了I/O请求“饿死”的极端情况。除了CFQ本身具有的I/O排序队列，DEADLINE额外分别为读I/O和写I/O提供了FIFO队列。读FIFO队列的最大等待时间为500ms，写FIFO队列的最大等待时间为5s。FIFO队列内的I/O请求优先级要比CFQ队列中的高，而读FIFO队列的优先级又比写FIFO队列的优先级高。 ANTICIPATORY ANTICIPATORY在DEADLINE的基础上，为每个读I/O都设置了6ms的等待时间窗口。如果在6ms内OS收到了相邻位置的读I/O请求，就可以立即满足 零拷贝将文件传出出去时，文件A经历了4次复制的过程： 调用read（）时，文件A中的内容被复制到了内核模式下的Read Buffer中。 CPU控制将内核模式数据复制到用户模式下。 调用write（）时，将用户模式下的内容复制到内核模式下的Socket Buffer中。 将内核模式下的Socket Buffer的数据复制到网卡设备中传送。 从上面的过程可以看出，数据平白无故地从内核模式到用户模式“走了一圈”，浪费了 2次复制过程：第一次是从内核模式复制到用户模式；第二次是从用户模式再复制回内核模式，即上面4次过程中的第2步和第3步。而且在上面的过程中，内核和用户模式的上下文的切换也是4次。 零拷贝技术通过DMA（Direct Memory Access）技术将文件内容复制到内核模式下的Read Buffer 中。不过没有数据被复制到 Socket Buffer，相反只有包含数据的位置和长度的信息的文件描述符被加到Socket Buffer DMA引擎直接将数据从内核模式中传递到网卡设备（协议引擎）。这里数据只经历了2次复制就从磁盘中传送出去了，并且上下文切换也变成了2次。零拷贝是针对内核模式而言的，数据在内核模式下实现了零拷贝。","categories":[],"tags":[]},{"title":"4.主题与分区","slug":"4-主题与分区","date":"2021-06-24T10:54:23.000Z","updated":"2021-06-25T12:42:44.673Z","comments":true,"path":"2021/06/24/4-主题与分区/","link":"","permalink":"https://sk-xinye.github.io/2021/06/24/4-%E4%B8%BB%E9%A2%98%E4%B8%8E%E5%88%86%E5%8C%BA/","excerpt":"","text":"主题作为消息的归类，可以再细分为一个或多个分区，分区也可以看作对消息的二次归类。从Kafka的底层实现来说，主题和分区都是逻辑上的概念，分区可以有一至多个副本，每个副本对应一个日志文件，每个日志文件对应一至多个日志分段（LogSegment），每个日志分段还可以细分为索引文件、日志存储文件和快照文件等。 主题和分区都是提供给上层用户的抽象，而在副本层面或更加确切地说是Log层面才有实际物理上的存在。 主题管理 可以通过 Kafka提供的 kafka-topics.sh 脚本来执行这些操作 其实质上是调用了kafka.admin.TopicCommand类来执行主题管理的操作。 还可以通过KafkaAdminClient 的方式实现 直接操纵日志文件和ZooKeeper节点来实现。 创建主题 auto.create.topics.enable设置为true 那么当生产者向一个尚未创建的主题发送消息时，会自动创建一个分区数为num.partitions （默认值为1）、副本因子为default.replication.factor（默认值为1）的主题。 当一个消费者开始从未知主题中读取消息时，或者当任意一个客户端向未知主题发送元数据请求时，都会按照配置参数num.partitions和default.replication.factor的值来创建一个相应的主题。 不建议将auto.create.topics.enable参数设置为true 通过zookeeper /brokers/topics 可以查看主题分区情况 kafka-topics.sh脚本在创建主题时还会检测是否包含“.”或“_”字符。为什么要检测这两个字符呢？因为在Kafka的内部做埋点时会根据主题的名称来命名metrics的名称，并且会将点号“.”改成下画线“_”。假设遇到一个名称为“topic.1_2”的主题，还有一个名称为“topic_1.2”的主题，那么最后的metrics的名称都会为“topic_1_2”，这样就发生了名称冲突。 分区副本的分配 生产者的分区分配是指为每条消息指定其所要发往的分区， 消费者中的分区分配是指为消费者指定其可以消费消息的分区 在创建主题时，如果使用了replica-assignment参数，那么就按照指定的方案来进行分区副本的创建 查看主题create list describe alter delete 修改主题通过kafka-topics.sh 脚本中alter 指令提供 分区管理优先副本的选举 分区使用多副本机制来提升可靠性，但只有leader副本对外提供读写服务，而follower副本只负责在内部进行消息的同步。 如果一个分区的leader副本不可用，那么就意味着整个分区变得不可用，此时就需要Kafka从剩余的follower副本中挑选一个新的leader副本来继续对外提供服务。 leader 副本个数的多少决定了这个节点负载的高低。 针对同一个分区而言，同一个broker节点中不可能出现它的多个副本，即Kafka集群的一个broker中最多只能有它的一个副本 我们可以将leader副本所在的broker节点叫作分区的leader节点，而follower副本所在的broker节点叫作分区的follower节点。 当原来的leader节点恢复之后重新加入集群时，它只能成为一个新的follower节点而不再对外提供服务。 Kafka引入了优先副本（preferred replica）的概念。所谓的优先副本是指在 AR 集合列表中的第一个副本。 在 Kafka 中可以提供分区自动平衡的功能，与此对应的 broker 端参数是 auto.leader.rebalance.enable，此参数的默认值为true，即默认情况下此功能是开启的。 如果开启分区自动平衡的功能，则 Kafka 的控制器会启动一个定时任务，这个定时任务会轮询所有的 broker节点，计算每个broker节点的分区不平衡率（broker中的不平衡率=非优先副本的leader个数/分区总数）是否超过leader.imbalance.per.broker.percentage参数配置的比值，默认值为 10%，如果超过设定的比值则会自动执行优先副本的选举动作以求分区平衡。 执行周期由参数leader.imbalance.check.interval.seconds控制，默认值为300秒，即5分钟。 分区重分配当集群中加入节点或者减少节点时，需要重新分配分区，以达到负载均衡的目的 Kafka提供了 kafka-reassign-partitions.sh 脚本来执行分区重分配的工作，它可以在集群扩容、broker节点失效的场景下对分区进行迁移。 首先创建需要一个包含主题清单的JSON 文件， 其次根据主题清单和 broker 节点清单生成一份重分配方案， 最后根据这份方案执行具体的重分配动作。 分区重分配的基本原理是先通过控制器为每个分区添加新副本（增加副本因子）， 新的副本将从分区的leader副本那里复制所有的数据。 根据分区的大小不同，复制过程可能需要花一些时间，因为数据是通过网络复制到新副本上的。 在复制完成之后，控制器将旧副本从副本清单里移除（恢复为原先的副本因子数）。 注意在重分配的过程中要确保有足够的空间。 复制限流kafka-config.sh脚本主要以动态配置的方式来达到限流的目的，在broker级别有两个与复制限流相关的配置参数：follower.replication.throttled.rate和leader.replication.throttled.rate，前者用于设置follower副本复制的速度，后者用于设置leader副本传输的速度，它们的单位都是B/s。 修改副本因子创建主题之后我们还可以修改分区的个数，同样可以修改副本因子（副本数）。 如何选择合适的分区数性能测试工具Kafka 本身提供的用于生产者性能测试的 kafka-producer-perf-test.sh和用于消费者性能测试的kafka-consumer-perf-test.sh。 分区数越多吞吐量就越高吗分区是Kafka 中最小的并行操作单元，对生产者而言，每一个分区的数据写入是完全可以并行化的；对消费者而言，Kafka 只允许单个分区中的消息被一个消费者线程消费，一个消费组的消费并行度完全依赖于所消费的分区数。 本次案例中使用的测试环境为一个由3台普通云主机组成的3节点的Kafka集群，每台云主机的内存大小为8GB、磁盘大小为40GB、4核CPU的主频为2600MHz。JVM版本为1.8.0_112，Linux系统版本为2.6.32-504.23.4.el6.x86_64。 分区数上限由于文件描述符限制 ulimit -n，不能超过该值，会报错 考量因素一个“恰如其分”的答案就是视具体情况而定。Kafka本身、业务应用、硬件资源、环境配置等多方面的考量而做出的选择。在设定完分区数，或者更确切地说是创建主题之后，还要对其追踪、监控、调优以求更好地利用它。 如果一定要给一个准则，则建议将分区数设定为集群中broker的倍数，即假定集群中有3个broker节点，可以设定分区数为3、6、9等","categories":[],"tags":[]},{"title":"3.消费者","slug":"3-消费者","date":"2021-06-22T09:11:12.000Z","updated":"2021-06-24T13:03:12.305Z","comments":true,"path":"2021/06/22/3-消费者/","link":"","permalink":"https://sk-xinye.github.io/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/","excerpt":"","text":"消费者与消费组消费者（Consumer）负责订阅Kafka中的主题（Topic），并且从订阅的主题上拉取消息。与其他一些消息中间件不同的是：在Kafka的消费理念中还有一层消费组（Consumer Group）的概念，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。 每一个分区只能被一个消费组中的一个消费者所消费。 但是一味增加消费者个数，并不一定会增加消费能力，对于分区数固定的主题，当消费者个数大于分区数，就会有消费者不能分配到任何分区的情况 默认分区分配策略partition.assignment.strategy 消费组是一个逻辑上的概念，它将旗下的消费者归为一类，每一个消费者只隶属于一个消费组。每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数group.id来配置，默认值为空字符串。 消费者并非逻辑上的概念，它是实际的应用实例，它可以是一个线程，也可以是一个进程。同一个消费组内的消费者既可以部署在同一台机器上，也可以部署在不同的机器上。 消息投递模式对于消息中间件而言，一般有两种消息投递模式：点对点（P2P，Point-to-Point）模式和发布/订阅（Pub/Sub）模式。kafka都支持，得益于消费者组 如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。 如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布/订阅模式的应用。 客户端开发主要步骤 配置消费者客户端参数及创建相应的消费者实例。 订阅主题。 拉取消息并消费。 提交消费位移。 关闭消费者实例。 必要的参数配置 在initConfig（）方法，在Kafka消费者客户端KafkaConsumer中有4个参数是必填的。 bootstrap.servers：该参数的释义和生产者客户端 KafkaProducer 中的相同，用来 指 定 连 接 Kafka 集 群 所 需 的 broker 地 址 清 单，具 体 内 容 形 式 为host1：port1，host2：post， group.id：消费者隶属的消费组的名称，默认值为“”。如果设置为空，则会报出异常：Exception in thread “main” org.apache.kafka.common.errors.InvalidGroupIdException：The configured groupId is invalid。一般而言，这个参数需要设置成具有一定的业务意义的名称。 key.deserializer 和 value.deserializer：与生产者客户端 KafkaProducer中的key.serializer和value.serializer参数对应。 订阅主题与分区 订阅主题既可以以集合方式，也可以以正则表达式方式订阅（consumer.subscribe(Pattern.compile(“topic-.*”))），如图的构造函数 如果前后两次订阅了不同的主题，那么消费者以最后一次的为准。 其中一个构造参数ConsumerRebalance-Listener，这个是用来设置相应的再均衡监听器的 消费者不仅可以通过KafkaConsumer.subscribe（）方法订阅主题，还可以直接订阅某些主题的特定分区，在KafkaConsumer中还提供了一个assign（）方法来实现这些功能 public void assign(Collection&lt; TopicPartition &gt; partitions) 这个方法只接受一个参数partitions，用来指定需要订阅的分区集合。这里补充说明一下TopicPartition类，在Kafka的客户端中，它用来表示分区 TopicPartition类只有2个属性：topic和partition，分别代表分区所属的主题和自身的分区编号，这个类可以和我们通常所说的主题—分区的概念映射起来。 可以将subscribe 替换为assign consumer.assign(Arrays.asList(new TopicPrtition(“topic-demo”,0))) KafkaConsumer 中的partitionsFor（）方法可以用来查询指定主题的元数据信息 public List&lt; PartitionInfo &gt; partitionsFor(String topic) PartitionInfo类中的属性topic表示主题名称，partition代表分区编号，leader代表分区的leader副本所在的位置，replicas代表分区的AR集合，inSyncReplicas代表分区的ISR集合，offlineReplicas代表分区的OSR集合。 既然有订阅，那么就有取消订阅，可以使用 KafkaConsumer 中的 unsubscribe（）方法来取消主题的订阅。 反序列化 Kafka所提供的反序列化器有ByteBufferDeserializer、ByteArrayDeserializer、BytesDeserializer、DoubleDeserializer、FloatDeserializer、IntegerDeserializer、LongDeserializer、ShortDeserializer、StringDeserializer，它们分别用于ByteBuffer、ByteArray、Bytes、Double、Float、Integer、Long、Short 及String类型的反序列化，这些序列化器也都实现了 Deserializer 接口 与KafkaProducer中提及的Serializer接口一样，Deserializer接口也有三个方法 public void configure（Map＜String，？＞ configs，boolean isKey）：用来配置当前类。 public byte[] serialize（String topic，T data）：用来执行反序列化。如果data为null，那么处理的时候直接返回null而不是抛出一个异常。 public void close（）：用来关闭当前序列化器。 消息消费 Kafka中的消费是基于拉模式的。 Kafka中的消息消费是一个不断轮询的过程，消费者所要做的就是重复地调用poll（）方法，而poll（）方法返回的是所订阅的主题（分区）上的一组消息。 public ConsumerRecords&lt; k,v &gt; poll(final Duration timeout) 线程不安全的 https://www.pianshen.com/article/88711219115/ ConsumerRecords ConsumerRecords类提供了一个records（TopicPartition）方法来获取消息集中指定分区的消息 ConsumerRecords 类中并没提供与 partitions（）类似的 topics（）方法来查看拉取的消息集中所包含的主题列表，如果要按照主题维度来进行消费，那么只能根据消费者订阅主题时的列表来进行逻辑处理了。下面的示例演示了如何使用ConsumerRecords中的record（String topic）方法： 位移提交 对于消息在分区中的位置，我们将offset称为“偏移量”；对于消费者消费到的位置，将 offset 称为“位移”，有时候也会更明确地称之为“消费位移” 在 Kafka 中默认的消费位移的提交方式是自动提交 enable.auto.commit 配置，默认值为 true 当然这个默认的自动提交不是每消费一条消息就提交一次，而是定期提交，这个定期的周期时间由客户端参数auto.commit.interval.ms配置，默认值为5秒，此参数生效的前提是enable.auto.commit参数为true。 自动提交缺点： 自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象，但是在编程的世界里异常无可避免，与此同时，自动位移提交也无法做到精确的位移管理。 手动提交 enable.auto.commit配置为false 手动提交可以细分为同步提交和异步提交，对应于 KafkaConsumer 中的 commitSync（）和commitAsync（）两种类型的方法 commitSync（）commitAsync（） 有重复消费问题 12345while(isRunning.get()): ConsumerRecords&lt;String,String&gt; records = consumer.poll(1000)； for (ConsumerRecord&lt;String,String&gt; record: records)&#123; //do some logical processing &#125; 控制或关闭消费 KafkaConsumer中使用pause（）和resume（）方法来分别实现暂停某些分区在拉取操作时返回数据给客户端和恢复某些分区向客户端返回数据的操作 指定位移消费 如果将auto.offset.reset参数配置为“earliest”，那么消费者会从起始处，也就是0开始消费 再均衡 再均衡是指分区的所属权从一个消费者转移到另一消费者的行为，它为消费组具备高可用性和伸缩性提供保障，使我们可以既方便又安全地删除消费组内的消费者或往消费组内添加消费者。 不过在再均衡发生期间，消费组内的消费者是无法读取消息的。也就是说，在再均衡发生期间的这一小段时间内，消费组会变得不可用。 消费者拦截器多线程实现 为了加速消费者的消费能力，我们可以通过多线程的方式实现消息消费。 KafkaProducer是线程安全的，然而KafkaConsumer却是非线程安全的。 第一种也是最常见的方式：线程封闭，即为每个线程实例化一个KafkaConsumer对象 一个线程对应一个KafkaConsumer实例，我们可以称之为消费线程。 一个消费线程可以消费一个或多个分区中的消息，所有的消费线程都隶属于同一个消费组。 这种实现方式的并发度受限于分区的实际个数当消费线程的个数大于分区数时，就有部分消费线程一直处于空闲的状态。 上面这种多线程的实现方式和开启多个消费进程的方式没有本质上的区别，它的优点是每个线程可以按顺序消费各个分区中的消息。 缺点也很明显，每个消费线程都要维护一个独立的TCP连接，如果分区数和consumerThreadNum的值都很大，那么会造成不小的系统开销。 第二种方式是多个消费线程同时消费同一个分区 通过 assign（）、seek（）等方法实现,这样可以打破原有的消费线程的个数不能超过分区数的限制，进一步提高了消费的能力。 不过这种实现方式对于位移提交和顺序控制的处理就会变得非常复杂，实际应用中使用得极少，笔者也并不推荐。 一般而言，分区是消费线程的最小划分单位。 第三种实现方式，将处理消息模块改成多线程的实现方式一般而言，poll（）拉取消息的速度是相当快的，而整体消费的瓶颈也正是在处理消息这一块，如果我们通过一定的方式来改进这一部分，那么我们就能带动整体消费性能的提升。 KafkaConsumerThread类对应的是一个消费线程，里面通过线程池的方式来调用 RecordHandler 处理一批批的消息。 第三种实现方式还可以横向扩展，通过开启多个 KafkaConsumerThread 实例来进一步提升整体的消费能力。 第三种实现方式相比第一种实现方式而言，除了横向扩展的能力，还可以减少TCP连接对系统资源的消耗，不过缺点就是对于消息的顺序处理就比较困难了。 对于消费位移的处理可以通过滑动窗口来解决 每一个方格代表一个批次的消息，一个滑动窗口包含若干方格，startOffset标注的是当前滑动窗口的起始位置，endOffset标注的是末尾位置。 每当startOffset指向的方格中的消息被消费完成，就可以提交这部分的位移，与此同时，窗口向前滑动一格，删除原来startOffset所指方格中对应的消息，并且拉取新的消息进入窗口。 滑动窗口的大小固定，所对应的用来暂存消息的缓存大小也就固定了，这部分内存开销可控。 方格大小和滑动窗口的大小同时决定了消费线程的并发数：一个方格对应一个消费线程，对于窗口大小固定的情况，方格越小并行度越高； 对于方格大小固定的情况，窗口越大并行度越高。 不过，若窗口设置得过大，不仅会增大内存的开销，而且在发生异常（比如Crash）的情况下也会引起大量的重复消费，同时还考虑线程切换的开销，建议根据实际情况设置一个合理的值，不管是对于方格还是窗口而言，过大或过小都不合适。 如果一个方格内的消息无法被标记为消费完成，重试失败就转入重试队列，如果还不奏效就转入死信队列 重要的消费者参数 fetch.min.bytes 该参数用来配置Consumer在一次拉取请求（调用poll（）方法）中能从Kafka中拉取的最小数据量，默认值为1（B）。Kafka在收到Consumer的拉取请求时，如果返回给Consumer的数据量小于这个参数所配置的值，那么它就需要进行等待，直到数据量满足这个参数的配置大小。可以适当调大这个参数的值以提高一定的吞吐量，不过也会造成额外的延迟（latency），对于延迟敏感的应用可能就不可取了。 fetch.max.bytes 该参数与fetch.max.bytes参数对应，它用来配置Consumer在一次拉取请求中从Kafka中拉取的最大数据量，默认值为 52428800（B），也就是 50MB。 与此相关的，Kafka中所能接收的最大消息的大小通过服务端参数message.max.bytes（对应于主题端参数max.message.bytes）来设置。 fetch.max.wait.ms fetch.max.wait.ms参数用于指定Kafka的等待时间，默认值为500（ms）。 如果Kafka中没有足够多的消息而满足不了fetch.min.bytes参数的要求，那么最终会等待500ms。 这个参数的设定和Consumer与Kafka之间的延迟也有关系，如果业务应用对延迟敏感，那么可以适当调小这个参数。 max.partition.fetch.bytes 这个参数用来配置从每个分区里返回给Consumer的最大数据量，默认值为1048576（B），即1MB。 这个参数与 fetch.max.bytes 参数相似，只不过前者用来限制一次拉取中每个分区的消息大小，而后者用来限制一次拉取中整体消息的大小。 同样，如果这个参数设定的值比消息的大小要小，那么也不会造成无法消费，Kafka 为了保持消费逻辑的正常运转不会对此做强硬的限制。 max.poll.records 这个参数用来配置Consumer在一次拉取请求中拉取的最大消息数，默认值为500（条）。如果消息的大小都比较小，则可以适当调大这个参数值来提升一定的消费速度。 connections.max.idle.ms 这个参数用来指定在多久之后关闭限制的连接，默认值是540000（ms），即9分钟 exclude.internal.topics Kafka中有两个内部的主题：__consumer_offsets和__transaction_state exclude.internal.topics用来指定Kafka中的内部主题是否可以向消费者公开，默认值为true。 如果设置为true，那么只能使用subscribe（Collection）的方式而不能使用subscribe（Pattern）的方式来订阅内部主题，设置为false则没有这个限制。 receive.buffer.bytes 这个参数用来设置Socket接收消息缓冲区（SO_RECBUF）的大小，默认值为65536（B），即64KB。 如果设置为-1，则使用操作系统的默认值。 如果Consumer与Kafka处于不同的机房，则可以适当调大这个参数值。 send.buffer.bytes 这个参数用来设置Socket发送消息缓冲区（SO_SNDBUF）的大小，默认值为131072（B），即128KB。 与receive.buffer.bytes参数一样，如果设置为-1，则使用操作系统的默认值。 request.timeout.ms 这个参数用来配置Consumer等待请求响应的最长时间，默认值为30000（ms）。 metadata.max.age.ms 这个参数用来配置元数据的过期时间，默认值为300000（ms），即5分钟。 如果元数据在此参数所限定的时间范围内没有进行更新，则会被强制更新，即使没有任何分区变化或有新的broker加入。 reconnect.backoff.ms 这个参数用来配置尝试重新连接指定主机之前的等待时间（也称为退避时间），避免频繁地连接主机，默认值为50（ms）。 这种机制适用于消费者向broker发送的所有请求。 retry.backoff.ms 个参数用来配置尝试重新发送失败的请求到指定的主题分区之前的等待（退避）时间，避免在某些故障情况下频繁地重复发送，默认值为100（ms）。 isolation.level 这个参数用来配置消费者的事务隔离级别。 字符串类型，有效值为“read_uncommitted”和“read_committed”，表示消费者所消费到的位置，如果设置为“read_committed”，那么消费者就会忽略事务未提交的消息，即只能消费到 LSO（LastStableOffset）的位置，默认情况下为“read_uncommitted”，即可以消费到HW（High Watermark）处的位置。","categories":[],"tags":[]},{"title":"2.生产者","slug":"2-生产者","date":"2021-06-22T03:34:09.000Z","updated":"2021-06-22T23:13:18.209Z","comments":true,"path":"2021/06/22/2-生产者/","link":"","permalink":"https://sk-xinye.github.io/2021/06/22/2-%E7%94%9F%E4%BA%A7%E8%80%85/","excerpt":"","text":"客户端开发 配置生产者客户端参数及创建相应的生产者实例。 构建待发送的消息。 发送消息。 关闭生产者实例。 ProducerRecord topic 主题必填 key 用来分区使用，二次分类（主题分为一次分类），相同key 放到相同分区，而且拥有key的值还支持压缩功能。 value 为消息体，必填，一般不为空，如果为空则表示特定的消息–墓碑消息 timestamp 是指消息的时间戳，它有CreateTime和LogAppendTime两种类型，前者表示消息创建的时间，后者表示消息追加到日志文件的时间 必要的参数配置initConfig（）方法 bootstrap.servers：该参数用来指定生产者客户端连接Kafka集群所需的broker地址清单 key.serializer 和 value.serializer：broker 端接收的消息必须以字节数组（byte[]）的形式存在 client.id：这个参数用来设定KafkaProducer对应的客户端id，默认值为“”。如果客户端不设置，则KafkaProducer会自动生成一个非空字符串，内容形式如“producer-1”“producer-2” KafkaProducer 线程安全的，可以在多个线程中共享单个KafkaProducer实例，也可以将其进行池化供其他线程调用 其内部原理和无序列化器的构造方法一样，不过就实际应用而言，一般都选用 public KafkaProducer（Properties properties）这个构造方法来创建KafkaProducer实例。 异常类型 可重试异常：NetworkException、LeaderNotAvailableException、UnknownTopicOrPartitionException、NotEnoughReplicasException、NotCoordinatorException等 props.put(ProducerConfig.RETRIES_CONFIG,10) 出现异常后重试10次，在不行就抛出异常 不可重试异常：RecordTooLargeException异常等 send (在KafkaProducer中) 发后即忘（fire-and-forget） 同步（sync） 异步（async） send 方法可以通过返回Future &lt; RecordMetadata &gt;对象，调用 get 来阻塞kafka的相应，直到消息发送成功，当然也提供了超时阻塞的方法 回调函数也是保证顺序性的 经过拦截器（Interceptor）、序列化器（Serializer）和分区器（Partitioner） 序列化器生产者需要用序列化器（Serializer）把对象转换成字节数组才能通过网络发送给Kafka。而在对侧，消费者需要用反序列化器（Deserializer）把从 Kafka 中收到的字节数组转换成相应的对象。 分区器拦截器（下一章会详细介绍）一般不是必需的，而序列化器是必需的。消息经过序列化之后就需要确定它发往的分区，如果消息ProducerRecord中指定了partition字段，那么就不需要分区器的作用，因为partition代表的就是所要发往的分区号。 其中包含两个方法 Kafka中提供的默认分区器是org.apache.kafka.clients.producer.internals.DefaultPartitioner，它实现了org.apache.kafka.clients.producer.Partitioner接口 其中partition（）方法用来计算分区号，返回值为int类型。partition（）方法中的参数分别表示主题、键、序列化后的键、值、序列化后的值，以及集群的元数据信息，通过这些信息可以实现功能丰富的分区器。close（）方法在关闭分区器的时候用来回收一些资源。 分区方式： key 不为null,通过key计算哈希值，（采用MurmurHash2算法，具备高运算性能及低碰撞率） 如果key为null，那么消息将会以轮询的方式发往主题内的各个可用分区。 拦截器在序列化器和分区器之前调用拦截器的onSend()方法 整体架构有可能需要经历拦截器（Interceptor）、序列化器（Serializer）和分区器（Partitioner）等一系列的作用 整体分为2个线程，主线程和sender线程 在主线程中由KafkaProducer创建消息，然后通过可能的拦截器、序列化器和分区器的作用之后缓存到消息累加器（RecordAccumulator，也称为消息收集器）中。Sender 线程负责从RecordAccumulator中获取消息并将其发送到Kafka中。 RecordAccumulator 主要用来缓存消息以便 Sender 线程可以批量发送，进而减少网络传输的资源消耗以提升性能。 RecordAccumulator 缓存的大小可以通过生产者客户端参数buffer.memory 配置，默认值为 33554432B，即 32MB 在 RecordAccumulator 的内部为每个分区都维护了一个双端队列，队列中的内容就是ProducerBatch 消息写入缓存时，追加到双端队列的尾部；Sender读取消息时，从双端队列的头部读取。 注意ProducerBatch不是ProducerRecord，ProducerBatch中可以包含一至多个 ProducerRecord。 ProducerBatch的大小和batch.size参数也有着密切的关系 当一条消息（ProducerRecord）流入RecordAccumulator时，会先寻找与消息分区所对应的双端队列（如果没有则新建），再从这个双端队列的尾部获取一个 ProducerBatch（如果没有则新建），查看 ProducerBatch 中是否还可以写入这个 ProducerRecord，如果可以则写入，如果不可以则需要创建一个新的ProducerBatch。在新建ProducerBatch时评估这条消息的大小是否超过batch.size参数的大小，如果不超过，那么就以 batch.size 参数的大小来创建 ProducerBatch，这样在使用完这段内存区域之后，可以通过BufferPool 的管理来进行复用；如果超过，那么就以评估的大小来创建ProducerBatch，这段内存区域不会被复用。 Sender Sender 从 RecordAccumulator 中获取缓存的消息之后，会进一步将原本＜分区，Deque＜ProducerBatch＞＞的保存形式转变成＜Node，List＜ ProducerBatch＞的形式，其中Node表示Kafka集群的broker节点。 在转换成＜Node，List＜ProducerBatch＞＞的形式之后，Sender 还会进一步封装成＜Node，Request＞的形式 请求在从Sender线程发往Kafka之前还会保存到InFlightRequests中，InFlightRequests保存对象的具体形式为 Map＜NodeId，Deque＜Request＞＞，它的主要作用是缓存了已经发出去但还没有收到响应的请求（NodeId 是一个 String 类型，表示节点的 id 编号）。 InFlightRequests还可以获得leastLoadedNode，即负载最小的node 发送之前要获取元数据信息https://blog.csdn.net/yuanshangshenghuo/article/details/112625489 KafkaProducer要将此消息追加到指定主题的某个分区所对应的leader副本之前，首先需要知道主题的分区数量，然后经过计算得出（或者直接指定）目标分区，之后KafkaProducer需要知道目标分区的leader副本所在的broker 节点的地址、端口等信息才能建立连接，最终才能将消息发送到 Kafka，在这一过程中所需要的信息都属于元数据信息。 这些元数据具体记录了集群中有哪些主题，这些主题有哪些分区，每个分区的leader副本分配在哪个节点上，follower副本分配在哪些节点上，哪些副本在AR、ISR等集合中，集群中有哪些节点，控制器节点又是哪一个等信息。 时机 一个是业务线程获取topic分区信息元数据的时候发现没有，就会立马唤醒sender线程去拉取元数据 sender线程检查已经准备好的batch的时候，发现没有partition 没有leader副本的信息 一个是距离上次更新元数据已经过了5分钟了，这个时候就会去更新下，这个参数配置metadata.max.age.ms，默认是5分钟 broker 响应发送消息结果里面带着异常，InvalidMetadataException，就是元数据异常。 业务线程等待元数据 long waitedOnMetadataMs = waitOnMetadata(record.topic(), this.maxBlockTimeMs); 这行代码就是检查消息对应topic 的partition元数据信息是不是存在，如果不存在的话，就会不断尝试获取并且 “通知”sender线程去更新这个元数据信息。 sender线程拉取元数据 重要的生产者参数 acks acks=1。默认值即为1。生产者发送消息之后，只要分区的leader副本成功写入消息，那么它就会收到来自服务端的成功响应。 acks=0。生产者发送消息之后不需要等待任何服务端的响应。 acks=-1或acks=all。生产者在消息发送之后，需要等待ISR中的所有副本都成功写入消息之后才能够收到来自服务端的成功响应。 不一定可靠，因为isr中可能只有leader副本，需要配合参数min.insync.replicas max.request.size 这个参数用来限制生产者客户端能发送的消息的最大值，默认值为 1048576B，即 1MB。、 不建议设置过大，因为这个参数还涉及一些其他参数的联动，比如broker端的message.max.bytes参数 retries和retry.backoff.ms retries参数用来配置生产者重试的次数，默认值为0，即在发生异常的时候不进行任何重试动作。 重试还和另一个参数retry.backoff.ms有关，这个参数的默认值为100，它用来设定两次重试之间的时间间隔，避免无效的频繁重试。 顺序保证：max.in.flight.requests.per.connection配置为1，而不是把acks配置为0，不过这样也会影响整体的吞吐。 compression.type 这个参数用来指定消息的压缩方式，默认值为“none”，即默认情况下，消息不会被压缩。该参数还可以配置为“gzip”“snappy”和“lz4”。 消息压缩是一种使用时间换空间的优化方式，如果对时延有一定的要求，则不推荐对消息进行压缩。 connections.max.idle.ms 这个参数用来指定在多久之后关闭限制的连接，默认值是540000（ms），即9分钟。 linger.ms 这个参数用来指定生产者发送 ProducerBatch 之前等待更多消息（ProducerRecord）加入ProducerBatch 的时间，默认值为 0。 增加会提升系统的吞吐，但是会有时延 receive.buffer.bytes 这个参数用来设置Socket接收消息缓冲区（SO_RECBUF）的大小，默认值为32768（B），即32KB。如果设置为-1，则使用操作系统的默认值。如果Producer与Kafka处于不同的机房，则可以适地调大这个参数值。 send.buffer.bytes 这个参数用来设置Socket发送消息缓冲区（SO_SNDBUF）的大小，默认值为131072（B），即128KB。与receive.buffer.bytes参数一样，如果设置为-1，则使用操作系统的默认值。 request.timeout.ms 这个参数用来配置Producer等待请求响应的最长时间，默认值为30000（ms）。请求超时之后可以选择进行重试。注意这个参数需要比broker端参数replica.lag.time.max.ms的值要大，这样可以减少因客户端重试而引起的消息重复的概率。 总结 本章主要讲述了生产者客户端的具体用法及其整体架构，主要内容包括配置参数的详解、消息的发送方式、序列化器、分区器、拦截器等。 在实际应用中，一套封装良好的且灵活易用的客户端可以避免开发人员重复劳动，也提高了开发效率，还可以提高程序的健壮性和可靠性，而Kafka的客户端正好包含了这些特质。 对于KafkaProducer而言，它是线程安全的，我们可以在多线程的环境中复用它， 而对于下一章的消费者客户端KafkaConsumer而言，它是非线程安全的，因为它具备了状态，具体怎么使用我们不妨继续来了解下一章的内容。","categories":[],"tags":[]},{"title":"常见命令总结","slug":"常见命令总结","date":"2021-06-22T02:59:01.000Z","updated":"2023-07-16T13:14:55.046Z","comments":true,"path":"2021/06/22/常见命令总结/","link":"","permalink":"https://sk-xinye.github.io/2021/06/22/%E5%B8%B8%E8%A7%81%E5%91%BD%E4%BB%A4%E6%80%BB%E7%BB%93/","excerpt":"","text":"启动kafka程序 bin/kafka-server-start.sh config/server.properties Bin/kafka_server_start.sh -daemon config/server.properties 后台启动 创建主题 bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 24 –topic sensor_input_demo bin/kafka-topics.sh –create –zookeeper 192.168.10.233:2181 –replication-factor 2 –partitions 24 –topic sensor_input_test 生产者发送消息 bin/kafka-console-producer.sh –broker-list 172.18.31.161:9092 –topic sensor_input bin/kafka-console-producer.sh –broker-list 172.18.30.24:9092 –topic monitor bin/kafka-console-producer.sh –broker-list 172.18.30.24:9092,172.18.30.48:9092,172.18.30.16:9092 –topic feifei docker exec -it kafka /opt/kafka/bin/kafka-producer-perf-test.sh –topic test –record-size 1000 –num-records 1000000 –throughput -1 –producer-props bootstrap.servers=192.168.81.192:9092,192.168.81.193:9092,192.168.81.194:9092 消费者发送消息 Jxf |grep FILEPARSE_RULE_CHANGED bin/kafka-console-consumer.sh –bootstrap-server 172.18.31.19:9092 –from-beginning –topic sensor_input &gt; /opt/kafka/config/2222.txt bin/kafka-console-consumer.sh –bootstrap-server 172.18.30.24:9092,172.18.30.48:9092,172.18.30.16:9092 –from-beginning –topic feifei &gt; /opt/kafka/config/2222.txt bin/kafka-console-consumer.sh –bootstrap-server 192.168.10.232:9092,192.168.10.233:9092 –from-beginning –topic sensor_input_test docker exec -it kafka /opt/kafka/bin/kafka-console-consumer.sh –bootstrap-server localhost:9092 –from-beginning –topic monitor –max-messages 1 –property print.key=true 查看主题 bin/kafka-topics.sh –zookeeper localhost:2181 –list 查看所有主题 bin/kafka-topics.sh –zookeeper 192.168.10.226:2181 –list 删除创建的主题(注意这里一定要写ip，千万不要写localhost)磁盘 bin/kafka-topics.sh –delete –zookeeper 192.168.10.226:2181 –topic sensor_input bin/kafka-topics.sh –delete –zookeeper 192.168.10.226:2181 –topic sensor_input_three bin/kafka-topics.sh –delete –zookeeper 192.168.9.191:2181 –topic sensor_input_three 1)登录zookeeper客户端的命令：zookeeper/bin/zkCli.sh 2)找到topic所在的目录：ls /brokers/topics 3)找到要删除的topic，执行如下命令即可，此时topic被彻底删除：rmr /brokers/topics/topic名称 查看kafka消费情况 #查看某一个消费者组的消费情况 bin/kafka-consumer-groups.sh –bootstrap-server 172.18.31.49:9092 –describe –group sensor-es –members（查看成员信息） bin/kafka-consumer-groups.sh –bootstrap-server 192.168.10.232:9092,192.168.10.233:9092 –describe –group sensor-es #查看有哪些消费者组 bin/kafka-consumer-groups.sh –bootstrap-server localhost:9092 –list #删除消费者组 bin/kafka-consumer-groups.sh –bootstrap-server localhost:9092 –delete –group sensor-stream333 查看topic各个分区的消息的信息（time为-1时表示最大值，time为-2时表示最小值） bin/kafka-run-class.sh kafka.tools.GetOffsetShell –broker-list localhost:9092 –topic sensor_input_update –time -1 bin/kafka-run-class.sh kafka.tools.GetOffsetShell –broker-list 192.168.10.226:9092 –topic xy_test –time -1 bin/kafka-run-class.sh kafka.tools.GetOffsetShell –broker-list localhost:9092 –topic sensor_input bin/kafka-run-class.sh kafka.tools.GetOffsetShell –topic hive-mdatabase-hostsltable –time -1 –broker-list node86:9092 –partitions 0 修改kafka分区的数量 bin/kafka-topics.sh –zookeeper localhost:2181 –alter –topic sensor_input –partitions 24 查看consumer组的状态 bin/kafka-consumer-groups.sh –bootstrap-server 192.168.10.226:9092 –group sensor-stream –describe –state 重新消费 bin/kafka-consumer-groups.sh –bootstrap-server 192.168.10.226:9092 –reset-offsets –execute –group sensor_stream_zhangjian –all-topics –to-earliest 统计数量 bin/kafka-run-class.sh kafka.tools.GetOffsetShell –topic sensor_input –broker-list 192.168.10.226:9092 | awk -F ‘:’ ‘{sum += $3 } END {print sum}’ bin/kafka-run-class.sh kafka.tools.GetOffsetShell –topic webui_input –broker-list localhost:9092 | awk -F ‘:’ ‘{sum += $3 } END {print sum}’ bin/kafka-run-class.sh kafka.tools.GetOffsetShell –topic monitor –broker-list 172.18.30.24:9092 | awk -F ‘:’ ‘{sum += $3 } END {print sum}’ &gt; /opt/kafka/config/2222.txt 查看主题描述信息 docker exec -it kafka /opt/kafka/bin/kafka-topics.sh –zookeeper localhost:2181 –topic monitor –describe 查看消费者组描述信息 bin/kafka-consumer-groups.sh –bootstrap-server localhost:9092 –describe –group my-group –members 查看消费者组的状态 bin/kafka-consumer-groups.sh –bootstrap-server localhost:9092 –describe –group my-group –state 为什么快 在生产者发送的时候将消息包装成ProducerRecord后拼凑成紧凑的ProducerBatch,可以减少网络请求次数，提升整体吞吐。主要通过linger.ms 参数控制，默认为0,增加会提升系统的吞吐，但是会有时延 提供compression.type参数，可以进行消息的压缩，减少网络IO ，达到加速的目的。但是如果对时延有要求，就不建议开启这个了，是时间换空间的方法 提出了消费者组的概念，加快消费速度 使用索引，偏移量索引，时间索引，加速查询 大量使用页缓存。mmap技术，减少了一次开销 页缓存免去了I/O的资源消耗 页缓存比使用进程缓存省去了一次缓存的开销 顺序读写 零拷贝 所谓的零拷贝是指将数据直接从磁盘文件复制到网卡设备中，而不需要经由应用程序之手。https://www.infoq.cn/article/ukoqjkuwr0v0cs7u6p8o 写入时用的顺序写和mmap读取时用到了压缩和零拷贝 遇到问题获取元数据错误，导致生产者失败，官方从2.4开始对java 客户端进行了修改（将计时器进行了前置，即将计时器放到了获取元数据信息之前，来规避这个问题，或者将元数据信息存储到第三方服务中，这样也可以达到同样的效果），但是conflunce kafka 和kafka python 并没有做相应的修改 raid5 问题，改为raid0 就好 https://issues.apache.org/jira/browse/ZOOKEEPER-2348 broker.id=445listeners=INTERNAL://0.0.0.0:19092,EXTERNAL://0.0.0.0:9092listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXTadvertised.listeners=INTERNAL://127.0.0.1:19092,EXTERNAL://127.0.0.1:9092inter.broker.listener.name=INTERNALnum.network.threads=3num.io.threads=8socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600log.dirs=/opt/kafka/logsnum.partitions=24num.recovery.threads.per.data.dir=1offsets.topic.replication.factor=1transaction.state.log.replication.factor=1transaction.state.log.min.isr=1num.replica.fetchers=4replica.lag.time.max.ms=60000unclean.leader.election.enable=truelog.retention.hours=168log.segment.bytes=1073741824log.retention.check.interval.ms=300000zookeeper.connect=localhost:2181zookeeper.connection.timeout.ms=56000zookeeper.session.timeout.ms=400000group.initial.rebalance.delay.ms=0 broker.id=2listeners=PLAINTEXT://:9092#advertised.listeners=PLAINTEXT://kafka-0.kafka-service.test.svc.cluster.local:9092num.network.threads=3num.io.threads=8socket.send.buffer.bytes=102400socket.receive.buffer.bytes=102400socket.request.max.bytes=104857600log.dirs=/opt/kafka/logsnum.partitions=24default.replication.factor=2num.recovery.threads.per.data.dir=1offsets.topic.replication.factor=2transaction.state.log.replication.factor=1transaction.state.log.min.isr=1log.retention.hours=8760log.segment.bytes=1073741824log.retention.check.interval.ms=300000zookeeper.connect=10.43.33.143:2181zookeeper.connection.timeout.ms=18000group.initial.rebalance.delay.ms=0","categories":[],"tags":[]},{"title":"初识kafka","slug":"1-初识kafka","date":"2021-06-21T23:41:18.000Z","updated":"2021-06-22T23:13:18.190Z","comments":true,"path":"2021/06/22/1-初识kafka/","link":"","permalink":"https://sk-xinye.github.io/2021/06/22/1-%E5%88%9D%E8%AF%86kafka/","excerpt":"","text":"三大角色 消息系统 具备系统解耦、冗余存储、流量削峰、缓冲、异步通信、扩展性、可恢复性等功能。 消息顺序性保障及回溯消费的功能。 存储系统 支持永久存储到磁盘 副本机制 压缩机制 流式处理平台 窗口、连接、变换和聚合等各类操作。 基本概念 Producer：生产者，也就是发送消息的一方。生产者负责创建消息，然后将其投递到Kafka中。 Consumer：消费者，也就是接收消息的一方。消费者连接到Kafka上并接收消息，进而进行相应的业务逻辑处理。 Broker：服务代理节点。对于Kafka而言，Broker可以简单地看作一个独立的Kafka服务节点或Kafka服务实例。大多数情况下也可以 Topic: Kafka中的消息以主题为单位进行归类，生产者负责将消息发送到特定的主题（发送到Kafka集群中的每一条消息都要指定一个主题），而消费者负责订阅主题并进行消费。 Topic-Partition: 主题是一个逻辑上的概念，它还可以细分为多个分区，一个分区只属于单个主题,分区在存储层面可以看作一个可追加的日志（Log）文件 offset: offset是消息在分区中的唯一标识，Kafka通过它来保证消息在分区内的顺序性，不过offset并不跨越分区，也就是说，Kafka保证的是分区有序而不是主题有序。 Replica: 每个分区都有相应的副本，只有leader对外提供读写 AR（Assigned Replicas）：所有副本总和。等于ISR 和OSR总和 HW是High Watermark：所有副本中最小的offset +1 LEO是Log End Offset：单个副本的最大offset +1 安装安装JDK 下载JDK jdk-8u181-linux-x64.tar.gz tar zxvf jdk-8u181-linux-x64.tar.gz 配置环境变量 /etc/profile source后生效 export JAVA_HOME=/usr/local/java-se-8u41-ri export PATH=$JAVA_HOME/bin:$PATH export JRE_HOME=$JAVA_HOME/jre export CLASSPATH=./://$JAVA_HOME/lib:$JAVA_HOME/lib 查看安装是否成功 java -version 安装zookeeper 下载zookeeper zookeeper-3.4.12.tar.gz tar zxvf zookeeper-3.4.12.tar.gz 配置环境变量 export ZOOKEEPER_HOME=/opt/zookeeper-3.4.12.tar.gz export PATH=$PATH:$ZOOKEEPER_HOME/bin 修改zookeeper配置文件 conf目录下 cp zoo_sample.cfg zoo.cfg 修改配置文件如图 创建日志路径及数据路径 在数据目录${dataDir}目录 下创建myid文件，只包含一个数值，例如0，是服务器编号 启动 zkServer.sh start 查看状态 zkServer.sh status 集群模式下，需要修改/etc/hosts 中IP 与域名映射，并在zoo.cfg 中增加如下配置 server.0=192.168.1.0:2888:3888 2888是集群内机器通讯使用（Leader监听此端口） 3888选举leader使用 安装kafka 下载kafka kafka_2.11-2.0.0.tgz 配置环境变量 export KAFKA_HOME=/opt/zookeeper-3.4.12.tar.gz export PATH=$PATH:$KAFKA_HOME/bin 配置文件修改 $KAFKA_HOME/conf/server.properties broker.id=0 listeners=PLAINTEXT://localhost:9092 log.dirs=/tmp/kafka-logs zookeeper.connect=localhost:2181/kafka 启动kafka bin/kafka-server-start.sh config/server.properties &amp; jps 查看进程 生产消费服务端参数配置在$KAFKA_HOME/config/server.properties文件中 参数 zookeeper.connect zookeeper 服务地址 advertised.listeners 用来对外提供服务的网卡，listeners用来对内提供的服务网卡 broker.id 该参数用来指定Kafka集群中broker的唯一标识，默认值为-1。如果没有设置，那么Kafka会自动生成一个。这个参数还和meta.properties文件及服务端参数broker.id.generation.enable和reserved.broker.max.id有关 log.dir和log.dirs Kafka 把所有的消息都保存在磁盘上，而这两个参数用来配置 Kafka 日志文件存放的根目录。一般情况下，log.dir 用来配置单个根目录，而 log.dirs 用来配置多个根目录（以逗号分隔），但是Kafka并没有对此做强制性限制，也就是说，log.dir和log.dirs都可以用来配置单个或多个根目录。log.dirs 的优先级比 log.dir 高，但是如果没有配置log.dirs，则会以 log.dir 配置为准。默认情况下只配置了 log.dir 参数，其默认值为/tmp/kafka-logs。 message.max.bytes 该参数用来指定broker所能接收消息的最大值，默认值为1000012（B），约等于976.6KB。如果 Producer 发送的消息大于这个参数所设置的值，那么（Producer）就会报出RecordTooLargeException的异常。","categories":[],"tags":[]},{"title":"初识kafka","slug":"初识kafka","date":"2021-06-21T23:41:18.000Z","updated":"2021-08-08T13:43:29.808Z","comments":true,"path":"2021/06/22/初识kafka/","link":"","permalink":"https://sk-xinye.github.io/2021/06/22/%E5%88%9D%E8%AF%86kafka/","excerpt":"","text":"三大角色 消息系统 具备系统解耦、冗余存储、流量削峰、缓冲、异步通信、扩展性、可恢复性等功能。 消息顺序性保障及回溯消费的功能。 存储系统 支持永久存储到磁盘 副本机制 压缩机制 流式处理平台 窗口、连接、变换和聚合等各类操作。 基本概念","categories":[],"tags":[]},{"title":"概览","slug":"概览","date":"2021-06-17T13:13:35.000Z","updated":"2021-12-19T07:12:26.880Z","comments":true,"path":"2021/06/17/概览/","link":"","permalink":"https://sk-xinye.github.io/2021/06/17/%E6%A6%82%E8%A7%88/","excerpt":"","text":"概念Zookeeper概念Zookeeper 是一个分布式协调服务，可用于服务发现，分布式锁，分布式领导选举，配置管理等。Zookeeper 提供了一个类似于 Linux 文件系统的树形结构（可认为是轻量级的内存文件系统，但只适合存少量信息，完全不适合存储大量文件或者大文件），同时提供了对于每个节点的监控与通知机制 Zookeeper角色Zookeeper 集群是一个基于主从复制的高可用集群，每个服务器承担如下三种角色中的一种 Leader 一个 Zookeeper 集群同一时间只会有一个实际工作的 Leader，它会发起并维护与各 Follwer及 Observer 间的心跳。 所有的写操作必须要通过 Leader 完成再由 Leader 将写操作广播给其它服务器。只要有超过半数节点（不包括 observeer 节点）写入成功，该写请求就会被提交（类 2PC 协议）。 Follower 一个 Zookeeper 集群可能同时存在多个 Follower，它会响应 Leader 的心跳， Follower 可直接处理并返回客户端的读请求，同时会将写请求转发给 Leader 处理， 并且负责在 Leader 处理写请求时对请求进行投票 Observer角色与 Follower 类似，但是无投票权。Zookeeper 需保证高可用和强一致性，为了支持更多的客户端，需要增加更多 Server；Server 增多，投票阶段延迟增大，影响性能；引入 Observer，Observer 不参与投票； Observers 接受客户端的连接，并将写请求转发给 leader 节点； 加入更多 Observer 节点，提高伸缩性，同时不影响吞吐率。 ZAB 协议事务编号 Zxid （事务请求 计数器 + epoch ）在 ZAB ( ZooKeeper Atomic Broadcast , ZooKeeper 原子消息广播协议） 协议的事务编号 Zxid设计中，Zxid 是一个 64 位的数字，其中低 32 位是一个简单的单调递增的计数器，针对客户端每一个事务请求，计数器加 1；而高 32 位则代表 Leader 周期 epoch 的编号，每个当选产生一个新的 Leader 服务器，就会从这个 Leader 服务器上取出其本地日志中最大事务的 ZXID，并从中读取epoch 值，然后加 1，以此作为新的 epoch，并将低 32 位从 0 开始计数。 Zxid（Transaction id）类似于 RDBMS 中的事务 ID，用于标识一次更新操作的 Proposal（提议）ID。为了保证顺序性，该 zkid 必须单调递增。 epochepoch：可以理解为当前集群所处的年代或者周期，每个 leader 就像皇帝，都有自己的年号，所以每次改朝换代，leader 变更之后，都会在前一个年代的基础上加 1。这样就算旧的 leader 崩溃恢复之后，也没有人听他的了，因为 follower 只听从当前年代的 leader 的命令。 Zab 协议有两种模式 - 恢复模式（选主）、广播模式（同步 ）Zab协议有两种模式，它们分别是恢复模式（选主）和广播模式（同步）。当服务启动或者在领导者崩溃后，Zab 就进入了恢复模式，当领导者被选举出来，且大多数 Server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 Server 具有相同的系统状态。 ZAB 协议 4 阶段 Leader election （选举阶段 - 选出准 Leader ） Leader election（选举阶段）：节点在一开始都处于选举阶段，只要有一个节点得到超半数节点的票数，它就可以当选准 leader。只有到达 广播阶段（broadcast） 准 leader 才会成为真正的 leader。这一阶段的目的是就是为了选出一个准 leader，然后进入下一个阶段。 Discovery （发现阶段 - 接受提议、生成 epoch 、接受 epoch ） Discovery（发现阶段）：在这个阶段，followers 跟准 leader 进行通信，同步 followers最近接收的事务提议。这个一阶段的主要目的是发现当前大多数节点接收的最新提议，并且准 leader 生成新的 epoch，让 followers 接受，更新它们的 accepted Epoch 一个 follower 只会连接一个 leader，如果有一个节点 f 认为另一个 follower p 是 leader，f在尝试连接 p 时会被拒绝，f 被拒绝之后，就会进入重新选举阶段。 Synchronization （同步阶段 - 同步 follower 副本 ） 同步阶段主要是利用 leader 前一阶段获得的最新提议历史，同步集群中所有的副本。只有当 大多数节点都同步完成，准 leader 才会成为真正的 leader。follower 只会接收 zxid 比自己的 lastZxid 大的提议。 Broadcast （广播阶段 -leader 消息广播 Broadcast（广播阶段）：到了这个阶段，Zookeeper 集群才能正式对外提供事务服务，并且 leader 可以进行消息广播。同时如果有新的节点加入，还需要对新节点进行同步 ZAB 提交事务并不像 2PC 一样需要全部 follower 都 ACK，只需要得到超过半数的节点的 ACK 就可以了。 ZAB 协议 JAVA 实现 （ FLE-发现阶段和同步合并为 Recovery Phase（恢复阶段）协议的 Java 版本实现跟上面的定义有些不同，选举阶段使用的是 Fast Leader Election（FLE），它包含了 选举的发现职责。因为 FLE 会选举拥有最新提议历史的节点作为 leader，这样就省去了发现最新提议的步骤。实际的实现将 发现阶段 和 同步合并为 Recovery Phase（恢复阶段）。所以，ZAB 的实现只有三个阶段：Fast Leader Election；Recovery Phase；Broadcast Phase。 投票机制每个 sever 首先给自己投票，然后用自己的选票和其他 sever 选票对比，权重大的胜出，使用权重较大的更新自身选票箱。具体选举过程如下： 每个 Server 启动以后都询问其它的 Server 它要投票给谁。对于其他 server 的询问，server 每次根据自己的状态都回复自己推荐的 leader 的 id 和上一次处理事务的 zxid（系统启动时每个 server 都会推荐自己） 收到所有 Server 回复以后，就计算出 zxid 最大的哪个 Server，并将这个 Server 相关信息设置成下一次要投票的 Server。 计算这过程中获得票数最多的的 sever 为获胜者，如果获胜者的票数超过半数，则改server 被选为 leader。否则，继续这个过程，直到 leader 被选举出来 leader 就会开始等待 server 连接 Follower 连接 leader，将最大的 zxid 发送给 leader Leader 根据 follower 的 zxid 确定同步点，至此选举阶段完成。 选举阶段完成 Leader 同步后通知 follower 已经成为 uptodate 状态 Follower 收到 uptodate 消息后，又可以重新接受 client 的请求进行服务了 目前有 5 台服务器，每台服务器均没有数据，它们的编号分别是 1,2,3,4,5,按编号依次启动，它们的选择举过程如下 服务器 1 启动，给自己投票，然后发投票信息，由于其它机器还没有启动所以它收不到反馈信息，服务器 1 的状态一直属于 Looking。 服务器 2 启动，给自己投票，同时与之前启动的服务器 1 交换结果，由于服务器 2 的编号大所以服务器 2 胜出，但此时投票数没有大于半数，所以两个服务器的状态依然是LOOKING。 服务器 3 启动，给自己投票，同时与之前启动的服务器 1,2 交换信息，由于服务器 3 的编号最大所以服务器 3 胜出，此时投票数正好大于半数，所以服务器 3 成为领导者，服务器1,2 成为小弟。 服务器 4 启动，给自己投票，同时与之前启动的服务器 1,2,3 交换信息，尽管服务器 4 的编号大，但之前服务器 3 已经胜出，所以服务器 4 只能成为小弟。 服务器 5 启动，后面的逻辑同服务器 4 成为小弟。 Zookeeper 工作原理 工作原理 （ 原子广播） Zookeeper 的核心是原子广播，这个机制保证了各个 server 之间的同步。实现这个机制的协议叫做 Zab 协议。Zab 协议有两种模式，它们分别是恢复模式和广播模式。 当服务启动或者在领导者崩溃后，Zab 就进入了恢复模式，当领导者被选举出来，且大多数 server 的完成了和 leader 的状态同步以后，恢复模式就结束了。 状态同步保证了 leader 和 server 具有相同的系统状态 一旦 leader 已经和多数的 follower 进行了状态同步后，他就可以开始广播消息了，即进入广播状态。这时候当一个 server 加入 zookeeper 服务中，它会在恢复模式下启动，发现 leader，并和 leader 进行状态同步。待到同步结束，它也参与消息广播。Zookeeper服务一直维持在 Broadcast 状态，直到 leader 崩溃了或者 leader 失去了大部分的followers 支持。 广播模式需要保证 proposal 被按顺序处理，因此 zk 采用了递增的事务 id 号(zxid)来保证。所有的提议(proposal)都在被提出的时候加上了 zxid。 实现中 zxid 是一个 64 为的数字，它高 32 位是 epoch 用来标识 leader 关系是否改变，每次一个 leader 被选出来，它都会有一个新的 epoch。低 32 位是个递增计数。 当 leader 崩溃或者 leader 失去大多数的 follower，这时候 zk 进入恢复模式，恢复模式需要重新选举出一个新的 leader，让所有的 server 都恢复到一个正确的状态。 Znode 有四种形式的目录节点 PERSISTENT：持久的节点。 EPHEMERAL：暂时的节点。 PERSISTENT_SEQUENTIAL：持久化顺序编号目录节点。 EPHEMERAL_SEQUENTIAL：暂时化顺序编号目录节点。 server.A=B：C：Dserver.A=B：C：D：其中 A 是一个数字，表示这个是第几号服务器；B 是这个服务器的 ip 地址；C 表示的是这个服务器与集群中的 Leader 服务器交换信息的端口；D 表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方式，由于 B 都是一样，所以不同的 Zookeeper 实例通信端口号不能一样，所以要给它们分配不同的端口号。Zookeeper 启动时会读取dataDir里面的myid文件，拿到里面的数据与 zoo.cfg 里面的配置信息比较从而判断到底是那个 server。","categories":[],"tags":[]},{"title":"总览","slug":"总览","date":"2021-06-17T06:04:08.000Z","updated":"2023-07-16T13:14:55.048Z","comments":true,"path":"2021/06/17/总览/","link":"","permalink":"https://sk-xinye.github.io/2021/06/17/%E6%80%BB%E8%A7%88/","excerpt":"","text":"磁盘原理 Linux 概述什么是LinuxLinux是一套免费使用和自由传播的类Unix操作系统，是一个基于POSIX和Unix的多用户、多任务、支持多线程和多CPU的操作系统。它能运行主要的Unix工具软件、应用程序和网络协议。它支持32位和64位硬件。Linux继承了Unix以网络为核心的设计思想，是一个性能稳定的多用户网络操作系统。 Unix和Linux有什么区别Linux和Unix都是功能强大的操作系统，都是应用广泛的服务器操作系统，有很多相似之处，甚至有一部分人错误地认为Unix和Linux操作系统是一样的，然而，事实并非如此，以下是两者的区别。 开源性 Linux是一款开源操作系统，不需要付费，即可使用；Unix是一款对源码实行知识产权保护的传统商业软件，使用需要付费授权使用。 跨平台性 Linux操作系统具有良好的跨平台性能，可运行在多种硬件平台上；Unix操作系统跨平台性能较弱，大多需与硬件配套使用。 可视化界面 Linux除了进行命令行操作，还有窗体管理系统；Unix只是命令行下的系统。 硬件环境 Linux操作系统对硬件的要求较低，安装方法更易掌握；Unix对硬件要求比较苛刻，安装难度较大。 用户群体 Linux的用户群体很广泛，个人和企业均可使用；Unix的用户群体比较窄，多是安全性要求高的大型企业使用，如银行、电信部门等，或者Unix硬件厂商使用，如Sun等。相比于Unix操作系统，Linux操作系统更受广大计算机爱好者的喜爱，主要原因是Linux操作系统具有Unix操作系统的全部功能，并且能够在普通PC计算机上实现全部的Unix特性，开源免费的特性，更容易普及使用！ 什么是 Linux 内核Linux 系统的核心是内核。内核控制着计算机系统上的所有硬件和软件，在必要时分配硬件，并根据需要执行软件。 系统内存管理 应用程序管理 硬件设备管理 文件系统管理 Linux的基本组件是什么就像任何其他典型的操作系统一样，Linux拥有所有这些组件：内核，shell和GUI，系统实用程序和应用程序。Linux比其他操作系统更具优势的是每个方面都附带其他功能，所有代码都可以免费下载。 Linux 的体系结构从大的方面讲，Linux 体系结构可以分为两块： 用户空间(User Space) ：用户空间又包括用户的应用程序(User Applications)、C 库(C Library) 。内核空间(Kernel Space) ：内核空间又包括系统调用接口(System Call Interface)、内核(Kernel)、平台架构相关的代码(Architecture-Dependent Kernel Code) 。 为什么 Linux 体系结构要分为用户空间和内核空间的原因 现代 CPU 实现了不同的工作模式，不同模式下 CPU 可以执行的指令和访问的寄存器不同。 实模式 （Real-Address Mode）：是指8086时代的工作模式，也就是通过 段寄存器«4 +偏移地址的寻址方式。 保护模式 （Protected Mode）: 从80386开始引入了保护模式，它的寄存器从16位扩展到了32位，而且不再使用之前的段寄存器«4 +偏移地址的寻址方式，而是使用段描述符的方式描述基址和限长，并通过描述符中的属性设置实现对内存段的访问限制和数据 保护。 虚拟8086模式（Virtual-8086 Mode）：这个就是所谓的准操作模式，在保护模式下，CPU支持运行8086的软件。 系统管理模式（System Management Mode ): 从80386以后引入的一个标准功能，最初应该是用于实现电源管理的功能或者用来让OEM做一些差异性的feature，它对OS或者其它应用是透明的。当有系统管理事件产生时，CPU上的SMI#（external system interrupt pin）就会被触发，CPU就会进入SMM mode并切换到一个单独的地址空间保存上下文然后执行相应的任务，当RSM从SMM返回时，CPU恢复之前的工作继续执行。 Linux 从 CPU 的角度出发，为了保护内核的安全，把系统分成了两部分。 用户空间和内核空间是程序执行的两种不同的状态，我们可以通过两种方式完成用户空间到内核空间的转移：1）系统调用；2）硬件中断。 BASH和DOS之间的基本区别是什么BASH和DOS控制台之间的主要区别在于3个方面： BASH命令区分大小写，而DOS命令则不区分; 在BASH下，/character是目录分隔符，\\作为转义字符。在DOS下，/用作命令参数分隔符，\\是目录分隔符 DOS遵循命名文件中的约定，即8个字符的文件名后跟一个点，扩展名为3个字符。BASH没有遵循这样的惯例。 Linux 开机启动过程 主机加电自检，加载 BIOS 硬件信息。 读取 MBR 的引导文件(GRUB、LILO)。 引导 Linux 内核。 运行第一个进程 init (进程号永远为 1 )。 进入相应的运行级别。 运行终端，输入用户名和密码 Linux 使用的进程间通信方式 管道(pipe)、流管道(s_pipe)、有名管道(FIFO)。 信号(signal) 。 消息队列。 共享内存。 信号量。 套接字(socket)。 Linux 有哪些系统日志文件比较重要的是 /var/log/messages 日志文件。 该日志文件是许多进程日志文件的汇总，从该文件可以看出任何入侵企图或成功的入侵。 另外，如果胖友的系统里有 ELK 日志集中收集，它也会被收集进去。 什么是交换空间交换空间是Linux使用的一定空间，用于临时保存一些并发运行的程序。当RAM没有足够的内存来容纳正在执行的所有程序时，就会发生这种情况。 什么是root帐户root帐户就像一个系统管理员帐户，允许你完全控制系统。你可以在此处创建和维护用户帐户，为每个帐户分配不同的权限。每次安装Linux时都是默认帐户。 什么是LILOLILO是Linux的引导加载程序。它主要用于将Linux操作系统加载到主内存中，以便它可以开始运行。 什么是BASHBASH是Bourne Again SHell的缩写。它由Steve Bourne编写，作为原始Bourne Shell（由/ bin / sh表示）的替代品。它结合了原始版本的Bourne Shell的所有功能，以及其他功能，使其更容易使用。从那以后，它已被改编为运行Linux的大多数系统的默认shell。 磁盘、目录、文件简单 Linux 文件系统在 Linux 操作系统中，所有被操作系统管理的资源，例如网络接口卡、磁盘驱动器、打印机、输入输出设备、普通文件或是目录都被看作是一个文件。 也就是说在 Linux 系统中有一个重要的概念：一切都是文件。其实这是 Unix 哲学的一个体现，而 Linux 是重写 Unix 而来，所以这个概念也就传承了下来。在 Unix 系统中，把一切资源都看作是文件，包括硬件设备。UNIX系统把每个硬件都看成是一个文件，通常称为设备文件，这样用户就可以用读写文件的方式实现对硬件的访问。 Linux 支持 5 种文件类型，如下图所示： Linux 的目录结构是怎样的Linux 文件系统的结构层次鲜明，就像一棵倒立的树，最顶层是其根目录： 常见目录说明： /bin： 存放二进制可执行文件(ls,cat,mkdir等)，常用命令一般都在这里； /etc： 存放系统管理和配置文件； /home： 存放所有用户文件的根目录，是用户主目录的基点，比如用户user的主目录就是/home/user，可以用~user表示； /usr： 用于存放系统应用程序； /opt： 额外安装的可选应用程序包所放置的位置。一般情况下，我们可以把tomcat等都安装到这里； /proc： 虚拟文件系统目录，是系统内存的映射。可直接访问这个目录来获取系统信息； /root： 超级用户（系统管理员）的主目录（特权阶级o）； /sbin: 存放二进制可执行文件，只有root才能访问。这里存放的是系统管理员使用的系统级别的管理命令和程序。如ifconfig等； /dev： 用于存放设备文件； /mnt： 系统管理员安装临时文件系统的安装点，系统提供这个目录是让用户临时挂载其他的文件系统； /boot： 存放用于系统引导时使用的各种文件； /lib： 存放着和系统运行相关的库文件 ； /tmp： 用于存放各种临时文件，是公用的临时文件存储点； /var： 用于存放运行时需要改变数据的文件，也是某些大文件的溢出区，比方说各种服务的日志文件（系统启动日志等。）等； /lost+found： 这个目录平时是空的，系统非正常关机而留下“无家可归”的文件（windows下叫什么.chk）就在这里。 什么是 inode理解inode，要从文件储存说起。 文件储存在硬盘上，硬盘的最小存储单位叫做”扇区”（Sector）。每个扇区储存512字节（相当于0.5KB）。 操作系统读取硬盘的时候，不会一个个扇区地读取，这样效率太低，而是一次性连续读取多个扇区，即一次性读取一个”块”（block）。这种由多个扇区组成的”块”，是文件存取的最小单位。”块”的大小，最常见的是4KB，即连续八个 sector组成一个 block。 文件数据都储存在”块”中，那么很显然，我们还必须找到一个地方储存文件的元信息，比如文件的创建者、文件的创建日期、文件的大小等等。这种储存文件元信息的区域就叫做inode，中文译名为”索引节点”。 每一个文件都有对应的inode，里面包含了与该文件有关的一些信息。 简述 Linux 文件系统通过 i 节点把文件的逻辑结构和物理结构转换的工作过程Linux 通过 inode 节点表将文件的逻辑结构和物理结构进行转换。 inode 节点是一个 64 字节长的表，表中包含了文件的相关信息，其中有文件的大小、文件所有者、文件的存取许可方式以及文件的类型等重要信息。在 inode 节点表中最重要的内容是磁盘地址表。在磁盘地址表中有 13 个块号，文件将以块号在磁盘地址表中出现的顺序依次读取相应的块。 Linux 文件系统通过把 inode 节点和文件名进行连接，当需要读取该文件时，文件系统在当前目录表中查找该文件名对应的项，由此得到该文件相对应的 inode 节点号，通过该 inode 节点的磁盘地址表把分散存放的文件物理块连接成文件的逻辑结构。 什么是硬链接和软链接1）硬链接 由于 Linux 下的文件是通过索引节点(inode)来识别文件，硬链接可以认为是一个指针，指向文件索引节点的指针，系统并不为它重新分配 inode 。每添加一个一个硬链接，文件的链接数就加 1 。 不足：1）不可以在不同文件系统的文件间建立链接；2）只有超级用户才可以为目录创建硬链接。 2）软链接 ln -s [源文件或目录] [目标文件或目录] 正确的删除方式（删除软链接，但不删除实际数据）rm -rf ./test_chk_ln 错误的删除方式rm -rf ./test_chk_ln/ (这样就会把原来test_chk下的内容删除) 软链接克服了硬链接的不足，没有任何文件系统的限制，任何用户可以创建指向目录的符号链接。因而现在更为广泛使用，它具有更大的灵活性，甚至可以跨越不同机器、不同网络对文件进行链接。 不足：因为链接文件包含有原文件的路径信息，所以当原文件从一个目录下移到其他目录中，再访问链接文件，系统就找不到了，而硬链接就没有这个缺陷，你想怎么移就怎么移；还有它要系统分配额外的空间用于建立新的索引节点和保存原文件的路径。 实际场景下，基本是使用软链接。总结区别如下： 硬链接不可以跨分区，软件链可以跨分区。 硬链接指向一个 inode 节点，而软链接则是创建一个新的 inode 节点。 删除硬链接文件，不会删除原文件，删除软链接文件，会把原文件删除。 安全一台 Linux 系统初始化环境后需要做一些什么安全工作 添加普通用户登陆，禁止 root 用户登陆，更改 SSH 端口号,不一定干这个事。 服务器使用密钥登陆，禁止密码登陆。 开启防火墙，关闭 SElinux ，根据业务需求设置相应的防火墙规则。 装 fail2ban 这种防止 SSH 暴力破击的软件 设置只允许公司办公网出口 IP 能登陆服务器(看公司实际需要) 修改历史命令记录的条数为 10 条。 只允许有需要的服务器可以访问外网，其它全部禁止。 做好软件层面的防护。 8.1 设置 nginx_waf 模块防止 SQL 注入。 8.2 把 Web 服务使用 www 用户启动，更改网站目录的所有者和所属组为 www 。 什么叫 CC 攻击？什么叫 DDOS 攻击 CC 攻击，主要是用来攻击页面的，模拟多个用户不停的对你的页面进行访问，从而使你的系统资源消耗殆尽。 DDOS 攻击，中文名叫分布式拒绝服务攻击，指借助服务器技术将多个计算机联合起来作为攻击平台，来对一个或多个目标发动 DDOS 攻击。 攻击，即是通过大量合法的请求占用大量网络资源，以达到瘫痪网络的目的。 预防 CC、DDOS 攻击，这些只能是用硬件防火墙做流量清洗，将攻击流量引入黑洞。 什么是网站数据库注入 由于程序员的水平及经验参差不齐，大部分程序员在编写代码的时候，没有对用户输入数据的合法性进行判断。 应用程序存在安全隐患。用户可以提交一段数据库查询代码，根据程序返回的结果，获得某些他想得知的数据，这就是所谓的 SQL 注入。 SQL注入，是从正常的 WWW 端口访问，而且表面看起来跟一般的 Web 页面访问没什么区别，如果管理员没查看日志的习惯，可能被入侵很长时间都不会发觉。 如何过滤与预防？ 数据库网页端注入这种，可以考虑使用 nginx_waf 做过滤与预防。 ShellShell 脚本是什么一个 Shell 脚本是一个文本文件，包含一个或多个命令。作为系统管理员，我们经常需要使用多个命令来完成一项任务，我们可以添加这些所有命令在一个文本文件(Shell 脚本)来完成这些日常工作任务。 什么是默认登录 Shell在 Linux 操作系统，”/bin/bash” 是默认登录 Shell，是在创建用户时分配的。 使用 chsh 命令可以改变默认的 Shell 。示例如下所示： 12## chsh &lt;用户名&gt; -s &lt;新shell&gt;## chsh ThinkWon -s /bin/sh 语法级可以在 Shell 脚本中使用哪些类型的变量 系统定义变量 系统变量是由系统自己创建的。这些变量通常由大写字母组成，可以通过 set 命令查看。 用户定义变量 用户变量由系统用户来生成和定义，变量的值可以通过命令 “echo $&lt;变量名&gt;” 查看。 Shell脚本中 $? 标记的用途是什么在写一个 Shell 脚本时，如果你想要检查前一命令是否执行成功，在 if 条件中使用 $? 可以来检查前一命令的结束状态。 如果结束状态是 0 ，说明前一个命令执行成功。如果结束状态不是0，说明命令执行失败。例如： 1234root@localhost:~## ls /usr/bin/shar/usr/bin/sharroot@localhost:~## echo $?0 Bourne Shell(bash) 中有哪些特殊的变量12345678内建变量 解释$0 命令行中的脚本名字$1 第一个命令行参数$2 第二个命令行参数….. …….$9 第九个命令行参数$## 命令行参数的数量$* 所有命令行参数，以空格隔开 如何取消变量或取消变量赋值unset 命令用于取消变量或取消变量赋值。语法如下所示：## unset &lt;变量名&gt; Shell 脚本中 if 语法如何嵌套1234567891011121314151617if [ 条件 ]then命令1命令2…..elseif [ 条件 ]then命令1命令2….else命令1命令2…..fifi 在 Shell 脚本中如何比较两个数字在 if-then 中使用测试命令（ -gt 等）来比较两个数字。例如： 123456789#!/bin/bashx=10y=20if [ $x -gt $y ]thenecho “x is greater than y”elseecho “y is greater than x”fi Shell 脚本中 for 循环语法1234567for 变量 in 循环列表do命令1命令2….最后命令done Shell 脚本中 while 循环语法如同 for 循环，while 循环只要条件成立就重复它的命令块。不同于 for循环，while 循环会不断迭代，直到它的条件不为真。 123456789while [ 条件 ]do命令…donedo&#123;命令&#125; while (条件) Shell 脚本中 break 命令的作用break 命令一个简单的用途是退出执行中的循环。我们可以在 while 和 until 循环中使用 break 命令跳出循环。 Shell 脚本中 continue 命令的作用continue 命令不同于 break 命令，它只跳出当前循环的迭代，而不是整个循环。continue 命令很多时候是很有用的，例如错误发生，但我们依然希望继续执行大循环的时候。 如何使脚本可执行使用 chmod 命令来使脚本可执行。例子如下：chmod a+x myscript.sh #!/bin/bash 的作用1#!/bin/bash 是 Shell 脚本的第一行，称为释伴（shebang）行。 这里 # 符号叫做 hash ，而 ! 叫做 bang。它的意思是命令通过 /bin/bash 来执行。 如何将标准输出和错误输出同时重定向到同一位置方法一：2&gt;&amp;1 (如## ls /usr/share/doc &gt; out.txt 2&gt;&amp;1 ) 。方法二：&amp;&gt; (如## ls /usr/share/doc &amp;&gt; out.txt ) 在 Shell 脚本如何定义函数呢1234567$ diskusage () &#123; df -h ; &#125;译注：下面是我给的shell函数语法，原文没有[ function ] 函数名 [()]&#123;命令;[return int;]&#125; 如何让 Shell 就脚本得到来自终端的输入123456789## vi /tmp/test.sh#!/bin/bashecho ‘Please enter your name’read nameecho “My Name is $name”## ./test.shPlease enter your nameThinkWonMy Name is ThinkWon 如何执行算术运算 使用 expr 命令：## expr 5 + 2 。 用一个美元符号和方括号（$[ 表达式 ]）：test=$[16 + 4] ; test=$[16 + 4] 。 文件管理命令 cat 命令 cp 命令 ln 命令 mv 命令 rm 命令 tail 命令 vim 命令 chmod 命令-rw-r–r– 1 root root 296K 11-13 06:03 log2012.log 第一列共有 10 个位置，第一个字符指定了文件类型。在通常意义上，一个目录也是一个文件。如果第一个字符是横线，表示是一个非目录的文件。如果是 d，表示是一个目录。从第二个字符开始到第十个 9 个字符，3 个字符一组，分别表示了 3 组用户对文件或者目录的权限。权限字符用横线代表空许可，r 代表只读，w 代表写，x 代表可执行。 chown 命令-c 显示更改的部分的信息-R 处理指定目录及子目录 chown -c mail:mail log2012.log find find ./ -name ‘*.log’ 在当前目录下xun find /opt -perm 777 查找 /opt 目录下 权限为 777 的文件 find -size +1000c 查找大于 1K 的文件 find -size 1000c 查找等于 1000 字符的文件 head 命令 head 1.log -n 20 显示 1.log 文件中前 20 行 head -n -10 t.log 显示 t.log最后 10 行 which 命令which ls 查看 ls 命令是否存在，执行哪个 文档编辑命令替换:%s/well/good/g（等同于 :g/well/s//good/g） 替换每一行中所有 well 为 good grep -c night restart.07014命令 wc 命令wc(word count)功能为统计指定的文件中字节数、字数、行数，并将统计结果输出 -c 统计字节数 -l 统计行数 -m 统计字符数 -w 统计词数，一个字被定义为由空白、跳格或换行字符分隔的字符串 wc text.txt 查找文件的 行数 单词数 字节数 文件名 cat test.txt | wc -l 统计输出结果的行数 磁盘管理命令 cd 命令 df 命令 df -h du 命令 du -h scf/ 以易读方式显示文件夹内及子文件夹大小 du -ah scf/ 以易读方式显示文件夹内所有文件大小 du -hc test/ scf/ 显示几个文件或目录各自占用磁盘空间的大小，还统计它们的总和 du -hc –max-depth=1 scf/ 输出当前目录下各个子目录所使用的空间 du -sh * ls命令 mkdir 命令 pwd 命令 rmdir 命令 网络通讯命令 ifconfig 命令 iptables 命令 netstat 命令 ping 命令 telnet 命令 telnet 192.168.0.5 登录IP为 192.168.0.5 的远程主机 系统管理命令date 命令free 命令kill 命令ps 命令top 命令yum 命令 备份压缩命令 zip 命令 zip -r unzip 命令 解压 *.zip 文件：unzip test.zip 。 查看 *.zip 文件的内容：unzip -l jasper.zip tar 命令 tar -zcvf /tmp/etc.tar.gz /etc 将 /etc 下的所有文件及目录打包到指定目录，并使用 gz 压缩sed -i ‘$a添加的内容’ file #这是在最后一行行后添加字符串 文件描述符ulimit -n 查看系统允许的最大文件描述符数量ulimit -S/-Hn 分别查看软限制和硬限制。硬限制设定之后不能在添加，软限制可以增加到硬限制规定的值。ls /proc/31796进程号/fd |wc -l 查看某个进程占用的文件描述符（遇到问题,时间一长就crash 了，通过拉取/var/message中日志，找到时间，然后去/var/crash 查看，发现是系统申请资源的时候申请不到，导致了crash，最终定位是有个程序的文件句柄未关闭，导致超过了限制，所以才发生的问题）/etc/security/limits.conf文件中进行修改 root soft nofile 65535 或者ulimit -n 65535 系统日志watch journalctl _PID=2409 磁盘读写操作前线执行下面命令得其中一条先清除cache/sbin/sysctl -w vm.drop_caches=3 echo 3 &gt; /proc/sys/vm/drop_caches 随机读fio -name=randread -direct=1 -iodepth=64 -rw=randread -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sda （根据实际盘符来写） 随机写fio -name=randwrite -direct=1 -iodepth=64 -rw=randwrite -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sda （根据实际盘符来写） 顺序读fio -name=read -direct=1 -iodepth=64 -rw=read -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sda （根据实际盘符来写） 顺序写fio -name=write -direct=1 -iodepth=64 -rw=write -ioengine=libaio -bs=4k -size=1G -numjobs=1 -runtime=1000 -group_reporting -filename=/dev/sda （根据实际盘符来写） 执行完命令后将打印结果粘贴到文本中 免密登录ssh-keygen -t rsa 一直按回车ssh-copy-id -p 10022 &#x7a;&#115;&#104;&#x69;&#101;&#x6c;&#x64;&#64;&#49;&#57;&#x32;&#46;&#49;&#x36;&#x38;&#x2e;&#x38;&#51;&#46;&#x32;&#x35;&#x32; 下载包yum -y install yum-utils 下载工具包 repotrack ansible -p /home/zshield #下载rpm包rpm -Uvh –force –nodeps *.rpm 安装rpm -ivh foo-1.0-l.i386.rpm 查看进程内存cat /proc/2913/statustop resps aux | sort -k4,4nr | head -n 10 查看内存占用前10名的程序 linux cache不能被释放发现Linux系统内存中的cache并不是在所有情况下都能被释放当做空闲空间用的。即使可以释放cache，也并不是对系统来说没有成本的。总结一下要点，我们应该记得这样几点： 当cache作为文件缓存被释放的时候会引发IO变高，这是cache加快文件访问速度所要付出的成本。 tmpfs中存储的文件会占用cache空间，除非文件删除否则这个cache不会被自动释放。 使用shmget方式申请的共享内存会占用cache空间，除非共享内存被ipcrm或者使用shmctl去IPC_RMID，否则相关的cache空间都不会被自动释放。 使用mmap方法申请的MAP_SHARED标志的内存会占用cache空间，除非进程将这段内存munmap，否则相关的cache空间都不会被自动释放。实际上shmget、mmap的共享内存，在内核层都是通过tmpfs实现的，tmpfs实现的存储用的都是cache。 僵尸进程ps -A |grep defunctps -ef |grep defunct |morekill -9 shell12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182set -e#source_hostname=$1source_hostname=`grep MASTER_NODE= /etc/ansible/hosts |cut -d &#x27;=&#x27; -f 2`echo &quot;source_hostname is $source_hostname&quot;#dest_hostname=$2dest_hostname=`hostname`echo &quot;dest_hostname is $dest_hostname&quot;#source_ip=$3source_ip=`grep WEB_IP= /etc/ansible/hosts |cut -d &#x27;=&#x27; -f 2`echo &quot;source_ip is $source_ip&quot;#dest_ip=$4dest_ip=`grep IPADDR= /etc/sysconfig/network-scripts/ifcfg-ens192 |cut -d &#x27;=&#x27; -f 2`echo &quot;dest_ip is $dest_ipecho &quot;开始设置宿主机名字&quot;hostnamectl set-hostname $dest_hostnameecho &quot;设置名字完成&quot;array=(/home/zshield/docker/compose /home/zshield/conf /home/zshield/web /home/zshield/zs_power /home/zshield/zy /home/zshield/blind /home/zshield/init_compose /home/zshield/init_mysql /home/zshield/nginx /home/zshield/task_scheduler /home/zshield/web /home/zshield/zs_release)for element in $&#123;array[*]&#125;doecho &quot;开始处理目录：$element&quot;sed -i &quot;s/$source_ip/$dest_ip/g&quot; `grep $source_ip -rl $element`sed -i &quot;s/$source_hostname/$dest_hostname/g&quot; `grep $source_hostname -rl $element`donefile_array=(/etc/sysconfig/network-scripts/ifcfg-ens192 /etc/ansible/facts.d/zshield.fact /etc/ansible/hosts /etc/hosts)for file in $&#123;file_array[*]&#125;doecho &quot;开始处理文件：$file&quot;sed -i &quot;s/$source_ip/$dest_ip/g&quot; $filesed -i &quot;s/$source_hostname/$dest_hostname/g&quot; $filedoneecho &quot;启动docker 并等待30s&quot;systemctl start dockersleep 30echo &quot;等待es启动&quot;real_es_result=&quot;red&quot;es_finish=0while [ $es_finish -eq 0 ]do echo &quot;未完成，等待10s再次检测,$es_finish&quot; sleep 2 es_result_tmp=$(curl zshield:Zx123456_shining10@localhost:19200/_cat/health) es_result=$(echo $&#123;es_result_tmp&#125;) echo &quot;$es_result_tmp&quot; if [[ $es_result =~ $real_es_result ]] then es_finish=0 sleep 10 else es_finish=1 fi echo &quot;本次检查结果：$es_finish&quot;doneecho &quot;完成es检查，es_finish is $es_finish&quot;echo &quot;准备进入mysql 执行sql&quot;docker exec -i mysql bash &lt;&lt;EOFmysql -u root -p&#x27;Zx123456@shining11&#x27; -P 13306 -h 127.0.0.1use walle;update servers set name = &#x27;node81109&#x27; , host = &#x27;192.168.81.109&#x27; where name = &#x27;node81104&#x27;;EOFecho &quot;完成&quot;./zx.sh node81104 node81109 192.168.81.104 192.168.81.109 &gt; f.log 2&gt;&amp;1echo &quot;准备进入mysql 执行sql&quot;result_path=&quot;/var/lib/mysql-files/new_server_message.csv&quot;docker exec -i mysql bash &lt;&lt;EOFmysql -u root -p&#x27;Zx123456@shining11&#x27; -P 13306 -h 127.0.0.1use walle;select p.name,s.name from projects as p,servers as s where instr(p.server_ids,s.id) into outfile &quot;$result_path&quot;; # 模糊查询EOFdocker cp mysql:/var/lib/mysql-files/new_server_message.csv /tmp/new_server_message.csvecho &quot;完成&quot; lvm操作1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980811.查看磁盘ls /dev/sd*2.创建pvpvcreate /dev/sdb /dev/sdc | pvremove /dev/sdb3.创建vgvgcreate xfgvg1 /dev/sdb /dev/sdc | vgreduce xfgvg1 /dev/sdb4.扩容vgextend xfgvg1 /dev/sdd5.创建lvlvcreate -n lv1 -L +500M xfgvg1lvcreate -n lv2 -L 195G xfgvg16.格式化文件系统mkfs.xfs /dev/xfgvg1/lv1mkfs.xfs /dev/xfgvg1/lv27.创建目录用于挂载mkdir /xfs_lv1mkdir /xfs_lv28.挂载mount /dev/xfgvg1/lv1 /xfs_lv1/mount /dev/xfgvg1/lv2 /xfs_lv2/umount /xfs_lv1umount /xfs_lv29.写入测试数据echo &#123;1..10000000&#125; &gt; /xfs_lv1/test1.txt扩容lv1.umount /xfs_lv1/2.lvextend -L +4.5G /dev/xfgvg1/lv13.mount /dev/xfgvg1/lv1 /xfs_lv1/清理大的pv中的PE 将小的pv迁移到大的pv上xfs缩容重建第1步：备份lv对应文件系统所有数据 yum install xfsdump -yxfsdump -l 0 -L xfs_all -M xfs_all -f /home/xfs_data.dump /xfs_lv2第2步：备份完成之后，查看一下/var/lib/xfsdump/inventory目录下目前备份的信息状态。session 0代表第一次完整备份，status为SUCCESS 就是备份完成xfsdump -I第3步：取消挂载逻辑卷umount /xfs_lv2第4步：移除逻辑卷lvremove /dev/xfgvg1/lv2第5步：迁移pv数据到指定pvpvmove /dev/sdb /dev/sdc第6步：删除VG中的pvvgreduce xfgvg1 /dev/sdb第6步：重建lvlvcreate -n lv2 -L 95G xfgvg1第7步：格式化mkfs.xfs /dev/xfgvg1/lv2第8步：挂载mount /dev/xfgvg1/lv2 /xfs_lv2/第9步：恢复备份xfsrestore -f /home/xfs_data.dump -L xfs_all /xfs_lv2第10步：查看数据df -Th第11步：删除pvpvremove /dev/sdb第1步：取消挂载逻辑卷umount /xfs_lv2第2步：检查文件系统完整性 fsck -fe2fsck -f /dev/xfgvg1/lv2第3步：缩减文件系统resize2fs /dev/xfgvg1/lv2 90G#缩减文件系统容量为90G第4步：缩减逻辑卷lvreduce -L 90G /dev/xfgvg1/lv2#缩减逻辑卷容量为90G#缩减fs或者lv时都会有警告提示，所以要考虑清楚再行动，做实验就没关系了第5步：迁移pv数据到指定pvpvmove /dev/sdb /dev/sdc第6步：删除VG中的pvvgreduce xfgvg1 /dev/sdb第7步：重新挂载mount /dev/xfgvg1/lv2 /xfs_lv2/第8步：查看数据df -Th第9步：删除pvpvremove /dev/sdb","categories":[],"tags":[]},{"title":"面试","slug":"面试","date":"2021-06-16T10:59:08.000Z","updated":"2021-06-17T13:06:33.155Z","comments":true,"path":"2021/06/16/面试/","link":"","permalink":"https://sk-xinye.github.io/2021/06/16/%E9%9D%A2%E8%AF%95/","excerpt":"","text":"概述什么是RedisRedis(Remote Dictionary Server) 是一个使用 C 语言编写的，开源的（BSD许可）高性能非关系型（NoSQL）的键值对数据库。 Redis 可以存储键和五种不同类型的值之间的映射。键的类型只能为字符串，值支持五种数据类型：字符串、列表、集合、散列表、有序集合。 与传统数据库不同的是 Redis 的数据是存在内存中的，所以读写速度非常快，因此 redis 被广泛应用于缓存方向，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。另外，Redis 也经常用来做分布式锁。除此之外，Redis 支持事务 、持久化、LUA脚本、LRU驱动事件、多种集群方案。 Redis有哪些优缺点优点 读写性能优异， Redis能读的速度是110000次/s，写的速度是81000次/s。 支持数据持久化，支持AOF和RDB两种持久化方式。 支持事务，Redis的所有操作都是原子性的，同时Redis还支持对几个操作合并后的原子性执行。 数据结构丰富，除了支持string类型的value外还支持hash、set、zset、list等数据结构。 支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。 缺点 数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。 Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。 主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。 Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。 为什么要用 Redis /为什么要用缓存主要从“高性能”和“高并发”这两点来看待这个问题 高性能： 假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在数缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！ 高并发： 直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。 为什么要用 Redis 而不用 map/guava 做缓存缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 或者 guava 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。 使用 redis 或 memcached 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 redis 或 memcached服务的高可用，整个程序架构上较为复杂。 Redis为什么这么快 完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是O(1)； 数据结构简单，对数据操作也简单，Redis 中的数据结构是专门进行设计的； 采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗； 使用多路 I/O 复用模型，非阻塞 IO； 使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis 直接自己构建了 VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求； 数据类型Redis有哪些数据类型Redis主要有5种数据类型，包括String，List，Set，Zset，Hash，满足大部分的使用要求 数据类型 可以存储的值 操作 应用场景 STRING 字符串、整数或者浮点数 对整个字符串或者字符串的其中一部分执行操作 做简单的键值对缓存 对整数和浮点数执行自增或者自减操作 LIST 列表 从两端压入或者弹出元素 存储一些列表型的数据结构，类似粉丝列表、文章的评论列表之类的数据 对单个或者多个元素进行修剪， 只保留一个范围内的元素 SET 无序集合 添加、获取、移除单个元素 交集、并集、差集的操作，比如交集，可以把两个人的粉丝列表整一个交集 检查一个元素是否存在于集合中 计算交集、并集、差集 从集合里面随机获取元素 HASH 包含键值对的无序散列表 添加、获取、移除单个键值对 结构化的数据，比如一个对象 获取所有键值对 检查某个键是否存在 ZSET 有序集合 添加、获取、删除元素 去重但可以排序，如获取排名前几名的用户 根据分值范围或者成员来获取元素 计算一个键的排名 Redis的应用场景 计数器 可以对 String 进行自增自减运算，从而实现计数器功能。Redis 这种内存型数据库的读写性能非常高，很适合存储频繁读写的计数量。 缓存 将热点数据放到内存中，设置内存的最大使用量以及淘汰策略来保证缓存的命中率。 会话缓存 可以使用 Redis 来统一存储多台应用服务器的会话信息。当应用服务器不再存储用户的会话信息，也就不再具有状态，一个用户可以请求任意一个应用服务器，从而更容易实现高可用性以及可伸缩性。 全页缓存（FPC） 除基本的会话token之外，Redis还提供很简便的FPC平台。以Magento为例，Magento提供一个插件来使用Redis作为全页缓存后端。此外，对WordPress的用户来说，Pantheon有一个非常好的插件 wp-redis，这个插件能帮助你以最快速度加载你曾浏览过的页面。 查找表 例如 DNS 记录就很适合使用 Redis 进行存储。查找表和缓存类似，也是利用了 Redis 快速的查找特性。但是查找表的内容不能失效，而缓存的内容可以失效，因为缓存不作为可靠的数据来源。 消息队列(发布/订阅功能) List 是一个双向链表，可以通过 lpush 和 rpop 写入和读取消息。不过最好使用 Kafka、RabbitMQ 等消息中间件。 分布式锁实现 在分布式场景下，无法使用单机环境下的锁来对多个节点上的进程进行同步。可以使用 Redis 自带的 SETNX 命令实现分布式锁，除此之外，还可以使用官方提供的 RedLock 分布式锁实现。 Set 可以实现交集、并集等操作，从而实现共同好友等功能。ZSet 可以实现有序性操作，从而实现排行榜等功能。 string——适合最简单的k-v存储，类似于memcached的存储结构，短信验证码，配置信息等，就用这种类型来存储。 hash——一般key为ID或者唯一标示，value对应的就是详情了。如商品详情，个人信息详情，新闻详情等。 list——因为list是有序的，比较适合存储一些有序且数据相对固定的数据。如省市区表、字典表等。因为list是有序的，适合根据写入的时间来排序，如：最新的***，消息队列等。 set——可以简单的理解为ID-List的模式，如微博中一个人有哪些好友，set最牛的地方在于，可以对两个set提供交集、并集、差集操作。例如：查找两个人共同的好友等。 Sorted Set——是set的增强版本，增加了一个score参数，自动会根据score的值进行排序。比较适合类似于top 10等不根据插入的时间来排序的数据。 持久化什么是Redis持久化持久化就是把内存的数据写到磁盘中去，防止服务宕机了内存数据丢失。 Redis 的持久化机制是什么？各自的优缺点RDBRedis 提供两种持久化机制 RDB（默认） 和 AOF 机制: RDB：是Redis DataBase缩写快照 RDB是Redis默认的持久化方式。按照一定的时间将内存的数据以快照的形式保存到硬盘中，对应产生的数据文件为dump.rdb。通过配置文件中的save参数来定义快照的周期。 优点： 只有一个文件 dump.rdb，方便持久化。 容灾性好，一个文件可以保存到安全的磁盘。 性能最大化，fork 子进程来完成写操作，让主进程继续处理命令，所以是 IO 最大化。使用单独子进程来进行持久化，主进程不会进行任何 IO 操作，保证了 redis 的高性能 相对于数据集大时，比 AOF 的启动效率更高。 缺点： 数据安全性低。RDB 是间隔一段时间进行持久化，如果持久化之间 redis 发生故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候) AOF（Append-only file)持久化方式： 是指所有的命令行记录以 redis 命令请 求协议的格式完全持久化存储)保存为 aof 文件。 AOF：持久化AOF持久化(即Append Only File持久化)，则是将Redis执行的每次写命令记录到单独的日志文件中，当重启Redis会重新将持久化的日志中文件恢复数据。 当两种方式同时开启时，数据恢复Redis会优先选择AOF恢复。 优点： 数据安全，aof 持久化可以配置 appendfsync 属性，有 always，每进行一次 命令操作就记录到 aof 文件中一次。 通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题。 AOF 机制的 rewrite 模式。AOF 文件没被 rewrite 之前（文件过大时会对命令 进行合并重写），可以删除其中的某些命令（比如误操作的 flushall）) 缺点： AOF 文件比 RDB 文件大，且恢复速度慢。 数据集大的时候，比 rdb 启动效率低。 优缺点是什么？ AOF文件比RDB更新频率高，优先使用AOF还原数据。 AOF比RDB更安全也更大 RDB性能比AOF好 如果两个都配了优先加载AOF 如何选择合适的持久化方式 一般来说， 如果想达到足以媲美PostgreSQL的数据安全性，你应该同时使用两种持久化功能。在这种情况下，当 Redis 重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整。 如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失，那么你可以只使用RDB持久化。 有很多用户都只使用AOF持久化，但并不推荐这种方式，因为定时生成RDB快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比AOF恢复的速度要快，除此之外，使用RDB还可以避免AOF程序的bug。 如果你只希望你的数据在服务器运行的时候存在，你也可以不使用任何持久化方式。 Redis持久化数据和缓存怎么做扩容 如果Redis被当做缓存使用，使用一致性哈希实现动态扩容缩容。 如果Redis被当做一个持久化存储使用，必须使用固定的keys-to-nodes映射关系，节点的数量一旦确定不能变化。否则的话(即Redis节点需要动态变化的情况），必须使用可以在运行时进行数据再平衡的一套系统，而当前只有Redis集群可以做到这样。 过期键的删除策略Redis的过期键的删除策略我们都知道，Redis是key-value数据库，我们可以设置Redis中缓存的key的过期时间。Redis的过期策略就是指当Redis中缓存的key过期了，Redis如何处理。 过期策略通常有以下三种： 定时过期：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。 惰性过期：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。 定期过期：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。(expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。) Redis中同时使用了惰性过期和定期过期两种过期策略。 Redis key的过期时间和永久有效分别怎么设置EXPIRE和PERSIST命令。 我们知道通过expire来设置key 的过期时间，那么对过期的数据怎么处理呢除了缓存服务器自带的缓存失效策略之外（Redis默认的有6中策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种： 定时去清理过期的缓存(定期过期)； 当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存（惰性过期）。 不进行数据淘汰的策略，只有 noeviction（默认） 这一种。 在设置了过期时间的数据中进行淘汰，包括 volatile-random、volatile-ttl、volatile-lru、volatile-lfu（Redis 4.0 后新增）四种。 在所有数据范围内进行淘汰，包括 allkeys-lru、allkeys-random、allkeys-lfu（Redis 4.0 后新增）三种。 内存相关Redis的内存用完了会发生什么如果达到设置的上限，Redis的写命令会返回错误信息（但是读命令还可以正常返回。）或者你可以配置内存淘汰机制，当Redis达到内存上限时会冲刷掉旧的内容。 Redis如何做内存优化可以好好利用Hash,list,sorted set,set等集合类型数据，因为通常情况下很多小的Key-Value可以用更紧凑的方式存放到一起。尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。比如你的web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key，而是应该把这个用户的所有信息存储到一张散列表里面 线程模型Redis基于Reactor模式开发了网络事件处理器，这个处理器被称为文件事件处理器（file event handler）。它的组成结构为4部分：多个套接字、IO多路复用程序、文件事件分派器、事件处理器。因为文件事件分派器队列的消费是单线程的，所以Redis才叫单线程模型。 文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字， 并根据套接字目前执行的任务来为套接字关联不同的事件处理器。 当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时， 与操作相对应的文件事件就会产生， 这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。 虽然文件事件处理器以单线程方式运行， 但通过使用 I/O 多路复用程序来监听多个套接字， 文件事件处理器既实现了高性能的网络通信模型， 又可以很好地与 redis 服务器中其他同样以单线程方式运行的模块进行对接， 这保持了 Redis 内部单线程设计的简单性。 事务Redis事务的概念Redis 事务的本质是通过MULTI、EXEC、WATCH等一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。 总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。 redis事务的三个阶段 事务开始 MULTI 命令入队 事务执行 EXEC 事务执行过程中，如果服务端收到有EXEC、DISCARD、WATCH、MULTI之外的请求，将会把请求放入队列中排队 Redis事务相关命令Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的 Redis会将一个事务中的所有命令序列化，然后按顺序执行。 redis 不支持回滚，“Redis 在事务失败时不进行回滚，而是继续执行余下的命令”， 所以 Redis 的内部可以保持简单且快速。 如果在一个事务中的命令出现错误，那么所有的命令都不会执行； 如果在一个事务中出现运行错误，那么正确的命令会被执行。 WATCH 命令是一个乐观锁，可以为 Redis 事务提供 check-and-set （CAS）行为。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令。 MULTI命令用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。 EXEC：执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排列。 当操作被打断时，返回空值 nil 。 通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出。 UNWATCH命令可以取消watch对所有key的监控。 事务管理（ACID）概述Redis的事务总是具有ACID中的一致性和隔离性，其他特性是不支持的。当服务器运行在AOF持久化模式下，并且appendfsync选项的值为always时，事务也具有耐久性。 Redis事务支持隔离性吗Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，Redis 的事务是总是带有隔离性的。 Redis事务保证原子性吗，支持回滚吗Redis中，单条命令是原子性执行的，但事务不保证原子性，且没有回滚。事务中任意命令执行失败，其余的命令仍会被执行。 Redis事务其他实现 基于Lua脚本，Redis可以保证脚本内的命令一次性、按顺序地执行，其同时也不提供事务运行错误的回滚，执行过程中如果部分命令运行错误，剩下的命令还是会继续运行完 基于中间标记变量，通过另外的标记变量来标识事务是否执行完成，读取数据时先读取该标记变量判断是否事务执行完成。但这样会需要额外写代码实现，比较繁琐 集群方案哨兵模式 哨兵的介绍sentinel，中文名是哨兵。哨兵是 redis 集群机构中非常重要的一个组件，主要有以下功能： 集群监控：负责监控 redis master 和 slave 进程是否正常工作。 消息通知：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。 哨兵用于实现 redis 集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作 故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了。 哨兵的核心知识 哨兵至少需要 3 个实例，来保证自己的健壮性。 哨兵 + redis 主从的部署架构，是不保证数据零丢失的，只能保证 redis 集群的高可用性。 对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。 官方Redis Cluster 方案(服务端路由查询) redis 集群模式的工作原理能说一下么？在集群模式下，redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 hash 算法吗？ 简介Redis Cluster是一种服务端Sharding技术，3.0版本开始正式提供。Redis Cluster并没有使用一致性hash，而是采用slot(槽)的概念，一共分成16384个槽。将请求发送到任意节点，接收到请求的节点会将查询请求发送到正确的节点上执行 方案说明 通过哈希的方式，将数据分片，每个节点均分存储一定哈希槽(哈希值)区间的数据，默认分配了16384 个槽位 每份数据分片会存储在多个互为主从的多节点上 数据写入先写主节点，再同步到从节点(支持配置为阻塞同步) 同一分片多个节点间的数据不保持一致性 读取数据时，当客户端操作的key没有分配在该节点上时，redis会返回转向指令，指向正确的节点 扩容时时需要需要把旧节点的数据迁移一部分到新节点 在 redis cluster 架构下，每个 redis 要放开两个端口号，比如一个是 6379，另外一个就是 加1w 的端口号，比如 16379。 16379 端口号是用来进行节点间通信的，也就是 cluster bus 的东西，cluster bus 的通信，用来进行故障检测、配置更新、故障转移授权。cluster bus 用了另外一种二进制的协议，gossip 协议，用于节点间进行高效的数据交换，占用更少的网络带宽和处理时间。 节点间的内部通信机制基本通信原理 集群元数据的维护有两种方式：集中式、Gossip 协议。redis cluster 节点间采用 gossip 协议进行通信。 分布式寻址算法 hash 算法（大量缓存重建） 直接用hash 函数进行映射 当系统需要对服务器进行扩容时，那么整个已经保存到服务器中的数据都得重新进行hash碰撞，当系统具有海量数据时将会是场灾难。 一致性 hash 算法（自动缓存迁移）+ 虚拟节点（自动负载均衡） 一致性哈希将整个哈希值空间组织成一个虚拟的圆环，在集群服务器确定以后，将各个服务器使用Hash函数进行一个哈希计算，哈希计算可以选择服务器的ip地址或者主机名等关键字进行哈希，这样就完成了节点在hash环上的位置分配， 每台服务器在hash环上的位置分配完成后，在客户端对缓存key进行同样函数的hash运算，得出hash值，同样得到环上的一个位置，从这个位置顺时针找到最近的一个服务器节点，比如遍历所有节点位置和key位置差值取最小值，这样就完成了路由。 为了避免节点分布不均匀情况，添加虚拟节点。具体操作就是将一台服务器加上编号尾缀进行哈希，每台服务器就会有多个结果，简单来说就是将一台服务器映射成不同的多个hash值。 redis cluster 的 hash slot 算法 记录和物理机之间引入了虚拟桶层，记录通过hash函数映射到虚拟桶，记录和虚拟桶是多对一的关系 第二层是虚拟桶和物理机之间的映射，同样也是多对一的关系，即一个物理机对应多个虚拟桶，这个层关系是通过内存表实现的 如果用户要从集群中移除节点 A ， 那么集群只需要将节点 A 中的所有哈希槽移动到节点 B 和节点 C ， 然后再移除空白（不包含任何哈希槽）的节点 A 就可以了。 因为将一个哈希槽从一个节点移动到另一个节点不会造成节点阻塞， 所以无论是添加新节点还是移除已存在节点， 又或者改变某个节点包含的哈希槽数量， 都不会造成集群下线。 https://www.jianshu.com/p/6b9ce31c0351 一致性哈希算法 https://blog.csdn.net/baidu_36161424/article/details/104945388 优点 无中心架构，支持动态扩容，对业务透明 具备Sentinel的监控和自动Failover(故障转移)能力 客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可 高性能，客户端直连redis服务，免去了proxy代理的损耗 缺点 运维也很复杂，数据迁移需要人工干预 只能使用0号数据库 不支持批量操作(pipeline管道操作) 分布式逻辑和存储模块耦合等 基于客户端分配 Redis Sharding是Redis Cluster出来之前，业界普遍使用的多Redis实例集群方法。其主要思想是采用哈希算法将Redis数据的key进行散列，通过hash函数，特定的key会映射到特定的Redis节点上。Java redis客户端驱动jedis，支持Redis Sharding功能，即ShardedJedis以及结合缓存池的ShardedJedisPool 优点 优势在于非常简单，服务端的Redis实例彼此独立，相互无关联，每个Redis实例像单服务器一样运行，非常容易线性扩展，系统的灵活性很强 缺点 由于sharding处理放到客户端，规模进一步扩大时给运维带来挑战。 客户端sharding不支持动态增删节点。服务端Redis实例群拓扑结构有变化时，每个客户端都需要更新调整。连接不能共享，当应用规模增大时，资源浪费制约优化 基于代理服务器分片 客户端发送请求到一个代理组件，代理解析客户端的数据，并将请求转发至正确的节点，最后将结果回复给客户端 特征 透明接入，业务程序不用关心后端Redis实例，切换成本低 Proxy 的逻辑和存储的逻辑是隔离的 代理层多了一次转发，性能有所损耗 业界开源方案 Twtter开源的Twemproxy 豌豆荚开源的Codis Redis 主从架构见数据同步 生产环境中的 redis 是怎么部署的edis cluster，10 台机器，5 台机器部署了 redis 主实例，另外 5 台机器部署了 redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰qps可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求/s。 机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 redis 进程的是10g内存，一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。 5 台机器对外提供读写，一共有 50g 内存。 因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，redis 从实例会自动变成主实例继续提供读写服务。 你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。 其实大型的公司，会有基础架构的 team 负责缓存集群的运维。 说说Redis哈希槽的概念Redis集群没有使用一致性hash,而是引入了哈希槽的概念，Redis集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽。 Redis集群会有写操作丢失吗？为什么Redis并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作。 Redis集群之间是如何复制的异步复制 Redis集群最大节点个数是多少16384个 Redis集群如何选择数据库Redis集群目前无法做数据库选择，默认在0数据库。 分布式问题见分布式锁 缓存异常见缓存","categories":[],"tags":[]},{"title":"面试","slug":"面试","date":"2021-06-15T13:41:30.000Z","updated":"2021-06-16T13:14:06.308Z","comments":true,"path":"2021/06/15/面试/","link":"","permalink":"https://sk-xinye.github.io/2021/06/15/%E9%9D%A2%E8%AF%95/","excerpt":"","text":"https://juejin.cn/post/6868270408534720525https://blog.csdn.net/ThinkWon/article/details/104778621 能说下myisam 和 innodb的区别吗myisam引擎是5.1版本之前的默认引擎，支持全文检索、压缩、空间函数等，但是不支持事务和行级锁，所以一般用于有大量查询少量插入的场景来使用，而且myisam不支持外键，并且索引和数据是分开存储的。innodb是基于聚簇索引建立的，和myisam相反它支持事务、外键，并且通过MVCC来支持高并发，索引和数据存储在一起。 说下mysql的索引有哪些吧，索引的数据结构，聚簇和非聚簇索引又是什么索引类别 主键索引: 数据列不允许重复，不允许为NULL，一个表只能有一个主键。 唯一索引: 数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。 可以通过 ALTER TABLE table_name ADD UNIQUE (column); 创建唯一索引 可以通过 ALTER TABLE table_name ADD UNIQUE (column1,column2); 创建唯一组合索引 普通索引: 基本的索引类型，没有唯一性的限制，允许为NULL值。 可以通过ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引 可以通过ALTER TABLE table_name ADD INDEX index_name(column1, column2, column3);创建组合索引 全文索引： 是目前搜索引擎使用的一种关键技术。 可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引 索引数据结构索引按照数据结构来说主要包含B+树和Hash索引。 B+树是左小右大的顺序存储结构，节点只包含id索引列，而叶子节点包含索引列和数据，这种数据和索引在一起存储的索引方式叫做聚簇索引，一张表只能有一个聚簇索引。假设没有定义主键，InnoDB会选择一个唯一的非空索引代替，如果没有的话则会隐式定义一个主键作为聚簇索引。将数据存储与索引放到了一块，找到索引也就找到了数据 非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因 索引覆盖，回表覆盖索引指的是在一次查询中，如果一个索引包含或者说覆盖所有需要查询的字段的值，我们就称之为覆盖索引，而不再需要回表查询。 而要确定一个查询是否是覆盖索引，我们只需要explain sql语句看Extra的结果是否是“Using index”即可。 锁的类型有哪些呢从数据库角度，线程是否能共享同一把锁（共享，排他）mysql锁分为共享锁和排他锁，也叫做读锁和写锁。 读锁是共享的，可以通过lock in share mode实现，这时候只能读不能写。 写锁是排他的，它会阻塞其他的写锁和读锁。从颗粒度来区分，可以分为表锁和行锁两种。 行锁又可以分为乐观锁和悲观锁，悲观锁可以通过for update实现，乐观锁则通过版本号实现。 你能说下事务的基本特性和隔离级别事务基本特性ACID 读未提交 读提交 可重复读 串行化 那ACID靠什么保证的 A原子性由undo log日志保证，它记录了需要回滚的日志信息，事务回滚时撤销已经执行成功的sql C一致性一般由代码层面来保证 I隔离性由MVCC来保证 D持久性由内存+redo log来保证，mysql修改数据同时在内存和redo log记录这次操作，事务提交的时候通过redo log刷盘，宕机的时候可以从redo log恢复 那你说说什么是幻读，什么是MVCC 幻读 在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。 MVCC 在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。 在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。 而每行数据也都是有多个版本的。数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。 说说mysql主从同步怎么做的首先先了解mysql主从同步的原理 master提交完事务后，写入binlog slave连接到master，获取binlog master创建dump线程，推送binglog到slave slave启动一个IO线程读取同步过来的master的binlog，记录到relay log中继日志中 slave再开启一个sql线程读取relay log事件并在slave执行，完成同步 slave记录自己的binglog 全同步复制 主库写入binlog后强制同步日志到从库，所有的从库都执行完成后才返回给客户端，但是很显然这个方式的话性能会受到严重影响。半同步复制 和全同步不同的是，半同步复制的逻辑是这样，从库写入日志成功后返回ACK确认给主库，主库收到至少一个从库的确认就认为写操作完成。 数据库基础知识为什么要使用数据库数据保存在数据库 数据永久保存 使用SQL语句，查询方便效率高。 管理数据方便 什么是SQL结构化查询语言(Structured Query Language)简称SQL，是一种数据库查询语言。 作用：用于存取数据、查询、更新和管理关系数据库系统。 数据库三大范式是什么 第一范式：每个列都不可以再拆分。 第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部- 分。 第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。 在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。 MySQL的binlog有有几种录入格式？分别有什么区别有三种格式，statement，row和mixed。 statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。 row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。 mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。 此外，新版的MySQL中对row级别也做了一些优化，当表结构发生变化的时候，会记录语句而不是逐行记录。 引擎MySQL存储引擎MyISAM与InnoDB区别 Innodb引擎：Innodb引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是处理大数据容量的数据库系统。 MyIASM引擎(原本Mysql的默认引擎)：不提供事务的支持，也不支持行级锁和外键。 MEMORY引擎：所有的数据都在内存中，数据的处理速度快，但是安全性不高。 存储引擎选择如果没有特别的需求，使用默认的Innodb即可。 MyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。 Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键。比如OA自动化办公系统。 索引什么是索引 索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。 索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。 更通俗的说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。索引是一个文件，它是要占据物理空间的。 索引有哪些优缺点索引的优点 可以大大加快数据的检索速度，这也是创建索引的最主要的原因。 通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。 索引的缺点 时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率； 空间方面：索引需要占物理空间。 索引使用场景 where 查询时，加速查询 order by 省去了排序的苦恼 join 对join语句匹配关系（on）涉及的字段建立索引能够提高效率 索引有哪几种类型 主键索引: 数据列不允许重复，不允许为NULL，一个表只能有一个主键。 唯一索引: 数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。 可以通过 ALTER TABLE table_name ADD UNIQUE (column); 创建唯一索引 可以通过 ALTER TABLE table_name ADD UNIQUE (column1,column2); 创建唯一组合索引 普通索引: 基本的索引类型，没有唯一性的限制，允许为NULL值。 可以通过ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引 可以通过ALTER TABLE table_name ADD INDEX index_name(column1, column2, column3);创建组合索引 全文索引： 是目前搜索引擎使用的一种关键技术。 可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引 索引数据结构1索引按照数据结构来说主要包含B+树和Hash索引。 B+树是左小右大的顺序存储结构，节点只包含id索引列，而叶子节点包含索引列和数据，这种数据和索引在一起存储的索引方式叫做聚簇索引，一张表只能有一个聚簇索引。假设没有定义主键，InnoDB会选择一个唯一的非空索引代替，如果没有的话则会隐式定义一个主键作为聚簇索引。将数据存储与索引放到了一块，找到索引也就找到了数据 非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因 索引的基本原理索引用来快速地寻找那些具有特定值的记录。如果没有索引，一般来说执行查询时遍历整张表。 索引的原理很简单，就是把无序的数据变成有序的查询 把创建了索引的列的内容进行排序 对排序结果生成倒排表 在倒排表内容上拼上数据地址链 在查询的时候，先拿到倒排表内容，再取出数据地址链，从而拿到具体数据 索引设计的原则 适合索引的列是出现在where子句中的列，或者连接子句中指定的列 基数较小的类，索引效果较差，没有必要在此列建立索引 使用短索引，如果对长字符串列进行索引，应该指定一个前缀长度，这样能够节省大量索引空间 不要过度索引。索引需要额外的磁盘空间，并降低写操作的性能。在修改表内容的时候，索引会进行更新甚至重构，索引列越多，这个时间就会越长。所以只保持需要的索引有利于查询即可。 创建索引的原则（重中之重）索引虽好，但也不是无限制的使用，最好符合一下几个原则 最左前缀匹配原则，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 较频繁作为查询条件的字段才去创建索引 更新频繁字段不适合创建索引 若是不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低) 尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。 定义有外键的数据列一定要建立索引。 对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。 对于定义为text、image和bit的数据类型的列不要建立索引。 创建索引的三种方式，删除索引第一种方式：在执行CREATE TABLE时创建索引12345678910CREATE TABLE user_index2 ( id INT auto_increment PRIMARY KEY, first_name VARCHAR (16), last_name VARCHAR (16), id_card VARCHAR (18), information text, KEY name (first_name, last_name), FULLTEXT KEY (information), UNIQUE KEY (id_card)); 第二种方式：使用ALTER TABLE命令去增加索引1ALTER TABLE table_name ADD INDEX index_name (column_list); 第三种方式：使用CREATE INDEX命令创建1CREATE INDEX index_name ON table_name (column_list); 删除索引123alter table user_index drop KEY name;alter table user_index drop KEY id_card;alter table user_index drop KEY information; 百万级别或以上的数据如何删除关于索引：由于索引需要额外的维护成本，因为索引文件是单独存在的文件,所以当我们对数据的增加,修改,删除,都会产生额外的对索引文件的操作,这些操作需要消耗额外的IO,会降低增/改/删的执行效率。所以，在我们删除数据库百万级别数据的时候，查询MySQL官方手册得知删除数据的速度和创建的索引数量是成正比的。 所以我们想要删除百万数据的时候可以先删除索引（此时大概耗时三分多钟） 然后删除其中无用数据（此过程需要不到两分钟） 删除完成后重新创建索引(此时数据较少了)创建索引也非常快，约十分钟左右。 与之前的直接删除绝对是要快速很多，更别说万一删除中断,一切删除会回滚。那更是坑了。 前缀索引 语法：index(field(10))，使用字段值的前10个字符建立索引，默认是使用字段的全部内容建立索引。 前提：前缀的标识度高。比如密码就适合建立前缀索引，因为密码几乎各不相同。 实操的难度：在于前缀截取的长度。 我们可以利用select count(*)/count(distinct left(password,prefixLen));，通过从调整prefixLen的值（从1自增）查看不同前缀长度的一个平均匹配度，接近1时就可以了（表示一个密码的前prefixLen个字符几乎能确定唯一一条记录） 什么是最左前缀原则？什么是最左匹配原则 顾名思义，就是最左优先，在创建多列索引时，要根据业务需求，where子句中使用最频繁的一列放在最左边。 最左前缀匹配原则，非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。 =和in可以乱序，比如a = 1 and b = 2 and c = 3 建立(a,b,c)索引可以任意顺序，mysql的查询优化器会帮你优化成索引可以识别的形式 B树和B+树的区别使用B树的好处B树可以在内部节点同时存储键和值，因此，把频繁访问的数据放在靠近根节点的地方将会大大提高热点数据的查询效率。这种特性使得B树在特定数据重复多次查询的场景中更加高效。 使用B+树的好处由于B+树的内部节点只存放键，不存放值，因此，一次读取，可以在内存页中获取更多的键，有利于更快地缩小查找范围。 B+树的叶节点由一条链相连，因此，当需要进行一次全数据遍历的时候，B+树只需要使用O(logN)时间找到最小的一个节点，然后通过链进行O(N)的顺序遍历即可。而B树则需要对树的每一层进行遍历，这会需要更多的内存置换次数，因此也就需要花费更多的时间 数据库为什么使用B+树而不是B树 B树只适合随机检索，而B+树同时支持随机检索和顺序检索； B+树空间利用率更高，可减少I/O次数，磁盘读写代价更低。一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗。B+树的内部结点并没有指向关键字具体信息的指针，只是作为索引使用，其内部结点比B树小，盘块能容纳的结点中关键字数量更多，一次性读入内存中可以查找的关键字也就越多，相对的，IO读写次数也就降低了。而IO读写次数是影响索引检索效率的最大因素； B+树的查询效率更加稳定。B树搜索有可能会在非叶子结点结束，越靠近根节点的记录查找时间越短，只要找到关键字即可确定记录的存在，其性能等价于在关键字全集内做一次二分查找。而在B+树中，顺序检索比较明显，随机检索时，任何关键字的查找都必须走一条从根节点到叶节点的路，所有关键字的查找路径长度相同，导致每一个关键字的查询效率相当。 B-树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。B+树的叶子节点使用指针顺序连接在一起，只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作。 增删文件（节点）时，效率更高。因为B+树的叶子节点包含所有关键字，并以有序的链表结构存储，这样可很好提高增删效率。 什么是聚簇索引？何时使用聚簇索引与非聚簇索引 聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据 非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因 非聚簇索引一定会回表查询吗不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。 联合索引是什么？为什么需要注意联合索引中的顺序MySQL可以使用多个字段同时建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。 具体原因为: MySQL使用索引时需要索引有序，假设现在建立了”name，age，school”的联合索引，那么索引的排序为: 先按照name排序，如果name相同，则按照age排序，如果age的值也相等，则按照school进行排序。 当进行查询时，此时索引仅仅按照name严格有序，因此必须首先使用name字段进行等值查询，之后对于匹配到的列而言，其按照age字段严格有序，此时可以使用age字段用做索引查找，以此类推。因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。 事务什么是数据库事务事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。 事务最经典也经常被拿出来说例子就是转账了。 事物的四大特性(ACID)介绍一下 原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； 一致性： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的； 隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； 持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 什么是脏读？幻读？不可重复读 脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。 不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。 幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。 什么是事务的隔离级别？MySQL的默认隔离级别是什么为了达到事务的四大特性，数据库定义了4种不同的事务隔离级别，由低到高依次为Read uncommitted、Read committed、Repeatable read、Serializable，这四个级别可以逐个解决脏读、不可重复读、幻读这几类问题。 READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。 READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。 REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。 锁隔离级别与锁的关系 在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突 在Read Committed级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁； 在Repeatable Read级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。 SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。 按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法在关系型数据库中，可以按照锁的粒度把数据库锁分为行级锁(INNODB引擎)、表级锁(MYISAM引擎)和页级锁(BDB引擎 )。 MyISAM和InnoDB存储引擎使用的锁： MyISAM采用表级锁(table-level locking)。 InnoDB支持行级锁(row-level locking)和表级锁，默认为行级锁 行级锁，表级锁和页级锁对比 行级锁 行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。 特点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。 表级锁 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。 特点：开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。 页级锁 页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。 特点：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 从锁的类别上分MySQL都有哪些锁呢？像上面那样子进行锁定岂不是有点阻碍并发效率了从锁的类别上来讲，有共享锁和排他锁。 共享锁: 又叫做读锁。 当用户要进行数据的读取时，对数据加上共享锁。共享锁可以同时加上多个。 排他锁: 又叫做写锁。 当用户要进行数据的写入时，对数据加上排他锁。排他锁只可以加一个，他和其他的排他锁，共享锁都相斥。 MySQL中InnoDB引擎的行锁是怎么实现的 答：InnoDB是基于索引来完成行锁 例: select * from tab_with_index where id = 1 for update; for update 可以根据条件来完成行锁锁定，并且 id 是有索引键的列，如果 id 不是索引键那么InnoDB将完成表锁，并发将无从谈起 InnoDB存储引擎的锁的算法有三种Record lock：单个行记录上的锁Gap lock：间隙锁，锁定一个范围，不包括记录本身Next-key lock：record+gap 锁定一个范围，包含记录本身 相关知识点： innodb对于行的查询使用next-key lock Next-locking keying为了解决Phantom Problem幻读问题 当查询的索引含有唯一属性时，将next-key lock降级为record key Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生 有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1 什么是死锁？怎么解决死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。 常见的解决死锁的方法 如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率； 如果业务处理不好可以用分布式事务锁或者使用乐观锁 数据库的乐观锁和悲观锁是什么？怎么实现的数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过version的方式来进行锁定。实现方式：乐一般会使用版本号机制或CAS算法实现。 两种锁的使用场景 从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。 但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。 常用SQL语句超键、候选键、主键、外键分别是什么 超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。 候选键：是最小超键，即没有冗余元素的超键。 主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。 外键：在一个表中存在的另一个表的主键称此表的外键。 SQL 约束有哪几种 NOT NULL: 用于控制字段的内容一定不能为空（NULL）。 UNIQUE: 控件字段内容不能重复，一个表允许有多个 Unique 约束。 PRIMARY KEY: 也是用于控件字段内容不能重复，但它在一个表只允许出现一个。 FOREIGN KEY: 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。 CHECK: 用于控制字段的值范围。 六种关联查询 交叉连接（CROSS JOIN） select r.,s. from r,s 笛卡尔积 内连接（INNER JOIN） 等值连接：ON A.id=B.id 不等值连接：ON A.id &gt; B.id 自连接：SELECT * FROM A T1 INNER JOIN A T2 ON T1.id=T2.pid select r.,s. from r inner join s on r.c=s.c 外连接（LEFT JOIN/RIGHT JOIN） 左外连接：LEFT OUTER JOIN, 以左表为主，先查询出左表，按照ON后的关联条件匹配右表，没有匹配到的用NULL填充，可以简写成LEFT JOIN 右外连接：RIGHT OUTER JOIN, 以右表为主，先查询出右表，按照ON后的关联条件匹配左表，没有匹配到的用NULL填充，可以简写成RIGHT JOIN 联合查询（UNION与UNION ALL） 就是把多个结果集集中在一起，UNION前的结果为基准，需要注意的是联合查询的列数要相等，相同的记录行会合并 如果使用UNION ALL，不会合并重复的记录行 效率 UNION 高于 UNION ALL 全连接（FULL JOIN） 交叉连接（CROSS JOIN） mysql中 in 和 exists 区别mysql中的in语句是把外表和内表作hash 连接，而exists语句是对外表作loop循环，每次loop循环再对内表进行查询。一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。 如果查询的两个表大小相当，那么用in和exists差别不大。 如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in。 not in 和not exists：如果查询语句使用了not in，那么内外表都进行全表扫描，没有用到索引；而not extsts的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快 varchar与char的区别char的特点 char表示定长字符串，长度是固定的； 如果插入数据的长度小于char的固定长度时，则用空格填充； 因为长度固定，所以存取速度要比varchar快很多，甚至能快50%，但正因为其长度固定，所以会占据多余的空间，是空间换时间的做法； 对于char来说，最多能存放的字符个数为255，和编码无关 varchar的特点 varchar表示可变长字符串，长度是可变的； 插入的数据是多长，就按照多长来存储； varchar在存取方面与char相反，它存取慢，因为长度不固定，但正因如此，不占据多余的空间，是时间换空间的做法； 对于varchar来说，最多能存放的字符个数为65532 总之，结合性能角度（char更快）和节省磁盘空间角度（varchar更小），具体情况还需具体来设计数据库才是妥当的做法 mysql中int(10)和char(10)以及varchar(10)的区别 int(10)的10表示显示的数据的长度，不是存储数据的大小；chart(10)和varchar(10)的10表示存储数据的大小，即表示存储多少个字符。 int(10) 10位的数据长度 9999999999，占32个字节，int型4位 char(10) 10位固定字符串，不足补空格 最多10个字符 varchar(10) 10位可变字符串，不足补空格 最多10个字符 char(10)表示存储定长的10个字符，不足10个就用空格补齐，占用更多的存储空间 varchar(10)表示存储10个变长的字符，存储多少个就是多少个，空格也按一个字符存储，这一点是和char(10)的空格不同的，char(10)的空格表示占位不算一个字符 FLOAT和DOUBLE的区别是什么 FLOAT类型数据可以存储至多8位十进制数，并在内存中占4字节。 DOUBLE类型数据可以存储至多18位十进制数，并在内存中占8字节。 drop、delete与truncate的区别 Delete Truncate Drop 类型 属于DML 属于DDL 属于DDL 回滚 可回滚 不可回滚 不可回滚 删除内容 表结构还在，删除表的全部或者一部分数据行 表结构还在，删除表中的所有数据 从数据库中删除表，所有的数据行，索引和权限也会被删除 删除速度 删除速度慢，需要逐行删除 删除速度快 删除速度最快 因此，在不再需要一张表的时候，用drop；在想删除部分数据行时候，用delete；在保留表而删除所有数据的时候用truncate。 SQL优化如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到?或者说怎么才可以知道这条语句运行很慢的原因MySQL提供了explain命令来查看语句的执行计划。 执行计划包含的信息 id 有一组数字组成。表示一个查询中各个子查询的执行顺序; id相同执行顺序由上至下。 id不同，id值越大优先级越高，越先被执行。 id为null时表示一个结果集，不需要使用它查询，常出现在包含union等查询语句中。 select_type 每个子查询的查询类型，一些常见的查询类型。 id select_type description Drop 1 SIMPLE 不包含任何子查询或union等查询 属于DDL 2 PRIMARY 包含子查询最外层查询就显示为 PRIMARY 不可回滚 3 SUBQUERY 在select或 where字句中包含的查询 从数据库中删除表，所有的数据行，索引和权限也会被删除 4 DERIVED from字句中包含的查询 删除速度最快 5 UNION 出现在union后的查询语句中 6 UNION RESULT 从UNION中获取结果集，例如上文的第三个例子 type(非常重要，可以看到有没有走索引) 访问类型 ALL 扫描全表数据 index 遍历索引 range 索引范围查找 index_subquery 在子查询中使用 ref unique_subquery 在子查询中使用 eq_ref ref_or_null 对Null进行索引的优化的 ref fulltext 使用全文索引 ref 使用非唯一索引查找数据 eq_ref 在join查询中使用PRIMARY KEYorUNIQUE NOT NULL索引关联。 possible_keys 可能使用的索引，注意不一定会使用。查询涉及到的字段上若存在索引，则该索引将被列出来。当该列为 NULL时就要考虑当前的SQL是否需要优化了。 key 显示MySQL在查询中实际使用的索引，若没有使用索引，显示为NULL。 TIPS:查询中若使用了覆盖索引(覆盖索引：索引的数据覆盖了需要查询的所有数据)，则该索引仅出现在key列表中 key_length 索引长度 ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值 rows 返回估算的结果集数目，并不是一个准确的值。 extra 的信息非常丰富，常见的有： Using index 使用覆盖索引 Using where 使用了用where子句来过滤结果集 Using filesort 使用文件排序，使用非索引列进行排序时出现，非常消耗性能，尽量优化。 Using temporary 使用了临时表 sql优化的目标可以参考阿里开发手册 大表数据查询，怎么优化 优化shema、sql语句+索引； 第二加缓存，memcached, redis； 主从复制，读写分离； 垂直拆分，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统； 水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key, 为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表； 慢查询日志于记录执行时间超过某个临界值的SQL日志，用于快速定位慢查询，为我们的优化做参考。 开启慢查询日志 配置项：slow_query_log 可以使用show variables like ‘slov_query_log’查看是否开启，如果状态值为OFF，可以使用set GLOBAL slow_query_log = on来开启，它会在datadir下产生一个xxx-slow.log的文件。 设置临界时间 配置项：long_query_time 查看：show VARIABLES like ‘long_query_time’，单位秒 设置：set long_query_time=0.5 实操时应该从长时间设置到短的时间，即将最慢的SQL优化掉 查看日志，一旦SQL超过了我们设置的临界时间就会被记录到xxx-slow.log中 为什么要尽量设定一个主键主键是数据库确保数据行在整张表唯一性的保障，即使业务上本张表没有主键，也建议添加一个自增长的ID列作为主键。设定了主键之后，在后续的删改查的时候可能更加快速以及确保操作数据范围安全。 主键使用自增ID还是UUID推荐使用自增ID，不要使用UUID。 因为在InnoDB存储引擎中，主键索引是作为聚簇索引存在的，也就是说，主键索引的B+树叶子节点上存储了主键索引以及全部的数据(按照顺序)，如果主键索引是自增ID，那么只需要不断向后排列即可，如果是UUID，由于到来的ID与原来的大小不确定，会造成非常多的数据插入，数据移动，然后导致产生很多的内存碎片，进而造成插入性能的下降。 总之，在数据量大一些的情况下，用自增主键性能会好一些。 关于主键是聚簇索引，如果没有主键，InnoDB会选择一个唯一键来作为聚簇索引，如果没有唯一键，会生成一个隐式的主键。 字段为什么要求定义为not nullnull值会占用更多的字节，且会在程序中造成很多与预期不符的情况。 如果要存储用户的密码散列，应该使用什么字段进行存储密码散列，盐，用户身份证号等固定长度的字符串应该使用char而不是varchar来存储，这样可以节省空间且提高检索效率。 优化查询过程中的数据访问 访问数据太多导致查询性能下降 确定应用程序是否在检索大量超过需要的数据，可能是太多行或列 确认MySQL服务器是否在分析大量不必要的数据行 避免犯如下SQL语句错误 查询不需要的数据。解决办法：使用limit解决 多表关联返回全部列。解决办法：指定列名 总是返回全部列。解决办法：避免使用SELECT * 重复查询相同的数据。解决办法：可以缓存数据，下次直接读取缓存 是否在扫描额外的记录。解决办法： 使用explain进行分析，如果发现查询需要扫描大量的数据，但只返回少数的行，可以通过如下技巧去优化： 使用索引覆盖扫描，把所有的列都放到索引中，这样存储引擎不需要回表获取对应行就可以返回结果。 改变数据库和表的结构，修改数据表范式 重写SQL语句，让优化器可以以更优的方式执行查询。 优化长难的查询语句 一个复杂查询还是多个简单查询 MySQL内部每秒能扫描内存中上百万行数据，相比之下，响应数据给客户端就要慢得多 使用尽可能小的查询是好的，但是有时将一个大的查询分解为多个小的查询是很有必要的。 切分查询 将一个大的查询分为多个小的相同的查询 一次性删除1000万的数据要比一次删除1万，暂停一会的方案更加损耗服务器开销。 分解关联查询，让缓存的效率更高。 执行单个查询可以减少锁的竞争。 在应用层做关联更容易对数据库进行拆分。查询效率会有大幅提升。 较少冗余记录的查询。 优化特定类型的查询语句 count(*)会忽略所有的列，直接统计所有列数，不要使用count(列名) MyISAM中，没有任何where条件的count(*)非常快。 当有where条件时，MyISAM的count统计不一定比其它引擎快。 可以使用explain查询近似值，用近似值替代count(*) 增加汇总表 使用缓存 优化关联查询 确定ON或者USING子句中是否有索引。 确保GROUP BY和ORDER BY只有一个表中的列，这样MySQL才有可能使用索引 优化子查询 用关联查询替代 优化GROUP BY和DISTINCT 这两种查询据可以使用索引来优化，是最有效的优化方法 关联查询中，使用标识列分组的效率更高 如果不需要ORDER BY，进行GROUP BY时加ORDER BY NULL，MySQL不会再进行文件排序。 WITH ROLLUP超级聚合，可以挪到应用程序处理 优化LIMIT分页 LIMIT偏移量大的时候，查询效率较低 可以记录上次查询的最大ID，下次查询时直接根据该ID来查询 优化UNION查询UNION ALL的效率高于UNION 优化WHERE子句解题方法 对于此类考题，先说明如何定位低效SQL语句，然后根据SQL语句可能低效的原因做排查，先从索引着手，如果索引没有问题，考虑以上几个方面，数据访问的问题，长难查询句的问题还是一些特定类型优化的问题，逐一回答。 SQL语句优化的一些方法 对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。 应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描 应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则引擎将放弃使用索引而进行全表扫描。 应尽量避免在 where 子句中使用or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描 123select id from t where num=10 or num=20 -- 可以这样查询：select id from t where num=10 union all select id from t where num=20 in 和 not in 也要慎用，否则会导致全表扫描 123select id from t where num in(1,2,3)-- 对于连续的数值，能用 between 就不要用 in 了：select id from t where num between 1 and 3 下面的查询也将导致全表扫描：select id from t where name like ‘%李%’若要提高效率，可以考虑全文检索 如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。如下面语句将进行全表扫描： 123select id from t where num=@num-- 可以改为强制查询使用索引：select id from t with(index(索引名)) where num=@num 应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描 123select id from t where num/2=100-- 应改为:select id from t where num=100*2 应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描 123select id from t where substring(name,1,3)=’abc’-- name以abc开头的id应改为:select id from t where name like ‘abc%’ 不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引 数据库优化为什么要优化 系统的吞吐量瓶颈往往出现在数据库的访问速度上 随着应用程序的运行，数据库中的数据会越来越多，处理时间会相应变慢 数据是存放在磁盘上的，读写速度无法和内存相比 优化原则：减少系统瓶颈，减少资源占用，增加系统的反应速度。 数据库结构优化一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果。 需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。 将字段很多的表分解成多个表 对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。 因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。 增加中间表 对于需要经常联合查询的表，可以建立中间表以提高查询效率。 通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。 增加冗余字段 设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。 表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。 冗余字段的值在一个表中修改了，就要想办法在其他表中更新，否则就会导致数据不一致的问题。 MySQL数据库cpu飙升到500%的话他怎么处理 当 cpu 飙升到 500%时，先用操作系统命令 top 命令观察是不是 mysqld 占用导致的，如果不是，找出占用高的进程，并进行相关处理。 如果是 mysqld 造成的， show processlist，看看里面跑的 session 情况，是不是有消耗资源的 sql 在运行。找出消耗高的 sql，看看执行计划是否准确， index 是否缺失，或者实在是数据量太大造成。 一般来说，肯定要 kill 掉这些线程(同时观察 cpu 使用率是否下降)，等进行相应的调整(比如说加索引、改 sql、改内存参数)之后，再重新跑这些 SQL。 也有可能是每个 sql 消耗资源并不多，但是突然之间，有大量的 session 连进来导致 cpu 飙升，这种情况就需要跟应用一起来分析为何连接数会激增，再做出相应的调整，比如说限制连接数等 当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下 限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内。； 读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读； 缓存： 使用MySQL的缓存，另外对重量级、更新少的数据可以考虑使用应用级别的缓存 通过分库分表的方式进行优化，主要有垂直分表和水平分表垂直拆分根据数据库里面数据表的相关性进行拆分。 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。 简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。 如下图所示，这样来说大家应该就更容易理解了。 垂直拆分的优点： 可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。 垂直拆分的缺点： 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂； 水平拆分保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。 分库分表后面临的问题 事务支持 分库分表后，就成了分布式事务了。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。 跨库join 只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。 分库分表方案产品 跨节点的count,order by,group by以及聚合函数问题 这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。 数据迁移，容量规划，扩容等问题 来自淘宝综合业务平台团队，它利用对2的倍数取余具有向前兼容的特性（如对4取余得1的数对2取余也是1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了Sharding扩容的难度。 ID问题 一旦数据库被切分到多个物理结点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的ID无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得ID,以便进行SQL路由. 一些常见的主键生成策略 MySQL的复制原理以及流程主从复制：将主数据库中的DDL和DML操作通过二进制日志（BINLOG）传输到从数据库上，然后将这些日志重新执行（重做）；从而使得从数据库的数据与主数据库保持一致。 主从复制的作用 主数据库出现问题，可以切换到从数据库。 可以进行数据库层面的读写分离。 可以在从数据库上进行日常备份 MySQL主从复制解决的问题 数据分布：随意开始或停止复制，并在不同地理位置分布数据备份 负载均衡：降低单个服务器的压力 高可用和故障切换：帮助应用程序避免单点失败 升级测试：可以用更高版本的MySQL作为从库 基本原理流程，3个线程以及之间的关联主：binlog线程——记录下所有改变了数据库数据的语句，放进master上的binlog中； 从：io线程——在使用start slave 之后，负责从master上拉取 binlog 内容，放进自己的relay log中； 从：sql执行线程——执行relay log中的语句； 第一步：master在每个事务更新数据完成之前，将该操作记录串行地写入到binlog文件中。 第二步：salve开启一个I/O Thread，该线程在master打开一个普通连接，主要工作是binlog dump process。如果读取的进度已经跟上了master，就进入睡眠状态并等待master产生新的事件。I/O线程最终的目的是将这些事件写入到中继日志中。 第三步：SQL Thread会读取中继日志，并顺序执行该日志中的SQL事件，从而与主数据库中的数据保持一致。 数据表损坏的修复方式有哪些使用 myisamchk 来修复，具体步骤： 1）修复前将mysql服务停止。2）打开命令行方式，然后进入到mysql的/bin目录。3）执行myisamchk –recover 数据库所在路径/*.MYI","categories":[],"tags":[]},{"title":"7数据不丢失保证","slug":"7数据不丢失保证","date":"2021-06-15T02:40:14.000Z","updated":"2021-06-15T13:39:48.071Z","comments":true,"path":"2021/06/15/7数据不丢失保证/","link":"","permalink":"https://sk-xinye.github.io/2021/06/15/7%E6%95%B0%E6%8D%AE%E4%B8%8D%E4%B8%A2%E5%A4%B1%E4%BF%9D%E8%AF%81/","excerpt":"","text":"WAL机制只要 redo log 和 binlog 保证持久化到磁盘，就能确保 MySQL 异常重启后，数据可以恢复。 binlog 的写入机制事务执行过程中，先把日志写到 binlog cache，事务提交的时候，再把 binlog cache 写到 binlog 文件中。 可以看到，每个线程有自己 binlog cache，但是共用同一份 binlog 文件。 图中的 write，指的就是指把日志写入到文件系统的 page cache，并没有把数据持久化到磁盘，所以速度比较快。 图中的 fsync，才是将数据持久化到磁盘的操作。一般情况下，我们认为 fsync 才占磁盘的 IOPS。 write 和 fsync 的时机，是由参数 sync_binlog 控制的： sync_binlog=0 的时候，表示每次提交事务都只 write，不 fsync； sync_binlog=1 的时候，表示每次提交事务都会执行 fsync； sync_binlog=N(N&gt;1) 的时候，表示每次提交事务都 write，但累积 N 个事务后才 fsync。 因此，在出现 IO 瓶颈的场景里，将 sync_binlog 设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢失日志量的可控性，一般不建议将这个参数设成 0，比较常见的是将其设置为 100~1000 中的某个数值。但是，将 sync_binlog 设置为 N，对应的风险是：如果主机发生异常重启，会丢失最近 N 个事务的 binlog 日志。 redo log 的写入机制事务在执行过程中，生成的 redo log 是要先写到 redo log buffer 的。 这三种状态分别是： 存在 redo log buffer 中，物理上是在 MySQL 进程内存中，就是图中的红色部分； 写到磁盘 (write)，但是没有持久化（fsync)，物理上是在文件系统的 page cache 里面，也就是图中的黄色部分； 持久化到磁盘，对应的是 hard disk，也就是图中的绿色部分. 为了控制 redo log 的写入策略，InnoDB 提供了 innodb_flush_log_at_trx_commit 参数，它有三种可能取值： 设置为 0 的时候，表示每次事务提交时都只是把 redo log 留在 redo log buffer 中 ; 设置为 1 的时候，表示每次事务提交时都将 redo log 直接持久化到磁盘； 设置为 2 的时候，表示每次事务提交时都只是把 redo log 写到 page cache。","categories":[],"tags":[]},{"title":"6数据删除了但是表大小不变","slug":"6数据删除了但是表大小不变","date":"2021-06-15T02:24:37.000Z","updated":"2021-06-15T13:39:48.070Z","comments":true,"path":"2021/06/15/6数据删除了但是表大小不变/","link":"","permalink":"https://sk-xinye.github.io/2021/06/15/6%E6%95%B0%E6%8D%AE%E5%88%A0%E9%99%A4%E4%BA%86%E4%BD%86%E6%98%AF%E8%A1%A8%E5%A4%A7%E5%B0%8F%E4%B8%8D%E5%8F%98/","excerpt":"","text":"数据删除表大小不变如果要收缩一个表，只是 delete 掉表里面不用的数据的话，表文件的大小是不会变的，你还要通过 alter table 命令重建表，才能达到表文件变小的目的。两种实现方式 Online DDL 的方式是可以考虑在业务低峰期使用的，而 MySQL 5.5 及之前的版本，这个命令是会阻塞 DML 的，这个你需要特别小心。alter table t engine=innodb,ALGORITHM=inplace; inplace","categories":[],"tags":[]},{"title":"5sql变慢","slug":"5sql变慢","date":"2021-06-15T02:08:37.000Z","updated":"2021-06-15T13:39:48.069Z","comments":true,"path":"2021/06/15/5sql变慢/","link":"","permalink":"https://sk-xinye.github.io/2021/06/15/5sql%E5%8F%98%E6%85%A2/","excerpt":"","text":"你的 SQL 语句为什么变“慢”了 当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。 内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”。 利用 WAL 技术，数据库将随机写转换成了顺序写，大大提升了数据库的性能。 但是，由此也带来了内存脏页的问题。脏页会被后台线程自动 flush，也会由于数据页淘汰而触发 flush，而刷脏页的过程由于会占用资源，可能会让你的更新和查询语句的响应时间长一些。 InnoDB 刷脏页的控制策略 innodb_io_capacity 告诉innoDB 磁盘能力，建议设置成IOPS (通过fio测试fio -filename=$filename -direct=1 -iodepth 1 -thread -rw=randrw -ioengine=psync -bs=16k -size=500M -numjobs=10 -runtime=10 -group_reporting -name=mytest ) innodb_max_dirty_pages_pct 脏页比例上限，默认为75%。 innodb_flush_neighbors 值为 1 的时候会有上述的“连坐”机制，值为 0 时表示不找邻居，自己刷自己的。在 MySQL 8.0 中，innodb_flush_neighbors 参数的默认值已经是 0 了。 短连接风暴max_connections 最大连接参数，当超过这个参数，就会报Too many connections，连接拒绝。 第一种方法：先处理掉那些占着连接但是不工作的线程。 第二种方法：减少连接过程的消耗。 跳过权限验证的方法是：重启数据库，并使用–skip-grant-tables 参数启动。这样，整个 MySQL 会跳过所有的权限验证阶段，包括连接过程和语句执行过程在内。 MySQL 8.0 版本里，如果你启用–skip-grant-tables 参数，MySQL 会默认把 –skip-networking 参数打开，表示这时候数据库只能被本地的客户端连接。 慢查询性能问题 索引没有设计好； SQL 语句没写好； MySQL 选错了索引。","categories":[],"tags":[]},{"title":"tcp/ip","slug":"tcp-ip","date":"2021-06-11T00:05:46.000Z","updated":"2021-06-14T23:39:01.453Z","comments":true,"path":"2021/06/11/tcp-ip/","link":"","permalink":"https://sk-xinye.github.io/2021/06/11/tcp-ip/","excerpt":"","text":"概念分层：解耦 和 扩展nc: 连接抓包：tcpdump -nn -i eth0 port 80路由：route -nmark: arp -n","categories":[],"tags":[]},{"title":"一条sql的执行过程","slug":"1一条sql的执行过程","date":"2021-06-07T23:37:56.000Z","updated":"2021-06-14T23:39:01.453Z","comments":true,"path":"2021/06/08/1一条sql的执行过程/","link":"","permalink":"https://sk-xinye.github.io/2021/06/08/1%E4%B8%80%E6%9D%A1sql%E7%9A%84%E6%89%A7%E8%A1%8C%E8%BF%87%E7%A8%8B/","excerpt":"","text":"逻辑架构图 大体来说，MySQL 可以分为 Server 层和存储引擎层两部分。 Server 层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖 MySQL 的大多数核心服务功能，以及所有的内置函数（如日期、时间、数学和加密函数等），所有跨存储引擎的功能都在这一层实现，比如存储过程、触发器、视图等。 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持 InnoDB、MyISAM、Memory 等多个存储引擎。现在最常用的存储引擎是 InnoDB，它从 MySQL 5.5.5 版本开始成为了默认存储引擎。 连接器第一步，你会先连接到这个数据库上，这时候接待你的就是连接器。mysql -h$ip -P$port -u$user -p 通过show processlist 查看系统的连接情况，其中command中sleep是空闲连接的意思 推荐使用长连接，但是长连接会占用系统内存资源，需要办法解决 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连。 如果你用的是 MySQL 5.7 或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection 来重新初始化连接资源。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。 查询缓存但是大多数情况下我会建议你不要使用查询缓存，为什么呢？因为查询缓存往往弊大于利。 查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。 query_cache_type 设置成 DEMAND MySQL 8.0 版本直接将查询缓存的整块功能删掉了，也就是说 8.0 开始彻底没有这个功能了。 分析器 分析器先会做“词法分析”。 做完了这些识别以后，就要做“语法分析”。 一般语法错误会提示第一个出现错误的位置，所以你要关注的是紧接“use near”的内容。 优化器优化器是在表里面有多个索引的时候，决定使用哪个索引；或者在一个语句有多表关联（join）的时候，决定各个表的连接顺序。 mysql&gt; select * from T where ID=10; 比如我们这个例子中的表 T 中，ID 字段没有索引，那么执行器的执行流程是这样的： 调用 InnoDB 引擎接口取这个表的第一行，判断 ID 值是不是 10，如果不是则跳过，如果是则将这行存在结果集中； 调用引擎接口取“下一行”，重复相同的判断逻辑，直到取到这个表的最后一行。 执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端。 重要的日志模块：redo logInnoDB 的 redo log 是固定大小的，比如可以配置为一组 4 个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录 4GB 的操作。 重要的日志模块：binlog","categories":[],"tags":[]},{"title":"数据倾斜","slug":"数据倾斜","date":"2021-06-06T05:16:02.000Z","updated":"2021-06-07T00:13:52.876Z","comments":true,"path":"2021/06/06/数据倾斜/","link":"","permalink":"https://sk-xinye.github.io/2021/06/06/%E6%95%B0%E6%8D%AE%E5%80%BE%E6%96%9C/","excerpt":"","text":"原因 数据中有 bigkey，导致某个实例的数据量增加； Slot 手工分配不均，导致某个或某些实例上有大量数据； 使用了 Hash Tag，导致数据集中到某些实例上。 解决：","categories":[],"tags":[]},{"title":"事务ACID","slug":"事务ACID","date":"2021-06-06T02:28:43.000Z","updated":"2021-06-06T05:22:55.068Z","comments":true,"path":"2021/06/06/事务ACID/","link":"","permalink":"https://sk-xinye.github.io/2021/06/06/%E4%BA%8B%E5%8A%A1ACID/","excerpt":"","text":"Redis 如何实现事务 第一步，客户端要使用一个命令显式地表示一个事务的开启。在 Redis 中，这个命令就是 MULTI。 第二步，客户端把事务中本身要执行的具体操作（例如增删改数据）发送给服务器端。这些操作就是 Redis 本身提供的数据读写命令，例如 GET、SET 等。不过，这些命令虽然被客户端发送到了服务器端，但 Redis 实例只是把这些命令暂存到一个命令队列中，并不会立即执行。 第三步，客户端向服务器端发送提交事务的命令，让数据库实际执行第二步中发送的具体操作。Redis 提供的 EXEC 命令就是执行事务提交的。当服务器端收到 EXEC 命令后，才会实际执行命令队列中的所有命令。 Redis 的事务机制能保证哪些属性原子性 命令入队时就报错，会放弃事务执行，保证原子性； 命令入队时没报错，实际执行时报错，不保证原子性； EXEC 命令执行时实例故障，如果开启了 AOF 日志，可以保证原子性。 一致性在命令执行错误或 Redis 发生故障的情况下，Redis 事务机制对一致性属性是有保证的。 隔离性 并发操作在 EXEC 命令前执行，此时，隔离性的保证要使用 WATCH 机制来实现，否则隔离性无法保证； 并发操作在 EXEC 命令后执行，此时，隔离性可以保证。 持久性不管 Redis 采用什么持久化模式，事务的持久性属性是得不到保证的。 总结Redis 的事务机制可以保证一致性和隔离性，但是无法保证持久性。不过，因为 Redis 本身是内存数据库，持久性并不是一个必须的属性，我们更加关注的还是原子性、一致性和隔离性这三个属性。原子性的情况比较复杂，只有当事务中使用的命令语法有误时，原子性得不到保证，在其它情况下，事务都可以原子性执行。","categories":[],"tags":[]},{"title":"分布式锁","slug":"分布式锁","date":"2021-06-06T01:27:00.000Z","updated":"2021-06-06T05:22:55.069Z","comments":true,"path":"2021/06/06/分布式锁/","link":"","permalink":"https://sk-xinye.github.io/2021/06/06/%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81/","excerpt":"","text":"为甚们需要分布式锁在分布式系统中，当有多个客户端需要获取锁时，我们需要分布式锁。此时，锁是保存在一个共享存储系统中的，可以被多个客户端共享访问和获取。 单机上的锁和分布式锁的联系与区别我们就可以得出实现分布式锁的两个要求： 要求一：分布式锁的加锁和释放锁的过程，涉及多个操作。所以，在实现分布式锁时，我们需要保证这些锁操作的原子性； 要求二：共享存储系统保存了锁变量，如果共享存储系统发生故障或宕机，那么客户端也就无法进行锁操作了。在实现分布式锁时，我们需要考虑保证共享存储系统的可靠性，进而保证锁的可靠性。 基于单个 Redis 节点实现分布式锁 SETNX 命令它用于设置键值对的值。具体来说，就是这个命令在执行时会判断键值对是否存在，如果不存在，就设置键值对的值，如果存在，就不做任何设置。 123456// 加锁SETNX lock_key 1// 业务逻辑DO THINGS// 释放锁DEL lock_key 为了防止锁无法释放，还会配置过期时间，并提供了set 命令，加上ex px 1SET key value [EX seconds | PX milliseconds] [NX] 例如： 12// 加锁, unique_value作为客户端唯一性的标识SET lock_key unique_value NX PX 10000 unique_value 是客户端的唯一标识，可以用一个随机生成的字符串来表示，PX 10000 则表示 lock_key 会在 10s 后过期，以免客户端在这期间发生异常而无法释放锁。 基于多个 Redis 节点实现高可靠的分布式锁Redlock 算法的基本思路，是让客户端和多个独立的 Redis 实例依次请求加锁，如果客户端能够和半数以上的实例成功地完成加锁操作，那么我们就认为，客户端成功地获得分布式锁了，否则加锁失败。这样一来，即使有单个 Redis 实例发生故障，因为锁变量在其它实例上也有保存，所以，客户端仍然可以正常地进行锁操作，锁变量并不会丢失 第一步是，客户端获取当前时间。 第二步是，客户端按顺序依次向 N 个 Redis 实例执行加锁操作。 如果客户端在和一个 Redis 实例请求加锁时，一直到超时都没有成功，那么此时，客户端会和下一个 Redis 实例继续请求加锁。加锁操作的超时时间需要远远地小于锁的有效时间，一般也就是设置为几十毫秒。 第三步是，一旦客户端完成了和所有 Redis 实例的加锁操作，客户端就要计算整个加锁过程的总耗时。 条件一：客户端从超过半数（大于等于 N/2+1）的 Redis 实例上成功获取到了锁； 条件二：客户端获取锁的总耗时没有超过锁的有效时间。 在满足了这两个条件后，我们需要重新计算这把锁的有效时间，计算的结果是锁的最初有效时间减去客户端为获取锁的总耗时。如果锁的有效时间已经来不及完成共享数据的操作了，我们可以释放锁，以免出现还没完成数据操作，锁就过期了的情况。 当然，如果客户端在和所有实例执行完加锁操作后，没能同时满足这两个条件，那么，客户端向所有 Redis 节点发起释放锁的操作。 总结： 在基于单个 Redis 实例实现分布式锁时，对于加锁操作，我们需要满足三个条件。 加锁包括了读取锁变量、检查锁变量值和设置锁变量值三个操作，但需要以原子操作的方式完成，所以，我们使用 SET 命令带上 NX 选项来实现加锁； 锁变量需要设置过期时间，以免客户端拿到锁后发生异常，导致锁一直无法释放，所以，我们在 SET 命令执行时加上 EX/PX 选项，设置其过期时间； 锁变量的值需要能区分来自不同客户端的加锁操作，以免在释放锁时，出现误释放操作，所以，我们使用 SET 命令设置锁变量值时，每个客户端设置的值是一个唯一值，用于标识客户端。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def acquire_lock_with_timeout(conn, lock_name, acquire_timeout=3, lock_timeout=2): &quot;&quot;&quot; 基于 Redis 实现的分布式锁 :param conn: Redis 连接 :param lock_name: 锁的名称 :param acquire_timeout: 获取锁的超时时间，默认 3 秒 :param lock_timeout: 锁的超时时间，默认 2 秒 :return: &quot;&quot;&quot; identifier = str(uuid.uuid4()) lockname = f&#x27;lock:&#123;lock_name&#125;&#x27; lock_timeout = int(math.ceil(lock_timeout)) end = time.time() + acquire_timeout while time.time() &lt; end: # 如果不存在这个锁则加锁并设置过期时间，避免死锁 if conn.set(lockname, identifier, ex=lock_timeout, nx=True): return identifier time.sleep(0.001) return Falsedef release_lock(conn, lockname, identifier): &quot;&quot;&quot; 释放锁 :param conn: Redis 连接 :param lockname: 锁的名称 :param identifier: 锁的标识 :return: &quot;&quot;&quot; # python中redis事务是通过pipeline的封装实现的 with conn.pipeline() as pipe: lockname = &#x27;lock:&#x27; + lockname while True: try: # watch 锁, multi 后如果该 key 被其他客户端改变, 事务操作会抛出 WatchError 异常 pipe.watch(lockname) iden = pipe.get(lockname) if iden and iden.decode(&#x27;utf-8&#x27;) == identifier: # 事务开始 pipe.multi() pipe.delete(lockname) pipe.execute() return True pipe.unwatch() break except WatchError: pass return False","categories":[],"tags":[]},{"title":"缓存","slug":"缓存","date":"2021-06-05T08:55:30.000Z","updated":"2021-06-06T05:22:55.068Z","comments":true,"path":"2021/06/05/缓存/","link":"","permalink":"https://sk-xinye.github.io/2021/06/05/%E7%BC%93%E5%AD%98/","excerpt":"","text":"缓存特征 第一个特征：在一个层次化的系统中，缓存一定是一个快速子系统，数据存在缓存中时，能避免每次从慢速子系统中存取数据。对应到互联网应用来说，Redis 就是快速子系统，而数据库就是慢速子系统了。 第二个特征：缓存系统的容量大小总是小于后端慢速系统的，我们不可能把所有数据都放在缓存系统中。 Redis 缓存处理请求的两种情况 缓存命中：Redis 中有相应数据，就直接读取 Redis，性能非常快。 缓存缺失：Redis 中没有保存相应数据，就从后端数据库中读取数据，性能就会变慢。而且，一旦发生缓存缺失，为了让后续请求能从缓存中读取到数据，我们需要把缺失的数据写入 Redis，这个过程叫作缓存更新。 使用 Redis 缓存时，我们基本有三个操作： 应用读取数据时，需要先读取 Redis； 发生缓存缺失时，需要从数据库读取数据； 发生缓存缺失时，还需要更新缓存。 Redis 作为旁路缓存的使用操作我们也把 Redis 称为旁路缓存，也就是说，读取缓存、读取数据库和更新缓存的操作都需要在应用程序中来完成。 123456789String cacheKey = “productid_11010003”;String cacheValue = redisCache.get(cacheKey)；//缓存命中if ( cacheValue != NULL) return cacheValue;//缓存缺失else cacheValue = getProductFromDB(); redisCache.put(cacheValue) //缓存更新 缓存的类型只读缓存 读请求正常发给redis进行查询 所有的写请求直接请求数据库，并删除redis中的缓存 读写缓存 所有的写请求也会发送到缓存，在缓存中直接对数据进行增删改操作。 这也就是说，应用的最新数据可能会丢失，给应用业务带来风险。 会有同步直写和异步写回两种策略。其中，同步直写策略优先保证数据可靠性，而异步写回策略优先提供快速响应。 区别： 1、使用只读缓存时，是先把修改写到后端数据库中，再把缓存中的数据删除。当下次访问这个数据时，会以后端数据库中的值为准，重新加载到缓存中。这样做的优点是，数据库和缓存可以保证完全一致，并且缓存中永远保留的是经常访问的热点数据。缺点是每次修改操作都会把缓存中的数据删除，之后访问时都会先触发一次缓存缺失，然后从后端数据库加载数据到缓存中，这个过程访问延迟会变大。 2、使用读写缓存时，是同时修改数据库和缓存中的值。这样做的优点是，被修改后的数据永远在缓存中存在，下次访问时，能够直接命中缓存，不用再从后端数据库中查询，这个过程拥有比较好的性能，比较适合先修改又立即访问的业务场景。但缺点是在高并发场景下，如果存在多个操作同时修改同一个值的情况，可能会导致缓存和数据库的不一致。 3、当使用只读缓存时，如果修改数据库失败了，那么缓存中的数据也不会被删除，此时数据库和缓存中的数据依旧保持一致。而使用读写缓存时，如果是先修改缓存，后修改数据库，如果缓存修改成功，而数据库修改失败了，那么此时数据库和缓存数据就不一致了。如果先修改数据库，再修改缓存，也会产生上面所说的并发场景下的不一致。 我个人总结，只读缓存是牺牲了一定的性能，优先保证数据库和缓存的一致性，它更适合对于一致性要求比较要高的业务场景。而如果对于数据库和缓存一致性要求不高，或者不存在并发修改同一个值的情况，那么使用读写缓存就比较合适，它可以保证更好的访问性能。 缓存满了：缓存数据的淘汰机制设置多大的缓存容量合适我会建议把缓存容量设置为总数据量的 15% 到 30%，兼顾访问性能和内存空间开销。 1CONFIG SET maxmemory 4gb Redis 缓存有哪些淘汰策略 不进行数据淘汰的策略，只有 noeviction（默认） 这一种。 在设置了过期时间的数据中进行淘汰，包括 volatile-random、volatile-ttl、volatile-lru、volatile-lfu（Redis 4.0 后新增）四种。 在所有数据范围内进行淘汰，包括 allkeys-lru、allkeys-random、allkeys-lfu（Redis 4.0 后新增）三种。 LRULRU 算法的全称是 Least Recently Used，从名字上就可以看出，这是按照最近最少使用的原则来筛选数据，最不常用的数据会被筛选出来，而最近频繁使用的数据会留在缓存中。 LRU 算法在实际实现时，需要用链表管理所有的缓存数据，这会带来额外的空间开销。而且，当有数据被访问时，需要在链表上把该数据移动到 MRU 端，如果有大量数据被访问，就会带来很多链表移动操作，会很耗时，进而会降低 Redis 缓存性能。 Redis 默认会记录每个数据的最近一次访问的时间戳（由键值对数据结构 RedisObject 中的 lru 字段记录）。然后，Redis 在决定淘汰的数据时，第一次会随机选出 N 个数据，把它们作为一个候选集合。接下来，Redis 会比较这 N 个数据的 lru 字段，把 lru 字段值最小的数据从缓存中淘汰出去。 如何处理被淘汰的数据一般来说，一旦被淘汰的数据选定后，如果这个数据是干净数据，那么我们就直接删除；如果这个数据是脏数据，我们需要把它写回数据库， 使用redis缓存带来的问题概括来说有 4 个方面：缓存中的数据和数据库中的不一致；缓存雪崩；缓存击穿和缓存穿透。 缓存中的数据和数据库中的不一致缓存和数据库的数据不一致是如何发生的首先，我们得清楚“数据的一致性”具体是啥意思。其实，这里的“一致性”包含了两种情况： 缓存中有数据，那么，缓存的数据值需要和数据库中的值相同； 缓存中本身没有数据，那么，数据库中的值必须是最新值。 只读缓存中删改数据造成的数据不一致： 如果发生删改操作，应用既要更新数据库，也要在缓存中删除数据。这两个操作如果无法保证原子性，也就是说，要不都完成，要不都没完成，此时，就会出现数据不一致问题了。 如何解决呢： 重试机制。 缓存雪崩缓存雪崩是指大量的应用请求无法在 Redis 缓存中进行处理，紧接着，应用将大量请求发送到数据库层，导致数据库层的压力激增。 首先，我们可以避免给大量的数据设置相同的过期时间。如果业务层的确要求有些数据同时失效，你可以在用 EXPIRE 命令给每个数据设置过期时间时，给这些数据的过期时间增加一个较小的随机数（例如，随机增加 1~3 分钟），这样一来，不同数据的过期时间有所差别，但差别又不会太大，既避免了大量数据同时过期，同时也保证了这些数据基本在相近的时间失效，仍然能满足业务需求。 所谓的服务降级，是指发生缓存雪崩时，针对不同的数据采取不同的处理方式 当业务应用访问的是非核心数据（例如电商商品属性）时，暂时停止从缓存中查询这些数据，而是直接返回预定义信息、空值或是错误信息； 当业务应用访问的是核心数据（例如电商商品库存）时，仍然允许查询缓存，如果缓存缺失，也可以继续通过数据库读取。 第一个建议，是在业务系统中实现服务熔断或请求限流机制。 我给你的第二个建议就是事前预防。 缓存击穿缓存击穿是指，针对某个访问非常频繁的热点数据的请求，无法在缓存中进行处理，紧接着，访问该数据的大量请求，一下子都发送到了后端数据库，导致了数据库压力激增，会影响数据库处理其他请求。缓存击穿的情况，经常发生在热点数据过期失效时， 对于访问特别频繁的热点数据，我们就不设置过期时间了。这样一来，对热点数据的访问请求，都可以在缓存中进行处理，而 Redis 数万级别的高吞吐量可以很好地应对大量的并发请求访问。 缓存穿透缓存穿透是指要访问的数据既不在 Redis 缓存中，也不在数据库中，导致请求在访问缓存时，发生缓存缺失，再去访问数据库时，发现数据库中也没有要访问的数据。 第一种方案是，缓存空值或缺省值。 第二种方案是，使用布隆过滤器快速判断数据是否存在，避免从数据库中查询数据是否存在，减轻数据库压力。 建议： 针对缓存雪崩，合理地设置数据过期时间，以及搭建高可靠缓存集群； 针对缓存击穿，在缓存访问非常频繁的热点数据时，不要设置过期时间； 针对缓存穿透，提前在入口前端实现恶意请求检测，或者规范数据库的数据删除操作，避免误删除。 并发访问方法 对临界区加锁，但是会导致系统并发性能降低 把多个操作在 Redis 中实现成一个操作，也就是单命令操作； Redis 提供了 INCR/DECR 命令，把这三个操作转变为一个原子操作了。INCR/DECR 命令可以对数据进行增值 / 减值操作，而且它们本身就是单个命令操作，Redis 在执行它们时，本身就具有互斥性。 把多个操作写到一个 Lua 脚本中，以原子性方式执行单个 Lua 脚本。 https://haicoder.net/note/redis-interview/redis-interview-redis-lua-script.html redis 面试提 由于 Lua 脚本是提交到 Redis server 进行一次性执行的，整个执行过程中不会被其他任何工作打断，其它任何脚本或者命令都无法执行,也就不会引起竞争条件，从而本身就实现了事务的原子性 在执行脚本时，不会执行其他脚本或Redis命令。这个语义类似于MULTI（开启事务）/EXEC（触发事务，一并执行事务中的所有命令）。从所有其他客户端的角度来看，脚本的效果要么仍然不可见，要么已经完成。https://blog.csdn.net/mingtianhaiyouwo/article/details/51199489 lua 原理","categories":[],"tags":[]},{"title":"影响redis性能因素","slug":"影响redis性能因素","date":"2021-06-02T13:19:33.000Z","updated":"2021-06-04T00:16:50.128Z","comments":true,"path":"2021/06/02/影响redis性能因素/","link":"","permalink":"https://sk-xinye.github.io/2021/06/02/%E5%BD%B1%E5%93%8Dredis%E6%80%A7%E8%83%BD%E5%9B%A0%E7%B4%A0/","excerpt":"","text":"影响redis性能的5大因素 Redis 内部的阻塞式操作； CPU 核和 NUMA 架构的影响； Redis 关键系统配置； Redis 内存碎片； Redis 缓冲区。 Redis 内部的阻塞式操作Redis 实例有哪些阻塞点 客户端：网络 IO，键值对增删改查操作，数据库操作； 磁盘：生成 RDB 快照，记录 AOF 日志，AOF 日志重写； 主从节点：主库生成、传输 RDB 文件，从库接收 RDB 文件、清空数据库、加载 RDB 文件； 切片集群实例：向其他实例传输哈希槽信息，数据迁移。 阻塞点 第一个阻塞点：集合全量查询和聚合操作。 bigkey 删除操作就是 Redis 的第二个阻塞点 Redis 的第三个阻塞点：清空数据库。 第四个阻塞点了：AOF 日志同步写（罗盘方式）。 RDB生成及Aof重写采用子进程 从库加载 RDB 文件就成为了 Redis 的第五个阻塞点。 上面的阻塞点可以异步执行吗 第一个阻塞点，是在关键路径上的读，需要给客户端返回结果，不能异步执行 第二三个阻塞点，可以不用返回结果，可以异步 第四个，可以异步 第五个是 异步的子线程机制Redis 主线程启动后，会使用操作系统提供的 pthread_create 函数创建 3 个子线程，分别由它们负责 AOF 日志写操作、键值对删除以及文件关闭的异步执行。 删除 键值对删除：当你的集合类型中有大量元素（例如有百万级别或千万级别元素）需要删除时，我建议你使用 UNLINK 命令。 清空数据库：可以在 FLUSHDB 和 FLUSHALL 命令后加上 ASYNC 选项，这样就可以让后台子线程异步地清空数据库，如下所示： 12FLUSHDB ASYNCFLUSHALL AYSNC 建议集合全量查询和聚合操作、从库加载 RDB 文件是在关键路径上，无法使用异步操作来完成。对于这两个阻塞点，我也给你两个小建议。 集合全量查询和聚合操作：可以使用 SCAN 命令，分批读取数据，再在客户端进行聚合计算； 从库加载 RDB 文件：把主库的数据量大小控制在 2~4GB 左右，以保证 RDB 文件能以较快的速度加载。 CPU 核和 NUMA 架构的影响主流的 CPU 架构 一个 CPU 处理器中一般有多个运行核心，我们把一个运行核心称为一个物理核，每个物理核都可以运行应用程序。每个物理核都拥有私有的一级缓存（Level 1 cache，简称 L1 cache），包括一级指令缓存和一级数据缓存，以及私有的二级缓存（Level 2 cache，简称 L2 cache）。 在主流的服务器上，一个 CPU 处理器会有 10 到 20 多个物理核。同时，为了提升服务器的处理能力，服务器上通常还会有多个 CPU 处理器（也称为多 CPU Socket），每个处理器有自己的物理核（包括 L1、L2 缓存），L3 缓存，以及连接的内存，同时，不同处理器间通过总线连接。 问题1：在多 CPU 架构上，应用程序可以在不同的处理器上运行 远端内存访问。和访问 Socket 直接连接的内存相比，远端内存访问会增加应用程序的延迟。 所以，我们也把这个架构称为非统一内存访问架构（Non-Uniform Memory Access，NUMA 架构） 如果在 CPU 多核场景下，Redis 实例被频繁调度到不同 CPU 核上运行的话，那么，对 Redis 实例的请求处理时间影响就更大了。每调度一次，一些请求就会受到运行时信息、指令和数据重新加载过程的影响，这就会导致某些请求的延迟明显高于其他请求。 解决：我们可以使用 taskset 命令把一个程序绑定在一个核上运行。 taskset -c 0 ./redis-server 就把 Redis 实例绑在了 0 号核上，其中，“-c”选项用于设置要绑定的核编号。 问题2：当网络中断处理程序、Redis 实例分别和 CPU 核绑定后，就会有一个潜在的风险 如果网络中断处理程序和 Redis 实例各自所绑的 CPU 核不在同一个 CPU Socket 上，那么，Redis 实例读取网络数据时，就需要跨 CPU Socket 访问内存，这个过程会花费较多时间。 在 CPU 的 NUMA 架构下，对 CPU 核的编号规则，并不是先把一个 CPU Socket 中的所有逻辑核编完，再对下一个 CPU Socket 中的逻辑核编码，而是先给每个 CPU Socket 中每个物理核的第一个逻辑核依次编号，再给每个 CPU Socket 中的物理核的第二个逻辑核依次编号 可以通过lscpu查看 绑核的风险和解决方案方案一：一个 Redis 实例对应绑一个物理核在给 Redis 实例绑核时，我们不要把一个实例和一个逻辑核绑定，而要和一个物理核绑定，也就是说，把一个物理核的 2 个逻辑核都用上。taskset -c 0,12 ./redis-server 方案二：优化 Redis 源码这个方案就是通过修改 Redis 源码，把子进程和后台线程绑到不同的 CPU 核上。6.0以后支持配置绑核 123456789101112131415//线程函数void worker(int bind_cpu)&#123; cpu_set_t cpuset; //创建位图变量 CPU_ZERO(&amp;cpu_set); //位图变量所有位设置0 CPU_SET(bind_cpu, &amp;cpuset); //根据输入的bind_cpu编号，把位图对应为设置为1 sched_setaffinity(0, sizeof(cpuset), &amp;cpuset); //把程序绑定在cpu_set_t结构位图中为1的逻辑核 //实际线程函数工作&#125;int main()&#123; pthread_t pthread1 //把创建的pthread1绑在编号为3的逻辑核上 pthread_create(&amp;pthread1, NULL, (void *)worker, 3);&#125; 问题排查redis 真的变慢了吗 一个最直接的方法，就是查看 Redis 的响应延迟。 基于当前环境下的 Redis 基线性能做判断。 123456789./redis-cli --intrinsic-latency 120Max latency so far: 17 microseconds.Max latency so far: 44 microseconds.Max latency so far: 94 microseconds.Max latency so far: 110 microseconds.Max latency so far: 119 microseconds.36481658 total runs (avg latency: 3.2893 microseconds / 3289.32 nanoseconds per run).Worst run took 36x longer than the average latency. 如何应对 Redis 变慢 查找原因 逐一解决 Redis 自身操作特性的影响 慢查询命令解决： 用其他高效命令代替。比如说，如果你需要返回一个 SET 中的所有成员时，不要使用 SMEMBERS 命令，而是要使用 SSCAN 多次迭代返回，避免一次返回大量数据，造成线程阻塞。 当你需要执行排序、交集、并集操作时，可以在客户端完成，而不要用 SORT、SUNION、SINTER 这些命令，以免拖慢 Redis 实例。 KEYS 命令一般不被建议用于生产环境中。 过期 key 操作删除操作是阻塞的（Redis 4.0 后可以用异步线程机制来减少阻塞影响）解决： 排查过期 key 的时间设置，并根据实际使用需求，设置不同的过期时间。 文件系统：AOF 模式依赖于系统命令write fsync AOF 重写会对磁盘进行大量 IO 操作，同时，fsync 又需要等到数据写到磁盘后才能返回，所以，当 AOF 重写的压力比较大时，就会导致 fsync 被阻塞。 当主线程使用后台子线程执行了一次 fsync，需要再次把新接收的操作记录写回磁盘时，如果主线程发现上一次的 fsync 还没有执行完，那么它就会阻塞。所以，如果后台子线程执行的 fsync 频繁阻塞的话（比如 AOF 重写占用了大量的磁盘 IO 带宽），主线程也会阻塞，导致 Redis 性能变慢。 解决： no-appendfsync-on-rewrite yes 采用固态 硬盘作为AOF日志的写入设备 操作系统：swap解决： 增加内存 扩展集群 内存大页 Redis 就会采用写时复制机制时需要加载更多的内存2MB 与平时的4KB相比要好的多解决：echo never /sys/kernel/mm/transparent_hugepage/enabled 关闭内存大页 总结 获取 Redis 实例在当前环境下的基线性能。 是否用了慢查询命令？如果是的话，就使用其他命令替代慢查询命令，或者把聚合计算命令放在客户端做。 是否对过期 key 设置了相同的过期时间？对于批量删除的 key，可以在每个 key 的过期时间上加一个随机数，避免同时删除。 是否存在 bigkey？ 对于 bigkey 的删除操作，如果你的 Redis 是 4.0 及以上的版本，可以直接利用异步线程机制减少主线程阻塞；如果是 Redis 4.0 以前的版本，可以使用 SCAN 命令迭代删除； 对于 bigkey 的集合查询和聚合操作，可以使用 SCAN 命令在客户端完成。 Redis AOF 配置级别是什么？业务层面是否的确需要这一可靠性级别？如果我们需要高性能，同时也允许数据丢失，可以将配置项 no-appendfsync-on-rewrite 设置为 yes，避免 AOF 重写和 fsync 竞争磁盘 IO 资源，导致 Redis 延迟增加。当然， 如果既需要高性能又需要高可靠性，最好使用高速固态盘作为 AOF 日志的写入盘。 Redis 实例的内存使用是否过大？发生 swap 了吗？如果是的话，就增加机器内存，或者是使用 Redis 集群，分摊单机 Redis 的键值对数量和内存压力。同时，要避免出现 Redis 和其他内存需求大的应用共享机器的情况。 在 Redis 实例的运行环境中，是否启用了透明大页机制？如果是的话，直接关闭内存大页机制就行了。 是否运行了 Redis 主从集群？如果是的话，把主库实例的数据量大小控制在 2~4GB，以免主从复制时，从库因加载大的 RDB 文件而阻塞。 是否使用了多核 CPU 或 NUMA 架构的机器运行 Redis 实例？使用多核 CPU 时，可以给 Redis 实例绑定物理核；使用 NUMA 架构时，注意把 Redis 实例和网络中断处理程序运行在同一个 CPU Socket 上。 内存碎片命令： INFO memory，mem_fragmentation_ratio 大于 1.5，需要引起注意 如何清理内存碎片 当 Redis 发生内存碎片后，一个“简单粗暴”的方法就是重启 Redis 实例。 当 Redis 发生内存碎片后，一个“简单粗暴”的方法就是重启 Redis 实例。 即使 Redis 数据持久化了，我们还需要通过 AOF 或 RDB 进行恢复，恢复时长取决于 AOF 或 RDB 的大小，如果只有一个 Redis 实例，恢复阶段无法提供服务。 自动清理 config set activedefrag yes active-defrag-ignore-bytes 100mb：表示内存碎片的字节数达到 100MB 时，开始清理； active-defrag-threshold-lower 10：表示内存碎片空间占操作系统分配给 Redis 的总空间比例达到 10% 时，开始清理。为了尽可能减少碎片清理对 Redis 正常请求处理 active-defrag-cycle-min 25： 表示自动清理过程所用 CPU 时间的比例不低于 25%，保证清理能正常开展； active-defrag-cycle-max 75：表示自动清理过程所用 CPU 时间的比例不高于 75%，一旦超过，就停止清理，从而避免在清理时，大量的内存拷贝阻塞 Redis，导致响应延迟升高。 缓冲区溢出 针对命令数据发送过快过大的问题，对于普通客户端来说可以避免 bigkey，而对于复制缓冲区来说，就是避免过大的 RDB 文件。 针对命令数据处理较慢的问题，解决方案就是减少 Redis 主线程上的阻塞操作，例如使用异步的删除操作。 针对缓冲区空间过小的问题，解决方案就是使用 client-output-buffer-limit 配置项设置合理的输出缓冲区、复制缓冲区和复制积压缓冲区大小。当然，我们不要忘了，输入缓冲区的大小默认是固定的，我们无法通过配置来修改它，除非直接去修改 Redis 源码。","categories":[],"tags":[]},{"title":"消息队列","slug":"消息队列","date":"2021-06-02T11:34:45.000Z","updated":"2021-06-02T13:57:34.981Z","comments":true,"path":"2021/06/02/消息队列/","link":"","permalink":"https://sk-xinye.github.io/2021/06/02/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","excerpt":"","text":"消息队列的消息存取需求 需求一：消息保序 需求二：重复消息处理 需求三：消息可靠性保证 基于 List 的消息队列解决方案 LPUSH RPOP 天然保序 读取时，存在while(true) 浪费cpu 内存的风险；BRPOP 命令也称为阻塞式读取，客户端在没有读到队列数据时，自动阻塞，直到有新的数据写入队列，再开始读取新数据。 消费者程序本身能对重复消息进行判断。并且生产者需要带上全局唯一ID LPUSH mq “101030001:stock:5” 就把一条全局 ID 为 101030001、库存量为 5 的消息插入了消息队列 为了留存消息，List 类型提供了 BRPOPLPUSH 命令，这个命令的作用是让消费者程序从一个 List 中读取消息，同时，Redis 会把这个消息再插入到另一个 List（可以叫作备份 List）留存。这样一来，如果消费者程序读了消息但没能正常处理，等它重启后，就可以从备份 List 中重新读取消息并进行处理了 问题：生产者消息发送很快，而消费者处理消息的速度比较慢，这就导致 List 中的消息越积越多，给 Redis 的内存带来很大压力。 所以需要消费者组的支持，stream支持消费者组 基于 Streams 的消息队列解决方案操作 XADD：插入消息，保证有序，可以自动生成全局唯一 ID； XREAD：用于读取消息，可以按 ID 读取数据； XREADGROUP：按消费组形式读取消息； XPENDING 和 XACK：XPENDING 命令可以用来查询每个消费组内所有消费者已读取但尚未确认的消息，而 XACK 命令用于向消息队列确认消息处理已完成。 总结Redis 是一个非常轻量级的键值数据库，部署一个 Redis 实例就是启动一个进程，部署 Redis 集群，也就是部署多个 Redis 实例。而 Kafka、RabbitMQ 部署时，涉及额外的组件，例如 Kafka 的运行就需要再部署 ZooKeeper。相比 Redis 来说，Kafka 和 RabbitMQ 一般被认为是重量级的消息队列。","categories":[],"tags":[]},{"title":"集合","slug":"集合","date":"2021-06-01T23:39:06.000Z","updated":"2021-06-02T13:05:25.145Z","comments":true,"path":"2021/06/02/集合/","link":"","permalink":"https://sk-xinye.github.io/2021/06/02/%E9%9B%86%E5%90%88/","excerpt":"","text":"常用的集合统计模式聚合统计 统计多个集合的共有元素（交集统计）；SINTERSTORE destination key1 [key2] 返回给定所有集合的交集并存储在 destination 中 把两个集合相比，统计其中一个集合独有的元素（差集统计）；SDIFFSTORE destination key1 [key2] 返回给定所有集合的差集并存储在 destination 中 统计多个集合的所有元素（并集统计）。SUNIONSTORE destination key1 [key2] 所有给定集合的并集存储在 destination 集合中 例如统计新增用户数 可以用两个set ,一个全局的，一个当天的 Set 的差集、并集和交集的计算复杂度较高，在数据量较大的情况下，如果直接执行这些计算，会导致 Redis 实例阻塞。所以，我给你分享一个小建议：你可以从主从集群中选择一个从库，让它专门负责聚合计算，或者是把数据读取到客户端，在客户端来完成聚合统计，这样就可以规避阻塞主库实例和其他从库实例的风险了。 排序统计 比如统计最新评论时，就要用到有序集合了，redis集合中List 和 Sorted Set 是有序集合 List 是按照元素进入 List 的顺序进行排序的，而 Sorted Set 可以根据元素的权重来排序 在面对需要展示最新列表、排行榜等场景时，如果数据更新频繁或者需要分页显示，建议你优先考虑使用 Sorted Set。 二值状态统计bitmap 比如统计多人的签到情况， SETBIT uid:sign:3000:202008 2 1 GETBIT uid:sign:3000:202008 2 BITCOUNT uid:sign:3000:202008 基数统计HyperLogLog 基数统计就是指统计一个集合中不重复的元素个数。对应到我们刚才介绍的场景中，就是统计网页的 UV。 PFADD page1:uv user1 user2 user3 user4 user5 对于基数统计来说，如果集合元素量达到亿级别而且不需要精确统计时，我建议你使用 HyperLogLog。 GEO面向 LBS(Location-Based Service，LBS) 应用的 GEO 数据类型 底层是sorted set 利用了GeoHash 的编码方法（以区间定0、1来进行编码） 自定义结合类型 type：表示值的类型，涵盖了我们前面学习的五大基本类型； encoding：是值的编码方式，用来表示 Redis 中实现各个基本类型的底层数据结构，例如 SDS、压缩列表、哈希表、跳表等； lru：记录了这个对象最后一次被访问的时间，用于淘汰过期的键值对； refcount：记录了对象的引用计数； *ptr：是指向数据的指针。 基于 RedisTimeSeries 模块保存时间序列数据 RedisTimeSeries 是 Redis 的一个扩展模块。它专门面向时间序列数据提供了数据类型和访问接口，并且支持在 Redis 实例上直接对数据进行按时间范围的聚合计算。 当用于时间序列数据存取时，RedisTimeSeries 的操作主要有 5 个： 用 TS.CREATE 命令创建时间序列数据集合；TS.CREATE device:temperature RETENTION 600000 LABELS device_id 1 用 TS.ADD 命令插入数据；TS.ADD device:temperature 1596416700 25.11596416700TS.GET device:temperature25.1 用 TS.GET 命令读取最新数据； 用 TS.MGET 命令按标签过滤查询数据集合； 用 TS.RANGE 支持聚合计算的范围查询。","categories":[],"tags":[]},{"title":"string","slug":"string","date":"2021-06-01T13:25:39.000Z","updated":"2021-06-02T00:14:46.306Z","comments":true,"path":"2021/06/01/string/","link":"","permalink":"https://sk-xinye.github.io/2021/06/01/string/","excerpt":"","text":"string类型的问题 当使用string保存大量数据时，例如保存图片id:存储id 时，1亿张图片用了6.4G 内存，会造成RDB生成相应变慢的问题 就是它保存数据时所消耗的内存空间较多。 为什么 String 类型内存开销大(64K,整数) 当你保存 64 位有符号整数时，String 类型会把它保存为一个 8 字节的 Long 类型整数，这种保存方式通常也叫作 int 编码方式。 当你保存的数据中包含字符时，String 类型就会用简单动态字符串（Simple Dynamic String，SDS）结构体来保存 buf：字节数组，保存实际数据。为了表示字节数组的结束，Redis 会自动在数组最后加一个“\\0”，这就会额外占用 1 个字节的开销。 len：占 4 个字节，表示 buf 的已用长度。 alloc：也占个 4 字节，表示 buf 的实际分配长度，一般大于 len。 因为 Redis 的数据类型有很多，而且，不同数据类型都有些相同的元数据要记录（比如最后一次访问的时间、被引用的次数等），所以，Redis 会用一个 RedisObject 结构体来统一记录这些元数据，同时指向实际数据。一个 RedisObject 包含了 8 字节的元数据和一个 8 字节指针 6.4g 中4.8g保存了元数据 用什么数据结构可以节省内存Redis 有一种底层数据结构，叫压缩列表（ziplist），这是一种非常节省内存的结构。 表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量，以及列表中的 entry 个数。压缩列表尾还有一个 zlend，表示列表结束。 减少dictEntry的个数来进行压缩 如何用集合类型保存单值的键值对可以采用基于 Hash 类型的二级编码方法。这里说的二级编码，就是把一个单值的数据拆分成两部分，前一部分作为 Hash 集合的 key，后一部分作为 Hash 集合的 value，这样一来，我们就可以把单值数据保存到 Hash 集合中了。 以图片 ID 1101000060 和图片存储对象 ID 3302000080 为例，我们可以把图片 ID 的前 7 位（1101000）作为 Hash 类型的键，把图片 ID 的最后 3 位（060）和图片存储对象 ID 分别作为 Hash 类型值中的 key 和 value。 Redis Hash 类型的两种底层实现结构，分别是压缩列表和哈希表。一旦超过了阈值，Hash 类型就会用哈希表来保存数据了。 hash-max-ziplist-entries：表示用压缩列表保存时哈希集合中的最大元素个数。 hash-max-ziplist-value：表示用压缩列表保存时哈希集合中单个元素的最大长度。","categories":[],"tags":[]},{"title":"镜像制作","slug":"镜像制作","date":"2021-06-01T02:53:30.000Z","updated":"2023-07-16T13:14:55.044Z","comments":true,"path":"2021/06/01/镜像制作/","link":"","permalink":"https://sk-xinye.github.io/2021/06/01/%E9%95%9C%E5%83%8F%E5%88%B6%E4%BD%9C/","excerpt":"","text":"镜像瘦身原因：冗余文件，冗余系统，层会占用空间，层越多，最终的镜像就越大。 COPY、ADD和RUN语句会在镜像中添加新层。可以用&amp;&amp;让镜像更小 我们来试试Docker的多阶段构建。 12345678910FROM node:8 as buildWORKDIR /appCOPY package.json index.js ./RUN npm installFROM node:8COPY --from=build /app /EXPOSE 3000CMD [&quot;index.js&quot;] 使用Distroless移除容器中的所有累赘 123456789101112 FROM node:8 as build WORKDIR /app COPY package.json index.js ./ RUN npm install FROM gcr.io/distroless/nodejs COPY --from=build /app / EXPOSE 3000 CMD [&quot;index.js&quot;]docker build -t node-distroless . 验证：$ docker run -p 3000:3000 -ti –rm –init node-distroless 使用Alpine作为更小的基础镜像 编译完成后，清理编译环境和跟程序运行无关的软件和文件 docker基本操作docker iamge build -t dr.z/test:aliun .docker tag IMAGEID(镜像id) REPOSITORY:TAG（仓库：标签） 修改标签，仓库名 docker rmi IMAGEID(镜像id) 删除镜像docker rm container(容器id) 删除容器 docker save -o es.tar dr.z/elasticsearch/elasticsearch-oss:6.3.2 保存镜像docker load &lt; /root/docker_result/images/es.tar 加载镜像docker load -i zs_theft.xz 加载镜像 docker cp container(容器id):/opt/py ./ 拷贝容器内容到本地docker cp /opt/test/file.txt container(容器id):/opt/py 拷贝本地到容器内容docker commit container(容器id) centos-vim 保存容器为镜像 docker run -it IMAGEID(镜像id) /bin/bash –network host(extra_hosts) 运行并进入镜像 保存镜像docker save -o /home/dyufei/tensorflow.tar tensorflow:tensorflow 或者如下docker save tensorflow:tensorflow &gt; /home/dyufei/tensorflow.tar docker save alpine | gzip &gt; alpine-latest.tar.gzdocker load -i alpine-latest.tar.gz 加载本地镜像docker load -i tensorflow.tar export/import与 save/load区别A ：export/import 是根据容器来导出镜像（因此没有镜像的历史记录）而 save/load 操作的对象是镜像B ：export/import 镜像的历史记录再导后无法进行回滚操作，而save/load镜像有完整的历史记录可以回滚 docker export tensorboard &gt; /home/dyufei/tensorflow_tensorboard.tar 或者如下docker export -o /home/dyufei/tensorflow_tensorboard.tar tensorboard 导入容器的镜像sudo docker import - /home/dyufei/tensorflow_tensorboard.tar 将 pip 源修改为阿里云源pip config list 查看当前 pip 的配置pip config set global.index-url https://mirrors.aliyun.com/pypi/simple/ 接着修改配置文件 12pip config set global.index-url http://mirrors.aliyun.com/pypi/simple/pip config set install.trusted-host mirrors.aliyun.com docker内存限制12345678910111213141516docker stats 查看容器使用的内存docker run -d -p 8081:8080 -m 512M --memory-swap -1 --name tomcat01 -v /data/wwwroot/tomcat01/:/opt/tomcat8/webapps/ -v /data/wwwroot/log/tomcat01/:/opt/tomcat8/logs/ --restart=always 71dc929e155 添加内存限制docker run -d -p 8081:8080 -m 512M --memory-swap -1 --name tomcat01 -v /data/wwwroot/tomcat01/:/opt/tomcat8/webapps/ -v /data/wwwroot/log/tomcat01/:/opt/tomcat8/logs/ -v /data/wwwroot/tomcat01/ROOT/static/:/data/wwwroot/tomcat01/ROOT/static/ --restart=always 71dc929e155c上面的 docker run 命令中通过 -m 选项限制容器使用的内存上限为 512M。同时设置 memory-swap 值为-1，它表示容器程序使用内存的受限，而可以使用的 swap 空间使用不受限制(宿主机有多少 swap 容器就可以使用多少)。–memory=”300m” –memory-swap=”1g” 的含义为：容器可以使用 300M 的物理内存，并且可以使用 700M(1G -300M) 的 swap。–memory-swap 居然是容器可以使用的物理内存和可以使用的 swap 之和！把 --memory-swap 设置为 0 和不设置是一样的，此时如果设置了 --memory，容器可以使用的 swap 大小为 --memory 值的两倍。如果--memory-swap 的值和--memory 相同，则容器不能使用 swap。docker update --memory 1200m --memory-swap -1 tomcat01 更新已有容器内存限制 删除镜像失败docker rmi $(docker images –filter “dangling=true” -q –no-trunc)docker rmi imagessudo docker rm docker ps -a|grep Exited|awk &#39;&#123;print $1&#125;&#39; ps -ef |grep pp2_main |grep cyan |grep -v ‘grep’ |awk ‘{print $2}’|xargs kill -9docker images|grep none|awk ‘{print $3}’|xargs docker rmi linux 操作rm -rf ./test_chk_ln 删除软连接ln -s 源文件 软链接 entrypoint cmd123FROM ubuntu:trustyENTRYPOINT [&quot;/bin/ping&quot;,&quot;-c&quot;,&quot;3&quot;]CMD [&quot;localhost&quot;] 默认是 ENTRYPOINT CMDENTRYPOINT 是固定的，CMD可以替换 查看容器启动方式docker ps –no-trunc docker socket映射docker run -u root –rm \\ -d \\ -p 8080:8080 \\ -p 50000:50000 \\ -v jenkins-data:/var/jenkins_home \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v /usr/bin/docker:/usr/bin/docker jenkinsci/blueocean k8s 操作kubectl describe pod webods-0 -n testkubectl get nodes –show-labels=truekubectl -n test get svckubectl logs -f webods-0 -n testkubectl logs web-0 -n test -c webkubectl create namespace test_1kubectl get event -n kubectl exec -it web-0 – /bin/bash/etc/docker/daemon.jsonkubectl cp /tmp/test_pod.txt default/mycentos-7b59b5b755-8rbgc:/root 问题： 目录创建问题 /home/zshield/logs/supervisor/supervisord_pp2.log /home/zshield/logs/supervisor/supervisord_pre.log /home/zshield/logs/supervisor/supervisord_web.log /home/zshield/logs/supervisor/supervisord_pp1.log /home/zshield/conf/supervisor_pp1/supervisord.d/pp1.ini tornadofrom tornado.netutil import set_close_execdef main(): app = AnalyticApiApplication() http_serve = httpserver.HTTPServer(app) # http_serve.listen(options.port) # supervisor 创建的监听套接字文件描述符，通过 0 号传递给 tornado的所有进程 sock = socket.fromfd(0, family=socket.AF_INET, type=socket.SOCK_STREAM) set_close_exec(sock.fileno()) sock.setblocking(0) # 设置套接字为非阻塞调用 http_serve.add_socket(sock) ioloop.IOLoop.instance().start() supervisorcommand=/home/tao/.local/bin/pipenv run python app.py –connect=local-dev –debug=1socket=tcp://localhost:8888directory=/home/tao/projects/analytics_apiuser=taonumprocs=4process_name=%(program_name)s_%(process_num)02dstdout_logfile =/var/log/tornado_pyapi_stdout_%(process_num)02d.logstderr_logfile =/var/log/tornado_pyapi_stderr_%(process_num)02d.log 操作方式 上传对应镜像到所需要启动的机器上，加载镜像， 加载方式：docker load -i elasticsearch.xz 方法1：可以单独上传到需要的服务器 方式2：docker push 到他们的仓库，这样就只用上传到一台服务器即可 修改配置文件： 在配置文件中都有namespace的字段，该字段是命名空间的意思，即你的操作都会在此空间中进行,如果现场不需要指定，那删除即可，会直接使用默认命名空间default yaml相关1234567891011121314151617181920212223242526env: - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: MY_POD_SERVICE_ACCOUNT valueFrom: fieldRef: fieldPath: spec.serviceAccountName - name: MY_NODE_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.hostIP 相关yaml示例zookeeper123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150apiVersion: apps/v1kind: StatefulSetmetadata: labels: component: zookeeper name: zookeeperspec: serviceName: zookeeper-headless replicas: 2 selector: matchLabels: component: zookeeper template: metadata: labels: component: zookeeper spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1 - node2 podAntiAffinity: #设置pod亲和性 requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 - labelSelector: matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签 - key: component operator: In values: - zookeeper topologyKey: kubernetes.io/hostname initContainers: - name: init-sysctl image: h.dr.z/rancher/busybox:1.27.2 command: - sysctl - -w - vm.max_map_count=262144 securityContext: privileged: true - command: - /bin/bash - -c - |- set -ex; mkdir -p /data; if [[ ! -f &quot;/data/myid&quot; ]]; then hostindex=$HOSTNAME; let zooid=$&#123;hostindex: -1: 1&#125;+1; echo $zooid &gt; &quot;/data/myid&quot; echo &quot;Zookeeper MyId: &quot; $zooid fi env: - name: HOSTNAME valueFrom: fieldRef: fieldPath: metadata.name image: dr.z/zookeeper:3.6.2 name: zookeeper-init securityContext: runAsUser: 1000 volumeMounts: - name: zk-data mountPath: &quot;/data&quot; containers: - env: - name: ZOO_ADMINSERVER_ENABLED value: &quot;false&quot; - name: ZOO_LOG4J_PROP value: &quot;INFO,ROLLINGFILE&quot; - name: ZOO_TICK_TIME value: &quot;10000&quot; - name: TZ value: &quot;Asia/Shanghai&quot; - name: ZOO_4LW_COMMANDS_WHITELIST value: &quot;mntr&quot; - name: ZOO_SERVERS value: &quot;server.1=zookeeper-0.zookeeper-headless.default.svc.cluster.local:2888:3888;2181 server.2=zookeeper-1.zookeeper-headless.default.svc.cluster.local:2888:3888;2181&quot; image: dr.z/zookeeper:3.6.2 imagePullPolicy: IfNotPresent name: zookeeper ports: - containerPort: 2181 name: client - containerPort: 2888 name: server - containerPort: 3888 name: leader-election volumeMounts: - mountPath: /logs name: zk-log - mountPath: /data name: zk-data - mountPath: /datalog name: zk-datalog - mountPath: /etc/localtime name: zk-time volumes: - name: zk-log hostPath: path: /home/zshield/zookeeper/logs/ type: DirectoryOrCreate - name: zk-data hostPath: path: /home/zshield/BIGdata/zookeeper/data/ type: DirectoryOrCreate - name: zk-datalog hostPath: path: /home/zshield/BIGdata/zookeeper/datalog/ type: DirectoryOrCreate - name: zk-time hostPath: path: /etc/localtime---apiVersion: v1kind: Servicemetadata: name: zookeeper-headless labels: component: zookeeperspec: ports: - port: 2888 name: server - port: 3888 name: leader-election clusterIP: None selector: component: zookeeper---apiVersion: v1kind: Servicemetadata: name: zookeeper-service labels: component: zookeeperspec: ports: - port: 2181 name: client protocol: TCP nodePort: 30123 targetPort: 2181 clusterIP: 10.110.114.54 selector: component: zookeeper type: NodePort kafka123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338apiVersion: v1data: log4j2.properties: | # log action execution errors for easier debugging log4j.rootLogger=INFO, stdout, kafkaAppender log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.kafkaAppender=org.apache.log4j.RollingFileAppender #log4j.appender.kafkaAppender.DatePattern=&#x27;.&#x27;yyyy-MM log4j.appender.kafkaAppender.File=$&#123;kafka.logs.dir&#125;/server.log log4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout log4j.appender.kafkaAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.kafkaAppender.MaxFileSize=200MB log4j.appender.kafkaAppender.MaxBackupIndex=3 log4j.appender.stateChangeAppender=org.apache.log4j.RollingFileAppender #log4j.appender.stateChangeAppender.DatePattern=&#x27;.&#x27;yyyy-MM log4j.appender.stateChangeAppender.File=$&#123;kafka.logs.dir&#125;/state-change.log log4j.appender.stateChangeAppender.layout=org.apache.log4j.PatternLayout log4j.appender.stateChangeAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.stateAppender.MaxFileSize=200MB log4j.appender.stateAppender.MaxBackupIndex=3 log4j.appender.requestAppender=org.apache.log4j.RollingFileAppender #log4j.appender.requestAppender.DatePattern=&#x27;.&#x27;yyyy-MM log4j.appender.requestAppender.File=$&#123;kafka.logs.dir&#125;/kafka-request.log log4j.appender.requestAppender.layout=org.apache.log4j.PatternLayout log4j.appender.requestAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.requestAppender.MaxFileSize=200MB log4j.appender.requestAppender.MaxBackupIndex=3 log4j.appender.cleanerAppender=org.apache.log4j.RollingFileAppender #log4j.appender.cleanerAppender.DatePattern=&#x27;.&#x27;yyyy-MM log4j.appender.cleanerAppender.File=$&#123;kafka.logs.dir&#125;/log-cleaner.log log4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayout log4j.appender.cleanerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.cleanerAppender.MaxFileSize=200MB log4j.appender.cleanerAppender.MaxBackupIndex=3 # Licensed to the Apache Software Foundation (ASF) under one or more # contributor license agreements. See the NOTICE file distributed with # this work for additional information regarding copyright ownership. # The ASF licenses this file to You under the Apache License, Version 2.0 # (the &quot;License&quot;); you may not use this file except in compliance with # the License. You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an &quot;AS IS&quot; BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # Unspecified loggers and loggers with additivity=true output to server.log and stdout # Note that INFO only applies to unspecified loggers, the log level of the child logger is used otherwise log4j.rootLogger=INFO, stdout, kafkaAppender log4j.appender.stdout=org.apache.log4j.ConsoleAppender log4j.appender.stdout.layout=org.apache.log4j.PatternLayout log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.kafkaAppender=org.apache.log4j.RollingFileAppender #log4j.appender.kafkaAppender.DatePattern=&#x27;.&#x27;yyyy-MM log4j.appender.kafkaAppender.File=$&#123;kafka.logs.dir&#125;/server.log log4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout log4j.appender.kafkaAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.kafkaAppender.MaxFileSize=200MB log4j.appender.kafkaAppender.MaxBackupIndex=3 log4j.appender.stateChangeAppender=org.apache.log4j.RollingFileAppender #log4j.appender.stateChangeAppender.DatePattern=&#x27;.&#x27;yyyy-MM log4j.appender.stateChangeAppender.File=$&#123;kafka.logs.dir&#125;/state-change.log log4j.appender.stateChangeAppender.layout=org.apache.log4j.PatternLayout log4j.appender.stateChangeAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.stateAppender.MaxFileSize=200MB log4j.appender.stateAppender.MaxBackupIndex=3 log4j.appender.requestAppender=org.apache.log4j.RollingFileAppender #log4j.appender.requestAppender.DatePattern=&#x27;.&#x27;yyyy-MM log4j.appender.requestAppender.File=$&#123;kafka.logs.dir&#125;/kafka-request.log log4j.appender.requestAppender.layout=org.apache.log4j.PatternLayout log4j.appender.requestAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.requestAppender.MaxFileSize=200MB log4j.appender.requestAppender.MaxBackupIndex=3 log4j.appender.cleanerAppender=org.apache.log4j.RollingFileAppender #log4j.appender.cleanerAppender.DatePattern=&#x27;.&#x27;yyyy-MM log4j.appender.cleanerAppender.File=$&#123;kafka.logs.dir&#125;/log-cleaner.log log4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayout log4j.appender.cleanerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.cleanerAppender.MaxFileSize=200MB log4j.appender.cleanerAppender.MaxBackupIndex=3 log4j.appender.controllerAppender=org.apache.log4j.RollingFileAppender #log4j.appender.controllerAppender.DatePattern=&#x27;.&#x27;yyyy-MM log4j.appender.controllerAppender.File=$&#123;kafka.logs.dir&#125;/controller.log log4j.appender.controllerAppender.layout=org.apache.log4j.PatternLayout log4j.appender.controllerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.controllerAppender.MaxFileSize=200MB log4j.appender.controllerAppender.MaxBackupIndex=3 log4j.appender.authorizerAppender=org.apache.log4j.RollingFileAppender #log4j.appender.authorizerAppender.DatePattern=&#x27;.&#x27;yyyy-MM log4j.appender.authorizerAppender.File=$&#123;kafka.logs.dir&#125;/kafka-authorizer.log log4j.appender.authorizerAppender.layout=org.apache.log4j.PatternLayout log4j.appender.authorizerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n log4j.appender.authorizerAppender.MaxFileSize=200MB log4j.appender.authorizerAppender.MaxBackupIndex=3 # Change the two lines below to adjust ZK client logging log4j.logger.org.I0Itec.zkclient.ZkClient=INFO log4j.logger.org.apache.zookeeper=INFO # Change the two lines below to adjust the general broker logging level (output to server.log and stdout) log4j.logger.kafka=INFO log4j.logger.org.apache.kafka=INFO # Change to DEBUG or TRACE to enable request logging log4j.logger.kafka.request.logger=WARN, requestAppender log4j.additivity.kafka.request.logger=false # Uncomment the lines below and change log4j.logger.kafka.network.RequestChannel$ to TRACE for additional output # related to the handling of requests #log4j.logger.kafka.network.Processor=TRACE, requestAppender #log4j.logger.kafka.server.KafkaApis=TRACE, requestAppender #log4j.additivity.kafka.server.KafkaApis=false log4j.logger.kafka.network.RequestChannel$=WARN, requestAppender log4j.additivity.kafka.network.RequestChannel$=false log4j.logger.kafka.controller=TRACE, controllerAppender log4j.additivity.kafka.controller=false log4j.logger.kafka.log.LogCleaner=INFO, cleanerAppender log4j.additivity.kafka.log.LogCleaner=false log4j.logger.state.change.logger=TRACE, stateChangeAppender log4j.additivity.state.change.logger=false # Access denials are logged at INFO level, change to DEBUG to also log allowed accesses log4j.logger.kafka.authorizer.logger=INFO, authorizerAppender log4j.additivity.kafka.authorizer.logger=falsekind: ConfigMapmetadata: labels: component: kafka-config name: kafka-config---apiVersion: v1kind: Servicemetadata: name: kafka-service labels: component: kafkaspec: ports: - port: 9092 name: server clusterIP: None selector: component: kafka---apiVersion: v1kind: Servicemetadata: name: kafka-0 labels: component: kafkaspec: type: NodePort ports: - port: 30092 name: server nodePort: 30092 targetPort: 30092 selector: statefulset.kubernetes.io/pod-name: kafka-0---apiVersion: v1kind: Servicemetadata: name: kafka-1 labels: component: kafkaspec: type: NodePort ports: - port: 30093 name: server nodePort: 30093 targetPort: 30093 selector: statefulset.kubernetes.io/pod-name: kafka-1---apiVersion: v1kind: Servicemetadata: name: kafka-2 labels: component: kafkaspec: type: NodePort ports: - port: 30094 name: server nodePort: 30094 targetPort: 30094 selector: statefulset.kubernetes.io/pod-name: kafka-2---apiVersion: apps/v1kind: StatefulSetmetadata: labels: component: kafka name: kafkaspec: serviceName: kafka-service replicas: 3 selector: matchLabels: component: kafka template: metadata: labels: component: kafka spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - xz-1.25.84.141.86 - xz-1.25.84.141.87 - xz-1.25.84.141.88 podAntiAffinity: #设置pod亲和性 requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 - labelSelector: matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签 - key: component operator: In values: - kafka topologyKey: kubernetes.io/hostname initContainers: - name: init-sysctl image: h.dr.z/rancher/busybox:1.27.2 command: - sysctl - -w - vm.max_map_count=262144 securityContext: privileged: true containers: - env: - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_NODE_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.hostIP - name: KAFKA_OPTS value: &quot;-Dkafka_mx4jenable=true -Dmx4jport=9999&quot; - name: KAFKA_HEAP_OPTS value: &quot;-Xmx4G -Xms4G&quot;# - name: zookeeper.connect# value: 10.43.33.143:2181# - name: advertised.listeners# value: PLAINTEXT://$(meta_name).kafka-service.test.svc.cluster.local:9092 image: dr.z/kafka:2.7.0-1600-smt0.3 command: [&quot;bash&quot;,&quot;-c&quot;,&quot;/opt/kafka/config/start_kafka.sh&quot;] imagePullPolicy: IfNotPresent name: kafka ports: - containerPort: 9092 name: kafka-cs# readinessProbe:# tcpSocket:# port: 9092# initialDelaySeconds: 15# timeoutSeconds: 1 volumeMounts: - mountPath: /var/run/docker.sock name: kafka-sock - mountPath: /opt/kafka/config/log4j.properties name: log4j2-conf subPath: log4j2.properties - mountPath: /opt/kafka/config/server.properties name: kafka-properties - mountPath: /opt/kafka/logs name: kafka-log - mountPath: /opt/kafka/config/start_kafka.sh name: kafka-start volumes: - name: kafka-start hostPath: path: /home/zshield/kafka/start_kafka.sh - name: kafka-log hostPath: path: /var/lib/container/BIGdata/kafka/ type: DirectoryOrCreate - name: kafka-properties hostPath: path: /var/lib/container/kafka/server.properties - configMap: items: - key: log4j2.properties path: log4j2.properties name: kafka-config name: log4j2-conf - name: kafka-sock hostPath: path: /var/run/docker.sock#---#apiVersion: v1#kind: Service#metadata:# name: kafka-service# labels:# component: kafka# namespace: test#spec:# clusterIP: None# ports:# - port: 9093# name: kafka-service# selector:# component: kafka cronjob12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970apiVersion: v1data: trans_config.yml: | base: time: &#x27;2021-07-27&#x27; #传输日期 当有time 字段，那mode不生效，优先级高 mode: 0 #模式，0是当天，1是前一天，依次类推 data_num: 100 # 每次最多查100条 index: zndbszgh_test: &#x27;http://6a04773c4af84ce08e896b34251ef1c1.apigateway.res.sgmc.sgcc.com.cn/api_szgh/test1?appCode=da7b69111b254695b79db9b2ebd26a73&amp;uuid=3d9d93b9-24d0-42aa-bee7-dd48c8461926&#x27; #索引名：api接口 # handle_status_appraise: &#x27;http://192.168.83.230:8001/a&#x27; k8s_es: ES_HOST: [&#x27;webods-data.default.svc.cluster.local:19200&#x27;] index_map: zndbszgh_test: &quot;data_quality&quot; # handle_status_appraise: &quot;status_appraise&quot; index_sort_field: zndbszgh_test: &quot;ID&quot; # handle_status_appraise: &quot;&quot;kind: ConfigMapmetadata: name: semems-cron---apiVersion: batch/v1beta1kind: CronJobmetadata: labels: run: transjob name: transjobspec: concurrencyPolicy: Forbid failedJobsHistoryLimit: 1 successfulJobsHistoryLimit: 3 startingDeadlineSeconds: 600 schedule: &quot;0 0 * * *&quot; jobTemplate: spec: template: metadata: labels: run: transjob spec: containers: - name: transjob image: **************************************************** imagePullPolicy: IfNotPresent command: [&quot;/opt/py3.6/ve1/bin/python&quot;] args: [&quot;-u&quot;,&quot;-m&quot;,&quot;main.pp2_main&quot;] volumeMounts: - mountPath: /home/zshield/bin/ruleng/alies_to_k8s/config/trans_config.yml name: trans-config subPath: trans_config.yml - mountPath: /home/zshield/bin/ruleng/alies_to_k8s/cache name: cache-dir workingDir: /home/zshield/bin/ruleng restartPolicy: OnFailure volumes: - hostPath: path: /home/zshield/logs/cache/ *********************************确保宿主机上有这个目录 type: DirectoryOrCreate name: cache-dir - configMap: items: - key: trans_config.yml path: trans_config.yml name: semems-cron name: trans-config es123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471apiVersion: v1data: log4j2.properties: | status = error internal_users.yml: | --- # This is the internal user database # The hash value is a bcrypt hash and can be generated with plugin/tools/hash.sh _meta: type: &quot;internalusers&quot; config_version: 2 # Define your internal users here zxtech: hash: &quot;$2y$12$8YN3UwVGjXCGPLFRY5.Ch.Bf1bKVmUPZ0bOreizTQANgEReyqSd5O&quot; reserved: false backend_roles: - &quot;admin&quot; description: &quot;admin user&quot; zshield: hash: &quot;$2y$12$WddVri.rlw8W0rIca1fWL.1VAh9HCxvRzkR.2jN7xIOlBSfq5zsHq&quot; reserved: false description: &quot;zshield user of SEMEMS&quot; finebi: hash: &quot;$2y$12$WddVri.rlw8W0rIca1fWL.1VAh9HCxvRzkR.2jN7xIOlBSfq5zsHq&quot; reserved: false description: &quot;finebi user of SEMEMS&quot; # Demo users roles_mapping.yml: | --- # In this file users, backendroles and hosts can be mapped to Open Distro Security roles. # Permissions for Opendistro roles are configured in roles.yml _meta: type: &quot;rolesmapping&quot; config_version: 2 # Define your roles mapping here ## Demo roles mapping all_access: reserved: false backend_roles: - &quot;admin&quot; description: &quot;Maps admin to all_access&quot; own_index: reserved: false users: - &quot;*&quot; description: &quot;Allow full access to an index named like the username&quot; logstash: reserved: false backend_roles: - &quot;logstash&quot; kibana_user: reserved: false backend_roles: - &quot;kibanauser&quot; users: - &quot;zshield&quot; - &quot;finebi&quot; description: &quot;Maps kibanauser to kibana_user&quot; readall: reserved: false backend_roles: - &quot;readall&quot; manage_snapshots: reserved: false backend_roles: - &quot;snapshotrestore&quot; kibana_server: reserved: true users: - &quot;kibanaserver&quot; zshield_role: reserved: false users: - &quot;zshield&quot; finebi_role: reserved: false users: - &quot;finebi&quot; roles.yml: | _meta: type: &quot;roles&quot; config_version: 2 # Restrict users so they can only view visualization and dashboard on kibana kibana_read_only: reserved: true # The security REST API access role is used to assign specific users access to change the security settings through the REST API. security_rest_api_access: reserved: true # Allows users to view monitors, destinations and alerts alerting_read_access: reserved: true cluster_permissions: - &#x27;cluster:admin/opendistro/alerting/alerts/get&#x27; - &#x27;cluster:admin/opendistro/alerting/destination/get&#x27; - &#x27;cluster:admin/opendistro/alerting/monitor/get&#x27; - &#x27;cluster:admin/opendistro/alerting/monitor/search&#x27; # Allows users to view and acknowledge alerts alerting_ack_alerts: reserved: true cluster_permissions: - &#x27;cluster:admin/opendistro/alerting/alerts/*&#x27; # Allows users to use all alerting functionality alerting_full_access: reserved: true cluster_permissions: - &#x27;cluster_monitor&#x27; - &#x27;cluster:admin/opendistro/alerting/*&#x27; index_permissions: - index_patterns: - &#x27;*&#x27; allowed_actions: - &#x27;indices_monitor&#x27; - &#x27;indices:admin/aliases/get&#x27; - &#x27;indices:admin/mappings/get&#x27; # Allow users to read Anomaly Detection detectors and results anomaly_read_access: reserved: true cluster_permissions: - &#x27;cluster:admin/opendistro/ad/detector/info&#x27; - &#x27;cluster:admin/opendistro/ad/detector/search&#x27; - &#x27;cluster:admin/opendistro/ad/detectors/get&#x27; - &#x27;cluster:admin/opendistro/ad/result/search&#x27; # Allows users to use all Anomaly Detection functionality anomaly_full_access: reserved: true cluster_permissions: - &#x27;cluster_monitor&#x27; - &#x27;cluster:admin/opendistro/ad/*&#x27; index_permissions: - index_patterns: - &#x27;*&#x27; allowed_actions: - &#x27;indices_monitor&#x27; - &#x27;indices:admin/aliases/get&#x27; - &#x27;indices:admin/mappings/get&#x27; # Allows users to read and download Reports reports_instances_read_access: reserved: true cluster_permissions: - &#x27;cluster:admin/opendistro/reports/instance/list&#x27; - &#x27;cluster:admin/opendistro/reports/instance/get&#x27; - &#x27;cluster:admin/opendistro/reports/menu/download&#x27; # Allows users to read and download Reports and Report-definitions reports_read_access: reserved: true cluster_permissions: - &#x27;cluster:admin/opendistro/reports/definition/get&#x27; - &#x27;cluster:admin/opendistro/reports/definition/list&#x27; - &#x27;cluster:admin/opendistro/reports/instance/list&#x27; - &#x27;cluster:admin/opendistro/reports/instance/get&#x27; - &#x27;cluster:admin/opendistro/reports/menu/download&#x27; # Allows users to all Reports functionality reports_full_access: reserved: true cluster_permissions: - &#x27;cluster:admin/opendistro/reports/definition/create&#x27; - &#x27;cluster:admin/opendistro/reports/definition/update&#x27; - &#x27;cluster:admin/opendistro/reports/definition/on_demand&#x27; - &#x27;cluster:admin/opendistro/reports/definition/delete&#x27; - &#x27;cluster:admin/opendistro/reports/definition/get&#x27; - &#x27;cluster:admin/opendistro/reports/definition/list&#x27; - &#x27;cluster:admin/opendistro/reports/instance/list&#x27; - &#x27;cluster:admin/opendistro/reports/instance/get&#x27; - &#x27;cluster:admin/opendistro/reports/menu/download&#x27; # # Define your roles here zshield_role: reserved: false cluster_permissions: - &#x27;cluster_all&#x27; index_permissions: - index_patterns: - &#x27;*&#x27; allowed_actions: - &quot;indices:admin/aliases&quot; - &quot;indices:admin/aliases/exists&quot; - &quot;indices:admin/aliases/get&quot; - &quot;indices:admin/analyze&quot; - &quot;indices:admin/cache/clear&quot; - &quot;indices:admin/close&quot; - &quot;indices:admin/create&quot; - &quot;indices:admin/exists&quot; - &quot;indices:admin/flush&quot; - &quot;indices:admin/flush*&quot; - &quot;indices:admin/forcemerge&quot; - &quot;indices:admin/get&quot; - &quot;indices:admin/mapping/put&quot; - &quot;indices:admin/mappings/fields/get&quot; - &quot;indices:admin/mappings/fields/get*&quot; - &quot;indices:admin/mappings/get&quot; - &quot;indices:admin/open&quot; - &quot;indices:admin/refresh&quot; - &quot;indices:admin/refresh*&quot; - &quot;indices:admin/rollover&quot; - &quot;indices:admin/seq_no/global_checkpoint_sync&quot; - &quot;indices:admin/settings/update&quot; - &quot;indices:admin/shards/search_shards&quot; - &quot;indices:admin/shrink&quot; - &quot;indices:admin/synced_flush&quot; - &quot;indices:admin/template/delete&quot; - &quot;indices:admin/template/get&quot; - &quot;indices:admin/template/put&quot; - &quot;indices:admin/types/exists&quot; - &quot;indices:admin/upgrade&quot; - &quot;indices:admin/validate/query&quot; - &quot;indices:admin/resolve/index&quot; - &quot;indices:data/read/explain&quot; - &quot;indices:data/read/field_caps&quot; - &quot;indices:data/read/field_caps*&quot; - &quot;indices:data/read/get&quot; - &quot;indices:data/read/mget&quot; - &quot;indices:data/read/mget*&quot; - &quot;indices:data/read/msearch&quot; - &quot;indices:data/read/msearch/template&quot; - &quot;indices:data/read/mtv&quot; - &quot;indices:data/read/mtv*&quot; - &quot;indices:data/read/scroll&quot; - &quot;indices:data/read/scroll/clear&quot; - &quot;indices:data/read/search&quot; - &quot;indices:data/read/search*&quot; - &quot;indices:data/read/search/template&quot; - &quot;indices:data/read/tv&quot; - &quot;indices:data/write/bulk&quot; - &quot;indices:data/write/bulk*&quot; - &quot;indices:data/write/index&quot; - &quot;indices:data/write/reindex&quot; - &quot;indices:data/write/update&quot; - &quot;indices:data/write/update/byquery&quot; - &quot;indices:monitor/recovery&quot; - &quot;indices:monitor/segments&quot; - &quot;indices:monitor/settings/get&quot; - &quot;indices:monitor/shard_stores&quot; - &quot;indices:monitor/stats&quot; - &quot;indices:monitor/upgrade&quot; tenant_permissions: - tenant_patterns: - &quot;global_tenant&quot; allowed_actions: - &quot;kibana_all_write&quot; finebi_role: reserved: false cluster_permissions: - &#x27;cluster_all&#x27; index_permissions: - index_patterns: - &quot;tg_status_idx-*&quot; - &quot;meter_status_idx-*&quot; - &quot;bin_power_idx-*&quot; - &quot;power_sz_idx-*&quot; - &quot;power_status_appraise_idx-*&quot; - &quot;power_data_governance_idx-*&quot; - &quot;power_model_appraise_idx-*&quot; - &quot;power_data_quality_idx-*&quot; - &quot;power_status_change_idx-*&quot; - &quot;power_err_level_idx-*&quot; - &quot;power_eyear_idx-*&quot; - &quot;power_jtype_idx-*&quot; - &quot;power_meter_err_idx-*&quot; - &quot;power_abtypes_idx-*&quot; - &quot;power_manufacturer_idx-*&quot; - &quot;power_age_month_idx-*&quot; - &quot;power_data_quality_detail_idx-*&quot; - &quot;power_data_quality_ststc_idx-*&quot; - &quot;power_whitelist_ststc_idx-*&quot; - &quot;power_whitelist_idx-*&quot; - &quot;power_data_quality_type_idx-*&quot; allowed_actions: - &quot;search&quot; - &quot;indices:admin/resolve/index&quot; tenant_permissions: - tenant_patterns: - &quot;global_tenant&quot; allowed_actions: - &quot;kibana_all_write&quot;kind: ConfigMapmetadata: labels: component: semems-webods name: semems-webods namespace: hunan---apiVersion: apps/v1kind: StatefulSetmetadata: labels: component: webods name: webods namespace: hunanspec: serviceName: &quot;webods-discovery&quot; replicas: 2 selector: matchLabels: component: webods template: metadata: labels: component: webods spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1 - node2 initContainers: - name: init-sysctl image: h.dr.z/rancher/busybox:1.27.2 command: - sysctl - -w - vm.max_map_count=262144 securityContext: privileged: true containers: - env: - name: meta_name valueFrom: fieldRef: apiVersion: v1 fieldPath: metadata.name - name: node.name value: $(meta_name).webods-discovery.default.svc.cluster.local - name: network.publish_host value: $(meta_name).webods-discovery.default.svc.cluster.local - name: TAKE_FILE_OWNERSHIP value: &quot;1&quot; - name: opendistro_security.ssl.http.enabled value: &quot;false&quot; - name: cluster.name value: &quot;zshield-k8s&quot; - name: path.data value: &quot;/usr/share/elasticsearch/data&quot; - name: path.logs value: &quot;/usr/share/elasticsearch/logs&quot; - name: bootstrap.memory_lock value: &quot;false&quot; - name: network.host value: 0.0.0.0 - name: http.port value: &quot;19200&quot; - name: transport.port value: &quot;19300&quot; - name: discovery.seed_hosts value: webods-0.webods-discovery.default.svc.cluster.local,webods-1.webods-discovery.default.svc.cluster.local - name: cluster.initial_master_nodes value: webods-0.webods-discovery.default.svc.cluster.local,webods-1.webods-discovery.default.svc.cluster.local - name: ES_JAVA_OPTS value: -Xms4g -Xmx4g image: dr.z/opendistro-es:1.13.2-1600 imagePullPolicy: IfNotPresent name: webods ports: - containerPort: 19200 name: http protocol: TCP - containerPort: 19300 name: transport protocol: TCP volumeMounts: - mountPath: /usr/share/elasticsearch/plugins/opendistro_security/securityconfig/roles_mapping.yml name: rolesmapping-conf subPath: roles_mapping.yml - mountPath: /usr/share/elasticsearch/plugins/opendistro_security/securityconfig/roles.yml name: roles-conf subPath: roles.yml - mountPath: /usr/share/elasticsearch/plugins/opendistro_security/securityconfig/internal_users.yml name: internalusers-conf subPath: internal_users.yml - mountPath: /usr/share/elasticsearch/config/log4j2.properties name: log4j2-conf subPath: log4j2.properties - mountPath: /usr/share/elasticsearch/data name: ods-data volumes: - name: ods-data hostPath: path: /home/zshield/BIGdata/es_data/ type: DirectoryOrCreate - configMap: items: - key: roles_mapping.yml path: roles_mapping.yml name: semems-webods name: rolesmapping-conf - configMap: items: - key: roles.yml path: roles.yml name: semems-webods name: roles-conf - configMap: items: - key: internal_users.yml path: internal_users.yml name: semems-webods name: internalusers-conf - configMap: items: - key: log4j2.properties path: log4j2.properties name: semems-webods name: log4j2-conf---apiVersion: v1kind: Servicemetadata: labels: component: webods name: webods-discovery namespace: hunanspec: clusterIP: None ports: - name: transport port: 19300 protocol: TCP targetPort: 19300 selector: component: webods type: ClusterIP---apiVersion: v1kind: Servicemetadata: labels: component: webods name: webods-data namespace: hunanspec: ports: - name: data port: 19200 protocol: TCP targetPort: 19200 selector: component: webods sessionAffinity: None type: ClusterIP pod test包括各种数值的获取，以及限制指标获取 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192apiVersion: v1kind: Servicemetadata: name: env-test labels: name: dapi-envars-fieldrefspec: type: NodePort ports: - port: 30095 name: server nodePort: 30096 targetPort: 30097 selector: statefulset.kubernetes.io/pod-name: dapi-envars-fieldref---apiVersion: v1kind: Podmetadata: name: dapi-envars-fieldrefspec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1 containers: - name: test-container image: dr.z/zs_power:trans_20220324 command: [ &quot;bash&quot;, &quot;-c&quot;] args: - while true; do echo -en &#x27;\\n&#x27;; printenv MY_NODE_NAME MY_POD_NAME MY_POD_NAMESPACE; printenv MY_POD_IP MY_POD_SERVICE_ACCOUNT MY_NODE_IP s; sleep 30; done; env: - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: MY_POD_SERVICE_ACCOUNT valueFrom: fieldRef: fieldPath: spec.serviceAccountName - name: MY_NODE_IP valueFrom: fieldRef: apiVersion: v1 fieldPath: status.hostIP - name: s value: a - name: MY_CPU_REQUEST valueFrom: resourceFieldRef: containerName: test-container resource: requests.cpu - name: MY_CPU_LIMIT valueFrom: resourceFieldRef: containerName: test-container resource: limits.cpu - name: MY_MEM_REQUEST valueFrom: resourceFieldRef: containerName: test-container resource: requests.memory - name: MY_MEM_LIMIT valueFrom: resourceFieldRef: containerName: test-container resource: limits.memory #when: $(MY_NODE_IP)==&quot;192.168.8.162&quot; restartPolicy: Never deployment12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273apiVersion: v1data: trans_config.yml: | base: time: &#x27;&#x27; # 传输日期 当有time 字段，那mode不生效，优先级高 例如： &#x27;2021-01-01&#x27; mode: 0 # 模式，0是当天，1是前一天，依次类推 data_num: 100 # 每次查询数量100条 save_to: [&quot;es&quot;] # 保存结果到哪里，目前实现了 kafka，以及k8s_es 例如 [&quot;es&quot;]或者[&quot;kafka&quot;] index: # ads_cst_szgh_handle_data_quality_di: &#x27;http://192.168.83.230:8001&#x27; # datawork上数据索引名与api映射关系，即索引名：api接口 ads_cst_szgh_handle_status_appraise_di: &#x27;http://192.168.83.230:8001/a&#x27; source: province: &quot;hunan&quot; # 设置本地数据源的省份，目前需求包括湖南及宁夏 hunan: batch_size: 500 # 每次拉取多少条数据 # k8s es配置 es: ES_HOST: [&#x27;zxtech:Zxod112_shining10@webods-data.default.svc.cluster.local:19200&#x27;] # es host 配置，用逗号分开 index_map: ads_cst_szgh_meter_status_di: &quot;meter_status_type&quot; # datawork 上索引与es 索引映射关系，目的是防止索引名字随时变化 ads_cst_szgh_tg_status_di: &quot;tg_status_type&quot; ads_cst_szgh_handle_status_appraise_di: &quot;status_appraise&quot; ads_cst_szgh_handle_data_quality_di: &quot;data_quality&quot; ads_cst_szgh_handle_whitelist_ststc_di: &quot;whitelist_ststc&quot; index_aip_query: # ads_cst_szgh_handle_data_quality_di: &quot;ID&quot; # api分页查询时使用的字段 ads_cst_szgh_meter_status_di: &#123;&#x27;esQueries&#x27;: [&#123;&#x27;fieldName&#x27;: &#x27;CAL_TIME&#x27;, &#x27;operator&#x27;: &#x27;=&#x27;&#125;],&#x27;sort&#x27;: &#123;&#x27;_id&#x27;: &#x27;ASC&#x27;&#125;&#125; ads_cst_szgh_tg_status_di: &#123;&#x27;esQueries&#x27;: [&#123;&#x27;fieldName&#x27;: &#x27;CAL_TIME&#x27;, &#x27;operator&#x27;: &#x27;=&#x27;&#125;],&#x27;sort&#x27;: &#123;&#x27;_id&#x27;: &#x27;ASC&#x27;&#125;&#125; ads_cst_szgh_handle_status_appraise_di: &#123;&#x27;esQueries&#x27;: [&#123;&#x27;fieldName&#x27;: &#x27;ZZ_POST_CAL_TIME&#x27;, &#x27;operator&#x27;: &#x27;=&#x27;&#125;],&#x27;sort&#x27;: &#123;&#x27;ORG_NO&#x27;: &#x27;ASC&#x27;&#125;&#125; # kafka 配置 #kafka: # KAFKA_HOST: &#x27;192.168.83.229,192.168.83.233&#x27; # kafka host配置 #kafka_topic_map: # ads_cst_szgh_handle_data_quality_di: &quot;data_quality&quot; # datawork 上索引与kafka主题映射关系，目的是防止索引或者主题名字随时变化 # ads_cst_szgh_handle_status_appraise_di: &quot;status_appraise&quot; #kafka_sort_field: # api分页查询时使用的字段 # ads_cst_szgh_handle_data_quality_di: &quot;ID&quot; # ads_cst_szgh_handle_status_appraise_di: &quot;&quot; connect.env: | LOG_DIR = &#x27;/home/zshield/bin/ruleng/alies_to_k8s/cache/&#x27; # 日志文件位置，将和缓存cache 一起映射到主机 LOG_FILENAME = &#x27;trans.log&#x27; # 这个文件存在才会写日志进去 CONSOLE_LOG_LEVEL = &#x27;INFO&#x27;kind: ConfigMapmetadata: name: semems-cron #namespace: trans1---apiVersion: apps/v1kind: Deploymentmetadata: labels: component: trans name: trans #namespace: trans1spec: replicas: 2 selector: matchLabels: component: trans template: metadata: labels: component: trans spec: containers: - command: - tail - -f - /etc/profile image: cr.registry.hn-1.res.sgmc.sgcc.com.cn/dev_csy/szgh_dev:trans_v0412 imagePullPolicy: IfNotPresent name: trans workingDir: /home/zshield/bin/ruleng cronjob k8s on spark123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869apiVersion: batch/v1beta1kind: CronJobmetadata: labels: run: pp2 #任务名称 name: pp2 #任务名称 namespace: spark-sk #命名空间spec: concurrencyPolicy: Forbid failedJobsHistoryLimit: 1 successfulJobsHistoryLimit: 3 startingDeadlineSeconds: 600 # 设置任务的执行超时时间 schedule: &quot;*/1 * * * *&quot; # 设置定时启动时间 jobTemplate: spec: template: metadata: labels: run: pp2 # 任务名称 spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node-192 # 启动主节点，krb5.conf以及keytable需要在这个节点上 containers: - name: pp2 # 任务名 image: zx-cronjob-noenv:v1 # 镜像名，目前这个镜像可以执行 imagePullPolicy: IfNotPresent command: [&quot;/bin/bash&quot;,&quot;start.sh&quot;] #启动的任务脚本 volumeMounts: - mountPath: /etc/hadoop.keytab name: hadoop-keytable - mountPath: /etc/krb5.conf name: krb5-conf - mountPath: /root/spark-3.1.1-bin-hadoop3.2/bin/start.sh #路径不变脚本名要改 name: start-pp2 serviceAccount: spark-sk restartPolicy: OnFailure hostAliases: # 所有hadoop及k8s相关的hosts - ip: &quot;192.168.81.192&quot; hostnames: - &quot;node-192&quot; - ip: &quot;192.168.81.193&quot; hostnames: - &quot;node-193&quot; - ip: &quot;192.168.83.187&quot; hostnames: - &quot;node187&quot; - ip: &quot;192.168.82.236&quot; hostnames: - &quot;node236&quot; volumes: - name: hadoop-keytable hostPath: path: /home/ksun/k8s-env/package/cronjob_p/hadoop.keytab # keytab文件位置 type: File - name: krb5-conf hostPath: path: /home/ksun/k8s-env/package/cronjob_p/krb5.conf # krb5.conf 文件位置 type: File - name: start-pp2 hostPath: path: /home/ksun/k8s-env/package/cronjob_p/start.sh # 启动脚本 type: File","categories":[],"tags":[]},{"title":"切片集群","slug":"切片集群","date":"2021-05-31T23:58:46.000Z","updated":"2021-06-02T00:14:46.306Z","comments":true,"path":"2021/06/01/切片集群/","link":"","permalink":"https://sk-xinye.github.io/2021/06/01/%E5%88%87%E7%89%87%E9%9B%86%E7%BE%A4/","excerpt":"","text":"rdb引发的问题当数据量较大时，fork 操作时常会受数据量的影响，如果数据量过大，就会导致主线程被阻塞过长时间，此时使用切片集群就是个好的方案。 如何保存更多数据 纵向扩展：升级单个 Redis 实例的资源配置，包括增加内存容量、增加磁盘容量、使用更高配置的 CPU。就像下图中，原来的实例内存是 8GB，硬盘是 50GB，纵向扩展后，内存增加到 24GB，磁盘增加到 150GB。 横向扩展：横向增加当前 Redis 实例的个数，就像下图中，原来使用 1 个 8GB 内存、50GB 磁盘的实例，现在使用三个相同配置的实例。 问题在只使用单个实例的时候，数据存在哪儿，客户端访问哪儿，都是非常明确的，但是，切片集群不可避免地涉及到多个实例的分布式管理问题。要想把切片集群用起来，我们就需要解决两大问题： 数据切片后，在多个实例之间如何分布？ 客户端怎么确定想要访问的数据在哪个实例上？ 数据切片和实例的对应分布关系实际上，切片集群是一种保存大量数据的通用机制，这个机制可以有不同的实现方案。在 Redis 3.0 之前，官方并没有针对切片集群提供具体的方案。从 3.0 开始，官方提供了一个名为 Redis Cluster 的方案，用于实现切片集群。Redis Cluster 方案中就规定了数据和实例的对应规则。 具体来说，Redis Cluster 方案采用哈希槽（Hash Slot，接下来我会直接称之为 Slot），来处理数据和实例之间的映射关系。在 Redis Cluster 方案中，一个切片集群共有 16384 个哈希槽，这些哈希槽类似于数据分区，每个键值对都会根据它的 key，被映射到一个哈希槽中。 具体的映射过程分为两大步：首先根据键值对的 key，按照CRC16 算法计算一个 16 bit 的值；然后，再用这个 16bit 值对 16384 取模，得到 0~16383 范围内的模数，每个模数代表一个相应编号的哈希槽。关于 CRC16 算法，不是这节课的重点，你简单看下链接中的资料就可以了。 哈希槽与实例的映射关系可以手动配置或者默认均匀分布在各个实例之间 123redis-cli -h 172.16.19.3 –p 6379 cluster addslots 0,1redis-cli -h 172.16.19.4 –p 6379 cluster addslots 2,3redis-cli -h 172.16.19.5 –p 6379 cluster addslots 4 客户端如何定位数据 集群的实例增减，或者是为了实现负载均衡而进行的数据重新分布，会导致哈希槽和实例的映射关系发生变化，客户端发送请求时，会收到命令执行报错信息。了解了 MOVED 和 ASK 命令，你就不会为这类报错而头疼了。","categories":[],"tags":[]},{"title":"哨兵机制","slug":"哨兵机制","date":"2021-05-31T13:22:33.000Z","updated":"2021-06-01T00:16:16.998Z","comments":true,"path":"2021/05/31/哨兵机制/","link":"","permalink":"https://sk-xinye.github.io/2021/05/31/%E5%93%A8%E5%85%B5%E6%9C%BA%E5%88%B6/","excerpt":"","text":"主库挂了怎么办如果主库挂了，我们就需要运行一个新主库，比如说把一个从库切换为主库，把它当成主库。这就涉及到三个问题： 主库真的挂了吗？ 该选择哪个从库作为主库？ 怎么把新主库的相关信息通知给从库和客户端呢？ 哨兵机制的基本流程哨兵其实就是一个运行在特殊模式下的 Redis 进程，主从库实例运行的同时，它也在运行。哨兵主要负责的就是三个任务：监控、选主（选择主库）和通知。 我们先看监控。监控是指哨兵进程在运行时，周期性地给所有的主从库发送 PING 命令，检测它们是否仍然在线运行。如果从库没有在规定时间内响应哨兵的 PING 命令，哨兵就会把它标记为“下线状态”；同样，如果主库也没有在规定时间内响应哨兵的 PING 命令，哨兵就会判定主库下线，然后开始自动切换主库的流程。 这个流程首先是执行哨兵的第二个任务，选主。主库挂了以后，哨兵就需要从很多个从库里，按照一定的规则选择一个从库实例，把它作为新的主库。这一步完成后，现在的集群里就有了新主库。 然后，哨兵会执行最后一个任务：通知。在执行通知任务时，哨兵会把新主库的连接信息发给其他从库，让它们执行 replicaof 命令，和新主库建立连接，并进行数据复制。同时，哨兵会把新主库的连接信息通知给客户端，让它们把请求操作发到新主库上。 判断主库是否下线时，不能由一个哨兵说了算，只有大多数的哨兵实例，都判断主库已经“主观下线”了，主库才会被标记为“客观下线” 如何选定新主库一般来说，我把哨兵选择新主库的过程称为“筛选 + 打分” 首先来看筛选的条件： 一般情况下，我们肯定要先保证所选的从库仍然在线运行。不过，在选主时从库正常在线，这只能表示从库的现状良好，并不代表它就是最适合做主库的。 在选主时，除了要检查从库的当前在线状态，还要判断它之前的网络连接状态。如果从库总是和主库断连，而且断连次数超出了一定的阈值，我们就有理由相信，这个从库的网络状况并不是太好，就可以把这个从库筛掉了。 打分： 第一轮：优先级最高的从库得分高。用户可以通过 slave-priority 配置项，给不同的从库设置不同优先级。 第二轮：和旧主库同步程度最接近的从库得分高。 第三轮：ID 号小的从库得分高。在优先级和复制进度都相同的情况下，ID 号最小的从库得分最高，会被选为新主库。 我们再回顾下这个流程。首先，哨兵会按照在线状态、网络状态，筛选过滤掉一部分不符合要求的从库，然后，依次按照优先级、复制进度、ID 号大小再对剩余的从库进行打分，只要有得分最高的从库出现，就把它选为新主库。小结 哨兵集群使用https://www.jianshu.com/p/42ee966f96e5 基于 pub/sub 机制的哨兵集群组成1sentinel monitor &lt;master-name&gt; &lt;ip&gt; &lt;redis-port&gt; &lt;quorum&gt; 只有订阅了同一个频道的应用，才能通过发布的消息进行信息交换。 哨兵是如何知道从库的 IP 地址和端口的呢 哨兵是如何知道从库的 IP 地址和端口的呢这是由哨兵向主库发送 INFO 命令来完成的。就像下图所示，哨兵 2 给主库发送 INFO 命令，主库接受到这个命令后，就会把从库列表返回给哨兵。接着，哨兵就可以根据从库列表中的连接信息，和每个从库建立连接，并在这个连接上持续地对从库进行监控。哨兵 1 和 3 可以通过相同的方法和从库建立连接。 基于 pub/sub 机制的客户端事件通知每个哨兵实例也提供 pub/sub 机制，客户端可以从哨兵订阅消息。 由哪个哨兵执行主从切换","categories":[],"tags":[]},{"title":"数据同步","slug":"数据同步","date":"2021-05-31T05:30:00.000Z","updated":"2021-06-01T00:16:16.998Z","comments":true,"path":"2021/05/31/数据同步/","link":"","permalink":"https://sk-xinye.github.io/2021/05/31/%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5/","excerpt":"","text":"redis高可靠性 一是数据尽量少丢失（AOF &amp; RDB） 二是服务尽量少中断 (多副本机制，增加副本冗余量，将一份数据同时保存在多个实例上)。问题：数据一致性 数据一致性保证Redis 提供了主从库模式，以保证数据副本的一致，主从库之间采用的是读写分离的方式。 读操作：主库、从库都可以接收； 写操作：首先到主库执行，然后，主库将写操作同步给从库 主从库间如何进行第一次同步现在有实例 1（ip：172.16.19.3）和实例 2（ip：172.16.19.5），我们在实例 2 上执行以下这个命令后，实例 2 就变成了实例 1 的从库，并从实例 1 上复制数据：replicaof 172.16.19.3 6379 第一阶段是主从库间建立连接、协商同步的过程，主要是为全量复制做准备。在这一步，从库和主库建立起连接，并告诉主库即将进行同步，主库确认回复后，主从库间就可以开始同步了。 在第二阶段，主库将所有数据同步给从库。从库收到数据后，在本地完成数据加载。 第三个阶段，主库会把第二阶段执行过程中新收到的写命令，再发送给从库。具体的操作是，当主库完成 RDB 文件发送后，就会把此时 replication buffer 中的修改操作发给从库，从库再重新执行这些操作。这样一来，主从库就实现同步了。 主从级联模式分担全量复制时的主库压力通过“主 - 从 - 从”模式将主库生成 RDB 和传输 RDB 的压力，以级联的方式分散到从库上。 主从库间网络断了怎么办 Redis 2.8 之前：全量复制，开销非常大 Redis 2.8 以后：增量复制 当主从库断连后，主库会把断连期间收到的写操作命令，写入 replication buffer，同时也会把这些操作命令也写入 repl_backlog_buffer 这个缓冲区。 repl_backlog_buffer 是一个环形缓冲区，主库会记录自己写到的位置，从库则会记录自己已经读到的位置。","categories":[],"tags":[]},{"title":"RDB","slug":"RDB","date":"2021-05-31T00:02:45.000Z","updated":"2021-09-10T13:07:40.747Z","comments":true,"path":"2021/05/31/RDB/","link":"","permalink":"https://sk-xinye.github.io/2021/05/31/RDB/","excerpt":"","text":"内存快照RDB:Redis DataBase(可以保证可靠性，还能在宕机时实现快速恢复的其他方法呢)我们还要考虑两个关键问题： 对哪些数据做快照？这关系到快照的执行效率问题； 做快照时，数据还能被增删改吗？这关系到 Redis 是否被阻塞，能否同时正常处理请求。 给哪些内存数据做快照Redis 的数据都在内存中，为了提供所有数据的可靠性保证，它执行的是全量快照，也就是说，把内存中的所有数据都记录到磁盘中，这就类似于给 100 个人拍合影，把每一个人都拍进照片里。这样做的好处是，一次性记录了所有数据，一个都不少。 Redis 提供了两个命令来生成 RDB 文件，分别是 save 和 bgsave。save： 在主线程中执行，会导致阻塞； bgsave：创建一个子进程，专门用于写入 RDB 文件，避免了主线程的阻塞，这也是 Redis RDB 文件生成的默认配置。 快照时数据能修改吗写时复制技术（Copy-On-Write, COW） 增量快照Redis 4.0 中提出了一个混合使用 AOF 日志和内存快照的方法 这样一来，快照不用很频繁地执行，这就避免了频繁 fork 对主线程的影响。而且，AOF 日志也只用记录两次快照间的操作，也就是说，不需要记录所有操作了，因此，就不会出现文件过大的情况了，也可以避免重写开销。 使用建议 数据不能丢失时，内存快照和 AOF 的混合使用是一个很好的选择； 如果允许分钟级别的数据丢失，可以只使用 RDB； 如果只用 AOF，优先使用 everysec 的配置选项，因为它在可靠性和性能之间取了一个平衡。","categories":[],"tags":[]},{"title":"AOF日志","slug":"AOF日志","date":"2021-05-30T23:34:12.000Z","updated":"2021-05-31T00:14:31.134Z","comments":true,"path":"2021/05/31/AOF日志/","link":"","permalink":"https://sk-xinye.github.io/2021/05/31/AOF%E6%97%A5%E5%BF%97/","excerpt":"","text":"持久化机制（aof rdb）目前，Redis 的持久化主要有两大机制 即 AOF（Append Only File）日志 RDB 快照 AOF 日志是如何实现的？ 数据库的是写前日志（Write Ahead Log, WAL），也就是说，在实际写数据前，先把修改的数据记到日志文件中，以便故障时进行恢复。 不过，AOF 日志正好相反，它是写后日志，“写后”的意思是 Redis 是先执行命令，把数据写入内存，然后才记录日志 为什么要采用写后日志呢？ 传统数据库的日志，例如 redo log（重做日志），记录的是修改后的数据，而 AOF 里记录的是 Redis 收到的每一条命令，这些命令是以文本形式保存的。 为了避免额外的检查开销，Redis 在向 AOF 里面记录日志的时候，并不会先去对这些命令进行语法检查。所以，如果先记日志再执行命令的话，日志中就有可能记录了错误的命令，Redis 在使用日志恢复数据时，就可能会出错。 它是在命令执行后才记录日志，所以不会阻塞当前的写操作。 潜在问题？ 首先，如果刚执行完一个命令，还没有来得及记日志就宕机了，那么这个命令和相应的数据就有丢失的风险。如果此时 Redis 是用作缓存，还可以从后端数据库重新读入数据进行恢复，但是，如果 Redis 是直接用作数据库的话，此时，因为命令没有记入日志，所以就无法用日志进行恢复了。 其次，AOF 虽然避免了对当前命令的阻塞，但可能会给下一个操作带来阻塞风险。这是因为，AOF 日志也是在主线程中执行的，如果在把日志文件写入磁盘时，磁盘写压力大，就会导致写盘很慢，进而导致后续的操作也无法执行了。 三种写回策略 Always，同步写回：每个写命令执行完，立马同步地将日志写回磁盘； Everysec，每秒写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，每隔一秒把缓冲区中的内容写入磁盘； No，操作系统控制的写回：每个写命令执行完，只是先把日志写到 AOF 文件的内存缓冲区，由操作系统决定何时将缓冲区内容写回磁盘。 日志文件太大了怎么办？（AOF 重写机制） 和 AOF 日志由主线程写回不同，重写过程是由后台子进程 bgrewriteaof 来完成的，这也是为了避免阻塞主线程，导致数据库性能下降。 我把重写的过程总结为“一个拷贝，两处日志”。 “一个拷贝”就是指，每次执行重写时，主线程 fork 出后台的 bgrewriteaof 子进程。此时，fork 会把主线程的内存拷贝一份给 bgrewriteaof 子进程，这里面就包含了数据库的最新数据。然后，bgrewriteaof 子进程就可以在不影响主线程的情况下，逐一把拷贝的数据写成操作，记入重写日志。 因为主线程未阻塞，仍然可以处理新来的操作。此时，如果有写操作，第一处日志就是指正在使用的 AOF 日志，Redis 会把这个操作写到它的缓冲区。这样一来，即使宕机了，这个 AOF 日志的操作仍然是齐全的，可以用于恢复。 而第二处日志，就是指新的 AOF 重写日志。这个操作也会被写到重写日志的缓冲区。这样，重写日志也不会丢失最新的操作。等到拷贝数据的所有操作记录重写完成后，重写日志记录的这些最新操作也会写入新的 AOF 文件，以保证数据库最新状态的记录。此时，我们就可以用新的 AOF 文件替代旧文件了。 总结来说，每次 AOF 重写时，Redis 会先执行一个内存拷贝，用于重写；然后，使用两个日志保证在重写过程中，新写入的数据不会丢失。而且，因为 Redis 采用额外的线程进行数据重写，所以，这个过程并不会阻塞主线程。","categories":[],"tags":[]},{"title":"高性能IO模型","slug":"高性能IO模型","date":"2021-05-30T13:32:18.000Z","updated":"2021-06-17T00:11:06.145Z","comments":true,"path":"2021/05/30/高性能IO模型/","link":"","permalink":"https://sk-xinye.github.io/2021/05/30/%E9%AB%98%E6%80%A7%E8%83%BDIO%E6%A8%A1%E5%9E%8B/","excerpt":"","text":"单线程IO？ Redis 的网络 IO 和键值对读写是由一个线程来完成的，这也是 Redis 对外提供键值存储服务的主要流程。 但 Redis 的其他功能，比如持久化、异步删除、集群数据同步等，其实是由额外的线程执行的。 Redis 为什么用单线程？多线程的开销 一个关键的瓶颈在于，系统中通常会存在被多线程同时访问的共享资源，比如一个共享的数据结构。当有多个线程要修改这个共享资源时，为了保证共享资源的正确性，就需要有额外的机制进行保证，而这个额外的机制，就会带来额外的开销。 采用多线程开发一般会引入同步原语来保护共享资源的并发访问，这也会降低系统代码的易调试性和可维护性。为了避免这些问题，Redis 直接采用了单线程模式。 单线程 Redis 为什么那么快？ 一方面，Redis 的大部分操作在内存上完成，再加上它采用了高效的数据结构，例如哈希表和跳表，这是它实现高性能的一个重要原因。 另一方面，就是 Redis 采用了多路复用机制，使其在网络 IO 操作中能并发处理大量的客户端请求，实现高吞吐率。 基本 IO 模型与阻塞点以Get 请求为例，需要监听客户端请求（bind/listen），和客户端建立连接（accept），从 socket 中读取请求（recv），解析客户端发送请求（parse），根据请求类型读取键值数据（get），最后给客户端返回结果，即向 socket 中写回数据（send）。 基于多路复用的高性能 I/O 模型 为了在请求到达时能通知到 Redis 线程，select/epoll 提供了基于事件的回调机制，即针对不同事件的发生，调用相应的处理函数。","categories":[],"tags":[]},{"title":"数据结构","slug":"数据结构","date":"2021-05-30T07:37:42.000Z","updated":"2021-05-31T00:14:31.134Z","comments":true,"path":"2021/05/30/数据结构/","link":"","permalink":"https://sk-xinye.github.io/2021/05/30/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/","excerpt":"","text":"redis 为什么这么快 一方面，这是因为它是内存数据库，所有操作都在内存上完成，内存的访问速度本身就很快。 另一方面，这要归功于它的数据结构。这是因为，键值对是按一定的数据结构来组织的，操作键值对最终就是对数据结构进行增删改查操作，所以高效的数据结构是 Redis 快速处理数据的基础。 数据结构 简单动态字符串 双向链表 压缩列表 哈希表 跳表 整数数组 键和值用什么结构组织？ 为了实现从键到值的快速访问，Redis 使用了一个哈希表来保存所有键值对。 一个哈希表，其实就是一个数组，数组的每个元素称为一个哈希桶。所以，我们常说，一个哈希表是由多个哈希桶组成的，每个哈希桶中保存了键值对数据。 一个潜在的风险点，那就是哈希表的冲突问题和 rehash 可能带来的操作阻塞。 为什么哈希表操作变慢了？ Redis 解决哈希冲突的方式，就是链式哈希。链式哈希也很容易理解，就是指同一个哈希桶中的多个元素用一个链表来保存，它们之间依次用指针连接。 渐进式rehash Redis 默认使用了两个全局哈希表：哈希表 1 和哈希表 2。 当你刚插入数据时，默认使用哈希表 1，此时的哈希表 2 并没有被分配空间。 给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍； 把哈希表 1 中的数据重新映射并拷贝到哈希表 2 中(Redis 仍然正常处理客户端请求，每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中,等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries)； 集合数据操作效率 整数数组和双向链表也很常见，它们的操作特征都是顺序读写，也就是通过数组下标或者链表的指针逐个元素访问，操作复杂度基本是 O(N)，操作效率比较低； 压缩列表实际上类似于一个数组，数组中的每一个元素都对应保存一个数据。和数组不同的是，压缩列表在表头有三个字段 zlbytes、zltail 和 zllen，分别表示列表长度、列表尾的偏移量和列表中的 entry 个数；压缩列表在表尾还有一个 zlend，表示列表结束。 跳表在链表的基础上，增加了多级索引，通过索引位置的几个跳转，实现数据的快速定位，当数据量很大时，跳表的查找复杂度就是 O(logN) 不同集合操作效率","categories":[],"tags":[]},{"title":"基本架构","slug":"基本架构","date":"2021-05-30T07:08:46.000Z","updated":"2021-05-31T00:14:31.134Z","comments":true,"path":"2021/05/30/基本架构/","link":"","permalink":"https://sk-xinye.github.io/2021/05/30/%E5%9F%BA%E6%9C%AC%E6%9E%B6%E6%9E%84/","excerpt":"","text":"redis 全景知识图 “两大维度”就是指系统维度和应用维度，“三大主线”也就是指高性能、高可靠和高可扩展（可以简称为“三高”）。 高性能主线，包括线程模型、数据结构、持久化、网络框架； 高可靠主线，包括主从复制、哨兵机制； 高可扩展主线，包括数据分片、负载均衡。 问题画像 整体架构 Redis 主要通过网络框架进行访问，而不再是动态库了，这也使得 Redis 可以作为一个基础性的网络服务进行访问，扩大了 Redis 的应用范围。 Redis 数据模型中的 value 类型很丰富，因此也带来了更多的操作接口，例如面向列表的 LPUSH/LPOP，面向集合的 SADD/SREM 等。在下节课，我将和你聊聊这些 value 模型背后的数据结构和操作效率，以及它们对 Redis 性能的影响。 Redis 的持久化模块能支持两种方式：日志（AOF）和快照（RDB），这两种持久化方式具有不同的优劣势，影响到 Redis 的访问性能和可靠性。 Redis 支持高可靠集群和高可扩展集群，因此，Redis 中包含了相应的集群功能支撑模块。","categories":[],"tags":[]},{"title":"总览","slug":"总览","date":"2021-05-29T13:45:40.000Z","updated":"2023-02-04T02:33:07.962Z","comments":true,"path":"2021/05/29/总览/","link":"","permalink":"https://sk-xinye.github.io/2021/05/29/%E6%80%BB%E8%A7%88/","excerpt":"","text":"协议","categories":[],"tags":[]},{"title":"容器网络","slug":"容器网络","date":"2021-05-27T13:26:42.000Z","updated":"2021-05-29T13:45:31.835Z","comments":true,"path":"2021/05/27/容器网络/","link":"","permalink":"https://sk-xinye.github.io/2021/05/27/%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C/","excerpt":"","text":"概念容器就是 Container，而 Container 的另一个意思是集装箱。其实容器的思想就是要变成软件交付的集装箱。 一种是看起来是隔离的技术，称为 namespace，也即每个 namespace 中的应用看到的是不同的 IP 地址、用户空间、程号等。 另一种是用起来是隔离的技术，称为 cgroup，也即明明整台机器有很多的 CPU、内存，而一个应用只能用其中的一部分。 namespace 网络的 namespace 由 ip netns 命令操作。它可以创建、删除、查询 namespace。 创建 ip netns add routerns 打开forward 开关，目的是为了可以转发 ip netns exec routerns sysctl -w net.ipv4.ip_forward=1 初始化一下 iptables，因为这里面要配置 NAT 规则。 ip netns exec routerns iptables-save -c ip netns exec routerns iptables-restore -c 路由器需要有一张网卡连到 br0 上，因而要创建一个网卡。ovs-vsctl – add-port br0 taprouter – set Interface taprouter type=internal – set Interface taprouter external-ids:iface-status=active – set Interface taprouter external-ids:attached-mac=fa:16:3e:84:6e:cc 这个网络创建完了，但是是在 namespace 外面的，如何进去呢？可以通过这个命令：ip link set taprouter netns routerns 要给这个网卡配置一个 IP 地址，当然应该是虚拟机网络的网关地址。例如虚拟机私网网段为 192.168.1.0/24，网关的地址往往为 192.168.1.1。ip netns exec routerns ip -4 addr add 192.168.1.1/24 brd 192.168.1.255 scope global dev taprouter 为了访问外网，还需要另一个网卡连在外网网桥 br-ex 上，并且塞在 namespace 里面。 ovs-vsctl – add-port br-ex taprouterex – set Interface taprouterex type=internal – set Interface taprouterex external-ids:iface-status=active – set Interface taprouterex external-ids:attached-mac=fa:16:3e:68:12:c0 ip link set taprouterex netns routerns 我们还需要为这个网卡分配一个地址，这个地址应该和物理外网网络在一个网段。假设物理外网为 16.158.1.0/24，可以分配一个外网地址 16.158.1.100/24。ip netns exec routerns ip -4 addr add 16.158.1.100/24 brd 16.158.1.255 scope global dev taprouterex 接下来，既然是路由器，就需要配置路由表，路由表是这样的： ip netns exec routerns route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 16.158.1.1 0.0.0.0 UG 0 0 0 taprouterex 192.168.1.0 0.0.0.0 255.255.255.0 U 0 0 0 taprouter 16.158.1.0 0.0.0.0 255.255.255.0 U 0 0 0 taprouterex 机制网络（cgroup）cgroup 全称 control groups，是 Linux 内核提供的一种可以限制、隔离进程使用的资源机制。 cgroup 提供了一个虚拟文件系统，作为进行分组管理和各子系统设置的用户接口。要使用 cgroup，必须挂载 cgroup 文件系统，一般情况下都是挂载到 /sys/fs/cgroup 目录下。 总结 容器是一种比虚拟机更加轻量级的隔离方式，主要通过 namespace 和 cgroup 技术进行资源的隔离，namespace 用于负责看起来隔离，cgroup 用于负责用起来隔离。 容器网络连接到物理网络的方式和虚拟机很像，通过桥接的方式实现一台物理机上的容器进行相互访问，如果要访问外网，最简单的方式还是通过 NAT。","categories":[],"tags":[]},{"title":"状态模式","slug":"状态模式","date":"2021-05-27T13:14:31.000Z","updated":"2021-05-28T00:11:15.465Z","comments":true,"path":"2021/05/27/状态模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/27/%E7%8A%B6%E6%80%81%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 有限状态机，英文翻译是 Finite State Machine，缩写为 FSM，简称为状态机。状态机有 3 个组成部分：状态（State）、事件（Event）、动作（Action）。其中，事件也称为转移条件（Transition Condition）。事件触发状态的转移及动作的执行。不过，动作不是必须的，也可能只转移状态，不执行任何动作。","categories":[],"tags":[]},{"title":"迭代器模式","slug":"迭代器模式","date":"2021-05-27T13:09:11.000Z","updated":"2021-05-28T00:11:15.465Z","comments":true,"path":"2021/05/27/迭代器模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/27/%E8%BF%AD%E4%BB%A3%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 迭代器模式（Iterator Design Pattern），也叫作游标模式（Cursor Design Pattern） 迭代器模式，也叫游标模式。它用来遍历集合对象。这里说的“集合对象”，我们也可以叫“容器”“聚合对象”，实际上就是包含一组对象的对象，比如，数组、链表、树、图、跳表。 一个完整的迭代器模式，一般会涉及容器和容器迭代器两部分内容。为了达到基于接口而非实现编程的目的，容器又包含容器接口、容器实现类，迭代器又包含迭代器接口、迭代器实现类。容器中需要定义 iterator() 方法，用来创建迭代器。迭代器接口中需要定义 hasNext()、currentItem()、next() 三个最基本的方法。容器对象通过依赖注入传递到迭代器类中。 遍历集合一般有三种方式：for 循环、foreach 循环、迭代器遍历。后两种本质上属于一种，都可以看作迭代器遍历。相对于 for 循环遍历，利用迭代器来遍历有下面三个优势： 迭代器模式封装集合内部的复杂数据结构，开发者不需要了解如何遍历，直接使用容器提供的迭代器即可； 迭代器模式将集合对象的遍历操作从集合类中拆分出来，放到迭代器类中，让两者的职责更加单一； 迭代器模式让添加新的遍历算法更加容易，更符合开闭原则。除此之外，因为迭代器都实现自相同的接口，在开发中，基于接口而非实现编程，替换迭代器也变得更加容易。","categories":[],"tags":[]},{"title":"云计算中网络--虚拟化技术","slug":"云计算中网络-虚拟化技术","date":"2021-05-27T01:58:48.000Z","updated":"2023-02-04T02:33:07.941Z","comments":true,"path":"2021/05/27/云计算中网络-虚拟化技术/","link":"","permalink":"https://sk-xinye.github.io/2021/05/27/%E4%BA%91%E8%AE%A1%E7%AE%97%E4%B8%AD%E7%BD%91%E7%BB%9C-%E8%99%9A%E6%8B%9F%E5%8C%96%E6%8A%80%E6%9C%AF/","excerpt":"","text":"概念 我们常把物理机比喻为自己拿地盖房子，而虚拟机则相当于购买公寓，更加灵活方面，随时可买可卖。 qemu-kvm。从名字上来讲，emu 就是 Emulator（模拟器）的意思，主要会模拟 CPU、内存、网络、硬盘，使得虚拟机感觉自己在使用独立的设备，但是真正使用的时候，当然还是使用物理的设备。 KVM（Kernel Virtual Machine）是Linux的一个内核驱动模块，它能够让Linux主机成为一个Hypervisor（虚拟机监控器）。 QEMU（quick emulator)本身并不包含或依赖KVM模块，而是一套由Fabrice Bellard编写的模拟计算机的自由软件。QEMU虚拟机是一个纯软件的实现，可以在没有KVM模块的情况下独立运行，但是性能比较低。QEMU有整套的虚拟机实现，包括处理器虚拟化、内存虚拟化以及I/O设备的虚拟化。 KVM只模拟CPU和内存，因此一个客户机操作系统可以在宿主机上跑起来，但是你看不到它，无法和它沟通。于是，有人修改了QEMU代码，把他模拟CPU、内存的代码换成KVM，而网卡、显示器等留着，因此QEMU+KVM就成了一个完整的虚拟化平台。 KVM只是内核模块，用户并没法直接跟内核模块交互，需要借助用户空间的管理工具，而这个工具就是QEMU。KVM和QEMU相辅相成，QEMU通过KVM达到了硬件虚拟化的速度，而KVM则通过QEMU来模拟设备。对于KVM来说，其匹配的用户空间工具并不仅仅只有QEMU，还有其他的，比如RedHat开发的libvirt、virsh、virt-manager等，QEMU并不是KVM的唯一选择。 所以简单直接的理解就是：QEMU是个计算机模拟器，而KVM为计算机的模拟提供加速功能。 虚拟网卡设备tun/tap(操作系统内核中的虚拟网络设备) tun是三层设备，其封装的外层是IP头 tap是二层设备，其封装的外层是以太网帧(frame)头 tun是PPP点对点设备，没有MAC地址 tap是以太网设备，有MAC地址 tap比tun更接近于物理网卡，可以认为，tap设备等价于去掉了硬件功能的物理网卡 tap设备通常用来连接其它网络设备(它更像网卡)换句话说，tap设备通常接入到虚拟交换机(bridge)上作为局域网的一个节点 tun设备通常用来结合用户空间程序实现再次封装.tun设备通常用来实现三层的ip隧道 VLAN(Virtual Local Area Network，即“虚拟局域网”) 隔离广播域，让每个节点（比如电脑、手机）不需要收到太多无关的广播包，从而减少计算性能和网络带宽的无谓消耗。从而保证局域网的性能。 隔离常见病毒与攻击，这样即使某个主机感染了arp攻击病毒、dhcp攻击病毒等常见局域网病毒，影响的范围也只限于本vlan，不会影响到其他vlan，可以将故障限制在比较小的范围。一来造成的影响小，二来排查故障也更加容易。 虚拟网卡原理 虚拟机是物理机上跑着的一个软件。这个软件可以像其他应用打开文件一样，打开一个称为 TUN/TAP 的 Char Dev（字符设备文件）。打开了这个字符设备文件之后，在物理机上就能看到一张虚拟 TAP 网卡。 简单的说就是通过qemu-kvm技术 通过打开操作系统的虚拟网络设备TUN-TAP 来创建一个虚拟网卡TAP,接收虚拟化软件的文件流并转化为网络包交给TCP/IP协议栈，然后交由虚拟网卡TAP 发出去，也就成为了真正的网络包 虚拟网卡连接到云中 共享：尽管每个虚拟机都会有一个或者多个虚拟网卡，但是物理机上可能只有有限的网卡。那这么多虚拟网卡如何共享同一个出口？ 隔离：分两个方面，一个是安全隔离，两个虚拟机可能属于两个用户，那怎么保证一个用户的数据不被另一个用户窃听？一个是流量隔离，两个虚拟机，如果有一个疯狂下片，会不会导致另外一个上不了网？ 互通：分两个方面，一个是如果同一台机器上的两个虚拟机，属于同一个用户的话，这两个如何相互通信？另一个是如果不同物理机上的两个虚拟机，属于同一个用户的话，这两个如何相互通信？ 灵活：虚拟机和物理不同，会经常创建、删除，从一个机器漂移到另一台机器，有的互通、有的不通等等，灵活性比物理网络要好得多，需要能够灵活配置。 共享与互通问题 一台物理机上的多个虚拟机，通过虚拟交换机（网桥），linux上命令brctl,可以创建虚拟的网桥brctl addbr br0。这样多台虚拟机配置相同的子网网段就可以相互通信了。 如何与外界通信呢？ 桥接（如果使用桥接网络，当你登录虚拟机里看 IP 地址的时候会发现，你的虚拟机的地址和你的笔记本电脑的，以及你旁边的同事的电脑的网段是一个网段） Net(虚拟机的网络是虚拟机的，物理机的网络是物理机的，两个不相同。虚拟机要想访问物理机的时候，需要将地址 NAT 成为物理机的地址) 隔离问题 brctl 创建的网桥也是支持 VLAN 功能的，可以设置两个虚拟机的 tag，这样在这个虚拟网桥上，两个虚拟机是不互通的。 外界互通（由于 brctl 创建的网桥上面的 tag 是没办法在网桥之外的范围内起作用的，因此我们需要使用命令 vconfig）。通过使用vconfig，可以基于物理网卡 eth0 创建带 VLAN 的虚拟网卡，所有从这个虚拟网卡出去的包，都带这个 VLAN，如果这样，跨物理机的互通和隔离就可以通过这个网卡来实现。","categories":[],"tags":[]},{"title":"职责链模式","slug":"职责链模式","date":"2021-05-26T23:18:58.000Z","updated":"2021-05-27T00:11:33.905Z","comments":true,"path":"2021/05/27/职责链模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/27/%E8%81%8C%E8%B4%A3%E9%93%BE%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 Chain Of Responsibility Design Pattern 将请求的发送和接收解耦，让多个接收对象都有机会处理这个请求。将这些接收对象串成一条链，并沿着这条链传递这个请求，直到链上的某个接收对象能够处理它为止。 实现方式 为每个handler 设计继任者，即 通过链表来存储 通过数组存储，需要执行时，直接遍历，当然也可以处理链上所有结果","categories":[],"tags":[]},{"title":"策略模式","slug":"策略模式","date":"2021-05-26T13:51:59.000Z","updated":"2021-05-26T14:03:04.787Z","comments":true,"path":"2021/05/26/策略模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/26/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 最常见的应用场景是，利用它来避免冗长的 if-else 或 switch 分支判断。不过，它的作用还不止如此。它也可以像模板模式那样，提供框架的扩展点等等。 定义一族算法类，将每个算法分别封装起来，让它们可以互相替换。策略模式可以使算法的变化独立于使用它们的客户端（这里的客户端代指使用算法的代码）。 工厂模式是解耦对象的创建和使用，观察者模式是解耦观察者和被观察者。策略模式跟两者类似，也能起到解耦的作用，不过，它解耦的是策略的定义、创建、使用这三部分。 实现123456789101112131415161718192021222324252627282930// 策略的定义public interface DiscountStrategy &#123; double calDiscount(Order order);&#125;// 省略NormalDiscountStrategy、GrouponDiscountStrategy、PromotionDiscountStrategy类代码...// 策略的创建public class DiscountStrategyFactory &#123; private static final Map&lt;OrderType, DiscountStrategy&gt; strategies = new HashMap&lt;&gt;(); static &#123; strategies.put(OrderType.NORMAL, new NormalDiscountStrategy()); strategies.put(OrderType.GROUPON, new GrouponDiscountStrategy()); strategies.put(OrderType.PROMOTION, new PromotionDiscountStrategy()); &#125; public static DiscountStrategy getDiscountStrategy(OrderType type) &#123; return strategies.get(type); &#125;&#125;// 策略的使用public class OrderService &#123; public double discount(Order order) &#123; OrderType type = order.getType(); DiscountStrategy discountStrategy = DiscountStrategyFactory.getDiscountStrategy(type); return discountStrategy.calDiscount(order); &#125;&#125; 重点 策略类的定义比较简单，包含一个策略接口和一组实现这个接口的策略类。 策略的创建由工厂类来完成，封装策略创建的细节。 策略模式包含一组策略可选，客户端代码如何选择使用哪个策略，有两种确定方法：编译时静态确定和运行时动态确定。其中，“运行时动态确定”才是策略模式最典型的应用场景。","categories":[],"tags":[]},{"title":"模板模式","slug":"模板模式","date":"2021-05-26T13:13:32.000Z","updated":"2021-05-26T14:03:04.787Z","comments":true,"path":"2021/05/26/模板模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/26/%E6%A8%A1%E6%9D%BF%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 模板模式主要是用来解决复用和扩展两个问题。 全称是模板方法设计模式，英文是 Template Method Design Pattern。 模板方法模式在一个方法中定义一个算法骨架，并将某些步骤推迟到子类中实现。模板方法模式可以让子类在不改变算法整体结构的情况下，重新定义算法中的某些步骤。 重点 模板方法模式在一个方法中定义一个算法骨架，并将某些步骤推迟到子类中实现。模板方法模式可以让子类在不改变算法整体结构的情况下，重新定义算法中的某些步骤。这里的“算法”，我们可以理解为广义上的“业务逻辑”，并不特指数据结构和算法中的“算法”。这里的算法骨架就是“模板”，包含算法骨架的方法就是“模板方法”，这也是模板方法模式名字的由来。 在模板模式经典的实现中，模板方法定义为 final，可以避免被子类重写。需要子类重写的方法定义为 abstract，可以强迫子类去实现。不过，在实际项目开发中，模板模式的实现比较灵活，以上两点都不是必须的。 模板模式有两大作用：复用和扩展。其中，复用指的是，所有的子类可以复用父类中提供的模板方法的代码。扩展指的是，框架通过模板模式提供功能扩展点，让框架用户可以在不修改框架源码的情况下，基于扩展点定制化框架的功能。 回调与模板区别回调的原理解析A 类事先注册某个函数 F 到 B 类，A 类在调用 B 类的 P 函数的时候，B 类反过来调用 A 类注册给它的 F 函数。这里的 F 函数就是“回调函数”。A 调用 B，B 反过来又调用 A，这种调用机制就叫作“回调”。 123456789101112131415161718192021222324public interface ICallback &#123; void methodToCallback();&#125;public class BClass &#123; public void process(ICallback callback) &#123; //... callback.methodToCallback(); //... &#125;&#125;public class AClass &#123; public static void main(String[] args) &#123; BClass b = new BClass(); b.process(new ICallback() &#123; //回调对象 @Override public void methodToCallback() &#123; System.out.println(&quot;Call back me.&quot;); &#125; &#125;); &#125;&#125; 实际上，回调不仅可以应用在代码设计上，在更高层次的架构设计上也比较常用。比如，通过三方支付系统来实现支付功能，用户在发起支付请求之后，一般不会一直阻塞到支付结果返回，而是注册回调接口（类似回调函数，一般是一个回调用的 URL）给三方支付系统，等三方支付系统执行完成之后，将结果通过回调接口返回给用户。 回调可以分为同步回调和异步回调（或者延迟回调）。同步回调指在函数返回之前执行回调函数；异步回调指的是在函数返回之后执行回调函数。上面的代码实际上是同步回调的实现方式，在 process() 函数返回之前，执行完回调函数 methodToCallback()。而上面支付的例子是异步回调的实现方式，发起支付之后不需要等待回调接口被调用就直接返回。从应用场景上来看，同步回调看起来更像模板模式，异步回调看起来更像观察者模式。 从代码实现上来看，回调和模板模式完全不同。回调基于组合关系来实现，把一个对象传递给另一个对象，是一种对象之间的关系；模板模式基于继承关系来实现，子类重写父类的抽象方法，是一种类之间的关系。","categories":[],"tags":[]},{"title":"观察者模式","slug":"观察者模式","date":"2021-05-25T23:42:33.000Z","updated":"2021-05-26T14:03:04.787Z","comments":true,"path":"2021/05/26/观察者模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/26/%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 观察者模式（Observer Design Pattern）也被称为发布订阅模式（Publish-Subscribe Design Pattern）。 在对象之间定义一个一对多的依赖，当一个对象状态改变的时候，所有依赖的对象都会自动收到通知 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public interface Subject &#123; void registerObserver(Observer observer); void removeObserver(Observer observer); void notifyObservers(Message message);&#125;public interface Observer &#123; void update(Message message);&#125;public class ConcreteSubject implements Subject &#123; private List&lt;Observer&gt; observers = new ArrayList&lt;Observer&gt;(); @Override public void registerObserver(Observer observer) &#123; observers.add(observer); &#125; @Override public void removeObserver(Observer observer) &#123; observers.remove(observer); &#125; @Override public void notifyObservers(Message message) &#123; for (Observer observer : observers) &#123; observer.update(message); &#125; &#125;&#125;public class ConcreteObserverOne implements Observer &#123; @Override public void update(Message message) &#123; //TODO: 获取消息通知，执行自己的逻辑... System.out.println(&quot;ConcreteObserverOne is notified.&quot;); &#125;&#125;public class ConcreteObserverTwo implements Observer &#123; @Override public void update(Message message) &#123; //TODO: 获取消息通知，执行自己的逻辑... System.out.println(&quot;ConcreteObserverTwo is notified.&quot;); &#125;&#125;public class Demo &#123; public static void main(String[] args) &#123; ConcreteSubject subject = new ConcreteSubject(); subject.registerObserver(new ConcreteObserverOne()); subject.registerObserver(new ConcreteObserverTwo()); subject.notifyObservers(new Message()); &#125;&#125; 基于不同应用场景的不同实现方式观察者模式的应用场景非常广泛，小到代码层面的解耦，大到架构层面的系统解耦，再或者一些产品的设计思路，都有这种模式的影子，比如: 邮件订阅、RSS Feeds，本质上都是观察者模式。 不同的应用场景和需求下，这个模式也有截然不同的实现方式， 有同步阻塞的实现方式，也有异步非阻塞的实现方式； 有进程内的实现方式，也有跨进程的实现方式。 同步阻塞同步阻塞是最经典的实现方式，主要是为了代码解耦 异步非阻塞进程间的观察者模式解耦更加彻底，一般是基于消息队列来实现，用来实现不同进程间的被观察者和观察者之间的交互","categories":[],"tags":[]},{"title":"享元模式","slug":"享元模式","date":"2021-05-25T13:39:03.000Z","updated":"2021-05-25T14:07:33.222Z","comments":true,"path":"2021/05/25/享元模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/25/%E4%BA%AB%E5%85%83%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 所谓“享元”，顾名思义就是被共享的单元。享元模式的意图是复用对象，节省内存，前提是享元对象是不可变对象。 具体来讲，当一个系统中存在大量重复对象的时候，如果这些重复的对象是不可变对象，我们就可以利用享元模式将对象设计成享元，在内存中只保留一份实例，供多处代码引用。这样可以减少内存中对象的数量，起到节省内存的目的。实际上，不仅仅相同对象可以设计成享元，对于相似对象，我们也可以将这些对象中相同的部分（字段）提取出来，设计成享元，让这些大量相似对象引用这些享元。 实际上，它的代码实现非常简单，主要是通过工厂模式，在工厂类中，通过一个 Map 来缓存已经创建过的享元对象，来达到复用的目的。 享元模式 vs 单例、缓存、对象池 应用单例模式是为了保证对象全局唯一。 应用享元模式是为了实现对象复用，节省内存。 缓存是为了提高访问效率，而非复用。 池化技术中的“复用”理解为“重复使用”，主要是为了节省时间（比如从数据库池中取一个连接，不需要重新创建）。 场景 java Integer 12345public static Integer valueOf(int i) &#123; if (i &gt;= IntegerCache.low &amp;&amp; i &lt;= IntegerCache.high) return IntegerCache.cache[i + (-IntegerCache.low)]; return new Integer(i);&#125; java string 123456String s1 = &quot;小争哥&quot;;String s2 = &quot;小争哥&quot;;String s3 = new String(&quot;小争哥&quot;);System.out.println(s1 == s2); trueSystem.out.println(s1 == s3); false 问题实际上，享元模式对 JVM 的垃圾回收并不友好。因为享元工厂类一直保存了对享元对象的引用，这就导致享元对象在没有任何代码使用的情况下，也并不会被 JVM 垃圾回收机制自动回收掉。因此，在某些情况下，如果对象的生命周期很短，也不会被密集使用，利用享元模式反倒可能会浪费更多的内存。所以，除非经过线上验证，利用享元模式真的可以大大节省内存，否则，就不要过度使用这个模式，为了一点点内存的节省而引入一个复杂的设计模式，得不偿失啊。","categories":[],"tags":[]},{"title":"组合模式","slug":"组合模式","date":"2021-05-25T00:01:25.000Z","updated":"2021-05-25T00:10:51.405Z","comments":true,"path":"2021/05/25/组合模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/25/%E7%BB%84%E5%90%88%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 组合模式的设计思路，与其说是一种设计模式，倒不如说是对业务场景的一种数据结构和算法的抽象。其中，数据可以表示成树这种数据结构，业务需求可以通过在树上的递归遍历算法来实现。 应用场景组合模式，将一组对象组织成树形结构，将单个对象和组合对象都看做树中的节点，以统一处理逻辑，并且它利用树形结构的特点，递归地处理每个子树，依次简化代码实现。使用组合模式的前提在于，你的业务场景必须能够表示成树形结构。所以，组合模式的应用场景也比较局限，它并不是一种很常用的设计模式。","categories":[],"tags":[]},{"title":"门面模式","slug":"门面模式","date":"2021-05-24T23:55:37.000Z","updated":"2021-05-25T00:10:51.405Z","comments":true,"path":"2021/05/25/门面模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/25/%E9%97%A8%E9%9D%A2%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 门面模式原理和实现都特别简单，应用场景也比较明确，主要在接口设计方面使用。门面模式为子系统提供一组统一的接口，定义一组高层接口让子系统更易用。 应用场景 解决易用性问题 解决性能问题 解决分布式事务问题 我们知道，类、模块、系统之间的“通信”，一般都是通过接口调用来完成的。接口设计的好坏，直接影响到类、模块、系统是否好用。所以，我们要多花点心思在接口设计上。我经常说，完成接口设计，就相当于完成了一半的开发任务。只要接口设计得好，那代码就差不到哪里去。 接口粒度设计得太大，太小都不好。太大会导致接口不可复用，太小会导致接口不易用。在实际的开发中，接口的可复用性和易用性需要“微妙”的权衡。针对这个问题，我的一个基本的处理原则是，尽量保持接口的可复用性，但针对特殊情况，允许提供冗余的门面接口，来提供更易用的接口。","categories":[],"tags":[]},{"title":"适配器模式","slug":"适配器模式","date":"2021-05-24T23:48:27.000Z","updated":"2021-05-25T00:10:51.405Z","comments":true,"path":"2021/05/25/适配器模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/25/%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 代理、桥接、装饰器、适配器 4 种设计模式的区别 代理模式：代理模式在不改变原始类接口的条件下，为原始类定义一个代理类，主要目的是控制访问，而非加强功能，这是它跟装饰器模式最大的不同。 桥接模式：桥接模式的目的是将接口部分和实现部分分离，从而让它们可以较为容易、也相对独立地加以改变。 装饰器模式：装饰者模式在不改变原始类接口的情况下，对原始类功能进行增强，并且支持多个装饰器的嵌套使用。 适配器模式：适配器模式是一种事后的补救策略。适配器提供跟原始类不同的接口，而代理模式、装饰器模式提供的都是跟原始类相同的接口。 适用场景一般来说，适配器模式可以看作一种“补偿模式”，用来补救设计上的缺陷。应用这种模式算是“无奈之举”，如果在设计初期，我们就能协调规避接口不兼容的问题，那这种模式就没有应用的机会了。 封装有缺陷的接口 设计统一多个类的接口 设计替换依赖的外部系统 兼容老版本接口 适配不同格式的数据","categories":[],"tags":[]},{"title":"装饰者模式","slug":"装饰者模式","date":"2021-05-24T23:40:39.000Z","updated":"2021-05-25T00:10:51.405Z","comments":true,"path":"2021/05/25/装饰者模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/25/%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 场景 装饰器模式主要解决继承关系过于复杂的问题，通过组合来替代继承。 它主要的作用是给原始类添加增强功能。这也是判断是否该用装饰器模式的一个重要的依据。除此之外，装饰器模式还有一个特点，那就是可以对原始类嵌套使用多个装饰器。为了满足这个应用场景，在设计的时候，装饰器类需要跟原始类继承相同的抽象类或者接口。 python 装饰器12345678910111213141516171819def Derectory(func): @wraps(func) def d(*args,**kwargs): print(&quot;装饰前&quot;) s = func(*args,**kwargs) print(&quot;装饰后&quot;) return ddef D(*args,**kwargs): def Derectory(func): @functools.wraps(func) def d(*args,**kwargs): print(&quot;装饰前&quot;) obj = args[0] s = args[1:] func(*args,**kwargs) print(&quot;装饰后&quot;) return d return Derectory","categories":[],"tags":[]},{"title":"桥接模式","slug":"桥接模式","date":"2021-05-24T23:35:07.000Z","updated":"2021-05-25T00:10:51.405Z","comments":true,"path":"2021/05/25/桥接模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/25/%E6%A1%A5%E6%8E%A5%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 桥接模式，也叫作桥梁模式，英文是 Bridge Design Pattern。将抽象和实现解耦，让它们可以独立变化。 一个类存在两个（或多个）独立变化的维度，我们通过组合的方式，让这两个（或多个）维度可以独立进行扩展。”通过组合关系来替代继承关系，避免继承层次的指数级爆炸。这种理解方式非常类似于，我们之前讲过的“组合优于继承”设计原则 应用场景一个类存在两个（或多个）独立变化的维度，我们通过组合的方式，让这两个（或多个）维度可以独立进行扩展的场景； 手机有很多品牌，每个品牌又有很多自己的软件","categories":[],"tags":[]},{"title":"总体介绍","slug":"总体介绍","date":"2021-05-24T13:17:16.000Z","updated":"2021-05-28T00:11:15.464Z","comments":true,"path":"2021/05/24/总体介绍/","link":"","permalink":"https://sk-xinye.github.io/2021/05/24/%E6%80%BB%E4%BD%93%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"设计模式分类 创建型（创建型设计模式主要解决“对象的创建”问题）创建型模式主要解决对象的创建问题，封装复杂的创建过程，解耦对象的创建代码和使用代码。 单例模式用来创建全局唯一的对象。 工厂模式用来创建不同但是相关类型的对象（继承同一父类或者接口的一组子类），由给定的参数来决定创建哪种类型的对象。 建造者模式是用来创建复杂对象，可以通过设置不同的可选参数，“定制化”地创建不同的对象。 原型模式针对创建成本比较大的对象，利用对已有对象进行复制的方式进行创建，以达到节省创建时间的目的。 结构型（结构型设计模式主要解决“类或对象的组合或组装”问题）结构型模式主要总结了一些类或对象组合在一起的经典结构，这些经典的结构可以解决特定应用场景的问题。 代理模式 桥接模式 装饰器模式 适配器模式 门面模式 组合模式 享元模式 行为型（那行为型设计模式主要解决的就是“类或对象之间的交互”问题） 观察者模式 模板模式 策略模式 职责链模式 状态模式 迭代器模式 访问者模式 备忘录模式 命令模式 解释器模式 中介模式。 代码书写方式 需求分析(重点) 抽象（根据需求分析的结果抽象出具体的类，其中动词为方法，名词为属性） 接口（扩展点，面向接口编程） 数据流（数据流向，具体体现在上帝类中）","categories":[],"tags":[]},{"title":"代理模式","slug":"代理模式","date":"2021-05-24T13:15:22.000Z","updated":"2021-05-24T13:51:32.122Z","comments":true,"path":"2021/05/24/代理模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/24/%E4%BB%A3%E7%90%86%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 它在不改变原始类（或叫被代理类）代码的情况下，通过引入代理类来给原始类附加功能。一般情况下，我们让代理类和原始类实现同样的接口。但是，如果原始类并没有定义接口，并且原始类代码并不是我们开发维护的。在这种情况下，我们可以通过让代理类继承原始类的方法来实现代理模式。 1234567891011121314151617181920212223242526public interface HelloInterface &#123; void sayHello();&#125;public class Hello implements HelloInterface&#123; @Override public void sayHello() &#123; System.out.println(&quot;Hello zhanghao!&quot;); &#125;&#125;public class HelloProxy implements HelloInterface&#123; private HelloInterface helloInterface = new Hello(); @Override public void sayHello() &#123; System.out.println(&quot;Before invoke sayHello&quot; ); helloInterface.sayHello(); System.out.println(&quot;After invoke sayHello&quot;); &#125;&#125;public static void main(String[] args) &#123; HelloProxy helloProxy = new HelloProxy(); helloProxy.sayHello(); &#125; 动态代理静态代理需要针对每个类都创建一个代理类，并且每个代理类中的代码都有点像模板式的“重复”代码，增加了维护成本和开发成本。对于静态代理存在的问题，我们可以通过动态代理来解决。我们不事先为每个原始类编写代理类，而是在运行的时候动态地创建原始类对应的代理类，然后在系统中用代理类替换掉原始类。 12jdkcglib 应用场景 业务系统的非功能性需求开发监控、统计、鉴权、限流、事务、幂等、日志。我们将这些附加功能与业务功能解耦，放到代理类中统一处理，让程序员只需要关注业务方面的开发。 代理模式在 RPC、缓存中的应用","categories":[],"tags":[]},{"title":"原型模式","slug":"原型模式","date":"2021-05-24T13:02:30.000Z","updated":"2021-05-28T00:11:15.464Z","comments":true,"path":"2021/05/24/原型模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/24/%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 如果对象的创建成本比较大，而同一个类的不同对象之间差别不大（大部分字段都相同），在这种情况下，我们可以利用对已有对象（原型）进行复制（或者叫拷贝）的方式，来创建新对象，以达到节省创建时间的目的。这种基于原型来创建对象的方式就叫作原型设计模式，简称原型模式。 状态机实现方式 原型模式有两种实现方法，深拷贝和浅拷贝。浅拷贝只会复制对象中基本数据类型数据和引用对象的内存地址，不会递归地复制引用对象，以及引用对象的引用对象……而深拷贝得到的是一份完完全全独立的对象。所以，深拷贝比起浅拷贝来说，更加耗时，更加耗内存空间。 如果要拷贝的对象是不可变对象，浅拷贝共享不可变对象是没问题的，但对于可变对象来说，浅拷贝得到的对象和原始对象会共享部分数据，就有可能出现数据被修改的风险，也就变得复杂多了。除非像我们今天实战中举的那个例子，需要从数据库中加载 10 万条数据并构建散列表索引，操作非常耗时，这种情况下比较推荐使用浅拷贝，否则，没有充分的理由，不要为了一点点的性能提升而使用浅拷贝。 分支逻辑法利用 if-else 或者 switch-case 分支逻辑，参照状态转移图，将每一个状态转移原模原样地直译成代码。对于简单的状态机来说，这种实现方式最简单、最直接，是首选。 查表法对于状态很多、状态转移比较复杂的状态机来说，查表法比较合适。通过二维数组来表示状态转移图，能极大地提高代码的可读性和可维护性。 状态模式对于状态并不多、状态转移也比较简单，但事件触发执行的动作包含的业务逻辑可能比较复杂的状态机来说，我们首选这种实现方式。","categories":[],"tags":[]},{"title":"基本操作","slug":"基本操作","date":"2021-05-24T02:20:42.000Z","updated":"2023-07-16T13:22:57.369Z","comments":true,"path":"2021/05/24/基本操作/","link":"","permalink":"https://sk-xinye.github.io/2021/05/24/%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/","excerpt":"","text":"准备搭建MaxCompute Spark环境https://help.aliyun.com/document_detail/118144.html?spm=a2c4g.11186623.6.889.390213d9Wiuds5 下载spark客户端 设置环境变量 配置spark-defaults.conf 搭建MaxCompute客户端（odpscmd）https://help.aliyun.com/document_detail/27971.html?spm=5176.21213303.J_6028563670.7.51513edahE6LWJ&amp;scm=20140722.S_help%40%40%E6%96%87%E6%A1%A3%40%4027971.S_hot.ID_27971-RL_odps%E5%AE%A2%E6%88%B7%E7%AB%AF-OR_s%2Bhelpmain-V_1-P0_0 数据迁移https://help.aliyun.com/document_detail/90930.htm?spm=a2c4g.11186623.2.2.505f4805bfZMIU#concept-phx-dgc-dfb ODPS客户端下载数据tunnel download new_binary result_v3; ODPS客户端上传数据tunnel upload power_json_es.csv test_project.test_table; ODPS客户端上传py文件add py|jar|archive localfiledrop table if exist tablenameset odps.sql.allow.fullscan=true; 打包python环境docker cp python3.7:/usr/local/python3.7.zip ./ali-py3.5package-py3.7.ziphttps://help.aliyun.com/document_detail/118328.html 使用 打开阿里云https://cn.aliyun.com/ 登录后，搜索dataworks 控制台https://workbench.data.aliyun.com/console#/ 点击左侧导航栏中工作空间列表 创建好自己的工作空间 在创建好的工作空间右侧点击进入数据开发 在左侧业务流程 上 右键，点击新建业务流程 在新建的流程中点击maxcompute,右键新建数据表，与结果保存表，并通过odps客户端上传数据，上传py文件及jar包https://help.aliyun.com/document_detail/27831.htm?spm=a2c4g.11186623.2.7.167813d900HgjO#section-lqp-41f-vdb 添加已经上传的py文件到资源，或者直接右键创建py文件 创建虚拟节点Vi，配置相关流程参数，试运行 创建ODPS Spark节点Sp 双击Sp节点，配置参数 运行流程，右键节点查看日志 相关网址https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html spark 2.4 apihttps://developer.aliyun.com/article/781030 maxcompute spark 使用及常见问题https://github.com/aliyun/MaxCompute-Spark/wiki/06.-Spark-2.4.5-%E4%BD%BF%E7%94%A8%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9 Spark 2.4.5 使用注意事项https://help.aliyun.com/document_detail/27971.html MaxCompute客户端（odpscmd） https://github.com/aliyun/MaxCompute-Spark/wiki/03.-Spark%E9%85%8D%E7%BD%AE%E8%AF%A6%E8%A7%A3 配置参数详解 内置包http://odps-repo.oss-cn-hangzhou.aliyuncs.com/pyspark/py37/py37-default_req.txt 虚拟环境问题阿里云环境问题测试结果说明： 由于阿里云目前环境的限制，不支持python3.7以上版本，所以python 版本从3.8 变为3.6 当使用的镜像不包含虚拟环境时，测试通过 当使用虚拟环境时，无法找到对应的软件包，通过排查发现： 阿里云后台的环境为centos 上传python环境并不会被解压到我们制作镜像的路径下。例如，我们制作镜像时使用的是’/opt/py3.6’,解压后的路径并非’/opt/py3.6’,而是一个用户项目目录下 尝试修改ve1/bin 中python3.6的软链接为相对路径，无果。应该在别的地方还有相关路径（activate?）。例如，原来的软链接路径’/opt/py3.6/bin/python3.6’ 改为’../../bin/python3.6’ 通过与阿里云那边的工作人员沟通，他们的反馈是目前不支持虚拟环境的使用 您的邮件已收到，通过近期与阿里云工作人员的沟通与测试，得到以下几点： 目前，非虚拟环境的方案可行，不在尝试虚拟环境的方案。 路径和每次创建的工作空间名称有关（在使用阿里云时，需要先创建一个工作空间），但是与对方工作人员沟通后，他们并不清楚解压的绝对路径是否可变（我理解应该是不会的），也没能给出路径。因为目前只有阿里云涉及到这块，非虚拟环境的方案也是可行的，不在追问这个问题。 由于和我们沟通的工作人员应该不是开发，昨天和他们沟通，他们没有能给出具体的centos版本，需要上班后继续询问相关内容。 Wiki page上看到Dockerfile中有修改”elasticsearch/client/init.py”的動作 这个是原来的，并未做修改，后面会和相关人员进行说明沟通。 感谢您的回复，学到了很多东西！ 配置参数spark.executor.instancesspark.executor.coresspark.executor.memoryspark.default.parallelismspark.hadoop.odps.cupid.resources platform_test_dev.py3.6_aliyun_oss_0624.zip,public.hadoop-fs-oss-shaded.jarspark.pyspark.python ./platform_test_dev.py3.6_aliyun_oss_0624.zip/py3.6/bin/python3.6spark.hadoop.odps.spark.version spark-2.4.5-odps0.33.0spark.hadoop.fs.oss.credentials.provider org.apache.hadoop.fs.aliyun.oss.AliyunStsTokenCredentialsProviderspark.hadoop.odps.cupid.internet.access.list es-cn-7mz28s8zq002kqlqy.public.elasticsearch.aliyuncs.com:9200 背景 为支持上云，需要将现有的后处理模块进行改造，以适配阿里云目前环境 目前以阿里云datawork 为工作台，进行代码的调试 使用spark 相关算子，为后处理提供所需数据源及写入服务 准备工作 登录阿里云，创建新业务流，详见wiki: 跑pp2模块，得到结果数据表tg_status_idx及meter_status_idx，详见wiki: 创建后处理模块结果表，建表语句见附件，建表方式见wiki(数据表创建): 主页内容后处理（对应原索引：handle_status_appraise）主要步骤 查询需要的数据源。本索引需要的数据源为tg_status_idx及meter_status_idx 对tg数据及meter数据进行分组聚合为一行数据，供对应的后处理handler模块使用 通过map算子启动后处理 将结果写入准备过程中创建的表 清单DataWorks华为云https://support.huaweicloud.com/devg-apig/apig-dev-180307016.html kafka检测彩色照片转黑白https://uutool.cn/img-color-remove/ processon","categories":[],"tags":[]},{"title":"建造者模式","slug":"建造者模式","date":"2021-05-23T23:39:59.000Z","updated":"2021-05-24T00:11:07.054Z","comments":true,"path":"2021/05/24/建造者模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/24/%E5%BB%BA%E9%80%A0%E8%80%85%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"概念 如果一个类中有很多属性，为了避免构造函数的参数列表过长，影响代码的可读性和易用性，我们可以通过构造函数配合 set() 方法来解决。但是，如果存在下面情况中的任意一种，我们就要考虑使用建造者模式了。 我们把类的必填属性放到构造函数中，强制创建对象的时候就设置。如果必填的属性有很多，把这些必填属性都放到构造函数中设置，那构造函数就又会出现参数列表很长的问题。如果我们把必填属性通过 set() 方法设置，那校验这些必填属性是否已经填写的逻辑就无处安放了。 如果类的属性之间有一定的依赖关系或者约束条件，我们继续使用构造函数配合 set() 方法的设计思路，那这些依赖关系或约束条件的校验逻辑就无处安放了。 如果我们希望创建不可变对象，也就是说，对象在创建好之后，就不能再修改内部的属性值，要实现这个功能，我们就不能在类中暴露 set() 方法。构造函数配合 set() 方法来设置属性值的方式就不适用了。 与工厂模式区别工厂模式是用来创建不同但是相关类型的对象（继承同一父类或者接口的一组子类），由给定的参数来决定创建哪种类型的对象。建造者模式是用来创建一种类型的复杂对象，通过设置不同的可选参数，“定制化”地创建不同的对象。 实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public class ResourcePoolConfig &#123; private String name; private int maxTotal; private int maxIdle; private int minIdle; private ResourcePoolConfig(Builder builder) &#123; this.name = builder.name; this.maxTotal = builder.maxTotal; this.maxIdle = builder.maxIdle; this.minIdle = builder.minIdle; &#125; //...省略getter方法... //我们将Builder类设计成了ResourcePoolConfig的内部类。 //我们也可以将Builder类设计成独立的非内部类ResourcePoolConfigBuilder。 public static class Builder &#123; private static final int DEFAULT_MAX_TOTAL = 8; private static final int DEFAULT_MAX_IDLE = 8; private static final int DEFAULT_MIN_IDLE = 0; private String name; private int maxTotal = DEFAULT_MAX_TOTAL; private int maxIdle = DEFAULT_MAX_IDLE; private int minIdle = DEFAULT_MIN_IDLE; public ResourcePoolConfig build() &#123; // 校验逻辑放到这里来做，包括必填项校验、依赖关系校验、约束条件校验等 if (StringUtils.isBlank(name)) &#123; throw new IllegalArgumentException(&quot;...&quot;); &#125; if (maxIdle &gt; maxTotal) &#123; throw new IllegalArgumentException(&quot;...&quot;); &#125; if (minIdle &gt; maxTotal || minIdle &gt; maxIdle) &#123; throw new IllegalArgumentException(&quot;...&quot;); &#125; return new ResourcePoolConfig(this); &#125; public Builder setName(String name) &#123; if (StringUtils.isBlank(name)) &#123; throw new IllegalArgumentException(&quot;...&quot;); &#125; this.name = name; return this; &#125; public Builder setMaxTotal(int maxTotal) &#123; if (maxTotal &lt;= 0) &#123; throw new IllegalArgumentException(&quot;...&quot;); &#125; this.maxTotal = maxTotal; return this; &#125; public Builder setMaxIdle(int maxIdle) &#123; if (maxIdle &lt; 0) &#123; throw new IllegalArgumentException(&quot;...&quot;); &#125; this.maxIdle = maxIdle; return this; &#125; public Builder setMinIdle(int minIdle) &#123; if (minIdle &lt; 0) &#123; throw new IllegalArgumentException(&quot;...&quot;); &#125; this.minIdle = minIdle; return this; &#125; &#125;&#125;// 这段代码会抛出IllegalArgumentException，因为minIdle&gt;maxIdleResourcePoolConfig config = new ResourcePoolConfig.Builder() .setName(&quot;dbconnectionpool&quot;) .setMaxTotal(16) .setMaxIdle(10) .setMinIdle(12) .build();","categories":[],"tags":[]},{"title":"工厂模式","slug":"工厂模式","date":"2021-05-23T13:06:48.000Z","updated":"2021-05-23T13:42:37.131Z","comments":true,"path":"2021/05/23/工厂模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/23/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"分类与实现简单工厂 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public interface Shape &#123; void draw();&#125;public class Rectangle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Inside Rectangle::draw() method.&quot;); &#125;&#125;public class Square implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Inside Square::draw() method.&quot;); &#125;&#125;public class Circle implements Shape &#123; @Override public void draw() &#123; System.out.println(&quot;Inside Circle::draw() method.&quot;); &#125;&#125;public class ShapeFactory &#123; //使用 getShape 方法获取形状类型的对象 public Shape getShape(String shapeType)&#123; if(shapeType == null)&#123; return null; &#125; if(shapeType.equalsIgnoreCase(&quot;CIRCLE&quot;))&#123; return new Circle(); &#125; else if(shapeType.equalsIgnoreCase(&quot;RECTANGLE&quot;))&#123; return new Rectangle(); &#125; else if(shapeType.equalsIgnoreCase(&quot;SQUARE&quot;))&#123; return new Square(); &#125; return null; &#125;&#125;public class FactoryPatternDemo &#123; public static void main(String[] args) &#123; ShapeFactory shapeFactory = new ShapeFactory(); //获取 Circle 的对象，并调用它的 draw 方法 Shape shape1 = shapeFactory.getShape(&quot;CIRCLE&quot;); //调用 Circle 的 draw 方法 shape1.draw(); //获取 Rectangle 的对象，并调用它的 draw 方法 Shape shape2 = shapeFactory.getShape(&quot;RECTANGLE&quot;); //调用 Rectangle 的 draw 方法 shape2.draw(); //获取 Square 的对象，并调用它的 draw 方法 Shape shape3 = shapeFactory.getShape(&quot;SQUARE&quot;); //调用 Square 的 draw 方法 shape3.draw(); &#125;&#125; 工厂 抽象工厂 总结当创建逻辑比较复杂，是一个“大工程”的时候，我们就考虑使用工厂模式，封装对象的创建过程，将对象的创建和使用相分离。何为创建逻辑比较复杂呢？我总结了下面两种情况。 第一种情况：类似规则配置解析的例子，代码中存在 if-else 分支判断，动态地根据不同的类型创建不同的对象。针对这种情况，我们就考虑使用工厂模式，将这一大坨 if-else 创建对象的代码抽离出来，放到工厂类中。 还有一种情况，尽管我们不需要根据不同的类型创建不同的对象，但是，单个对象本身的创建过程比较复杂，比如前面提到的要组合其他类对象，做各种初始化操作。在这种情况下，我们也可以考虑使用工厂模式，将对象的创建过程封装到工厂类中。 使用标准 封装变化：创建逻辑有可能变化，封装成工厂类之后，创建逻辑的变更对调用者透明。 代码复用：创建代码抽离到独立的工厂类之后可以复用。 隔离复杂性：封装复杂的创建逻辑，调用者无需了解如何创建对象。 控制复杂度：将创建代码抽离出来，让原本的函数或类职责更单一，代码更简洁。","categories":[],"tags":[]},{"title":"单例模式","slug":"单例模式","date":"2021-05-23T08:08:44.000Z","updated":"2023-02-04T02:33:07.963Z","comments":true,"path":"2021/05/23/单例模式/","link":"","permalink":"https://sk-xinye.github.io/2021/05/23/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/","excerpt":"","text":"定义单例设计模式（Singleton Design Pattern）理解起来非常简单。一个类只允许创建一个对象（或者实例），那这个类就是一个单例类，这种设计模式就叫作单例设计模式，简称单例模式。 实现java 饿汉式饿汉式的实现方式，在类加载的期间，就已经将 instance 静态实例初始化好了，所以，instance 实例的创建是线程安全的。不过，这样的实现方式不支持延迟加载实例。 123456public class Singleton &#123; private static Singleton instance = new Singleton(); private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; return instance; &#125; &#125; 懒汉式懒汉式相对于饿汉式的优势是支持延迟加载。这种实现方式会导致频繁加锁、释放锁，以及并发度低等问题，频繁的调用会产生性能瓶颈。 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125; &#125; 双重检测双重检测实现方式既支持延迟加载、又支持高并发的单例实现方式。只要 instance 被创建之后，再调用 getInstance() 函数都不会进入到加锁逻辑中。所以，这种实现方式解决了懒汉式并发度低的问题。 1234567891011121314public class Singleton &#123; private volatile static Singleton singleton; private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (singleton == null) &#123; synchronized (Singleton.class) &#123; if (singleton == null) &#123; singleton = new Singleton(); &#125; &#125; &#125; return singleton; &#125; &#125; 静态内部类SingletonHolder 是一个静态内部类，当外部类 IdGenerator 被加载的时候，并不会创建 SingletonHolder 实例对象。只有当调用 getInstance() 方法时，SingletonHolder 才会被加载，这个时候才会创建 instance。instance 的唯一性、创建过程的线程安全性，都由 JVM 来保证。所以，这种实现方法既保证了线程安全，又能做到延迟加载。 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 枚举最简单的实现方式，基于枚举类型的单例实现。这种实现方式通过 Java 枚举类型本身的特性，保证了实例创建的线程安全性和实例的唯一性。 12345public enum Singleton &#123; INSTANCE; public void whateverMethod() &#123; &#125; &#125; python 装饰器讲解 闭包概念：在一个内部函数中，对外部作用域的变量进行引用，(并且一般外部函数的返回值为内部函数)，那么内部函数就被认为是闭包。 1234def startAt(x): def increamentBy(y): return x+y return increamentBy 在函数startAt中定义了一个incrementBy函数，incrementBy访问了外部函数startAt的变量，并且函数返回值为incrementBy函数（注意python是可以返回一个函数的，这也是python的特性之一） 最主要的就是python语法糖 被装饰函数=装饰器（被装饰函数） Cls=singleton(Cls) 使用函数装饰器实现单例 1234567891011121314151617def singleton(cls): _instance = &#123;&#125; def inner(*args, **kw): if cls not in _instance: _instance[cls] = cls(*args, **kw) return _instance[cls] return inner@singletonclass Cls(object): def __init__(self): passcls1 = Cls()cls2 = Cls()print(id(cls1) == id(cls2)) 使用类装饰器实现单例 1234567891011121314151617class Singleton(object): def __init__(self, cls): self._cls = cls self._instance = &#123;&#125; def __call__(self): if self._cls not in self._instance: self._instance[self._cls] = self._cls() return self._instance[self._cls]@Singletonclass Cls2(object): def __init__(self): passcls1 = Cls2()cls2 = Cls2()print(id(cls1) == id(cls2)) 使用 new 关键字实现单例模式 123456789101112class Single(object): _instance = None def __new__(cls, *args, **kw): if cls._instance is None: cls._instance = object.__new__(cls, *args, **kw) return cls._instance def __init__(self): passsingle1 = Single()single2 = Single()print(id(single1) == id(single2)) 使用 metaclass 实现单例模式 12345678910111213class Singleton(type): _instances = &#123;&#125; def __call__(cls, *args, **kwargs): if cls not in cls._instances: cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs) return cls._instances[cls]class Cls4(metaclass=Singleton): passcls1 = Cls4()cls2 = Cls4()print(id(cls1) == id(cls2)) 单例模式存在问题 单例对 OOP 特性的支持不友好 单例会隐藏类之间的依赖关系 单例对代码的扩展性不友好 单例对代码的可测试性不友好 单例不支持有参数的构造函数 分布式单例模式如何理解单例模式的唯一性单例类中对象的唯一性的作用范围是“进程唯一”的。“进程唯一”指的是进程内唯一，进程间不唯一；“线程唯一”指的是线程内唯一，线程间可以不唯一。实际上，“进程唯一”就意味着线程内、线程间都唯一，这也是“进程唯一”和“线程唯一”的区别之处。“集群唯一”指的是进程内唯一、进程间也唯一。 如何实现线程唯一的单例我们通过一个 HashMap 来存储对象，其中 key 是线程 ID，value 是对象。这样我们就可以做到，不同的线程对应不同的对象，同一个线程只能对应一个对象。实际上，Java 语言本身提供了 ThreadLocal 并发工具类，可以更加轻松地实现线程唯一单例。 如何实现集群环境下的单例我们需要把这个单例对象序列化并存储到外部共享存储区（比如文件）。进程在使用这个单例对象的时候，需要先从外部共享存储区中将它读取到内存，并反序列化成对象，然后再使用，使用完成之后还需要再存储回外部共享存储区。为了保证任何时刻在进程间都只有一份对象存在，一个进程在获取到对象之后，需要对对象加锁，避免其他进程再将其获取。在进程使用完这个对象之后，需要显式地将对象从内存中删除，并且释放对对象的加锁。 如何实现一个多例模式“单例”指的是一个类只能创建一个对象。对应地，“多例”指的就是一个类可以创建多个对象，但是个数是有限制的，比如只能创建 3 个对象。多例的实现也比较简单，通过一个 Map 来存储对象类型和对象之间的对应关系，来控制对象的个数。","categories":[],"tags":[]},{"title":"规范与重构","slug":"规范与重构","date":"2021-05-20T01:42:34.000Z","updated":"2021-05-23T08:07:28.027Z","comments":true,"path":"2021/05/20/规范与重构/","link":"","permalink":"https://sk-xinye.github.io/2021/05/20/%E8%A7%84%E8%8C%83%E4%B8%8E%E9%87%8D%E6%9E%84/","excerpt":"","text":"why what when how重构的目的：为什么重构（why）对于项目来言，重构可以保持代码质量持续处于一个可控状态，不至于腐化到无可救药的地步。对于个人而言，重构非常锻炼一个人的代码能力，并且是一件非常有成就感的事情。它是我们学习的经典设计思想、原则、模式、编程规范等理论知识的练兵场。 重构的对象：重构什么（what）按照重构的规模，我们可以将重构大致分为大规模高层次的重构和小规模低层次的重构。大规模高层次重构包括对代码分层、模块化、解耦、梳理类之间的交互关系、抽象复用组件等等。这部分工作利用的更多的是比较抽象、比较顶层的设计思想、原则、模式。小规模低层次的重构包括规范命名、注释、修正函数参数过多、消除超大类、提取重复代码等等编程细节问题，主要是针对类、函数级别的重构。小规模低层次的重构更多的是利用编码规范这一理论知识。 重构的时机：什么时候重构（when）我反复强调，我们一定要建立持续重构意识，把重构作为开发必不可少的部分，融入到日常开发中，而不是等到代码出现很大问题的时候，再大刀阔斧地重构。 重构的方法：如何重构（how）大规模高层次的重构难度比较大，需要组织、有计划地进行，分阶段地小步快跑，时刻让代码处于一个可运行的状态。而小规模低层次的重构，因为影响范围小，改动耗时短，所以，只要你愿意并且有时间，随时随地都可以去做。 如果保证重构的正确性（单元测试）单元测试与集成测试概念 单元测试：测试对象是类或者函数，用来测试一个类和函数是否都按照预期的逻辑执行。这是代码层级的测试。 集成测试：测试对象是整个系统或者某个功能模块，比如测试用户注册、登录功能是否正常，是一种端到端（end to end）的测试 为什么要写单元测试 单元测试能有效地帮你发现代码中的 bug能否写出 bug free 的代码，是判断工程师编码能力的BUG 写单元测试能帮你发现代码设计上的问题 单元测试是对集成测试的有力补充 写单元测试的过程本身就是代码重构的过程 阅读单元测试能帮助你快速熟悉代码 单元测试是 TDD 可落地执行的改进方案测试驱动开发（Test-Driven Development，简称 TDD) 如何编写单元测试写单元测试就是针对代码设计各种测试用例，以覆盖各种输入、异常、边界情况，并将其翻译成代码。我们可以利用一些测试框架来简化单元测试的编写。除此之外，对于单元测试，我们需要建立以下正确的认知： 编写单元测试尽管繁琐，但并不是太耗时； 我们可以稍微放低对单元测试代码质量的要求； 覆盖率作为衡量单元测试质量的唯一标准是不合理的； 单元测试不要依赖被测代码的具体实现逻辑； 单元测试框架无法测试，多半是因为代码的可测试性不好。 代码可测试性什么是代码可测试性粗略地讲，所谓代码的可测试性，就是针对代码编写单元测试的难易程度。对于一段代码，如果很难为其编写单元测试，或者单元测试写起来很费劲，需要依靠单元测试框架中很高级的特性，那往往就意味着代码设计得不够合理，代码的可测试性不好。 编写可测试性代码有效手段依赖注入是编写可测试性代码的最有效手段。通过依赖注入，我们在编写单元测试的时候，可以通过 mock 的方法解依赖外部服务，这也是我们在编写单元测试的过程中最有技术挑战的地方。 常见的反模式 代码中包含未决行为逻辑 滥用可变全局变量 滥用静态方法 使用复杂的继承关系 高度耦合的代码 如何解耦封装与抽象封装和抽象作为两个非常通用的设计思想，可以应用在很多设计场景中，比如系统、模块、lib、组件、接口、类等等的设计。封装和抽象可以有效地隐藏实现的复杂性，隔离实现的易变性，给依赖的模块提供稳定且易用的抽象接口。 中间层引入中间层能简化模块或类之间的依赖关系 模块化代码质量把控命名 命名的关键是能准确达意 我们可以借助类的信息来简化属性、函数的命名，利用函数的信息来简化函数参数的命名。 命名要可读、可搜索 接口有两种命名方式：一种是在接口中带前缀“I”；另一种是在接口的实现类中带后缀“Impl”。对于抽象类的命名，也有两种方式，一种是带上前缀“Abstract”，一种是不带前缀。这两种命名方式都可以，关键是要在项目中统一 注释 注释的目的就是让代码更容易看懂。只要符合这个要求的内容，你就可以将它写到注释里。总结一下，注释的内容主要包含这样三个方面：做什么、为什么、怎么做。对于一些复杂的类和接口，我们可能还需要写明“如何用”。 注释本身有一定的维护成本，所以并非越多越好。类和函数一定要写注释，而且要写得尽可能全面、详细，而函数内部的注释要相对少一些，一般都是靠好的命名、提炼函数、解释性变量、总结性注释来提高代码可读性。","categories":[],"tags":[]},{"title":"设计原则","slug":"设计原则","date":"2021-05-19T01:22:18.000Z","updated":"2021-05-23T08:07:28.028Z","comments":true,"path":"2021/05/19/设计原则/","link":"","permalink":"https://sk-xinye.github.io/2021/05/19/%E8%AE%BE%E8%AE%A1%E5%8E%9F%E5%88%99/","excerpt":"","text":"主要原则（SOLID、KISS、YAGNI、DRY、LOD 等）单一职责（Single Responsibility Principle，缩写为 SRP）一个类或者模块只负责完成一个职责（或者功能） 类中的代码行数、函数或者属性过多； 类依赖的其他类过多，或者依赖类的其他类过多； 私有方法过多； 比较难给类起一个合适的名字； 类中大量的方法都是集中操作类中的某几个属性 开闭原则（Open Closed Principle，简写为 OCP） 我们要时刻具备扩展意识、抽象意识、封装意识。在写代码的时候，我们要多花点时间思考一下，这段代码未来可能有哪些需求变更，如何设计代码结构，事先留好扩展点，以便在未来需求变更的时候，在不改动代码整体结构、做到最小代码改动的情况下，将新的代码灵活地插入到扩展点上。 很多设计原则、设计思想、设计模式，都是以提高代码的扩展性为最终目的的。特别是 23 种经典设计模式，大部分都是为了解决代码的扩展性问题而总结出来的，都是以开闭原则为指导原则的。最常用来提高代码扩展性的方法有：多态、依赖注入、基于接口而非实现编程，以及大部分的设计模式（比如，装饰、策略、模板、职责链、状态） 里氏替换原则（Liskov Substitution Principle，缩写为 LSP）子类对象（object of subtype/derived class）能够替换程序（program）中父类对象（object of base/parent class）出现的任何地方，并且保证原来程序的逻辑行为（behavior）不变及正确性不被破坏。 子类违背父类声明要实现的功能 子类违背父类对输入、输出、异常的约定 子类违背父类注释中所罗列的任何特殊说明 接口隔离原则（Interface Segregation Principle，缩写为 ISP） 一组 API 接口集合 单个 API 接口或函数 OOP 中的接口概念接下来，我就按照这三种理解 控制反转（Inversion Of Control，缩写为 IOC）将需要程序员控制的代码交给框架来管理，扩展代码时只要增加扩展点就好，流程不用管 依赖注入（Dependency Injection，缩写为 DI）不通过 new() 的方式在类内部创建依赖类对象，而是将依赖的类对象在外部创建好之后，通过构造函数、函数参数等方式传递（或注入）给类使用。 依赖反转原则（Dependency Inversion Principle，缩写为 DIP）依赖反转原则也叫作依赖倒置原则。这条原则跟控制反转有点类似，主要用来指导框架层面的设计。高层模块不依赖低层模块，它们共同依赖同一个抽象。抽象不要依赖具体实现细节，具体实现细节依赖抽象。 KISS原则（Keep It Short and Simple）YAGNI原则（You Ain’t Gonna Need It）DRY原则（Don’t Repeat Yourself） 实现的逻辑重复简单的copy-pase 实现的功能语义重复代码干的一件事，只是实现方式不同 代码执行重复重复的执行某段代码 提高代码复用性 减少代码耦合 满足单一职责原则 模块化 业务与非业务逻辑分离 通用代码下沉 继承、多态、抽象、封装 应用模板等设计模式 迪米特法则（Law of Demeter，缩写是 LOD The Least Knowledge Principle）不该有直接依赖关系的类之间，不要有依赖；有依赖关系的类之间，尽量只依赖必要的接口。迪米特法则是希望减少类之间的耦合，让类越独立越好。每个类都应该少了解系统的其他部分。一旦发生变化，需要了解这一变化的类就会比较少。 分层作用 分层能起到代码复用的作用 分层能起到隔离变化的作用 分层能起到隔离关注点的作用 分层能提高代码的可测试性 分层能应对系统的复杂性 需求分析 画产品线框图、 聚焦简单应用场景、 设计实现最小原型、 画系统设计图 面向对象设计与实现（就是将合适的代码放到合适的类中） 划分职责进而识别出有哪些类 定义类及类与类之间的关系 将类组装起来并提供执行入口 Review 设计与实现","categories":[],"tags":[]},{"title":"面向对象","slug":"面向对象","date":"2021-05-17T13:25:03.000Z","updated":"2021-05-23T08:07:28.029Z","comments":true,"path":"2021/05/17/面向对象/","link":"","permalink":"https://sk-xinye.github.io/2021/05/17/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/","excerpt":"","text":"概念 什么是面向对象编程：面向对象编程是一种编程范式或编程风格。它以类或对象作为组织代码的基本单元，并将封装、抽象、继承、多态四个特性，作为代码设计和实现的基石 。 什么是面向对象编程语言：面向对象编程语言是支持类或对象的语法机制，并有现成的语法机制，能方便地实现面向对象编程四大特性（封装、抽象、继承、多态）的编程语言。 如何判定一个编程语言是否是面向对象编程语言：如果按照严格的的定义，需要有现成的语法支持类、对象、四大特性才能叫作面向对象编程语言。如果放宽要求的话，只要某种编程语言支持类、对象语法机制，那基本上就可以说这种编程语言是面向对象编程语言了，不一定非得要求具有所有的四大特性。 什么是面向对象分析和面向对象设计：简单点讲，面向对象分析就是要搞清楚做什么，面向对象设计就是要搞清楚怎么做。两个阶段最终的产出是类的设计，包括程序被拆解为哪些类，每个类有哪些属性方法、类与类之间如何交互等等。 关于封装特性：封装也叫作信息隐藏或者数据访问保护。类通过暴露有限的访问接口，授权外部仅能通过类提供的方式来访问内部信息或者数据。它需要编程语言提供权限访问控制语法来支持，例如 Java 中的 private、protected、public 关键字。封装特性存在的意义，一方面是保护数据不被随意修改，提高代码的可维护性；另一方面是仅暴露有限的必要接口，提高类的易用性。 关于抽象特性：封装主要讲如何隐藏信息、保护数据，那抽象就是讲如何隐藏方法的具体实现，让使用者只需要关心方法提供了哪些功能，不需要知道这些功能是如何实现的。抽象可以通过接口类或者抽象类来实现，但也并不需要特殊的语法机制来支持。抽象存在的意义，一方面是提高代码的可扩展性、维护性，修改实现不需要改变定义，减少代码的改动范围；另一方面，它也是处理复杂系统的有效手段，能有效地过滤掉不必要关注的信息。 关于继承特性：继承是用来表示类之间的 is-a 关系，分为两种模式：单继承和多继承。单继承表示一个子类只继承一个父类，多继承表示一个子类可以继承多个父类。为了实现继承这个特性，编程语言需要提供特殊的语法机制来支持。继承主要是用来解决代码复用的问题。 关于多态特性：多态是指子类可以替换父类，在实际的代码运行过程中，调用子类的方法实现。多态这种特性也需要编程语言提供特殊的语法机制来实现，比如继承、接口类、duck-typing。多态可以提高代码的扩展性和复用性，是很多设计模式、设计原则、编程技巧的代码实现基础。 接口和抽象类 抽象类和接口的语法特性：抽象类不允许被实例化，只能被继承。它可以包含属性和方法。方法既可以包含代码实现，也可以不包含代码实现。不包含代码实现的方法叫作抽象方法。子类继承抽象类，必须实现抽象类中的所有抽象方法。接口不能包含属性，只能声明方法，方法不能包含代码实现。类实现接口的时候，必须实现接口中声明的所有方法。 抽象类和接口存在的意义：抽象类是对成员变量和方法的抽象，是一种 is-a 关系，是为了解决代码复用问题。接口仅仅是对方法的抽象，是一种 has-a 关系，表示具有某一组行为特性，是为了解决解耦问题，隔离接口和具体的实现，提高代码的扩展性。 抽象类和接口的应用场景区别：什么时候该用抽象类？什么时候该用接口？实际上，判断的标准很简单。如果要表示一种 is-a 的关系，并且是为了解决代码复用问题，我们就用抽象类；如果要表示\b一种 has-a 关系，并且是为了解决抽象而非代码复用问题，那我们就用接口。 技巧 基于接口变成而非实现类编程：越抽象、越顶层、越脱离具体某一实现的设计，越能提高代码的灵活性，越能应对未来的需求变化。好的代码设计，不仅能应对当下的需求，而且在将来需求发生变化的时候，仍然能够在不破坏原有代码设计的情况下灵活应对。而抽象就是提高代码扩展性、灵活性、可维护性最有效的手段之一。 多用组合少用继承 贫血模型OOP 与充血模型DDD其实就是service 层中BO的数据类中是否包含有相应的操作，即，操作与数据是否在一块","categories":[],"tags":[]},{"title":"动态规划","slug":"动态规划","date":"2021-05-15T12:11:46.000Z","updated":"2021-06-04T00:16:50.128Z","comments":true,"path":"2021/05/15/动态规划/","link":"","permalink":"https://sk-xinye.github.io/2021/05/15/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/","excerpt":"","text":"动态规划学习路线动态规划比较适合用来求解最优问题，比如求最大值、最小值等等 明确「状态」 -&gt; 定义 dp 数组/函数的含义 -&gt; 明确「选择」-&gt; 明确 basecase。 应用","categories":[],"tags":[]},{"title":"回溯算法","slug":"回溯算法","date":"2021-05-15T10:36:16.000Z","updated":"2021-05-15T14:21:02.032Z","comments":true,"path":"2021/05/15/回溯算法/","link":"","permalink":"https://sk-xinye.github.io/2021/05/15/%E5%9B%9E%E6%BA%AF%E7%AE%97%E6%B3%95/","excerpt":"","text":"如何理解“回溯算法”（最直观的就是DFS）？回溯的处理思想，有点类似枚举搜索。我们枚举所有的解，找到满足期望的解。为了有规律地枚举所有可能的解，避免遗漏和重复，我们把问题求解的过程分为多个阶段。每个阶段，我们都会面对一个岔路口，我们先随意选一条路走，当发现这条路走不通的时候（不符合期望的解），就回退到上一个岔路口，另选一种走法继续走。 应用八皇后12345678910111213141516171819202122232425262728293031323334353637383940int[] result = new int[8];//全局或成员变量,下标表示行,值表示queen存储在哪一列public void cal8queens(int row) &#123; // 调用方式：cal8queens(0); if (row == 8) &#123; // 8个棋子都放置好了，打印结果 printQueens(result); return; // 8行棋子都放好了，已经没法再往下递归了，所以就return &#125; for (int column = 0; column &lt; 8; ++column) &#123; // 每一行都有8中放法 if (isOk(row, column)) &#123; // 有些放法不满足要求 result[row] = column; // 第row行的棋子放到了column列 cal8queens(row+1); // 考察下一行 &#125; &#125;&#125;private boolean isOk(int row, int column) &#123;//判断row行column列放置是否合适 int leftup = column - 1, rightup = column + 1; for (int i = row-1; i &gt;= 0; --i) &#123; // 逐行往上考察每一行 if (result[i] == column) return false; // 第i行的column列有棋子吗？ if (leftup &gt;= 0) &#123; // 考察左上对角线：第i行leftup列有棋子吗？ if (result[i] == leftup) return false; &#125; if (rightup &lt; 8) &#123; // 考察右上对角线：第i行rightup列有棋子吗？ if (result[i] == rightup) return false; &#125; --leftup; ++rightup; &#125; return true;&#125;private void printQueens(int[] result) &#123; // 打印出一个二维矩阵 for (int row = 0; row &lt; 8; ++row) &#123; for (int column = 0; column &lt; 8; ++column) &#123; if (result[row] == column) System.out.print(&quot;Q &quot;); else System.out.print(&quot;* &quot;); &#125; System.out.println(); &#125; System.out.println();&#125; 深度优先搜索、八皇后、0-1 背包问题、图的着色、旅行商问题、数独、全排列、正则表达式匹配","categories":[],"tags":[]},{"title":"分治算法","slug":"分治算法","date":"2021-05-15T08:44:15.000Z","updated":"2021-05-24T23:32:32.467Z","comments":true,"path":"2021/05/15/分治算法/","link":"","permalink":"https://sk-xinye.github.io/2021/05/15/%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/","excerpt":"","text":"如何理解分治算法？分治算法是一种处理问题的思想，递归是一种编程技巧，一般用递归实现，分为三步： 分解：将原问题分解成一系列子问题； 解决：递归地求解各个子问题，若子问题足够小，则直接求解； 合并：将子问题的结果合并成原问题。 分治算法能解决的问题，一般需要满足下面这几个条件： 原问题与分解成的小问题具有相同的模式； 原问题分解成的子问题可以独立求解，子问题之间没有相关性，这一点是分治算法跟动态规划的明显区别，等我们讲到动态规划的时候，会详细对比这两种算法； 具有分解终止条件，也就是说，当问题足够小时，可以直接求解； 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。 分治算法应用举例分析归并求逆序度1234567891011121314151617181920212223242526272829303132333435363738private int num = 0; // 全局变量或者成员变量public int count(int[] a, int n) &#123; num = 0; mergeSortCounting(a, 0, n-1); return num;&#125;private void mergeSortCounting(int[] a, int p, int r) &#123; if (p &gt;= r) return; int q = (p+r)/2; mergeSortCounting(a, p, q); mergeSortCounting(a, q+1, r); merge(a, p, q, r);&#125;private void merge(int[] a, int p, int q, int r) &#123; int i = p, j = q+1, k = 0; int[] tmp = new int[r-p+1]; while (i&lt;=q &amp;&amp; j&lt;=r) &#123; if (a[i] &lt;= a[j]) &#123; tmp[k++] = a[i++]; &#125; else &#123; num += (q-i+1); // 统计p-q之间，比a[j]大的元素个数 tmp[k++] = a[j++]; &#125; &#125; while (i &lt;= q) &#123; // 处理剩下的 tmp[k++] = a[i++]; &#125; while (j &lt;= r) &#123; // 处理剩下的 tmp[k++] = a[j++]; &#125; for (i = 0; i &lt;= r-p; ++i) &#123; // 从tmp拷贝回a a[p+i] = tmp[i]; &#125;&#125; 练习关于分治算法，我这还有两道比较经典的问题，你可以自己练习一下。 二维平面上有 n 个点，如何快速计算出两个距离最近的点对？ 有两个 nn 的矩阵 A，B，如何快速求解两个矩阵的乘积 C=AB？","categories":[],"tags":[]},{"title":"贪心算法","slug":"贪心算法","date":"2021-05-15T08:26:50.000Z","updated":"2021-05-15T14:21:02.033Z","comments":true,"path":"2021/05/15/贪心算法/","link":"","permalink":"https://sk-xinye.github.io/2021/05/15/%E8%B4%AA%E5%BF%83%E7%AE%97%E6%B3%95/","excerpt":"","text":"如何理解“贪心算法”？ 第一步，当我们看到这类问题的时候，首先要联想到贪心算法：针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制值的情况下，期望值最大。 第二步，我们尝试看下这个问题是否可以用贪心算法解决：每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据。 第三步，我们举几个例子看下贪心算法产生的结果是否是最优的。大部分情况下，举几个例子验证一下就可以了。 第四步，当前面的选择会影响后面的选择时，就无法用贪心策略了，无后效性，即每一次选择都是同样的 贪心算法实战分析背包装豆子 限制值：背包载重100kg 期望值：总价值最大 在同等重量下，期望获取最大的价值的豆子即可 无后效性 分糖果 限制值：糖果数量M 期望值：孩子的数量N 在相同的糖果下，孩子的个数最多，即在一个糖果的情况下，孩子需求越小，孩子越可能满足条件 无后效性 钱币找零 限制值：钱总数 期望值：纸币数量最少 在相同钱的情况下，面值越大，纸币数量越少 无后效性 区间覆盖 限制值：区间长度 期望值：子区间越多越好 在同样长度的区间下，子区间越小，越能满足条件，即找一个满足条件的尾区间最小的 无后效性","categories":[],"tags":[]},{"title":"Trie树","slug":"Trie树","date":"2021-05-13T23:51:27.000Z","updated":"2021-05-14T00:08:18.081Z","comments":true,"path":"2021/05/14/Trie树/","link":"","permalink":"https://sk-xinye.github.io/2021/05/14/Trie%E6%A0%91/","excerpt":"","text":"Trie树（字典树）Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。 如何实现一棵 Trie 树？借助散列表的思想，我们通过一个下标与字符一一映射的数组，来存储子节点的指针。 1234567891011121314151617181920212223242526272829303132333435363738394041public class Trie &#123; private TrieNode root = new TrieNode(&#x27;/&#x27;); // 存储无意义字符 // 往Trie树中插入一个字符串 public void insert(char[] text) &#123; TrieNode p = root; for (int i = 0; i &lt; text.length; ++i) &#123; int index = text[i] - &#x27;a&#x27;; if (p.children[index] == null) &#123; TrieNode newNode = new TrieNode(text[i]); p.children[index] = newNode; &#125; p = p.children[index]; &#125; p.isEndingChar = true; &#125; // 在Trie树中查找一个字符串 public boolean find(char[] pattern) &#123; TrieNode p = root; for (int i = 0; i &lt; pattern.length; ++i) &#123; int index = pattern[i] - &#x27;a&#x27;; if (p.children[index] == null) &#123; return false; // 不存在pattern &#125; p = p.children[index]; &#125; if (p.isEndingChar == false) return false; // 不能完全匹配，只是前缀 else return true; // 找到pattern &#125; public class TrieNode &#123; public char data; public TrieNode[] children = new TrieNode[26]; public boolean isEndingChar = false; public TrieNode(char data) &#123; this.data = data; &#125; &#125;&#125; 在 Trie 树中，查找某个字符串的时间复杂度是多少？(构建是O(n),查找是O(k))","categories":[],"tags":[]},{"title":"字符串匹配","slug":"字符串匹配","date":"2021-05-13T23:26:57.000Z","updated":"2021-05-14T00:08:18.081Z","comments":true,"path":"2021/05/14/字符串匹配/","link":"","permalink":"https://sk-xinye.github.io/2021/05/14/%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%8C%B9%E9%85%8D/","excerpt":"","text":"BF 算法(Brute Force 强制匹配算法 O(n*m))我们在主串中，检查起始位置分别是 0、1、2….n-m 且长度为 m 的 n-m+1 个子串，看有没有跟模式串匹配的 尽管理论上，BF 算法的时间复杂度很高，是 O(n*m)，但在实际的开发中，它却是一个比较常用的字符串匹配算法。为什么这么说呢？原因有两点。 第一，实际的软件开发中，大部分情况下，模式串和主串的长度都不会太长。而且每次模式串与主串中的子串匹配的时候，当中途遇到不能匹配的字符的时候，就可以就停止了，不需要把 m 个字符都比对一下。所以，尽管理论上的最坏情况时间复杂度是 O(n*m)，但是，统计意义上，大部分情况下，算法执行效率要比这个高很多。 第二，朴素字符串匹配算法思想简单，代码实现也非常简单。简单意味着不容易出错，如果有 bug 也容易暴露和修复。在工程中，在满足性能要求的前提下，简单是首选。这也是我们常说的KISS（Keep it Simple and Stupid）设计原则。 RK 算法（Rabin-Karp 人名 O(n)）RK 算法的思路是这样的：我们通过哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值与模式串相等，那就说明对应的子串和模式串匹配了（这里先不考虑哈希冲突的问题，后面我们会讲到）。因为哈希值是一个数字，数字之间比较是否相等是非常快速的，所以模式串和子串比较的效率就提高了当我们发现一个子串的哈希值跟模式串的哈希值相等的时候，我们只需要再对比一下子串和模式串本身就好了。当然，如果子串的哈希值与模式串的哈希值不相等，那对应的子串和模式串肯定也是不匹配的，就不需要比对子串和模式串本身了。 BM（Boyer-Moore）算法BM 算法包含两部分，分别是坏字符规则（bad character rule）和好后缀规则（good suffix shift）。 坏字符规则:当发生不匹配的时候，我们把坏字符对应的模式串中的字符下标记作 si。如果坏字符在模式串中存在，我们把这个坏字符在模式串中的下标记作 xi。如果不存在，我们把 xi 记作 -1。那模式串往后移动的位数就等于 si-xi。（注意，我这里说的下标，都是字符在模式串的下标）。 好后缀规则： 我们可以分别计算好后缀和坏字符往后滑动的位数，然后取两个数中最大的，作为模式串往后滑动的位数。这种处理方法还可以避免我们前面提到的，根据坏字符规则，计算得到的往后滑动的位数，有可能是负数的情况。 KMP 算法基本原理","categories":[],"tags":[]},{"title":"深度优先广度优先","slug":"深度优先广度优先","date":"2021-05-12T23:44:34.000Z","updated":"2021-05-17T00:12:23.182Z","comments":true,"path":"2021/05/13/深度优先广度优先/","link":"","permalink":"https://sk-xinye.github.io/2021/05/13/%E6%B7%B1%E5%BA%A6%E4%BC%98%E5%85%88%E5%B9%BF%E5%BA%A6%E4%BC%98%E5%85%88/","excerpt":"","text":"BFS12345678910111213141516171819def bfs(root,n): visited = set() queue = [] result = [] pre = &#123;&#125; queue.append(root) while(queue): tmp = [] for i in range(len(queue)): nod = queue.pop(0) visited.add(nod) tmp.append(nod) nodes = nod.children rel_nodes = set(nodes).difference(visited) for i in rel_nodes: pre[i] = nod queue.extend(rel_nodes) result.append(tmp) return result DFS12345678visited = set()def dfs(root, visited): if root in visited: return visited.add(root) for node in root.children: if node not in visited: dfs(node,visited)","categories":[],"tags":[]},{"title":"递归树","slug":"递归树","date":"2021-05-12T23:28:55.000Z","updated":"2021-05-18T00:13:57.673Z","comments":true,"path":"2021/05/13/递归树/","link":"","permalink":"https://sk-xinye.github.io/2021/05/13/%E9%80%92%E5%BD%92%E6%A0%91/","excerpt":"","text":"实战一：分析快速排序的时间复杂度 快速排序的过程中，每次分区都要遍历待分区区间的所有数据，所以，每一层分区操作所遍历的数据的个数之和就是 n。我们现在只要求出递归树的高度 h，这个快排过程遍历的数据个数就是 h∗n ，也就是说，时间复杂度就是 O(h∗n)。h 为数据的高度logn","categories":[],"tags":[]},{"title":"红黑树","slug":"红黑树","date":"2021-05-12T23:11:55.000Z","updated":"2021-05-24T23:32:32.467Z","comments":true,"path":"2021/05/13/红黑树/","link":"","permalink":"https://sk-xinye.github.io/2021/05/13/%E7%BA%A2%E9%BB%91%E6%A0%91/","excerpt":"","text":"二叉查找树问题由于在插入过程中可能退化为链表，使其性能由O(logn)–&gt;O(n) 平衡二叉查找树 平衡二叉树的严格定义是这样的：二叉树中任意一个节点的左右子树的高度相差不能大于 1。 平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。 分类 AVL 红黑树 红黑树定义 所有节点是红黑和黑色组成 根节点是黑色的； 每个叶子节点都是黑色的空节点（NIL），也就是说，叶子节点不存储数据； 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的； 每个节点，从该节点到达其可达叶子节点的所有路径，都包含相同数目的黑色节点； 性质：红黑树的高度只比高度平衡的 AVL 树的高度（log2n）仅仅大了一倍，在性能上，下降得并不多。这样推导出来的结果不够精确，实际上红黑树的性能更好。","categories":[],"tags":[]},{"title":"二叉树","slug":"二叉树","date":"2021-05-12T13:44:33.000Z","updated":"2021-05-25T00:14:14.445Z","comments":true,"path":"2021/05/12/二叉树/","link":"","permalink":"https://sk-xinye.github.io/2021/05/12/%E4%BA%8C%E5%8F%89%E6%A0%91/","excerpt":"","text":"树 二叉树满二叉树除了叶子节点，其他节点都有左右两个子节点 完全二叉树叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大 二叉树的遍历- 前序遍历 123456def pre_order(root): if root is None: return print(root) pre_order(root.left) pre_order(root.right) - 中序遍历 123456def pre_order(root): if root is None: return pre_order(root.left) print(root) pre_order(root.right) - 后续遍历 123456def pre_order(root): if root is None: return pre_order(root.left) pre_order(root.right) print(root) 二叉查找树(Binary Search Tree) 二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值 中序遍历二叉查找树，可以输出有序的数据序列，时间复杂度是 O(n)，非常高效","categories":[],"tags":[]},{"title":"哈希算法","slug":"哈希算法","date":"2021-05-12T13:42:54.000Z","updated":"2021-05-13T23:20:59.907Z","comments":true,"path":"2021/05/12/哈希算法/","link":"","permalink":"https://sk-xinye.github.io/2021/05/12/%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95/","excerpt":"","text":"应用 应用一：安全加密 应用二：唯一标识 应用三：数据校验 应用四：散列函数 应用五：负载均衡 应用六：数据分片 应用七：分布式存储","categories":[],"tags":[]},{"title":"散列表","slug":"散列表","date":"2021-05-12T13:39:51.000Z","updated":"2021-05-13T23:20:59.909Z","comments":true,"path":"2021/05/12/散列表/","link":"","permalink":"https://sk-xinye.github.io/2021/05/12/%E6%95%A3%E5%88%97%E8%A1%A8/","excerpt":"","text":"散列冲突 开放寻址法 链表法","categories":[],"tags":[]},{"title":"二分","slug":"二分","date":"2021-05-12T00:14:17.000Z","updated":"2023-02-04T02:33:07.940Z","comments":true,"path":"2021/05/12/二分/","link":"","permalink":"https://sk-xinye.github.io/2021/05/12/%E4%BA%8C%E5%88%86/","excerpt":"","text":"易出错 循环退出条件注意是 low&lt;=high，而不是 low &lt; high mid 的取值mid = low+((high-low)&gt;&gt;1) low 和 high 的更新low=mid+1，high=mid-1 当寻找旋转排序数组的话，low &lt; hight 普通二分1234567891011def common_search(arr,dest): low ,high = 0,len(arr) while(low&lt;=high): mid = low+((high-low)&gt;&gt;1) if(arr[mid]==dest): return mid elif arr[mid]&lt;dest: low = mid+1 elif arr[mid]&gt;dest: high = mid-1 return -1 查找第一个等于给定元素值1234567891011121314def first_dest(arr,dest): low,high = 0,len(arr)-1 while(low&lt;=high): mid = low + ((high-low)&gt;&gt;1) if(arr[mid]&lt;dest): low = mid +1 elif arr[mid]&gt;dest: high = mid -1 else: if mid ==0 or arr[mid-1]!=dest: return mid else: high = mid -1 return -1 查找最后一个等于给定元素1234567891011121314def last_dest(arr,dest): low,high = 0,len(dest)-1 while(low&lt;=high): mid = low + ((high-low)&gt;&gt;1) if (arr[mid] &lt; dest): low = mid + 1 elif arr[mid] &gt; dest: high = mid - 1 else: if mid==len(arr)-1 or arr[mid+1]!= dest: return mid else: low = mid +1 return -1 查找第一个大于等于给定元素123456789101112def first_gte_dest(arr,dest): low,high = 0,len(arr)-1 while(low&lt;=high): mid = low + ((high-low)&gt;&gt;1) if arr[mid]&gt;=dest: if mid==0 or arr[mid-1]&lt;dest: return mid else: high = mid-1 else: low = mid +1 return -1 查找最后一个小于等于给定元素123456789101112def last_lt_dest(arr,dest): low,high = 0,len(arr)-1 while(low&lt;=high): mid = low+((high-low)&gt;&gt;1) if arr[mid]&lt;=dest: if mid==len(arr)-1 or arr[mid +1]&gt;dest: return mid else: low = mid+1 else: high = mid-1 return -1 x平方根123456789101112131415class Solution(object): def mySqrt(self, x): &quot;&quot;&quot; :type x: int :rtype: int &quot;&quot;&quot; l,r,ans = 0,x,-1 while l&lt;=r: mid = l+((r-l)&gt;&gt;1) if mid*mid&lt;=x: ans = mid l=mid+1 else: r=mid-1 return ans x的n次幂1234567891011121314def myPow(self, x: float, n: int) -&gt; float: if n&lt;0: x = 1/x n = -n def dfs(x,n): if n==0: return 1 tmp = n//2 t = dfs(x,tmp) if n%2==1: re = t*t*x else: re = t*t return re","categories":[],"tags":[]},{"title":"排序","slug":"排序","date":"2021-05-11T23:51:13.000Z","updated":"2021-05-17T00:12:21.454Z","comments":true,"path":"2021/05/12/排序/","link":"","permalink":"https://sk-xinye.github.io/2021/05/12/%E6%8E%92%E5%BA%8F/","excerpt":"","text":"冒泡123456789101112131415161718// 冒泡排序，a表示数组，n表示数组大小public void bubbleSort(int[] a, int n) &#123; if (n &lt;= 1) return; for (int i = 0; i &lt; n; ++i) &#123; // 提前退出冒泡循环的标志位 boolean flag = false; for (int j = 0; j &lt; n - i - 1; ++j) &#123; if (a[j] &gt; a[j+1]) &#123; // 交换 int tmp = a[j]; a[j] = a[j+1]; a[j+1] = tmp; flag = true; // 表示有数据交换 &#125; &#125; if (!flag) break; // 没有数据交换，提前退出 &#125;&#125; 插入12345678910111213141516171819// 插入排序，a表示数组，n表示数组大小public void insertionSort(int[] a, int n) &#123; if (n &lt;= 1) return; for (int i = 1; i &lt; n; ++i) &#123; int value = a[i]; int j = i - 1; // 查找插入的位置 for (; j &gt;= 0; --j) &#123; if (a[j] &gt; value) &#123; a[j+1] = a[j]; // 数据移动 &#125; else &#123; break; &#125; &#125; a[j+1] = value; // 插入数据 &#125;&#125; 归并123456789101112131415161718192021222324252627282930递推公式：merge_sort(p…r) = merge(merge_sort(p…q), merge_sort(q+1…r))终止条件：p &gt;= r 不用再继续分解def merge(arr): merge_sort(arr,0,len(arr)-1)def merge_sort(arr,start,end): if start&gt;=end: return mid = start + ((end-start)&gt;&gt;1) merge_sort(arr,start,mid) merge_sort(arr,mid+1,end) merge_all(arr,start,mid,end)def merge_all(arr,start,mid,end): i,j = start,mid+1 tmp = [] while i&lt;=mid and j&lt;=end: if arr[i]&lt;=arr[j]: tmp.append(arr[i]) i+=1 else: tmp.append(arr[j]) j+=1 if i&lt;=mid: tmp.extend(arr[i:mid+1]) if j&lt;=end: tmp.extend(arr[j:end+1]) for i in range(len(tmp)): arr[i+start] = tmp[i] 快排1234567891011121314151617181920212223242526递推公式：quick_sort(p…r) = quick_sort(p…q-1) + quick_sort(q+1… r)终止条件：p &gt;= rdef quick(arr): quick_sort(arr,0,len(arr)-1)def quick_sort(arr,start,end): if start&gt;=end: return partition = get_partition(arr,start,end) quick_sort(arr,start,partition-1) quick_sort(arr,partition+1,end)def get_partition(arr,start,end): tmp = arr[end] i = start for j in range(start,end): if arr[j]&lt;tmp: arr[i],arr[j]= arr[j],arr[i] i+=1 arr[i],arr[end]= arr[end],arr[i] return i O(n)找数组中最大元素我们选择数组区间 A[0…n-1]的最后一个元素 A[n-1]作为 pivot，对数组 A[0…n-1]原地分区，这样数组就分成了三部分，A[0…p-1]、A[p]、A[p+1…n-1]。如果 p+1=K，那 A[p]就是要求解的元素；如果 K&gt;p+1, 说明第 K 大元素出现在 A[p+1…n-1]区间，我们再按照上面的思路递归地在 A[p+1…n-1]这个区间内查找。同理，如果 K 123456789def get_k(arr,k): k(arr,0,len(arr-1))def k(arr,start,end,k): if start&gt;=end: return partition = get_partition(arr,start,end,k) if arr[partition+1] == k: return arr[partition+1] 堆排序12345678910111213141516171819202122232425262728293031323334353637383940414243444546def heapify(arr,last_num,top_num): while True: max_num = top_num if top_num*2&lt;=last_num and arr[top_num]&lt;arr[top_num*2]: max_num = top_num*2 if top_num*2+1&lt;=last_num and arr[max_num]&lt;arr[top_num*2+1]: max_num = top_num*2+1 if top_num == max_num: break arr[top_num],arr[max_num] = arr[max_num],arr[top_num] top_num = max_numdef build_heap(arr,last_num): i = last_num//2 while(i&gt;=1): heapify(arr,last_num,i) i-=1def heap_sort(arr,last_num): build_heap(arr,last_num) s = last_num while(s&gt;1): arr[1],arr[s] = arr[s],arr[1] s-=1 heapify(arr,s,1)topk def get_least_numbers(arr,k): if k==0: return list() hp = [-x for x in arr[:k]] heapq.heapify(hp) for i in range(k,len(arr)): if -hp[0]&gt;arr[i]: heapq.heappop(hp) heapq.heappush(hp,-arr[i]) ans = [-x for x in hp] return ansdef getMaxNumbers(arr,k): if k == 0: return list() hp = [x for x in arr[:k]] heapq.heapify(hp) for i in range(k, len(arr)): if hp[0] &lt; arr[i]: heapq.heappop(hp) heapq.heappush(hp, arr[i]) ans = [x for x in hp] return ans","categories":[],"tags":[]},{"title":"递归","slug":"递归","date":"2021-05-11T23:35:47.000Z","updated":"2021-05-27T00:11:33.905Z","comments":true,"path":"2021/05/12/递归/","link":"","permalink":"https://sk-xinye.github.io/2021/05/12/%E9%80%92%E5%BD%92/","excerpt":"","text":"条件 一个问题的解可以分解为几个子问题的解 这个问题与分解之后的子问题，除了数据规模不同，求解思路完全一样 存在递归终止条件 如何编写递归 写递归代码的关键就是找到如何将大问题分解为小问题的规律 并且基于此写出递推公式 然后再推敲终止条件 最后将递推公式和终止条件翻译成代码。 12345678910111213141516171819202122232425262728293031// 递推公式：f(n) = f(n-1)+f(n-2)// 条件n ==1 n==2 public int f(int n) &#123; if (n == 1) return 1; if (n == 2) return 2; // hasSolvedList可以理解成一个Map，key是n，value是f(n) if (hasSolvedList.containsKey(n)) &#123; return hasSolvedList.get(n); &#125; int ret = f(n-1) + f(n-2); hasSolvedList.put(n, ret); return ret;&#125;int f(int n) &#123; if (n == 1) return 1; if (n == 2) return 2; int ret = 0; int pre = 2; int prepre = 1; for (int i = 3; i &lt;= n; ++i) &#123; ret = pre + prepre; prepre = pre; pre = ret; &#125; return ret;&#125; 确定递归函数的参数和返回值：确定哪些参数是递归的过程中需要处理的，那么就在递归函数里加上这个参数， 并且还要明确每次递归的返回值是什么进而确定递归函数的返回类型。 确定终止条件：写完了递归算法, 运行的时候，经常会遇到栈溢出的错误，就是没写终止条件或者终止条件写的不对，操作系统也是用一个栈的结构来保存每一层递归的信息，如果递归没有终止，操作系统的内存栈必然就会溢出。 确定单层递归的逻辑：确定每一层递归需要处理的信息。在这里也就会重复调用自己来实现递归的过程。","categories":[],"tags":[]},{"title":"栈","slug":"栈","date":"2021-05-10T23:58:07.000Z","updated":"2021-06-17T13:06:33.159Z","comments":true,"path":"2021/05/11/栈/","link":"","permalink":"https://sk-xinye.github.io/2021/05/11/%E6%A0%88/","excerpt":"","text":"定义先进后出，后进先出；当某个数据集合只涉及在一端插入和删除数据，并且满足后进先出、先进后出的特性，这时我们就应该首选“栈”这种数据结构。 栈操作 有效括号 1234567891011121314151617class Solution(object): def isValid(self, s): &quot;&quot;&quot; :type s: str :rtype: bool &quot;&quot;&quot; if len(s)%2!=0: return False stack = [] for i in s: if i ==&#x27;(&#x27;: stack.append(&#x27;)&#x27;) elif i==&quot;&#123;&quot;: stack.append(&quot;&#125;&quot;) elif i==&quot;[&quot;: stack.append(&quot;]&quot;) elif not stack or stack.pop() != i: return False if stack: return False return True 最小栈 123456789101112131415161718192021222324252627282930313233343536class MinStack(object): def __init__(self): &quot;&quot;&quot; initialize your data structure here. &quot;&quot;&quot; self.cache = [] def push(self, val): &quot;&quot;&quot; :type val: int :rtype: None &quot;&quot;&quot; if not self.cache: self.cache.append((val,val)) else: self.cache.append((val,min(val,self.cache[-1][1]))) def pop(self): &quot;&quot;&quot; :rtype: None &quot;&quot;&quot; return self.cache.pop() def top(self): &quot;&quot;&quot; :rtype: int &quot;&quot;&quot; return self.cache[-1][0] def getMin(self): &quot;&quot;&quot; :rtype: int &quot;&quot;&quot; return self.cache[-1][1]","categories":[],"tags":[]},{"title":"链表","slug":"链表","date":"2021-05-09T09:49:37.000Z","updated":"2023-07-16T13:14:55.057Z","comments":true,"path":"2021/05/09/链表/","link":"","permalink":"https://sk-xinye.github.io/2021/05/09/%E9%93%BE%E8%A1%A8/","excerpt":"","text":"常见缓存 先进先出策略 FIFO（First In，First Out） 最少使用策略 LFU（Least Frequently Used） 最近最少使用策略 LRU（Least Recently Used） 1234567891011121314151617181920212223def josephus1(num, k, m):&quot;&quot;&quot;约瑟夫环（约瑟夫问题）是一个数学的应用问题：已知num个人（以编号1，2，3...n分别表示）围坐在一张圆桌周围。从编号为k的人开始报数，数到m的那个人出列；他的下一个人又从1开始报数，数到m的那个人又出列；依此规律重复下去，直到圆桌周围的人全部出列。:param num:总人数:param k:开始的编号:param m:数到m的出列:return:&quot;&quot;&quot;alist = [x + 1 for x in range(num)]print(alist) # [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]index, step = k-1, m # 从1号开始报数，数到3的那个人出列while len(alist) &gt; 1: index = (index + step - 1) % len(alist) # 列表的索引 print(&#x27;出去的数：&#x27;, alist[index]) # del alist[index] alist.pop(index)return &#x27;最后的一个数：%s&#x27; % alist[0]print(josephus1(13, 1, 3)) LRU实现12345678910111213141516171819202122232425262728293031323334class LRUCache(object):def __init__(self, capacity): &quot;&quot;&quot; :type capacity: int &quot;&quot;&quot; self.capacity = capacity self.cache = collections.OrderedDict()def get(self, key): &quot;&quot;&quot; :type key: int :rtype: int &quot;&quot;&quot; if key not in self.cache: return -1 value = self.cache.pop(key) self.cache[key]=value return valuedef put(self, key, value): &quot;&quot;&quot; :type key: int :type value: int :rtype: None &quot;&quot;&quot; if key in self.cache: self.cache.pop(key) else: if self.capacity==0: self.cache.popitem(last=False) else: self.capacity -=1 self.cache[key] = value 链表操作指针含义 有些语言有“指针”的概念，比如 C 语言；有些语言没有指针，取而代之的是“引用”，比如 Java、Python。不管是“指针”还是“引用”，实际上，它们的意思都是一样的，都是存储所指对象的内存地址。 将某个变量赋值给指针，实际上就是将这个变量的地址赋值给指针，或者反过来说，指针中存储了这个变量的内存地址，指向了这个变量，通过指针就能找到这个变量。 操作 12345678910错误实现p-&gt;next = x; // 将p的next指针指向x结点；x-&gt;next = p-&gt;next; // 将x的结点的next指针指向b结点；正确new_node-&gt;next = p-&gt;next;p-&gt;next = new_node;删除节点p-&gt;next = p-&gt;next-&gt;next; 我们插入结点时，一定要注意操作的顺序，要先将结点 x 的 next 指针指向结点 b，再把结点 a 的 next 指针指向结点 x，这样才不会丢失指针，导致内存泄漏。 利用哨兵在头节点前插入哨兵，指向头节点，这样下来，当遇到头节点为空，或者末尾节点为空的时候，就不用多余的判断了，可以继续使用上面的操作 重点留意边界问题 如果链表为空时，代码是否能正常工作？ 如果链表只包含一个结点时，代码是否能正常工作？ 如果链表只包含两个结点时，代码是否能正常工作？ 代码逻辑在处理头结点和尾结点的时候，是否能正常工作？、 常见链表操作 单链表反转 1234567891011121314class Solution(object): def reverseList(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; pre = None cur = head while cur is not None: tmp = cur.next cur.next = pre pre = cur cur = tmp return pre 链表中环的检测 1234567891011121314151617181920212223242526272829303132class Solution(object): def hasCycle(self, head): &quot;&quot;&quot; 是否有环 :type head: ListNode :rtype: bool &quot;&quot;&quot; fast,slow = head,head while True: if fast is None or fast.next is None: return False slow,fast = slow.next , fast.next.next if slow == fast: return True def detectCycle(self, head): &quot;&quot;&quot; 环的位置 :type head: ListNode :rtype: ListNode &quot;&quot;&quot; slow,fast = head,head while True: if fast is None or fast.next is None: return slow,fast = slow.next,fast.next.next if slow==fast: break slow = head while(slow!=fast): slow,fast = slow.next,fast.next return slow 两个有序的链表合并 1234567891011121314151617181920class Solution(object): def mergeTwoLists(self, l1, l2): &quot;&quot;&quot; :type l1: ListNode :type l2: ListNode :rtype: ListNode &quot;&quot;&quot; pre = ListNode() result = pre while l1 is not None and l2 is not None: if l1.val &lt;= l2.val: result.next = l1 l1 = l1.next else: result.next = l2 l2= l2.next result = result.next result.next = l1 if l1 is not None else l2 return pre.next 删除链表倒数第 n 个结点 12345678910111213141516171819class Solution(object): def removeNthFromEnd(self, head, n): &quot;&quot;&quot; :type head: ListNode :type n: int :rtype: ListNode &quot;&quot;&quot; pre = ListNode(-1,head) fast = pre slow = pre for i in range(1,n+2): fast = fast.next while(fast!=None): fast = fast.next slow = slow.next slow.next = slow.next.next return pre.next 求链表的中间结点 12345678910111213class Solution(object): def middleNode(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; if head is None or head.next ==None: return head slow,fast = head,head while(fast !=None and fast.next !=None): slow = slow.next fast = fast.next.next return slow 两两交换节点 123456789101112class Solution(object): def swapPairs(self, head): &quot;&quot;&quot; :type head: ListNode :rtype: ListNode &quot;&quot;&quot; if head is None or head.next is None: return head tmp = head.next head.next = self.swapPairs(tmp.next) tmp.next = head return tmp","categories":[],"tags":[]},{"title":"数组","slug":"数组","date":"2021-05-09T09:22:45.000Z","updated":"2021-05-17T00:12:23.182Z","comments":true,"path":"2021/05/09/数组/","link":"","permalink":"https://sk-xinye.github.io/2021/05/09/%E6%95%B0%E7%BB%84/","excerpt":"","text":"概念 数组（Array）是一种线性表数据结构。它用一组连续的内存空间，来存储一组具有相同类型的数据。 第一是线性表（Linear List）。顾名思义，线性表就是数据排成像一条线一样的结构。 非线性表，比如二叉树、堆、图等。 链表适合插入、删除，时间复杂度 O(1),数组支持随机访问，根据下标随机访问的时间复杂度为 O(1) 低效的“插入”和“删除” 如果需要数组有序，需要挪动数组数据，很低效 如果不需要有序，只需要将要插入位置的数据插入到末尾，然后将数据进行插入即可 删除操作时，可以先进行标记，有必要的时候在进行数组移动 经验总结 Java ArrayList 无法存储基本类型，比如 int、long，需要封装为 Integer、Long 类，而 Autoboxing、Unboxing 则有一定的性能消耗，所以如果特别关注性能，或者希望使用基本类型，就可以选用数组。 如果数据操作非常简大小事先已知，并且对数据的单，用不到 ArrayList 提供的大部分方法，也可以直接使用数组。 还有一个是我个人的喜好，当要表示多维数组时，用数组往往会更加直观。比如 Object[][] array；而用容器的话则需要这样定义：ArrayList &gt; array。 我总结一下，对于业务开发，直接使用容器就足够了，省时省力。毕竟损耗一丢丢性能，完全不会影响到系统整体的性能。但如果你是做一些非常底层的开发，比如开发网络框架，性能的优化需要做到极致，这个时候数组就会优于容器，成为首选。 为什么要从0开始不是1 如果从0开始，那a[k]的寻址公式即：a[k]_address = base_address + k * type_size 如果从1开始，那a[K]的寻址公式即：a[k]_address = base_address + (k-1)*type_size,每次多一个减法操作 历史原因 C语言最开始就用的0 ，然后别的语言都开始模仿，久而久之 题三数之和12345678910111213141516171819202122232425262728class Solution(object): def threeSum(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: List[List[int]] &quot;&quot;&quot; nums.sort() result = [] k = 0 for k in range(0,len(nums)-2): if nums[k]&gt;0:break if k&gt;0 and nums[k]==nums[k-1]:continue i,j = k+1,len(nums)-1 while i&lt;j: s = nums[i] + nums[k]+ nums[j] if s&lt;0: i+=1 while i&lt;j and nums[i]==nums[i-1]:i+=1 elif s&gt;0: j-=1 while i&lt;j and nums[j]==nums[j+1]:j-=1 elif s==0: result.append([nums[k],nums[i],nums[j]]) i+=1 j-=1 while i&lt;j and nums[i]==nums[i-1]:i+=1 while i&lt;j and nums[j]==nums[j+1]:j-=1 return result 两数之和1234567891011121314class Solution(object): def twoSum(self, nums, target): &quot;&quot;&quot; :type nums: List[int] :type target: int :rtype: List[int] &quot;&quot;&quot; cache = &#123;&#125; for i,num in enumerate(nums): if num in cache: return [cache[num],i] else: cache[target-num]=i return [] 多数元素12345678class Solution(object): def majorityElement(self, nums): &quot;&quot;&quot; :type nums: List[int] :rtype: int &quot;&quot;&quot; nums.sort() return nums[len(nums)//2]","categories":[],"tags":[]},{"title":"时间复杂度","slug":"时间复杂度","date":"2021-05-09T08:33:52.000Z","updated":"2021-05-13T23:20:59.909Z","comments":true,"path":"2021/05/09/时间复杂度/","link":"","permalink":"https://sk-xinye.github.io/2021/05/09/%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6/","excerpt":"","text":"时间复杂度大O复杂度表示法所有代码的执行时间 T(n) 与每行代码的执行次数 f(n) 成正比。 时间复杂度分析 只关注循环执行次数最多的一段代码我们在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了 加法法则：总复杂度等于量级最大的那段代码的复杂度总的时间复杂度就等于量级最大的那段代码的时间复杂度 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积 几种常见的时间复杂度 分类 我们可以粗略地分为两类，多项式量级和非多项式量级。其中，非多项式量级只有两个：O(2n) 和 O(n!)。 我们把时间复杂度为非多项式量级的算法问题叫作 NP（Non-Deterministic Polynomial，非确定多项式）问题。 具体分析 O(1) 一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。 O(logn)、O(nlogn) 归并排序、快速排序的时间复杂度都是 O(nlogn) O(m+n)、O(m*n) 空间复杂度渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。类比一下，空间复杂度全称就是渐进空间复杂度（asymptotic space complexity），表示算法的存储空间与数据规模之间的增长关系。 我们常见的空间复杂度就是 O(1)、O(n)、O(n2 )，像 O(logn)、O(nlogn) 这样的对数阶复杂度平时都用不到。","categories":[],"tags":[]},{"title":"分布式事务","slug":"分布式事务","date":"2021-05-06T08:34:18.000Z","updated":"2021-07-04T09:38:08.800Z","comments":true,"path":"2021/05/06/分布式事务/","link":"","permalink":"https://sk-xinye.github.io/2021/05/06/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"分布式事务解决方案数据库事务 原子性(Atomicity ) 一致性( Consistency ) 隔离性或独立性( Isolation) 持久性(Durabilily) 分布式理论CAP理论CAP定理是由加州大学伯克利分校Eric Brewer教授提出来的，他指出WEB服务无法同时满足一下3个属性： 一致性(Consistency) ： 客户端知道一系列的操作都会同时发生(生效) 可用性(Availability) ： 每个操作都必须以可预期的响应结束 分区容错性(Partition tolerance) ： 即使出现单个组件无法可用,操作依然可以完成 CAP原则指CAP三者不能同时满足，要么能同时满足CP即同时满足区分容错性和一致性，要么同时满足AP即同时满足区分容错性和可用性。从中可以看出，P是分布式系统的基础，没有区分容错性就谈不上分布式系统了 BASE理论前面讲到分布式系统的CAP原则要么同时满足AP要么同时满足CP，那么BASE理论则是CAP原则权衡的结果。BASE是指Basically Available（基本可用的）,Soft state（软状态）,Eventual consistency（最终一致性）。 Basically Available是指在分布式集群节点中，若某个节点宕机，或者在数据在节点间复制的过程中，只有部分数据不可用，但不影响整个系统的整体的可用性。 Soft state是指软状态即这个状态只是一个中间状态，允许数据在节点集群间操作过程中存在存在一个时延，这个中间状态最终会转化为最终状态。 Eventual consistency是指数据在分布式集群节点间操作过程中存在时延，与ACID相反，最终一致性不是强一致性，在经过一定时间后，分布式集群节点间的数据拷贝能达到最终一致的状态。 解决方案两阶段提交(2pc) 准备阶段 协调者节点向所有参与者节点询问是否可以执行提交操作(vote)，并开始等待各参与者节点的响应。 参与者节点执行询问发起为止的所有事务操作，并将Undo信息和Redo信息写入日志。（注意：若成功这里其实每个参与者已经执行了事务操作） 各参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个”同意”消息；如果参与者节点的事务操作实际执行失败，则它返回一个”中止”消息。由协调者判断是否继续 提交阶段 当协调者节点从所有参与者节点获得的相应消息都为”同意”时： 协调者节点向所有参与者节点发出”正式提交(commit)”的请求。 参与者节点正式完成操作，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”完成”消息。 协调者节点受到所有参与者节点反馈的”完成”消息后，完成事务。 如果任一参与者节点在第一阶段返回的响应消息为”中止”，或者 协调者节点在第一阶段的询问超时之前无法获取所有参与者节点的响应消息时： 协调者节点向所有参与者节点发出”回滚操作(rollback)”的请求。 参与者节点利用之前写入的Undo信息执行回滚，并释放在整个事务期间内占用的资源。 参与者节点向协调者节点发送”回滚完成”消息。 协调者节点受到所有参与者节点反馈的”回滚完成”消息后，取消事务。 不管最后结果如何，第二阶段都会结束当前事务。 问题 2PC是一个强一致性协议，同时它在实际应用中还存在几个问题： 同步阻塞，2PC的两个阶段中，协调者和参与者的通信都是同步的，这会导致整个事务的长时间阻塞 Coordinator的单点问题 数据不一致，在Commit阶段，可能存在只有部分参与者收到Commit消息（或处理成功）的情况 三阶段提交(3PC)3PC即三阶段提交，它比2PC多了一个阶段，即把原来2PC的准备阶段拆分成CanCommit和PreCommit两个阶段，同时引入超时机制来解决2PC的同步阻塞问题。 但是在我看来3PC并没有解决2PC的根本问题，它只是在2PC的基础上做了一些优化，它增加了一个阶段（也增加了1个RTT）来提高对方可用性的概率，这本质跟TCP的三次握手一样，同样也改为四次握手，五次握手等等。 第二阶段超时回滚，第三阶段超时提交 补偿事务(TCC:Try-Confirm-Cancel) TCC 其实就是采用的补偿机制，其核心思想是：针对每个操作，都要注册一个与其对应的确认和补偿（撤销）操作。它分为三个阶段： Try 阶段主要是对业务系统做检测及资源预留 Confirm 阶段主要是对业务系统做确认提交，Try阶段执行成功并开始执行 Confirm阶段时，默认 Confirm阶段是不会出错的。即：只要Try成功，Confirm一定成功。 Cancel 阶段主要是在业务执行错误，需要回滚的状态下执行的业务取消，预留资源释放。 举个例子，假入 Bob 要向 Smith 转账，思路大概是： 我们有一个本地方法，里面依次调用 1、首先在 Try 阶段，要先调用远程接口把 Smith 和 Bob 的钱给冻结起来。 2、在 Confirm 阶段，执行远程调用的转账的操作，转账成功进行解冻。 3、如果第2步执行成功，那么转账成功，如果第二步执行失败，则调用远程冻结接口对应的解冻方法 (Cancel)。 优点： 跟2PC比起来，实现以及流程相对简单了一些，但数据的一致性比2PC也要差一些 缺点： 缺点还是比较明显的，在2,3步中都有可能失败。TCC属于应用层的一种补偿方式，所以需要程序员在实现的时候多写很多补偿的代码，在一些场景中，一些业务流程可能用TCC不太好定义及处理。 本地消息表本地消息表方案应该是业界内使用最为广泛的，因为它使用简单，成本比较低。本地消息表的方案最初是由eBay提出（完整方案），核心思路是将分布式事务拆分成本地事务进行处理。它的处理流程如下： 事务发起方把要处理的业务事务和写消息表这两个操作放在同一个本地事务里 事务发起方有一个定时任务轮询消息表，把没处理的消息发送到消息中间件 事务被动方从消息中间件获取消息后，返回成功 事务发起方更新消息状态为已成功 一些分析: 把业务处理和写消息表放在同一个事务是为了失败/异常后可以同时回滚 为什么不直接发消息，而是先写消息表？试想，如果发送消息超时了，即不确定消息中间件收到消息没，那么你是重试还是抛异常回滚事务呢？回滚是不行的，因为可能消息中间件已经收到消息，接收方收到消息后做处理，导致双方数据不一致了；重试也是不行的，因为有可能会一直重试失败，导致事务阻塞。 基于上述分析，消息的接收方是需要做幂等操作的 本地消息表方案整体来说还是比较简单、可用的，但是也有以下缺点： 消息数据和业务数据耦合，消息表需要根据具体的业务场景制定，不能公用。就算可以公用消息表，对于分库的业务来说每个库都是需要消息表的。 只适用于最终一致的业务场景。例如在 A -&gt; B场景下，在不考虑网络异常、宕机等非业务异常的情况下，A成功的话，B肯定也会成功的。 生产者记录任务，处理消息，发送消息；接受者从队列拿取消息，并更改生产者的消息状态；定时轮询生产者中本地记录任务，当存在没有发送成功的，就进行重发；接受者需要启用幂等 Sagas事务模型Saga事务模型又叫做长时间运行的事务（Long-running-transaction）, 它是由普林斯顿大学的H.Garcia-Molina等人提出，它描述的是另外一种在没有两阶段提交的的情况下解决分布式系统中复杂的业务事务问题。我们这里说的是一种基于 Sagas 机制的工作流事务模型，这个模型的相关理论目前来说还是比较新的，以至于百度上几乎没有什么相关资料。 该模型其核心思想就是拆分分布式系统中的长事务为多个短事务，或者叫多个本地事务，然后由 Sagas 工作流引擎负责协调，如果整个流程正常结束，那么就算是业务成功完成，如果在这过程中实现失败，那么Sagas工作流引擎就会以相反的顺序调用补偿操作，重新进行业务回滚。 比如我们一次关于购买旅游套餐业务操作涉及到三个操作，他们分别是预定车辆，预定宾馆，预定机票，他们分别属于三个不同的远程接口。可能从我们程序的角度来说他们不属于一个事务，但是从业务角度来说是属于同一个事务的。 们的执行顺序如上图所示，所以当发生失败时，会依次进行取消的补偿操作。 因为长事务被拆分了很多个业务流，所以 Sagas 事务模型最重要的一个部件就是工作流或者你也可以叫流程管理器（Process Manager），工作流引擎和Process Manager虽然不是同一个东西，但是在这里，他们的职责是相同的 https://albenw.github.io/posts/425b6837/https://juejin.cn/post/6844903647197806605https://www.sofastack.tech/blog/sofa-meetup-3-seata-retrospect/https://albenw.github.io/posts/255bd548/ 可以参考他的分布式事务","categories":[],"tags":[]},{"title":"用户态及内核态","slug":"用户态及内核态","date":"2021-05-06T06:44:38.000Z","updated":"2021-05-08T13:59:05.035Z","comments":true,"path":"2021/05/06/用户态及内核态/","link":"","permalink":"https://sk-xinye.github.io/2021/05/06/%E7%94%A8%E6%88%B7%E6%80%81%E5%8F%8A%E5%86%85%E6%A0%B8%E6%80%81/","excerpt":"","text":"为什么要有用户态和内核态基本概念 Linux所谓的用户态和内核态，本质是对CPU提供的功能的一层封装抽象。 现代CPU，其设计目标主要是为了完美高效的实现一个多任务系统，多任务系统的三个核心特征是： 权限分级 数据隔离 任务切换。 以X86_64架构为例， 权限分级通过CPU的多模式机制和分段机制实现， 数据隔离通过分页机制实现， 任务切换通过中断机制和任务机制（TR/TSS）实现。 内核态和用户态的概念，是Linux为了有效实现CPU的权限分级和数据隔离的目标而出现的，是通过组合CPU的分段机制+分页机制而形成的。 当CPU处于保护模式下时（X86_64CPU有5种模式，保护模式是其中之一，此时CPU.CR0.PE=1）， 当CPU.CS=系统代码段时（CS.CPL=0）为内核态，此时通过CPU的指令有操控全部寄存器的权限（包括FLAGS和CR寄存器）， 当CPU.CS=用户代码段时（CS.CPL=3）为用户态，此时通过CPU的指令只有操控部分寄存器的权限。 所谓“一个进程主动跳进内核态”，是指该进程中的一个执行线程通过INT或者SYSCALL指令，使得当前线程的CS=系统代码段（这里还有不同的细节，不多说了）。 每个用户进程都有自己的虚拟地址空间，用户进程之间切换的时候，通过切换页表（CR3）来实现，不用改CS寄存器和DS寄存器。所以没有“代码映射在0-3G”一说。就像班级名称和学号，同样的学号在不同的班级代表不同的人，同样的虚拟地址在不同的页表中代表不同的物理内存空间。 内核态允许多个用户线程同时进入，不存在阻塞的现象，尤其是多核CPU的情况下。 除了用户线程可以进入内核态之外，还有内核线程，即内核自己要干的事情，这种线程只运行在内核态 图1：从CPU的寄存器视角看，指令运行所处的三种CPU状态 图2：从Linux线程角度看，指令运行所处的两种状态（要么属于某线程，要么黑色切换中） 图3：Linux下，线程类型*CPU状态决定数据空间","categories":[],"tags":[]},{"title":"虚拟内存管理","slug":"虚拟内存管理","date":"2021-05-06T05:47:07.000Z","updated":"2021-05-08T13:59:05.040Z","comments":true,"path":"2021/05/06/虚拟内存管理/","link":"","permalink":"https://sk-xinye.github.io/2021/05/06/%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/","excerpt":"","text":"内存管理 连续分配管理方式 分区式存储管理最大的缺点是碎片问题严重，内存利用率低。 原因，主要在于连续分配的限制，即它要求每个作用在内存中必须占一个连续的分区。 如果允许将一个进程分散地装入到许多不相邻的分区中，便可充分地利用内存，而无需再进行“紧凑”。基于这一思想，产生了“非连续分配方式”，或者称为“离散分配方式”。 实分页存储管理 优点： 较好地解决了碎片问题 打破了存储分配的连续性要求 提高了主存的利用率 缺点： 页内碎片 动态地址变换、方案实施需耗用额外的系统资源 存储扩充问题没有解决——作业大小受到限制，可用块数小于作业需求时需等待 虚拟存储管理系统局部性原理 指程序在执行过程中的一个较短时期，所执行的指令地址和指令的操作数地址，分别局限于一定区域。还可以表现为： 时间局部性：一条指令的一次执行和下次执行，一个数据的一次访问和下次访问都集中在一个较短时期内； 空间局部性：当前指令和邻近的几条指令，当前访问的数据和邻近的数据都集中在一个较小区域内。 局部性原理的具体体现： 程序在执行时，大部分是顺序执行的指令，少部分是转移和过程调用指令。 过程调用的嵌套深度一般不超过5，因此执行的范围不超过这组嵌套的过程。 程序中存在相当多的循环结构，它们由少量指令组成，而被多次执行。 程序中存在相当多对一定数据结构的操作，如数组操作，往往局限在较小范围内。 虚拟存储管理系统的好处 大程序：可在较小的可用内存中执行较大的用户程序； 大的用户空间：提供给用户可用的虚拟内存空间通常大于物理内存(real memory) 并发：可在内存中容纳更多程序并发执行； 易于开发：与覆盖技术比较，不必影响编程时的程序结构 虚拟存储技术特征 不连续性：物理内存分配的不连续，虚拟地址空间使用的不连续（数据段和栈段之间的空闲空间，共享段和动态链接库占用的空间） 部分交换：与交换技术相比较，虚拟存储的调入和调出是对部分虚拟地址空间进行的； 大空间：通过物理内存和快速外存相结合，提供大范围的虚拟地址空间 虚拟存储技术分类 虚拟页式 虚拟段式 虚拟段页式 虚拟页式存储管理基本原理 虚拟页式存储管理实际是实分页技术与虚拟存储技术相结合的产物，其分页思想与实分页是一样的。这里的请求调入和置换功能都是比实分页存储管理增加的内容，是实现虚拟存储的主要功能。 主存页面分配策略 在虚拟页式存储管理中，内存分配似实分页方式，但还必须考虑解决下面两个问题： （1）是否对各进程采用平均分配策略？ a、平均分配。 b、按进程长度比例分配。 c、按进程优先级分配。 d、按进程长度和优先级别分配。 （2）发生缺页中断时，如何为所缺的页面分配内存？ a、固定分配局部置换。 b、可变分配全局置换。 页面调入策略 （1）请求调入 当发生页面故障时进行调度，即当进程访问不在内存的页面引发缺页中断时，由系统根据这种访问请求把所缺页面装入内存。 优点：由请求调入策略装入的页一定会被访问，再加之比较容易实现，故在目前的虚拟存储器中，大多采用此策略。 缺点：每次仅调入一页，增加了磁盘I/O的启动频率。 （ 2）预调入 也称先行调度，即一页面被访问前就已经预先置入内存，以减少今后的缺页率。 主要适于进程的许多页存放在外存的连续区域中的情况。有的系统结合请求调入使用，即每次缺页时装入多个页面。 优点：提高调页的I/O效率。 缺点：基于预测，若调入的页在以后很少被访问，则效率低。常用于程序装入时的调页。 页面调度算法 a.最佳淘汰算法：OPT 淘汰以后不用的，但是这是个衡量指标，不能实现太理想化了 b.FIFO：先进先出，用队列实现 c.最近最久未使用算法(LRU)：根据调入内存的情况，选择内存中最久未使用的被置换 计时法： 堆栈法： 多维寄存器 d.二次机会淘汰算法。SC(Second Chance) e.时钟淘汰算法： f.最近未使用淘汰算法 缺页中断率因素 （1）调度算法不合理 （2）分配给作业的内存块数太少 （3）页面大小的选择不合理 （4）用户程序编制的方法不合理 在mysql中，选择B+树的原因就在于此，为了尽量较少磁盘IO","categories":[],"tags":[]},{"title":"mysql锁","slug":"4mysql锁","date":"2021-05-05T12:46:31.000Z","updated":"2021-06-14T23:57:02.646Z","comments":true,"path":"2021/05/05/4mysql锁/","link":"","permalink":"https://sk-xinye.github.io/2021/05/05/4mysql%E9%94%81/","excerpt":"","text":"分类范围分类 全局锁Flush tables with read lock (FTWRL)，对整个数据库实例加锁。全库的逻辑备份 表锁 lock tables … read/write。可以使用unlock tables 主动释放锁，也可以在客户端断开的时候自动释放。 自动加上的，当对一个表做增删改查操作的时候，加 MDL 读锁；当要对表做结构变更操作的时候，加 MDL 写锁。 行锁引擎自己实现的 间隙锁(不是的) 因为间隙锁在可重复读隔离级别下才有效，所以本篇文章接下来的描述，若没有特殊说明，默认是可重复读隔离级别。 程序员角度（线程是否对资源加锁，https://juejin.cn/post/6844904022202122248#heading-27） 悲观锁(多写) 假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制 乐观锁(多读) 假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过version的方式来进行锁定。实现方式：乐一般会使用版本号机制或CAS算法实现 数据库角度 （多个线程能否获取同一把锁） 共享锁 排它锁 隔离级别与锁的关系 在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突 在Read Committed级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁； 在Repeatable Read级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。 SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。 死锁与解决方案思路 一种头痛医头的方法，就是如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉。 另一个思路是控制并发度 死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。 常见的解决死锁的方法 如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率； 对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率； 如果业务处理不好可以用分布式事务锁或者使用乐观锁 mysql加锁规则我总结的加锁规则里面，包含了两个“原则”、两个“优化”和一个“bug”。 原则 1：加锁的基本单位是 next-key lock。希望你还记得，next-key lock 是前开后闭区间。 原则 2：查找过程中访问到的对象才会加锁。 优化 1：索引上的等值查询，给唯一索引加锁的时候，next-key lock 退化为行锁。 优化 2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock 退化为间隙锁。 一个 bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 等值查询中，普通索引回向右遍历到第一个不等于给定数据的行 索引覆盖发生时，不会阻塞主键索引的更新","categories":[],"tags":[]},{"title":"mysql索引","slug":"3mysql索引","date":"2021-05-05T12:42:07.000Z","updated":"2021-06-15T13:39:48.068Z","comments":true,"path":"2021/05/05/3mysql索引/","link":"","permalink":"https://sk-xinye.github.io/2021/05/05/3mysql%E7%B4%A2%E5%BC%95/","excerpt":"","text":"索引常见的索引类型 哈希表 哈希表这种结构适用于只有等值查询的场景 有序数组 有序数组在等值查询和范围查询场景中的性能就都非常优秀，但是当涉及到插入操作时就不行了，所以有序数组索引只适用于静态存储引擎 搜索树 二叉树 可以任意拥有左右两个节点，但是会演变为链表，查询复杂度变高. 二叉搜索树（BST) 左节点小于根节点 右节点大于根节点 时间复杂度最好是O(logn),最坏是演变为链表O(n) 平衡二叉搜索树（AVL） 左右子树高度不会差距超过1 需要大量的平衡操作，代价比较大，如果插入操作不高的话，AVL树更好点，否则红黑树更好 红黑树 每个节点非红即黑. 根节点是黑的。 每个叶节点(叶节点即树尾端NUL指针或NULL节点)都是黑的. 如果一个节点是红的,那么它的两儿子都是黑的. 对于任意节点而言,其到叶子点树NIL指针的每条路径都包含相同数目的黑节点. B树 定义任意非叶子结点最多只有M个儿子；且M&gt;2； 根结点的儿子数为[2, M]； 除根结点以外的非叶子结点的儿子数为[M/2, M]； 每个结点存放至少M/2-1（取上整）和至多M-1个关键字；（至少2个关键字） 非叶子结点的关键字个数=指向儿子的指针个数-1； 非叶子结点的关键字：K[1], K[2], …, K[M-1]；且K[i] &lt; K[i+1]； 非叶子结点的指针：P[1], P[2], …, P[M]；其中P[1]指向关键字小于K[1]的子树，P[M]指向关 键字大于K[M-1]的子树，其它P[i]指向关键字属于(K[i-1], K[i])的子树； 所有叶子结点位于同一层； B+树 相对于 B树来说更适合作为索引 B+-tree的内部节点并没有指向关键字具体信息的指针,因此其内部节点相对B树更小,如果把所有同一内部节点的关键字存放在同一盘块中,那么盘块所能容纳的关键字数量也越多,一次性读入内存的需要查找的关键字也就越多,相对IO读写次数就降低了. 由于非终结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。 支持范围查询较好，效率较B+树要高很多 其实选择B+树当索引要涉及到操作系统虚拟内存叶式存储有关，主要的性能瓶颈是在磁盘IO操作，并且为了更好的命中内存，将附近的存储区域一块抽取到内存中 索引相关概念 索引覆盖 需要查询的内容已经在索引树上，减少了回表，推荐使用索引覆盖 最左前缀原则 针对联合索引，从左边开始依次匹配，可以用来减少索引较多带来的维护问题 索引下推 为了减少回表的次数，Mysql5.6以后引入了索引下推，其实就是将联合索引中可用的判断用上，减少了回表的次数 表操作相关代码 创建表123456789CREATE TABLE `geek` (`a` int(11) NOT NULL,`b` int(11) NOT NULL,`c` int(11) NOT NULL,`d` int(11) NOT NULL,PRIMARY KEY (`a`,`b`),KEY `c` (`c`),KEY `ca` (`c`,`a`),KEY `cb` (`c`,`b`)) ENGINE=InnoDB; 普通索引唯一索引怎么选这个记录要更新的目标页不在内存中。 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束； 对于普通索引来说，则是将更新记录在 change buffer，语句执行就结束了。 将数据从磁盘读入内存涉及随机 IO 的访问，是数据库里面成本最高的操作之一。change buffer 因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的。 redo log 主要节省的是随机写磁盘的 IO 消耗（转成顺序写），而 change buffer 主要节省的则是随机读磁盘的 IO 消耗。 由于唯一索引用不上 change buffer 的优化机制，因此如果业务可以接受，从性能角度出发我建议你优先考虑非唯一索引。 mysql选错索引 对于由于索引统计信息不准确导致的问题，你可以用 analyze table 来解决。 而对于其他优化器误判的情况，你可以在应用端用 force index 来强行指定索引，也可以通过修改语句来引导优化器，还可以通过增加或者删除索引来绕过这个问题。 如何给字符串字段加索引 直接创建完整索引，这样可能比较占用空间； 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引； 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题； 创建 hash 字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支持范围扫描。","categories":[],"tags":[]},{"title":"mysql事务","slug":"2mysql事务","date":"2021-05-05T09:28:56.000Z","updated":"2021-06-20T13:36:27.478Z","comments":true,"path":"2021/05/05/2mysql事务/","link":"","permalink":"https://sk-xinye.github.io/2021/05/05/2mysql%E4%BA%8B%E5%8A%A1/","excerpt":"","text":"mysql事务视图 一个是 view。它是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果。创建视图的语法是 create view … ，而它的查询方法与表一样。 另一个是 InnoDB 在实现 MVCC 时用到的一致性读视图，即 consistent read view，用于支持 RC（Read Committed，读提交）和 RR（Repeatable Read，可重复读）隔离级别的实现。 MVCC 在可重复读隔离级别下，事务在启动的时候就“拍了个快照”。注意，这个快照是基于整库的。 在事务开始的时候向 InnoDB 的事务系统申请的，是按申请顺序严格递增的。 而每行数据也都是有多个版本的。数据表中的一行记录，其实可能有多个版本 (row)，每个版本有自己的 row trx_id。 低水位：已经提交的；高水位：已经开始的事务最大值加1 如果落在绿色部分，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的； 如果落在红色部分，表示这个版本是由将来启动的事务生成的，是肯定不可见的； 如果落在黄色部分，那就包括两种情况 a. 若 row trx_id 在数组中，表示这个版本是由还没提交的事务生成的，不可见； b. 若 row trx_id 不在数组中，表示这个版本是已经提交了的事务生成的，可见。 更新逻辑 更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read） 相关概念 Atomicity 原子性（undo log，undo log） 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用； Consistency 一致性（通过undo，回滚机制来保证） 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的； Isolation 隔离性（MVCC(undo log）） 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的； Durability 持久性（是通过redo来保证的。） 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。 脏读 某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。 不可重复读 在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。 幻读 在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。 一致性读（快照读） 当不涉及到更新操作时，select通过一致性视图MVCC 实现一致性读 当前读 更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）。 事务隔离级别对照表 隔离级别 脏读 不可重复读 幻读 READ-UNCOMMITTED √ √ √ READ-COMMITTED × √ √ REPEATABLE-READ × × √ SERIALIZABLE × × × Mysql 默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别 事务隔离机制的实现基于锁机制和并发调度。其中并发调度使用的是MVVC（多版本并发控制），通过保存修改的旧版本信息来支持并发一致性读和回滚等特性。 因为隔离级别越低，事务请求的锁越少，所以大部分数据库系统的隔离级别都是READ-COMMITTED(读取提交内容):，但是你要知道的是InnoDB 存储引擎默认使用 REPEATABLE-READ（可重读）并不会有任何性能损失。 InnoDB 存储引擎在 分布式事务 的情况下一般会用到**SERIALIZABLE(可串行化)**隔离级别。","categories":[],"tags":[]},{"title":"搬家","slug":"hello-world","date":"2021-05-01T16:00:00.000Z","updated":"2023-02-04T02:33:08.077Z","comments":true,"path":"2021/05/02/hello-world/","link":"","permalink":"https://sk-xinye.github.io/2021/05/02/hello-world/","excerpt":"","text":"Welcome to [sk-xinye] blog (https://sk-xinye.github.io/)!House-moving in labor day. 搬家方式1.安装nodes.js2.创建新的git 仓库3.ssh连接 ssh-keygen -t rsa -C “@qq.com” 增加到git中setting ssh 4.hexo https://hexo.io/zh-cn/ npm install hexo-cli -g hexo init blog cd blog npm install hexo -s p 5555 hexo g hexo d npm install hexo-deployer-git –save npm install hexo-generator-searchdb –save tanjuntao.github.io hexo new spark 5.内存管理 hexo hexo server -p 5555 hexo g","categories":[],"tags":[]},{"title":"搬家","slug":"搬家","date":"2021-05-01T16:00:00.000Z","updated":"2023-02-04T02:33:08.078Z","comments":true,"path":"2021/05/02/搬家/","link":"","permalink":"https://sk-xinye.github.io/2021/05/02/%E6%90%AC%E5%AE%B6/","excerpt":"","text":"Welcome to [sk-xinye] blog (https://sk-xinye.github.io/)! House-moving in labor day. 搬家方式1.安装nodes.js2.创建新的git 仓库3.ssh连接 ssh-keygen -t rsa -C “@qq.com”增加到git中setting ssh 4.hexo https://hexo.io/zh-cn/ npm install hexo-cli -ghexo init blogcd blognpm installhexo -s p 5555hexo ghexo dnpm install hexo-deployer-git –savenpm install hexo-generator-searchdb –savenpm install -S hexo-generator-json-content 全文搜索 83.11/12/13/253 32G内存，80G磁盘，16C 账户密码：root/zx12345683.252 254 root/pbu@2020 -P 10022","categories":[],"tags":[]}],"categories":[],"tags":[]}