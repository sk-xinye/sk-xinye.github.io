<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sk-xinye.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.yml"};
  </script>

  <meta name="description" content="愿所有努力都不被辜负">
<meta property="og:type" content="website">
<meta property="og:title" content="sk-xinyeの博客">
<meta property="og:url" content="https://sk-xinye.github.io/default-index/page/4/index.html">
<meta property="og:site_name" content="sk-xinyeの博客">
<meta property="og:description" content="愿所有努力都不被辜负">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="sk-xinye">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://sk-xinye.github.io/default-index/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>sk-xinyeの博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">sk-xinyeの博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录学习的脚步</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-fa fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">0</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">0</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">142</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/11/08/100-red%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/11/08/100-red%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5/" class="post-title-link" itemprop="url">100.red问题排查</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-11-08 10:59:16" itemprop="dateCreated datePublished" datetime="2021-11-08T10:59:16+08:00">2021-11-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-12-19 15:12:26" itemprop="dateModified" datetime="2021-12-19T15:12:26+08:00">2021-12-19</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="red-问题排查思路"><a href="#red-问题排查思路" class="headerlink" title="red 问题排查思路"></a>red 问题排查思路</h2><h3 id="集群状态排查"><a href="#集群状态排查" class="headerlink" title="集群状态排查"></a>集群状态排查</h3><p>curl -uelastic:pwd  -XGET “<a target="_blank" rel="noopener" href="http://localhost:9200/_cluster/health?pretty&quot;">http://localhost:9200/_cluster/health?pretty&quot;</a></p>
<ul>
<li>active_shards 是涵盖了所有索引的所有分片的汇总值，其中包括副本分片。</li>
<li>relocating_shards 显示当前正在从一个节点迁往其他节点的分片的数量。通常来说应该是 0，不过在 Elasticsearch 发现集群不太均衡时，该值会上涨。比如说：添加了一个新节点，或者下线了一个节点。</li>
<li>initializing_shards 显示的是刚刚创建的分片的个数。比如，当你刚创建第一个索引，分片都会短暂的处于 initializing 状态，分片不应该长期停留在 initializing 状态。你还可能在节点刚重启的时候看到initializing 分片：当分片从磁盘上加载后，它们会从 initializing 状态开始。所以这一般是临时状态。</li>
<li>unassigned_shards 是已经在集群状态中存在的分片，但是实际在集群里又找不着。最常见的体现在副本上。比如，我有两个es节点，索引设置分片数量为 10， 3 副本，那么在集群上，由于灾备原则，主分片和其对应副本不能同时在一个节点上,es无法找到其他节点来存放第三个副本的分片，所以就会有 10 个未分配副本分片。如果你的集群是 red 状态，也会长期保有未分配分片（因为缺少主分片）。<ul>
<li>上面说了一种造成 unassigned_shards的原因，就是副本太多，节点太少，es无法完成分片。</li>
<li>举一反三！由于索引的副本是可以动态修改的，那么，如果在修改时分配的副本数大于节点数目，那么肯定会有分片是这个状态。</li>
<li>目前集群爆红，但是所有节点都还在，有点诡异，从集群状态看，一共是两个分片有问题，一个正在初始化，一个是unassigned。确定了故障范围后，我们再来从索引层面、分片层面深入的分析具体原因把。</li>
</ul>
</li>
</ul>
<h3 id="索引层面分析"><a href="#索引层面分析" class="headerlink" title="索引层面分析"></a>索引层面分析</h3><p>curl -uelastic:pwd  -XGET “<a target="_blank" rel="noopener" href="http://localhost:9200/_cluster/health?pretty&amp;level=indices&quot;">http://localhost:9200/_cluster/health?pretty&amp;level=indices&quot;</a></p>
<h3 id="分片层面分析"><a href="#分片层面分析" class="headerlink" title="分片层面分析"></a>分片层面分析</h3><p>curl -uelastic:pwd  -XGET “<a target="_blank" rel="noopener" href="http://localhost:9200/_cluster/health?pretty&amp;level=shards&quot;">http://localhost:9200/_cluster/health?pretty&amp;level=shards&quot;</a></p>
<h3 id="诊断分片未分配原因"><a href="#诊断分片未分配原因" class="headerlink" title="诊断分片未分配原因"></a>诊断分片未分配原因</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">curl -uelastic:pwd -XGET &quot;http://localhost:9200/_cluster/allocation/explain&quot; -H&quot;Content-Type:application/json&quot; -d &#x27;&#123;</span><br><span class="line">  &quot;index&quot;: &quot;B_2020-01-05&quot;,</span><br><span class="line">  &quot;shard&quot;: 0,</span><br><span class="line">  &quot;primary&quot;: true</span><br><span class="line">&#125;&#x27;</span><br></pre></td></tr></table></figure>

<p>尝试集群reroute命令 curl -XPOST “<a target="_blank" rel="noopener" href="http://ip:9200/_cluster/reroute?retry_failed=true&quot;">http://ip:9200/_cluster/reroute?retry_failed=true&quot;</a></p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><h3 id="遇到集群Red时，我们可以从如下方法排查"><a href="#遇到集群Red时，我们可以从如下方法排查" class="headerlink" title="遇到集群Red时，我们可以从如下方法排查"></a>遇到集群Red时，我们可以从如下方法排查</h3><ul>
<li>集群层面：/_cluster/health。</li>
<li>索引层面：/_cluster/health?pretty&amp;level=indices。</li>
<li>分片层面：/_cluster/health?pretty&amp;level=shards。</li>
<li>看恢复情况：/_recovery?pretty</li>
</ul>
<h3 id="有unassigned分片的排查思路"><a href="#有unassigned分片的排查思路" class="headerlink" title="有unassigned分片的排查思路"></a>有unassigned分片的排查思路</h3><ul>
<li>_cluster/allocation/explain，先诊断。</li>
<li>/_cluster/reroute尝试重新分配</li>
</ul>
<h3 id="数据重放（最终解决方案）"><a href="#数据重放（最终解决方案）" class="headerlink" title="数据重放（最终解决方案）"></a>数据重放（最终解决方案）</h3><ul>
<li><p>先新建备份索引</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">curl -XPUT &#x27;http://xxxx:9200/a_index_copy/&#x27; -d &#x27;&#123;</span><br><span class="line">“settings”:&#123;</span><br><span class="line">        “index”:&#123;</span><br><span class="line">                “number_of_shards”:3,</span><br><span class="line">                “number_of_replicas”:2</span><br><span class="line">            &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;&#x27;</span><br></pre></td></tr></table></figure></li>
<li><p>通过reindex，将目前可用的数据导入</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">&quot;source&quot;: &#123;</span><br><span class="line">            &quot;index&quot;: &quot;a_index&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;dest&quot;: &#123;</span><br><span class="line">            &quot;index&quot;: &quot;a_index_copy&quot;,</span><br><span class="line">            &quot;op_type&quot;: &quot;create&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li>
<li><p>删除a_index索引，这个必须要先做，否则别名无法添加.</p>
<p>  curl -XDELETE ‘<a target="_blank" rel="noopener" href="http://xxxx:9200/a_index&#39;">http://xxxx:9200/a_index&#39;</a></p>
</li>
<li><p>给a_index_copy添加别名a_index</p>
<p>```shell<br>curl -XPOST ‘<a target="_blank" rel="noopener" href="http://xxxx:9200/_aliases&#39;">http://xxxx:9200/_aliases&#39;</a> -d ‘<br>  {</p>
<pre><code>      &quot;actions&quot;: [
          &#123;&quot;add&quot;: &#123;&quot;index&quot;: &quot;a_index_copy&quot;, &quot;alias&quot;: &quot;a_index&quot;&#125;&#125;
  ]
</code></pre>
<p>  }’</p>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/09/20/index-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/20/index-1/" class="post-title-link" itemprop="url">未命名</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-20 10:09:33" itemprop="dateCreated datePublished" datetime="2021-09-20T10:09:33+08:00">2021-09-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-08 21:43:29" itemprop="dateModified" datetime="2021-08-08T21:43:29+08:00">2021-08-08</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>镜像制作 </title></head><body>
<h2 id="docker">docker基本操作</h2>
<p>docker iamge build -t dr.z/test:aliun .
docker tag IMAGEID(镜像id) REPOSITORY:TAG（仓库：标签） 修改标签，仓库名</p>
<p>docker rmi IMAGEID(镜像id) 删除镜像
docker rm container(容器id) 删除容器</p>
<p>docker save -o es.tar dr.z/elasticsearch/elasticsearch-oss:6.3.2 保存镜像
docker load &lt; /root/docker_result/images/es.tar 加载镜像
docker load -i zs_theft.xz 加载镜像</p>
<p>docker cp container(容器id):/opt/py ./ 拷贝容器内容到本地
docker cp  /opt/test/file.txt container(容器id):/opt/py  拷贝本地到容器内容
docker commit container(容器id) centos-vim  保存容器为镜像</p>
<p>docker run -it IMAGEID(镜像id) /bin/bash 运行并进入镜像</p>
<h3 id="_1">保存镜像</h3>
<p>docker save -o  /home/dyufei/tensorflow.tar  tensorflow/tensorflow 或者如下
docker save  tensorflow/tensorflow &gt; /home/dyufei/tensorflow.tar</p>
<h3 id="_2">加载本地镜像</h3>
<p>docker load -i  tensorflow.tar</p>
<h3 id="exportimport-saveload">export/import与 save/load区别</h3>
<p>A ：export/import 是根据容器来导出镜像（因此没有镜像的历史记录）而 save/load 操作的对象是镜像
B ：export/import 镜像的历史记录再导后无法进行回滚操作，而save/load镜像有完整的历史记录可以回滚</p>
<p>docker export tensorboard &gt; /home/dyufei/tensorflow_tensorboard.tar 或者如下
docker export -o /home/dyufei/tensorflow_tensorboard.tar  tensorboard</p>
<h3 id="_3">导入容器的镜像</h3>
<p>sudo docker import - /home/dyufei/tensorflow_tensorboard.tar</p>
<h2 id="pip">将 pip 源修改为阿里云源</h2>
<p>pip config list 查看当前 pip 的配置</p>
<p>接着修改配置文件</p>
<p><code>shell
pip config set global.index-url http://mirrors.aliyun.com/pypi/simple/
pip config set install.trusted-host mirrors.aliyun.com</code></p>
<h2 id="_4">删除镜像失败</h2>
<p>docker rmi $(docker images --filter "dangling=true" -q --no-trunc)
docker rmi images</p>
<h2 id="linux">linux 操作</h2>
<p>rm -rf  ./test_chk_ln 删除软连接
ln -s 源文件 软链接</p>
</body></html>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/09/15/Page-Cache-%E9%9B%B6%E6%8B%B7%E8%B4%9D-%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/15/Page-Cache-%E9%9B%B6%E6%8B%B7%E8%B4%9D-%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98/" class="post-title-link" itemprop="url">Page_Cache_零拷贝_顺序读写_堆外内存</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-15 15:17:03" itemprop="dateCreated datePublished" datetime="2021-09-15T15:17:03+08:00">2021-09-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-09-20 10:09:06" itemprop="dateModified" datetime="2021-09-20T10:09:06+08:00">2021-09-20</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>在学习零拷贝等NIO技术之前，我们需要先知道什么是DMA。DMA(Direct Memory Access,直接存储器访问)。在ＤＭＡ出现之前，CPU与外设之间的数据传送方式有程序传送方式、中断传送方式。CPU是通过系统总线与其他部件连接并进行数据传输。不管何种传送方式，都要消耗CPU，间接影响了其他任务的执行。</p>
<h2 id="DMA原理"><a href="#DMA原理" class="headerlink" title="DMA原理"></a>DMA原理</h2><p>DMA的出现就是为了解决批量数据的输入/输出问题。DMA是指外部设备不通过CPU而直接与系统内存交换数据的接口技术。类比显卡，也是从CPU中剥离出来的功能。将这些特殊的模块进行剥离，使得CPU可以更加专注于计算工作。<br>通常系统总线是由CPU管理的，在DMA方式时，就希望CPU把这些总线让出来而由DMA控制器接管，控制传送的字节数，判断DMA是否结束，以及发出DMA结束信号。因此DMA控制器必须有以下功能:</p>
<ul>
<li>能向CPU发出系统保持(HOLD)信号，提出总线接管请求；</li>
<li>当CPU发出允许接管信号后，对总线的控制由DMA接管;</li>
<li>能对存储器寻址及能修改地址指针，实现对内存的读写；</li>
<li>能决定本次DMA传送的字节数，判断DMA传送是否借宿。</li>
<li>发出DMA结束信号，使CPU恢复正常工作状态。</li>
</ul>
<img src="/2021/09/15/Page-Cache-%E9%9B%B6%E6%8B%B7%E8%B4%9D-%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98/DMA.png" class="">

<h2 id="pagecache"><a href="#pagecache" class="headerlink" title="pagecache"></a>pagecache</h2><h3 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h3><p>从应用程序的角度看，操作系统提供了一个统一的虚拟机，在该虚拟机中没有各种机器的具体细节，只有进程、文件、地址空间以及进程间通信等逻辑概念。这种抽象虚拟机使得应用程序的开发变得相对容易。对于存储设备上的数据，操作系统向应用程序提供的逻辑概念就是”文件”。应用程序要存储或访问数据时，只需读或者写”文件”的一维地址空间即可，而这个地址空间与存储设备上存储块之间的对应关系则由操作系统维护。说白了，文件就是基于内核态Page Cache的一层抽象，下文有详细介绍。</p>
<h3 id="Page-Cache的作用"><a href="#Page-Cache的作用" class="headerlink" title="Page Cache的作用"></a>Page Cache的作用</h3><img src="/2021/09/15/Page-Cache-%E9%9B%B6%E6%8B%B7%E8%B4%9D-%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98/page_cache.png" class="">

<p>中描述了 Linux 操作系统中文件 Cache 管理与内存管理以及文件系统的关系示意图。从图中可以看到，在 Linux 中，具体文件系统，如 ext2/ext3、jfs、ntfs 等，负责在文件 Cache和存储设备之间交换数据，位于具体文件系统之上的虚拟文件系统VFS负责在应用程序和文件 Cache 之间通过 read/write 等接口交换数据，而内存管理系统负责文件 Cache 的分配和回收，同时虚拟内存管理系统(VMM)则允许应用程序和文件 Cache 之间通过 memory map的方式交换数据。可见，在 Linux 系统中，文件 Cache 是内存管理系统、文件系统以及应用程序之间的一个联系枢纽。</p>
<h3 id="Page-Cache相关的数据结构"><a href="#Page-Cache相关的数据结构" class="headerlink" title="Page Cache相关的数据结构"></a>Page Cache相关的数据结构</h3><p>每一个 Page Cache 包含若干 Buffer Cache。</p>
<ul>
<li>内存管理系统与Page Cache交互，负责维护每项 Page Cache 的分配和回收，同时在使用 memory map 方式访问时负责建立映射；</li>
<li>VFS 与Page Cache交互，负责 Page Cache 与用户空间的数据交换，即文件读写；</li>
<li>具体文件系统则一般只与 Buffer Cache 交互，它们负责在外围存储设备和 Buffer Cache 之间交换数据。</li>
</ul>
<img src="/2021/09/15/Page-Cache-%E9%9B%B6%E6%8B%B7%E8%B4%9D-%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98/page_cache2.png" class="">

<p>假定了 Page 的大小是 4K，则文件的每个4K的数据块最多只能对应一个 Page Cache 项，它通过一个是 radix tree来管理文件块和page cache的映射关系，Radix tree 是一种搜索树，Linux 内核利用这个数据结构来通过文件内偏移快速定位 Cache 项。</p>
<h2 id="零拷贝"><a href="#零拷贝" class="headerlink" title="零拷贝"></a>零拷贝</h2><p>Linux内核中与Page Cache操作相关的API有很多，按其使用方式可以分成两类：</p>
<ul>
<li>类是以拷贝方式操作的相关接口， 如read/write/sendfile等；</li>
<li>另一类是以地址映射方式操作的相关接口，如mmap。</li>
</ul>
<p>其中sendfile和mmap都是零拷贝的实现方案。</p>
<p>我们经常听说Kafka和RocketMQ等消息中间件有利用零拷贝技术来加速数据处理，提高吞吐量。所谓零拷贝，就是用户态与内核态的数据拷贝的次数为零</p>
<h3 id="常规文件读写"><a href="#常规文件读写" class="headerlink" title="常规文件读写"></a>常规文件读写</h3><p>我们先看下正常文件读写所经历的阶段，即FileChannel#read，FileChannel#write，共涉及四次上下文切换（内核态和用户态的切换，包括read调用，read返回，write调用，write返回）和四次数据拷贝</p>
<img src="/2021/09/15/Page-Cache-%E9%9B%B6%E6%8B%B7%E8%B4%9D-%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98/%E6%AD%A3%E5%B8%B8%E8%AF%BB%E5%86%99.png" class="">

<h3 id="mmap"><a href="#mmap" class="headerlink" title="mmap"></a>mmap</h3><p>mmap 把文件映射到用户空间里的虚拟地址空间，实现文件和进程虚拟地址空间中一段虚拟地址的一一对映关系。</p>
<p>省去了从内核缓冲区复制到用户空间的过程，进程就可以采用指针的方式读写操作这一段内存（文件 / page cache），而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作。相反，内核空间对这段区域的修改也直接反映到用户空间，从而可以实现用户态和内核态对此内存区域的共享。</p>
<p>但在真正使用到这些数据前却不会消耗物理内存，也不会有读写磁盘的操作，只有真正使用这些数据时，虚拟内存管理系统 VMS 才根据缺页加载的机制从磁盘加载对应的数据块到内核态的Page Cache。这样的文件读写文件方式少了数据从内核缓存到用户空间的拷贝，效率很高。</p>
<p>概括而言，mmap有以下特点：</p>
<ul>
<li>文件（page cache）直接映射到用户虚拟地址空间，内核态和用户态共享一片page cache，避免了一次数据拷贝</li>
<li>建立mmap之后，并不会立马加载数据到内存，只有真正使用数据时，才会引发缺页异常并加载数据到内存</li>
</ul>
<img src="/2021/09/15/Page-Cache-%E9%9B%B6%E6%8B%B7%E8%B4%9D-%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98/mmap1.png" class="">
<img src="/2021/09/15/Page-Cache-%E9%9B%B6%E6%8B%B7%E8%B4%9D-%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98/mmap2.png" class="">

<p>memory map具体步骤如下：</p>
<ul>
<li>首先，应用程序调用mmap（图中1）</li>
<li>陷入到内核中后调用do_mmap_pgoff（图中2）。该函数从应用程序的地址空间中分配一段区域作为映射的内存地址，并使用一个VMA（vm_area_struct）结构代表该区域，之后就返回到应用程序（图中3）。</li>
<li>当应用程序访问mmap所返回的地址指针时（图中4），由于虚实映射尚未建立，会触发缺页中断（图中5）。</li>
<li>之后系统会调用缺页中断处理函数（图中6），在缺页中断处理函数中，内核通过相应区域的VMA结构判断出该区域属于文件映射，于是调用具体文件系统的接口读入相应的Page Cache项（图中7、8、9），并填写相应的虚实映射表。</li>
<li>经过这些步骤之后，应用程序就可以正常访问相应的内存区域了。</li>
</ul>
<h3 id="sendfile"><a href="#sendfile" class="headerlink" title="sendfile"></a>sendfile</h3><p>从Linux 2.1版内核开始，Linux引入了sendfile，也能减少一次拷贝</p>
<img src="/2021/09/15/Page-Cache-%E9%9B%B6%E6%8B%B7%E8%B4%9D-%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98/sendfile.png" class="">

<p>这种方式避免了与用户空间进行交互，将四次拷贝减少到三次，内核态与用户态的切换从四次减少到两次。</p>
<p>在 Linux 内核 2.4 及后期版本中，针对套接字缓冲区描述符做了相应调整，DMA自带了收集功能，对于用户方面，用法还是一样。内部只把包含数据位置和长度信息的描述符追加到套接字缓冲区，DMA 引擎直接把数据从内核缓冲区传到协议引擎，从而消除了最后一次 CPU参与的拷贝动作。</p>
<h3 id="顺序读写"><a href="#顺序读写" class="headerlink" title="顺序读写"></a>顺序读写</h3><p>我们时常听到顺序读写比随机读写更高效的论断，那么什么是顺序读写？要想搞清楚顺序读写，我们首先要掌握文件的预读机制，它是一种将磁盘块预读到page cache的机制。</p>
<p>Linux内核中文件预读算法的具体过程是这样的：</p>
<ul>
<li>对于每个文件的第一个读请求，系统读入所请求的页面并读入紧随其后的少数几个页面(不少于一个页面，通常是三个页面)，这时的预读称为同步预读。</li>
<li>对于第二次读请求，如果所读页面不在Cache中，即不在前次预读的group中，则表明文件访问不是顺序访问，系统继续采用同步预读；</li>
<li>如果所读页面在Cache中，则表明前次预读命中，操作系统把预读group扩大一倍，并让底层文件系统读入group中剩下尚不在Cache中的文件数据块，这时的预读称为异步预读。</li>
<li>无论第二次读请求是否命中，系统都要更新当前预读group的大小。此外，系统中定义了一个window，它包括前一次预读的group和本次预读的group。</li>
<li>任何接下来的读请求都会处于两种情况之一：<ul>
<li>第一种情况是所请求的页面处于预读window中，这时继续进行异步预读并更新相应的window和group；</li>
<li>第二种情况是所请求的页面处于预读window之外，这时系统就要进行同步预读并重置相应的window和group。如下是Linux内核预读机制的一个示意图，其中a是某次读操作之前的情况，b是读操作所请求页面不在window中的情况，而c是读操作所请求页面在window中的情况。</li>
</ul>
</li>
</ul>
<img src="/2021/09/15/Page-Cache-%E9%9B%B6%E6%8B%B7%E8%B4%9D-%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98/%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99.png" class="">

<p>图中group指一次读入page cached的集合；window包括前一次预读的group和本次预读的group；浅灰色代表要用户想要查找的page cache，深灰色代表命中的page。</p>
<img src="/2021/09/15/Page-Cache-%E9%9B%B6%E6%8B%B7%E8%B4%9D-%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%99-%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98/%E9%A1%BA%E5%BA%8F%E8%AF%BB%E5%86%992.png" class="">

<p>以顺序读为例，当用户发起一个 fileChannel.read(4kb) 之后，实际发生了两件事</p>
<ul>
<li>操作系统从磁盘加载了 16kb 进入 PageCache，这被称为预读</li>
<li>操作通从 PageCache 拷贝 4kb 进入用户内存</li>
</ul>
<p>最终我们在用户内存访问到了 4kb，为什么顺序读快？很容量想到，当用户继续访问接下来的 [4kb,16kb] 的磁盘内容时，便是直接从 PageCache 去访问了。试想一下，当需要访问 16kb 的磁盘内容时，是发生 4 次磁盘 IO 快，还是发生 1 次磁盘 IO+4 次内存 IO 快呢？答案是显而易见的，这一切都是 PageCache 带来的优化。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/09/12/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/12/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/" class="post-title-link" itemprop="url">性能测试</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-12 09:25:02" itemprop="dateCreated datePublished" datetime="2021-09-12T09:25:02+08:00">2021-09-12</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-09-13 08:29:56" itemprop="dateModified" datetime="2021-09-13T08:29:56+08:00">2021-09-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="elasticsearch-esrally的使用"><a href="#elasticsearch-esrally的使用" class="headerlink" title="elasticsearch esrally的使用"></a>elasticsearch esrally的使用</h2><p>esrally 是Elasticsearch 官方出的集群基础测试框架，使用python编写的。它的工作原理是：先下载需要测试数据集，然后在本地执行测试。由于网络原因每次下载都很慢，而且每次测试都会重新下载，我们采用离线测试的方式，先将数据下载下来。同时，以为它本身是支持Docker使用的，为了方便使用，我使用docker-compose的方式，并且制作了镜像。</p>
<h3 id="数据和镜像准备"><a href="#数据和镜像准备" class="headerlink" title="数据和镜像准备"></a>数据和镜像准备</h3><h4 id="镜像准备"><a href="#镜像准备" class="headerlink" title="镜像准备"></a>镜像准备</h4><p>拉取官方镜像docker pull elastic/rally:latest</p>
<h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><ul>
<li>拉取rally-tracks项目</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/elastic/rally-tracks.git</span><br><span class="line">cd rally-tracks</span><br></pre></td></tr></table></figure>

<ul>
<li>下载测试数据，通过download下载或者直接找到数据直接下载</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 下载测试数据</span></span><br><span class="line">./download.sh geonames</span><br><span class="line"><span class="meta">#</span><span class="bash"> 查看rally-tracks/geonames/tack.json,<span class="string">&quot;base-url&quot;</span>+<span class="string">&quot;source-file&quot;</span>就是数据地址http://benchmarks.elasticsearch.org.s3.amazonaws.com/corpora/geonames/documents-2.json.b2</span></span><br><span class="line">cd track.json</span><br></pre></td></tr></table></figure>

<img src="/2021/09/12/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE.png" class="">

<ul>
<li><p>docker-compose文件编写<br>  在官方的Dockerfile中，使用的默认执行用户的1000，这里可能存在一些权限问题（创建uuid为1000的用户，并且把所有要映射的文件都改为该用户所属）。同时，官方建议是把/rally/.rally文件夹在本地进行映射，因为这些配置，以及数据集都是在该文件夹下的，如果不进行本地映射的话，不便于结果的保存及数据集的使用。而在映射了/rally/.rally文件夹后，有需要手动进行esrally configure,所以，直接调整了entrypoint.sh文件：</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/usr/bin/env bash</span></span><br><span class="line">set -Eeo pipefail</span><br><span class="line">esrally configure</span><br><span class="line">exec &quot;&amp;@&quot;</span><br></pre></td></tr></table></figure>

<p>  rally.yml 文件内容如下：<br>  –track=geonames,表示使用geonames数据集进行测试；<br>  –offline，表示离线使用，不去下载数据集；<br>  –target-hosts=:9200,表示需要测试的ES集群地址，端口为http端口</p>
  <figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&quot;3&quot;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">    <span class="attr">esrally:</span></span><br><span class="line">        <span class="attr">container_name:</span> <span class="string">esrally</span></span><br><span class="line">        <span class="attr">image:</span> <span class="string">dr.z/elastic/rally:latest</span></span><br><span class="line">        <span class="attr">volumns:</span> </span><br><span class="line">          <span class="bullet">-</span>  <span class="string">/opt/analytic_rally:/rally/.rally</span></span><br><span class="line">          <span class="bullet">-</span>  <span class="string">/opt/entrypoint.sh:/entrypoint.sh</span></span><br><span class="line">        <span class="attr">command:</span> <span class="string">&quot;esrally race --track=geonames --challenge=append-no-conflicts --pipeline=benchmark-only --target-hosts=192.168.13.132:9200&quot;</span></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="数据映射"><a href="#数据映射" class="headerlink" title="数据映射"></a>数据映射</h4><p>手动创建/opt/analytic_rally文件夹，启动镜像，使之初始化docker-compose -f rally.yml up,查看/opt/analytic_rally已经初始化了一些配置和文件。然后会报各种错误，一一解决这些错误就可以正常使用了：</p>
<ul>
<li><p>Expected a git repository at [/root/.rally/benchmarks/tracks/default] but the directory does not exist 这个错误很明显，我们只需要手动创建对应的文件夹就好了。</p>
</li>
<li><p>[/rally/.rally/benchmarks/tracks/default] must be a git repository.\n\nPlease run:\ngit -C /rally/.rally/benchmarks/tracks/default init 这个错误是因为需要git目录，也已经给出了解决方案，不同的是我们是在myrally文件夹进行操作：</p>
  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">cd /opt/analytic_rally/benchmarks/tracks/default</span><br><span class="line">git init</span><br><span class="line">touch .gitignore</span><br><span class="line">git add .</span><br><span class="line">git commit -m &quot;init default&quot;</span><br></pre></td></tr></table></figure></li>
</ul>
<p>Could not load ‘/rally/.rally/benchmarks/tracks/default/geonames/track.json’.The complete track has been written to ‘tmp/tmpyadqlaqi.json’ for diagnos is .”,’(could not load track from &#39;track.json&#39;)’这个错误就需要我们拉取下来的rally-tracks 项目了</p>
<p>cp rally-tracks/geonames/ myrally/benchmarks/tracks/default/ -r</p>
<p>cannot find /rally/.rally/benchmarks/data/geonames/documents-2.json.bz2. Please disable offilne mode and retry again. 这个错误也比较明显，这是，我们就可以直接手动下载数据集了</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir /opt/analytic_rally/benchmarks/data/geonames/ -P</span><br><span class="line">cp geonames/documents-2.jaon.bz2 myrally/benchmarks/data/geonames/</span><br></pre></td></tr></table></figure>

<p>到这里，就可以测试了</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><img src="/2021/09/12/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE.png" class="">
<img src="/2021/09/12/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%B5%8B%E8%AF%95%E6%95%B0%E6%8D%AE2.png" class="">

<h2 id="冷热分离"><a href="#冷热分离" class="headerlink" title="冷热分离"></a>冷热分离</h2><img src="/2021/09/12/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/%E5%86%B7%E7%83%AD%E5%88%86%E7%A6%BB.png" class="">

<h2 id="es-roll-over"><a href="#es-roll-over" class="headerlink" title="es roll over"></a>es roll over</h2><h2 id="单机单节点和单机多节点"><a href="#单机单节点和单机多节点" class="headerlink" title="单机单节点和单机多节点"></a>单机单节点和单机多节点</h2><p>es 集群一般情况下为多无服务器的单ES节点组成的集群，或为单台服务器的多ES 节点组成的集群。本文探讨多台服务器的单es节点组成的集群与多台服务器多ES节点组成的集群进行性能对比，通过探针查询规则以及es堆内存进行比较，搭建monitoring with Diamond+influxDB+Grafana对CPU、RAM、load average、GC及es堆内存监控，也可通过Elasticsearch kopf插件对ES进行监控</p>
<p>基础工作：</p>
<p>配置：3200单机（cpu:40,内存：256，硬盘：4块HHD组成的raid5），每个节点es的heap都设置为31G</p>
<ol>
<li>三台3200服务器各启动一个es节点组成三节点集群，所有节点均作为数据节点和master节点</li>
<li>三台3200服务器各启动3各es节点组成九节点集群，所有节点均作为数据节点和master节点（组成集群后需对数据进行负载均衡）</li>
</ol>
<p>结果：</p>
<p>对于该场景，计算较多，适合使用单机多节点，无论是查询速度，es堆内存使用等，都占有优势</p>
<img src="/2021/09/12/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/39%E5%AF%B9%E6%AF%94.png" class="">
<img src="/2021/09/12/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/%E5%86%99%E5%85%A5%E6%B5%8B%E8%AF%95.png" class="">


      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/09/09/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/09/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/" class="post-title-link" itemprop="url">性能优化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-09 14:35:09" itemprop="dateCreated datePublished" datetime="2021-09-09T14:35:09+08:00">2021-09-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-09-13 08:29:56" itemprop="dateModified" datetime="2021-09-13T08:29:56+08:00">2021-09-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="写入速度优化"><a href="#写入速度优化" class="headerlink" title="写入速度优化"></a>写入速度优化</h2><p>在es的默认设置下，是综合考虑数据可靠性、搜索实时性、写入速度等因素的。当离开默认配置、追求极致的写入速度时，很多是以牺牲可靠性和搜索实时性为代价的。有时候，业务上对数据可靠性和搜索实时性要求并不高，反而对写入速度要求很高，此时可以调整一些策略，最大化写入速度。接下来的优化基于集群正常运行的前提下，如果是集群首次批量导入数据，则可以将副本数设置为0，导入完毕在将副本数调整回去，这样副分片只需要复制，节省了构建索引过程。综合来说，提升写入速度从以下几方面入手：</p>
<ul>
<li>加大translog flush间隔，目的是降低iops、writeblock</li>
<li>加大index refresh间隔，除了降低I/O，更重要的是降低了segment merge频率</li>
<li>调整bulk请求</li>
<li>优化磁盘间的任务均匀情况，将shard 尽量均匀分布到物理主机的各个磁盘</li>
<li>优化节点间的任务分布，将任务尽量均匀地分发各节点</li>
<li>优化lucence层建立索引的过程，目的是降低CPU占用率及I/O，例如，禁用_all字段</li>
</ul>
<h3 id="translog-flush间隔调整"><a href="#translog-flush间隔调整" class="headerlink" title="translog flush间隔调整"></a>translog flush间隔调整</h3><p>这是影响ES写入速度的最大因素。但是只有这样，写操作才有可能是可靠的。如果系统可以接受一定概率的数据丢失（例如，数据写入主分片成功，尚未复制到副本分片时，主机断电。由于数据既没有刷到Lucene,translog 也没有刷盘，恢复时translog中没有这个数据，数据丢失），则调整translog持久化策略为周期性和一定大小的时候“flush”,例如：</p>
<ul>
<li>index.translog.durability:async<ul>
<li>设置为async标识translog的刷盘策略按sync_interval配置指定时间周期进行</li>
</ul>
</li>
<li>index.translog.sync_interval:120s<ul>
<li>加大translog刷盘间隔时间。默认为5s，不可低于100ms</li>
</ul>
</li>
<li>index.translog.flush_threshold_size:1024mb<ul>
<li>超过这个大小会导致refresh操作，产生新的lucene分段。默认为512Mb</li>
</ul>
</li>
</ul>
<h3 id="索引刷新间隔refresh-interval"><a href="#索引刷新间隔refresh-interval" class="headerlink" title="索引刷新间隔refresh interval"></a>索引刷新间隔refresh interval</h3><p>默认情况下索引的refresh_interval为1秒，这意味着数据写1秒后就可以被搜索到，每次索引的refresh会产生一个新的lucene段，这会导致频繁的segment merge行为，如果不需要这么高的搜索实时性，应该降低索引refresh周期，例如：index.refresh_interval:120s</p>
<h3 id="段合并"><a href="#段合并" class="headerlink" title="段合并"></a>段合并</h3><ul>
<li>segmnet merge 操作对系统I/O和内存占用都比较高，merge行为由lucene控制，位置为：index.merge.scheduler.max_thread_count。</li>
<li>最大线程数max_thread_count的默认值如下：Math.min(3, Runtime.getRuntime().availableProcessors() / 2)</li>
<li>以上是一个比较理想值，如果只有一块硬盘并且非SSD,则应该把它设置为1，因为在旋转存储介质算上并发写，由于寻址的原因，只会降低写入速度。</li>
</ul>
<h3 id="indexing-buffer"><a href="#indexing-buffer" class="headerlink" title="indexing buffer"></a>indexing buffer</h3><ul>
<li>indexing buffer 在为doc建立索引时使用，当缓冲满时会刷入磁盘，生成一个新的segmnet,这是除refresh_interval刷新索引外，另一个生成新segment的机会。</li>
<li>每个shard有自己的indexing buffer,下面的这个buffer大小的配置需要除以这个节点上所有shard的数量：<br>indices.memory.index_buffer_size。默认为整个堆空间的10%。</li>
<li>如果有了以上设置的话，那indices.memory.min_index_buffer_size默认为48MB，indices.memory.max_index_buffer_siz默认值为无限。</li>
<li>在执行大量的索引操作时，indices.memory.index_buffer_size的默认设置可能不够，这和可用堆内存、单节点上的shard数量有关，可以考虑适当增大该值</li>
</ul>
<h3 id="使用bulk"><a href="#使用bulk" class="headerlink" title="使用bulk"></a>使用bulk</h3><p>批量写比一个索引请求只写单个文档的效率高得多，但是要注意bulk请求的整体字节数不要太大，太大的请求可能会给集群带来内存压力，因此每个请求最好避免超过几十兆字节，即使较大的请求看上去执行得更好</p>
<h4 id="bulk线程池和队列"><a href="#bulk线程池和队列" class="headerlink" title="bulk线程池和队列"></a>bulk线程池和队列</h4><p>建立索引的过程属于计算密集型任务，应该使用固定大小的线程池配置，来不及处理的任务放入队列。线程池最大线程数量应配置为CPU核心数+1，这也是bulk线程池的默认设置，可以避免过多的上下文切换。队列大小可以适当增加，但一定要严格控制大小，过大的队列导致较高的GC压力，并可能导致Full GC 频繁发生</p>
<h4 id="并发执行bulk请求"><a href="#并发执行bulk请求" class="headerlink" title="并发执行bulk请求"></a>并发执行bulk请求</h4><p>bulk写请求是个长任务，为了给系统增加足够的写入压力，写入过程应该多个客户端、多线程地并发执行，如果要验证系统的极限写入能力，那么目标就是把CPU压满。磁盘util、内存等一般都不是瓶颈。如果CPU没有压满，则应该提高写入端的并发数量。但是要注意bulk线程池队列的reject情况，出现reject代表ES的bulk队列已满，客户端请求被拒绝，此时客户端会收到429错误（TOO_MANAY_REQUESTS）,客户端对此的处理策略应该是延迟重试。不应该忽略这个异常，否则写入系统的数据会少于预期。即使客户端正确处理了429错误，我们仍然应该尽量避免产生reject。因此，在评估极限的写入并发量应该控制在不reject前提下的最大值为宜。</p>
<h3 id="磁盘间的任务均衡"><a href="#磁盘间的任务均衡" class="headerlink" title="磁盘间的任务均衡"></a>磁盘间的任务均衡</h3><p>如果部署方案是为path.data配置多个路径来使用多块磁盘，则ES在分配shard时，落到各磁盘上的shard可能并不均匀，这种不均匀可能会导致某些磁盘繁忙，利用率在较长时间持续达到100%。这种不均匀达到一定程度会对写入性能产生负面影响。</p>
<p>ES在处理多路径时，有限将shard分配到可用空间占比最多的磁盘上，因此短时间创建的shard 可能被击中分配到各个磁盘上，即使可用空间是99%和98%的差别。后来es在2.x版本开始解决这个问题：预估一下shard会使用的空间，从磁盘可用空间中减去这部分，直到现在6.x版也是这种处理方式。但是实现也存在一些问题：</p>
<ul>
<li>从可用空间减去预估大小这种机制只存在于一次索引创建的过程中，下一次的索引创建，磁盘可用空间并不是上次做减法后的结果。这也可以理解，毕竟预估是不准的，一直减下去空间很快就减没了。</li>
<li>但是最终的效果是，这种机制并没有根本上解决问题，即使没有完美的解决方案，这种机制的效果也不够好。如果单一的机制不能解决所有的场景，那么至少应该为不同场景准备多种选择。为此，ES增加了两种策略。<ul>
<li>简单轮询：在系统初始阶段，简单轮询的效果是最均匀的</li>
<li>基于可用空间的动态加权轮询：以可用空间作为权重，在磁盘之间加权轮询</li>
</ul>
</li>
</ul>
<h3 id="节点间的任务均衡"><a href="#节点间的任务均衡" class="headerlink" title="节点间的任务均衡"></a>节点间的任务均衡</h3><p>为了节点间的任务林亮均衡，数据写入客户端应该把bulk请求轮询发送各个节点。<br>当适用JAVA API 或 REST API的bulk接口发送数据时，客户端将会轮询发送到集群节点，节点列表取决于：</p>
<ul>
<li>使用java API,当设置client.transport.sniff为true(默认为false)时，列表为所有数据节点，否则节点列表为构建客户端对象时传入的节点列表</li>
<li>使用REST API时，列表为构建对象时添加进去的节点</li>
</ul>
<p>java API的TransportClient和REST API的RestClient都是线程安全的，如果写入程序自己创建线程池控制并发，则应该使用同一个Client对象。再次建议使用REST API，java API会在未来的版本中废弃，REST API有良好的版本兼容性。理论上，java api在序列化上有性能优势，但是只有在吞吐量非常大时才值得考虑序列化的开销带来的影响，通常搜索并不是高吞吐量的业务</p>
<p>要观察bulk请求在不同节点间的均衡性，可以通过cat接口观察bulk线程池和队列情况：_cat/thread_pool</p>
<h3 id="索引过程调整和优化"><a href="#索引过程调整和优化" class="headerlink" title="索引过程调整和优化"></a>索引过程调整和优化</h3><h4 id="自动生成doc-ID"><a href="#自动生成doc-ID" class="headerlink" title="自动生成doc ID"></a>自动生成doc ID</h4><p>通过ES写入流程可以看出，写入doc时如果外部指定了id,则ES会先尝试读取doc的版本号，以判断是否需要更新。这会涉及一次读取磁盘的操作，通过自动生成doc ID可以避免这个环节。</p>
<h4 id="调整字段Mappings"><a href="#调整字段Mappings" class="headerlink" title="调整字段Mappings"></a>调整字段Mappings</h4><ul>
<li>减少字段数量，对于不需要建立索引的字段，不写入ES</li>
<li>将不需要建立索引的字段index属性设置为not_analyzed或no。对字段部分次，或者不索引，可以减少很多运算操作，降低CPU占用。尤其是binary类型，默认情况下占用CPU非常高，而这种类型进行分词通常没有什么意义。</li>
<li>减少字段内容长度，如果原始数据的大段内容无需全部建立索引，则可以尽量减少不必要的内容</li>
<li>使用不同的分词器（analyzer）,不同的分析器在索引过程中运算复杂度也有较大的差异</li>
</ul>
<h4 id="调整-source字段"><a href="#调整-source字段" class="headerlink" title="调整_source字段"></a>调整_source字段</h4><p>_source字段用于存储doc原始数据，对于部分不需要存储的字段，可以通过includes excludes 过滤，或者将_source禁用，一般用于索引和数据分离。这样可以降低I/O的压力，不过实际场景中大多不会禁用_source,而即使过滤掉某些字段，对于写入速度的提升作用也不大，满负荷情况下，基本是CPU先跑满了，瓶颈在与CPU。</p>
<h4 id="禁用-all字段"><a href="#禁用-all字段" class="headerlink" title="禁用_all字段"></a>禁用_all字段</h4><p>从ES6.0开始，_all字段默认为不启用，而在此前的版本中，_all字段默认是开启的，_all字段中包含所有字段分词后的关键词，作用是可以在搜索的时候不指定特定字段，从所有字段中检索。ES6.0默认禁用_all字段主要有以下几点原因：</p>
<ul>
<li>由于需要从其他的全部字段复制所有字段值，导致_all字段占用非常大的空间。</li>
<li>_all字段有自己的分词器，在进行某些查询时（例如，同义词），结果不符合预期，因为没有匹配同一个分词器</li>
<li>由于数据重复引起的额外建立索引的开销</li>
<li>想要调试时，其内容不容易检查</li>
<li>有些用户甚至不知道存在这个字段，导致查询混乱</li>
<li>有更好的替代方式</li>
</ul>
<p>关于此问题，可以参考 <a target="_blank" rel="noopener" href="https://github.com/elastic/elasticsearch/issues/19784">https:github.com/elastic/elasticsearch/issues/19784</a> 。在ES6.0之前的版本中，可以在mapping中将enabled设置为false 来禁用_all字段:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">curl -X PUT &quot;localhost:9200/my_index&quot; -H &#x27;Content-Type: application/json&#x27; -d &#x27;</span><br><span class="line">&#123;</span><br><span class="line">    &quot;mapping&quot;:&#123;</span><br><span class="line">    &quot;type_1&quot;:&#123;</span><br><span class="line">    &quot;_all&quot;:&#123;&quot;enabled&quot;:false&#125;,</span><br><span class="line">    &quot;properties&quot;:&#123;....&#125;</span><br><span class="line">&#125;&#125;&#125;&#x27;</span><br></pre></td></tr></table></figure>

<p>禁用_all字段可以明显降低对CPU和I/O的压力</p>
<h4 id="对Analyzed的字段禁用Norms"><a href="#对Analyzed的字段禁用Norms" class="headerlink" title="对Analyzed的字段禁用Norms"></a>对Analyzed的字段禁用Norms</h4><p>Norms用于在搜索时计算doc的评分，如果不需要评分，则可以将其禁用：<br>“title”:{“type”:”string”,”norms”:{“enabled”:false}}</p>
<h4 id="index-option设置"><a href="#index-option设置" class="headerlink" title="index option设置"></a>index option设置</h4><p>index_options用于控制在建立倒排索引过程中，哪些内容会被添加到倒排索引，例如，doc数量、词频、postions、offsets等信息，优化这些配置可以一定程度降低索引过程中的运算任务，节省CPU占用率。不过在实际场景中，通常很难确定业务将来会不会用到这个信息，除非一开始方案就明确这样设计的。</p>
<h3 id="要用raid-0-或者10"><a href="#要用raid-0-或者10" class="headerlink" title="要用raid 0 或者10"></a>要用raid 0 或者10</h3><h2 id="ES-搜索速度优化"><a href="#ES-搜索速度优化" class="headerlink" title="ES 搜索速度优化"></a>ES 搜索速度优化</h2><h3 id="为文件系统cache预留足够的内存空间"><a href="#为文件系统cache预留足够的内存空间" class="headerlink" title="为文件系统cache预留足够的内存空间"></a>为文件系统cache预留足够的内存空间</h3><p>在一般情况下，应用程序的读写都是被操作系统”cache”(除了direct方式)，cache保存在系统物理内存中（线上应该禁用swap）,命中cache可以降低对磁盘的直接访问频率。搜索很依赖对系统cache的命中，如果某个请求需要从磁盘读取数据，则一定会产生相对较高的延迟。应该至少为系统cache预留一般的可用物理内存，更大的内存有更高的cache命中率。</p>
<h3 id="使用更快的硬件"><a href="#使用更快的硬件" class="headerlink" title="使用更快的硬件"></a>使用更快的硬件</h3><p>写入性能对CPU的性能更敏感，而搜索性能在一般情况下更多的是在于I/O能力，使用ssd会比旋转类存储介质好得多。尽量避免使用NFS等远程文件系统，如果NFS比本地存储慢3倍，则在搜索场景下响应速度可能会慢10倍左右。这可能是因为搜索请求有更多的随机访问。</p>
<p>如果搜索类型属于计算比较多，则可以考虑使用更快CPU。</p>
<h3 id="文档类型"><a href="#文档类型" class="headerlink" title="文档类型"></a>文档类型</h3><p>为了让搜索时的成本更低，文档应该合理建模。特别是应该避免join操作，嵌套（nested）会使查询慢几倍，父子（parent-child）关系可能使查询慢数百倍，因此，如果可以通过非规范化（denormalizing）文档来回答相同的问题，则可以显著地提高搜索速度。</p>
<h3 id="预索引数据"><a href="#预索引数据" class="headerlink" title="预索引数据"></a>预索引数据</h3><p>还可以针对某些查询的模式来优化数据的索引方式。例如，如果所有文档都有一个price字段，并且大多数查询在一个固定的范围上运行range聚合，那么可以通过将范围“pre-indexing”到索引中并使用聚合来加快聚合速度。</p>
<h3 id="字段映射"><a href="#字段映射" class="headerlink" title="字段映射"></a>字段映射</h3><p>有些字段的内容是数值，但并不意味着其总是应该被映射为数值类型，例如，一些标识符，将他们映射为keyword可能会比integer或long更好。</p>
<h3 id="避免使用脚本"><a href="#避免使用脚本" class="headerlink" title="避免使用脚本"></a>避免使用脚本</h3><p>一般来说，应该避免使用脚本。如果一定要用，则应该优先考虑painless和expressions.</p>
<h3 id="优化日期搜索"><a href="#优化日期搜索" class="headerlink" title="优化日期搜索"></a>优化日期搜索</h3><p>在使用日期范围检索时，使用now的查询通常不能缓存，因为匹配到的范围一直在变化。但是，从用户体验角度来看，切换到完整的日期通常是可以接受的，这样可以更好地利用查询缓存。</p>
<h3 id="为只读索引执行force-merge"><a href="#为只读索引执行force-merge" class="headerlink" title="为只读索引执行force-merge"></a>为只读索引执行force-merge</h3><p>为不再更新的只读索引执行force merge,将lucene索引合并为单个分段，可以提升查询速度。当一个lucene索引存在多个分段时，每个分段会单独执行搜索再将结果合并，将只读索引强制合并为一个lucene分段不仅可以优化搜索过程，对索引恢复速度也有好处。</p>
<p>基于日期进行轮询的索引的旧数据一般都不会再更新。此前的章节中说过，应该避免持续地写一个固定的索引，直到它巨大无比，而应该按照一定的策略，例如，每天生成一个新的索引，然后用别名关联，或者使用索引通配符。这样，可以每天选一个时间点对昨天的索引执行force-merge、Shrink等操作。</p>
<h3 id="globle-ordinals"><a href="#globle-ordinals" class="headerlink" title="globle ordinals"></a>globle ordinals</h3><p>全局序号是一种数据结构，用于在keyword字段上运行terms聚合。他用一个数值来代表字段中的字符串值，然后为每一数值分配一个bucket.这需要一个对global ordinals和bucket的构建过程。默认情况下，他们被延迟构建，因为ES不知道哪些字段将用于terms聚合，哪些字段不会。<br>参考：<a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/elasticsearch/guide/current/preload-fielddata.html">https://www.elastic.co/guide/en/elasticsearch/guide/current/preload-fielddata.html</a> <a target="_blank" rel="noopener" href="https://blog.csdn.net/zwgdft/artical/details/83215977">https://blog.csdn.net/zwgdft/artical/details/83215977</a></p>
<h3 id="execution-hint"><a href="#execution-hint" class="headerlink" title="execution hint"></a>execution hint</h3><p>terms 聚合有两种不同机制：</p>
<ul>
<li>通过直接使用字段值来聚合每个桶的数据（map）</li>
<li>通过使用字段的全局序号并为每个全局序号分配一个bucket(global_ordinals).</li>
</ul>
<p>ES 使用global_ordinals作为keyword字段的默认选项，它使用全局序号动态地分配bucket,因此内存使用与聚合结果中的字段数量是线性关系。在大部分情况下，这种方式的速度很快。当查询只会匹配少量文档时，可以考虑使用map.默认情况下，map只在脚本上运行聚合时使用，因为他们没有序数。</p>
<h3 id="预热文件系统cache"><a href="#预热文件系统cache" class="headerlink" title="预热文件系统cache"></a>预热文件系统cache</h3><p>如果ES主机重启，则文件系统缓存将为空，此时搜索会比较慢。可以使用index.store.preload设置，通过指定文件扩展名，显式地告诉操作系统应该将哪些文件加载到内存中，例如，配置到elasticsearch.yml文件中：<br>index.store.preload:[“nvd”,”dvd”]</p>
<p>或者在索引创建时设置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PUT /my_index</span><br><span class="line">&#123;</span><br><span class="line">  &quot;settings&quot;:&#123;</span><br><span class="line">    &quot;index.store.preload&quot;:[&quot;nvd&quot;,&quot;dvd&quot;]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如果文件缓存不够大，则无法保存所有数据，那么为太多文件预加载数据到文件系统缓存中会使搜索速度变慢，应谨慎使用。</p>
<h3 id="转换查询表达式"><a href="#转换查询表达式" class="headerlink" title="转换查询表达式"></a>转换查询表达式</h3><p>在组合查询中通过bool过滤器进行and、or和not的多个逻辑组合检索，这种组合查询中的表达式在下面的情况下可以做等价转换：<br>（A|B）&amp;(C|D)==&gt;(A&amp;B)|（A&amp;D）|(B&amp;C)|(B&amp;D)</p>
<h3 id="使用近似聚合"><a href="#使用近似聚合" class="headerlink" title="使用近似聚合"></a>使用近似聚合</h3><p>近似聚合以牺牲少量的精度为代价，大幅度提高执行效率，降低了内存使用。近似聚合的方式可以参考官方手册：<br>Percentiles Aggregation(<a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-precentile-aggregation.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-precentile-aggregation.html</a>)</p>
<p>Cardinality Aggregation(<a target="_blank" rel="noopener" href="https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html">https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-metrics-cardinality-aggregation.html</a>)</p>
<h3 id="深度优先还是广度优先"><a href="#深度优先还是广度优先" class="headerlink" title="深度优先还是广度优先"></a>深度优先还是广度优先</h3><p>ES 有两种不同的聚合方式：深度优先和广度优先。深度优先是默认，先构建完整的树，然后修剪无用节点。大多数情况下深度聚合都能正常工作，但是在特殊场合适合广度优先，先执行第一次聚合，再继续下一层之前修剪，官方例子：<a target="_blank" rel="noopener" href="https://www.elastic.co/guide/cn/elasticsearch/guide/curretn/_preventing_combinatorial_explosions.html">https://www.elastic.co/guide/cn/elasticsearch/guide/curretn/_preventing_combinatorial_explosions.html</a></p>
<h3 id="限制搜索请求分片数"><a href="#限制搜索请求分片数" class="headerlink" title="限制搜索请求分片数"></a>限制搜索请求分片数</h3><p>一个搜索请求涉及的分片数量越多，协调节点的CPU和内存压力越大。默认情况下，ES会拒绝超过1000个分片的搜索请求。我们应该更好的组织数据，让搜索请求的分片数更少。如果想调节这个值，则可以通过actions.search.shard_count配置项进行修改</p>
<p>虽然限制搜索的分片数并不能直接提升单个索引请求的速度，但协调节点的压力会间接影响搜索速度，例如，占用更多内存会产生更多GC压力，可能导致更多的stop-the-world时间等，因此间接影响了协调节点的性能，所以我们仍把它列作本章的一部分</p>
<h3 id="利用自适应副本选择（ARS）提升ES响应速度"><a href="#利用自适应副本选择（ARS）提升ES响应速度" class="headerlink" title="利用自适应副本选择（ARS）提升ES响应速度"></a>利用自适应副本选择（ARS）提升ES响应速度</h3><p>为了充分利用计算资源和负载均衡，协调节点将搜索请求轮询转发到分片的每个副本，轮询策略是负载均衡中最简答的策略，任何一个负载均衡器都具备这种基础的策略，缺点是不考虑后端实际系统压力和健康水平。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/09/08/6-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/08/6-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/" class="post-title-link" itemprop="url">6.性能优化</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-08 19:24:46" itemprop="dateCreated datePublished" datetime="2021-09-08T19:24:46+08:00">2021-09-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-09-10 21:07:40" itemprop="dateModified" datetime="2021-09-10T21:07:40+08:00">2021-09-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="开发调优"><a href="#开发调优" class="headerlink" title="开发调优"></a>开发调优</h2><ul>
<li>避免创建重复的RDD  开发过程中忘记之前已经创建过了，导致了重复创建，浪费资源</li>
<li>尽可能复用同一个RDD  我们应该尽量复用一个RDD，这样可以尽可能地减少RDD的数量，从而尽可能减少算子执行的次数</li>
<li>对多次使用的RDD进行持久化  默认memory</li>
<li>尽量避免使用shuffle类算子</li>
<li>使用map-side预聚合的shuffle操作<ul>
<li>在可能的情况下，建议使用reduceByKey或者aggregateByKey算子来替代掉groupByKey算子。</li>
</ul>
</li>
<li>使用高性能的算子<ul>
<li>使用reduceByKey/aggregateByKey替代groupByKey</li>
<li>使用mapPartitions替代普通map</li>
<li>使用filter之后进行coalesce操作</li>
<li>使用repartitionAndSortWithinPartitions替代repartition与sort类操作</li>
</ul>
</li>
<li>广播大变量<ul>
<li>广播后的变量，会保证每个Executor的内存中，只驻留一份变量副本，而Executor中的task执行时共享该Executor中的那份变量副本。这样的话，可以大大减少变量副本的数量，从而减少网络传输的性能开销，并减少对Executor内存的占用开销，降低GC的频率。</li>
</ul>
</li>
<li>使用Kryo优化序列化性能</li>
</ul>
<h2 id="参数调优"><a href="#参数调优" class="headerlink" title="参数调优"></a>参数调优</h2><ul>
<li>num-executors<ul>
<li>这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的</li>
<li>每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</li>
</ul>
</li>
<li>executor-memory<ul>
<li>每个Executor进程的内存设置4G~8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少</li>
<li>阿里云上设置的是10G</li>
</ul>
</li>
<li>executor-cores<ul>
<li>Executor的CPU core数量设置为2~4个较为合适。</li>
</ul>
</li>
<li>driver-memory<ul>
<li>Driver的内存通常来说不设置，或者设置1G左右应该就够了。</li>
</ul>
</li>
<li>spark.default.parallelism<ul>
<li>Spark作业的默认task数量为500<del>1000个较为合适。设置该参数为num-executors * executor-cores的2</del>3倍较为合适</li>
<li>Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。</li>
</ul>
</li>
<li>spark.storage.memoryFraction<ul>
<li>默认是0.6。也就是说，默认Executor 60%的内存，如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。</li>
</ul>
</li>
<li>spark.shuffle.memoryFraction<ul>
<li>默认是0.2 如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用</li>
</ul>
</li>
</ul>
<h2 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h2><h3 id="如何定位导致数据倾斜的代码"><a href="#如何定位导致数据倾斜的代码" class="headerlink" title="如何定位导致数据倾斜的代码"></a>如何定位导致数据倾斜的代码</h3><ul>
<li>数据倾斜只会发生在shuffle过程中。这里给大家罗列一些常用的并且可能会触发shuffle操作的算子：distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup、repartition等。出现数据倾斜时，可能就是你的代码中使用了这些算子中的某一个所导致的。</li>
<li>通过web_ui确定哪个stage 最慢，然后进行对应的调优工作</li>
</ul>
<h4 id="某个task执行特别慢的情况"><a href="#某个task执行特别慢的情况" class="headerlink" title="某个task执行特别慢的情况"></a>某个task执行特别慢的情况</h4><ul>
<li>首先要看的，就是数据倾斜发生在第几个stage中。</li>
<li>如果是用yarn-client模式提交，那么本地是直接可以看到log的，可以在log中找到当前运行到了第几个stage；</li>
<li>如果是用yarn-cluster模式提交，则可以通过Spark Web UI来查看当前运行到了第几个stage</li>
</ul>
<h4 id="某个task莫名其妙内存溢出的情况"><a href="#某个task莫名其妙内存溢出的情况" class="headerlink" title="某个task莫名其妙内存溢出的情况"></a>某个task莫名其妙内存溢出的情况</h4><ul>
<li>我们建议直接看yarn-client模式下本地log的异常栈，或者是通过YARN查看yarn-cluster模式下的log中的异常栈。</li>
<li>一般来说，通过异常栈信息就可以定位到你的代码中哪一行发生了内存溢出。然后在那行代码附近找找，一般也会有shuffle类算子，此时很可能就是这个算子导致了数据倾斜。</li>
</ul>
<h4 id="导致数据倾斜的key的数据分布情况"><a href="#导致数据倾斜的key的数据分布情况" class="headerlink" title="导致数据倾斜的key的数据分布情况"></a>导致数据倾斜的key的数据分布情况</h4><ul>
<li>如果是Spark SQL中的group by、join语句导致的数据倾斜，那么就查询一下SQL中使用的表的key分布情况。</li>
<li>如果是对Spark RDD执行shuffle算子导致的数据倾斜，那么可以在Spark作业中加入查看key分布的代码，比如RDD.countByKey()。然后对统计出来的各个key出现的次数，collect/take到客户端打印一下，就可以看到key的分布情况。</li>
</ul>
<h3 id="数据倾斜的解决方案"><a href="#数据倾斜的解决方案" class="headerlink" title="数据倾斜的解决方案"></a>数据倾斜的解决方案</h3><h4 id="使用Hive-ETL预处理数据"><a href="#使用Hive-ETL预处理数据" class="headerlink" title="使用Hive ETL预处理数据"></a>使用Hive ETL预处理数据</h4><p>方案适用场景：导致数据倾斜的是Hive表。如果该Hive表中的数据本身很不均匀（比如某个key对应了100万数据，其他key才对应了10条数据），而且业务场景需要频繁使用Spark对Hive表执行某个分析操作，那么比较适合使用这种技术方案。</p>
<p>方案实现思路：此时可以评估一下，是否可以通过Hive来进行数据预处理（即通过Hive ETL预先对数据按照key进行聚合，或者是预先和其他表进行join），然后在Spark作业中针对的数据源就不是原来的Hive表了，而是预处理后的Hive表。此时由于数据已经预先进行过聚合或join操作了，那么在Spark作业中也就不需要使用原先的shuffle类算子执行这类操作了。</p>
<p>方案实现原理：这种方案从根源上解决了数据倾斜，因为彻底避免了在Spark中执行shuffle类算子，那么肯定就不会有数据倾斜的问题了。但是这里也要提醒一下大家，这种方式属于治标不治本。因为毕竟数据本身就存在分布不均匀的问题，所以Hive ETL中进行group by或者join等shuffle操作时，还是会出现数据倾斜，导致Hive ETL的速度很慢。我们只是把数据倾斜的发生提前到了Hive ETL中，避免Spark程序发生数据倾斜而已。</p>
<p>方案优点：实现起来简单便捷，效果还非常好，完全规避掉了数据倾斜，Spark作业的性能会大幅度提升。</p>
<p>方案缺点：治标不治本，Hive ETL中还是会发生数据倾斜。</p>
<p>方案实践经验：在一些Java系统与Spark结合使用的项目中，会出现Java代码频繁调用Spark作业的场景，而且对Spark作业的执行性能要求很高，就比较适合使用这种方案。将数据倾斜提前到上游的Hive ETL，每天仅执行一次，只有那一次是比较慢的，而之后每次Java调用Spark作业时，执行速度都会很快，能够提供更好的用户体验。</p>
<p>项目实践经验：在美团·点评的交互式用户行为分析系统中使用了这种方案，该系统主要是允许用户通过Java Web系统提交数据分析统计任务，后端通过Java提交Spark作业进行数据分析统计。要求Spark作业速度必须要快，尽量在10分钟以内，否则速度太慢，用户体验会很差。所以我们将有些Spark作业的shuffle操作提前到了Hive ETL中，从而让Spark直接使用预处理的Hive中间表，尽可能地减少Spark的shuffle操作，大幅度提升了性能，将部分作业的性能提升了6倍以上。</p>
<h4 id="过滤少数导致倾斜的key"><a href="#过滤少数导致倾斜的key" class="headerlink" title="过滤少数导致倾斜的key"></a>过滤少数导致倾斜的key</h4><p>方案适用场景：如果发现导致倾斜的key就少数几个，而且对计算本身的影响并不大的话，那么很适合使用这种方案。比如99%的key就对应10条数据，但是只有一个key对应了100万数据，从而导致了数据倾斜。</p>
<p>方案实现思路：如果我们判断那少数几个数据量特别多的key，对作业的执行和计算结果不是特别重要的话，那么干脆就直接过滤掉那少数几个key。比如，在Spark SQL中可以使用where子句过滤掉这些key或者在Spark Core中对RDD执行filter算子过滤掉这些key。如果需要每次作业执行时，动态判定哪些key的数据量最多然后再进行过滤，那么可以使用sample算子对RDD进行采样，然后计算出每个key的数量，取数据量最多的key过滤掉即可。</p>
<p>方案实现原理：将导致数据倾斜的key给过滤掉之后，这些key就不会参与计算了，自然不可能产生数据倾斜。</p>
<p>方案优点：实现简单，而且效果也很好，可以完全规避掉数据倾斜。</p>
<p>方案缺点：适用场景不多，大多数情况下，导致倾斜的key还是很多的，并不是只有少数几个。</p>
<p>方案实践经验：在项目中我们也采用过这种方案解决数据倾斜。有一次发现某一天Spark作业在运行的时候突然OOM了，追查之后发现，是Hive表中的某一个key在那天数据异常，导致数据量暴增。因此就采取每次执行前先进行采样，计算出样本中数据量最大的几个key之后，直接在程序中将那些key给过滤掉。</p>
<h4 id="提高shuffle操作的并行度"><a href="#提高shuffle操作的并行度" class="headerlink" title="提高shuffle操作的并行度"></a>提高shuffle操作的并行度</h4><p>方案适用场景：如果我们必须要对数据倾斜迎难而上，那么建议优先使用这种方案，因为这是处理数据倾斜最简单的一种方案。</p>
<p>方案实现思路：在对RDD执行shuffle算子时，给shuffle算子传入一个参数，比如reduceByKey(1000)，该参数就设置了这个shuffle算子执行时shuffle read task的数量。对于Spark SQL中的shuffle类语句，比如group by、join等，需要设置一个参数，即spark.sql.shuffle.partitions，该参数代表了shuffle read task的并行度，该值默认是200，对于很多场景来说都有点过小。</p>
<p>方案实现原理：增加shuffle read task的数量，可以让原本分配给一个task的多个key分配给多个task，从而让每个task处理比原来更少的数据。举例来说，如果原本有5个key，每个key对应10条数据，这5个key都是分配给一个task的，那么这个task就要处理50条数据。而增加了shuffle read task以后，每个task就分配到一个key，即每个task就处理10条数据，那么自然每个task的执行时间都会变短了。具体原理如下图所示。</p>
<img src="/2021/09/08/6-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/%E5%A2%9E%E5%8A%A0%E5%B9%B6%E8%A1%8C%E5%BA%A6.png" class="">

<p>方案优点：实现起来比较简单，可以有效缓解和减轻数据倾斜的影响。</p>
<p>方案缺点：只是缓解了数据倾斜而已，没有彻底根除问题，根据实践经验来看，其效果有限。</p>
<p>方案实践经验：该方案通常无法彻底解决数据倾斜，因为如果出现一些极端情况，比如某个key对应的数据量有100万，那么无论你的task数量增加到多少，这个对应着100万数据的key肯定还是会分配到一个task中去处理，因此注定还是会发生数据倾斜的。所以这种方案只能说是在发现数据倾斜时尝试使用的第一种手段，尝试去用嘴简单的方法缓解数据倾斜而已，或者是和其他方案结合起来使用。</p>
<h4 id="两阶段聚合（局部聚合-全局聚合）"><a href="#两阶段聚合（局部聚合-全局聚合）" class="headerlink" title="两阶段聚合（局部聚合+全局聚合）"></a>两阶段聚合（局部聚合+全局聚合）</h4><p>方案适用场景：对RDD执行reduceByKey等聚合类shuffle算子或者在Spark SQL中使用group by语句进行分组聚合时，比较适用这种方案。</p>
<p>方案实现思路：这个方案的核心实现思路就是进行两阶段聚合。第一次是局部聚合，先给每个key都打上一个随机数，比如10以内的随机数，此时原先一样的key就变成不一样的了，比如(hello, 1) (hello, 1) (hello, 1) (hello, 1)，就会变成(1_hello, 1) (1_hello, 1) (2_hello, 1) (2_hello, 1)。接着对打上随机数后的数据，执行reduceByKey等聚合操作，进行局部聚合，那么局部聚合结果，就会变成了(1_hello, 2) (2_hello, 2)。然后将各个key的前缀给去掉，就会变成(hello,2)(hello,2)，再次进行全局聚合操作，就可以得到最终结果了，比如(hello, 4)。</p>
<p>方案实现原理：将原本相同的key通过附加随机前缀的方式，变成多个不同的key，就可以让原本被一个task处理的数据分散到多个task上去做局部聚合，进而解决单个task处理数据量过多的问题。接着去除掉随机前缀，再次进行全局聚合，就可以得到最终的结果。具体原理见下图。</p>
<img src="/2021/09/08/6-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/%E4%B8%A4%E9%98%B6%E6%AE%B5%E8%81%9A%E5%90%88.png" class="">
<p>方案优点：对于聚合类的shuffle操作导致的数据倾斜，效果是非常不错的。通常都可以解决掉数据倾斜，或者至少是大幅度缓解数据倾斜，将Spark作业的性能提升数倍以上。</p>
<p>方案缺点：仅仅适用于聚合类的shuffle操作，适用范围相对较窄。如果是join类的shuffle操作，还得用其他的解决方案。</p>
<h4 id="将reduce-join转为map-join"><a href="#将reduce-join转为map-join" class="headerlink" title="将reduce join转为map join"></a>将reduce join转为map join</h4><p>方案适用场景：在对RDD使用join类操作，或者是在Spark SQL中使用join语句时，而且join操作中的一个RDD或表的数据量比较小（比如几百M或者一两G），比较适用此方案。</p>
<p>方案实现思路：不使用join算子进行连接操作，而使用Broadcast变量与map类算子实现join操作，进而完全规避掉shuffle类的操作，彻底避免数据倾斜的发生和出现。将较小RDD中的数据直接通过collect算子拉取到Driver端的内存中来，然后对其创建一个Broadcast变量；接着对另外一个RDD执行map类算子，在算子函数内，从Broadcast变量中获取较小RDD的全量数据，与当前RDD的每一条数据按照连接key进行比对，如果连接key相同的话，那么就将两个RDD的数据用你需要的方式连接起来。</p>
<p>方案实现原理：普通的join是会走shuffle过程的，而一旦shuffle，就相当于会将相同key的数据拉取到一个shuffle read task中再进行join，此时就是reduce join。但是如果一个RDD是比较小的，则可以采用广播小RDD全量数据+map算子来实现与join同样的效果，也就是map join，此时就不会发生shuffle操作，也就不会发生数据倾斜。具体原理如下图所示。</p>
<img src="/2021/09/08/6-%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/%E4%BA%94.png" class="">
<p>方案优点：对join操作导致的数据倾斜，效果非常好，因为根本就不会发生shuffle，也就根本不会发生数据倾斜。</p>
<p>方案缺点：适用场景较少，因为这个方案只适用于一个大表和一个小表的情况。毕竟我们需要将小表进行广播，此时会比较消耗内存资源，driver和每个Executor内存中都会驻留一份小RDD的全量数据。如果我们广播出去的RDD数据比较大，比如10G以上，那么就可能发生内存溢出了。因此并不适合两个都是大表的情况。</p>
<h4 id="采样倾斜key并分拆join操作"><a href="#采样倾斜key并分拆join操作" class="headerlink" title="采样倾斜key并分拆join操作"></a>采样倾斜key并分拆join操作</h4><h4 id="使用随机前缀和扩容RDD进行join"><a href="#使用随机前缀和扩容RDD进行join" class="headerlink" title="使用随机前缀和扩容RDD进行join"></a>使用随机前缀和扩容RDD进行join</h4><h4 id="多种方案组合使用"><a href="#多种方案组合使用" class="headerlink" title="多种方案组合使用"></a>多种方案组合使用</h4>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/09/08/5-shuffle%E4%B8%8E%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/08/5-shuffle%E4%B8%8E%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/" class="post-title-link" itemprop="url">5.shuffle与内存管理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-08 17:23:46" itemprop="dateCreated datePublished" datetime="2021-09-08T17:23:46+08:00">2021-09-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-09-10 21:07:40" itemprop="dateModified" datetime="2021-09-10T21:07:40+08:00">2021-09-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><h3 id="堆内和堆外内存规划"><a href="#堆内和堆外内存规划" class="headerlink" title="堆内和堆外内存规划"></a>堆内和堆外内存规划</h3><p>作为一个 JVM 进程，Executor 的内存管理建立在 JVM 的内存管理之上，Spark 对 JVM的堆内（On-heap）空间进行了更为详细的分配，以充分利用内存。同时，Spark 引入了堆外（Off-heap）内存，使之可以直接在工作节点的系统内存中开辟空间，进一步优化了内存的使用。堆内内存受到 JVM 统一管理，堆外内存是直接向操作系统进行内存的申请和释放。</p>
<img src="/2021/09/08/5-shuffle%E4%B8%8E%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/%E5%86%85%E5%AD%98.png" class="">

<h4 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h4><p>动态占用机制，存储区域、计算区域、其他区域</p>
<h2 id="shuffle"><a href="#shuffle" class="headerlink" title="shuffle"></a>shuffle</h2><h3 id="ShuffleManager发展概述"><a href="#ShuffleManager发展概述" class="headerlink" title="ShuffleManager发展概述"></a>ShuffleManager发展概述</h3><ul>
<li>在Spark的源码中，负责shuffle过程的执行、计算和处理的组件主要就是ShuffleManager，也即shuffle管理器。而随着Spark的版本的发展，ShuffleManager也在不断迭代，变得越来越先进。</li>
<li>在Spark 1.2以前，默认的shuffle计算引擎是HashShuffleManager。该ShuffleManager而HashShuffleManager有着一个非常严重的弊端，就是会产生大量的中间磁盘文件，进而由大量的磁盘IO操作影响了性能。</li>
<li>因此在Spark 1.2以后的版本中，默认的ShuffleManager改成了SortShuffleManager。SortShuffleManager相较于HashShuffleManager来说，有了一定的改进。主要就在于，每个Task在进行shuffle操作时，虽然也会产生较多的临时磁盘文件，但是最后会将所有的临时文件合并（merge）成一个磁盘文件，因此每个Task就只有一个磁盘文件。在下一个stage的shuffle read task拉取自己的数据时，只要根据索引读取每个磁盘文件中的部分数据即可。</li>
</ul>
<h3 id="HashShuffleManager运行原理"><a href="#HashShuffleManager运行原理" class="headerlink" title="HashShuffleManager运行原理"></a>HashShuffleManager运行原理</h3>


<h3 id="SortShuffleManager运行原理"><a href="#SortShuffleManager运行原理" class="headerlink" title="SortShuffleManager运行原理"></a>SortShuffleManager运行原理</h3><p>SortShuffleManager的运行机制主要分成两种，一种是普通运行机制，另一种是bypass运行机制。当shuffle read task的数量小于等于spark.shuffle.sort.bypassMergeThreshold参数的值时（默认为200），就会启用bypass机制。</p>
<h4 id="普通运行机制"><a href="#普通运行机制" class="headerlink" title="普通运行机制"></a>普通运行机制</h4><p>内存数据结构–&gt;排序，分批溢写磁盘，清空内存数据结构–&gt;临时磁盘文件都进行合并，这就是merge过程</p>


<h4 id="bypass运行机制"><a href="#bypass运行机制" class="headerlink" title="bypass运行机制"></a>bypass运行机制</h4><p>而该机制与普通SortShuffleManager运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。</p>


<h3 id="shuffle相关参数调优"><a href="#shuffle相关参数调优" class="headerlink" title="shuffle相关参数调优"></a>shuffle相关参数调优</h3><h4 id="spark-shuffle-file-buffer"><a href="#spark-shuffle-file-buffer" class="headerlink" title="spark.shuffle.file.buffer"></a>spark.shuffle.file.buffer</h4><ul>
<li>默认值：32k</li>
<li>参数说明：该参数用于设置shuffle write task的BufferedOutputStream的buffer缓冲大小。将数据写到磁盘文件之前，会先写入buffer缓冲中，待缓冲写满之后，才会溢写到磁盘。</li>
<li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如64k），从而减少shuffle write过程中溢写磁盘文件的次数，也就可以减少磁盘IO次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li>
</ul>
<h4 id="spark-reducer-maxSizeInFlight"><a href="#spark-reducer-maxSizeInFlight" class="headerlink" title="spark.reducer.maxSizeInFlight"></a>spark.reducer.maxSizeInFlight</h4><ul>
<li>默认值：48m</li>
<li>参数说明：该参数用于设置shuffle read task的buffer缓冲大小，而这个buffer缓冲决定了每次能够拉取多少数据。</li>
<li>调优建议：如果作业可用的内存资源较为充足的话，可以适当增加这个参数的大小（比如96m），从而减少拉取数据的次数，也就可以减少网络传输的次数，进而提升性能。在实践中发现，合理调节该参数，性能会有1%~5%的提升。</li>
</ul>
<h4 id="spark-shuffle-io-maxRetries"><a href="#spark-shuffle-io-maxRetries" class="headerlink" title="spark.shuffle.io.maxRetries"></a>spark.shuffle.io.maxRetries</h4><ul>
<li>默认值：3</li>
<li>参数说明：shuffle read task从shuffle write task所在节点拉取属于自己的数据时，如果因为网络异常导致拉取失败，是会自动进行重试的。该参数就代表了可以重试的最大次数。如果在指定次数之内拉取还是没有成功，就可能会导致作业执行失败。</li>
<li>调优建议：对于那些包含了特别耗时的shuffle操作的作业，建议增加重试最大次数（比如60次），以避免由于JVM的full gc或者网络不稳定等因素导致的数据拉取失败。在实践中发现，对于针对超大数据量（数十亿~上百亿）的shuffle过程，调节该参数可以大幅度提升稳定性。</li>
</ul>
<h4 id="spark-shuffle-io-retryWait"><a href="#spark-shuffle-io-retryWait" class="headerlink" title="spark.shuffle.io.retryWait"></a>spark.shuffle.io.retryWait</h4><ul>
<li>默认值：5s</li>
<li>参数说明：具体解释同上，该参数代表了每次重试拉取数据的等待间隔，默认是5s。</li>
<li>调优建议：建议加大间隔时长（比如60s），以增加shuffle操作的稳定性。</li>
</ul>
<h4 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h4><ul>
<li>默认值：0.2</li>
<li>参数说明：该参数代表了Executor内存中，分配给shuffle read task进行聚合操作的内存比例，默认是20%。</li>
<li>调优建议：在资源参数调优中讲解过这个参数。如果内存充足，而且很少使用持久化操作，建议调高这个比例，给shuffle read的聚合操作更多内存，以避免由于内存不足导致聚合过程中频繁读写磁盘。在实践中发现，合理调节该参数可以将性能提升10%左右。</li>
</ul>
<h4 id="spark-shuffle-manager"><a href="#spark-shuffle-manager" class="headerlink" title="spark.shuffle.manager"></a>spark.shuffle.manager</h4><ul>
<li>默认值：sort</li>
<li>参数说明：该参数用于设置ShuffleManager的类型。Spark 1.5以后，有三个可选项：hash、sort和tungsten-sort。HashShuffleManager是Spark 1.2以前的默认选项，但是Spark 1.2以及之后的版本默认都是SortShuffleManager了。tungsten-sort与sort类似，但是使用了tungsten计划中的堆外内存管理机制，内存使用效率更高。</li>
<li>调优建议：由于SortShuffleManager默认会对数据进行排序，因此如果你的业务逻辑中需要该排序机制的话，则使用默认的SortShuffleManager就可以；而如果你的业务逻辑不需要对数据进行排序，那么建议参考后面的几个参数调优，通过bypass机制或优化的HashShuffleManager来避免排序操作，同时提供较好的磁盘读写性能。这里要注意的是，tungsten-sort要慎用，因为之前发现了一些相应的bug。</li>
</ul>
<h4 id="spark-shuffle-sort-bypassMergeThreshold"><a href="#spark-shuffle-sort-bypassMergeThreshold" class="headerlink" title="spark.shuffle.sort.bypassMergeThreshold"></a>spark.shuffle.sort.bypassMergeThreshold</h4><ul>
<li>默认值：200</li>
<li>参数说明：当ShuffleManager为SortShuffleManager时，如果shuffle read task的数量小于这个阈值（默认是200），则shuffle write过程中不会进行排序操作，而是直接按照未经优化的HashShuffleManager的方式去写数据，但是最后会将每个task产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</li>
<li>调优建议：当你使用SortShuffleManager时，如果的确不需要排序操作，那么建议将这个参数调大一些，大于shuffle read task的数量。那么此时就会自动启用bypass机制，map-side就不会进行排序了，减少了排序的性能开销。但是这种方式下，依然会产生大量的磁盘文件，因此shuffle write性能有待提高。</li>
</ul>
<h4 id="spark-shuffle-consolidateFiles"><a href="#spark-shuffle-consolidateFiles" class="headerlink" title="spark.shuffle.consolidateFiles"></a>spark.shuffle.consolidateFiles</h4><ul>
<li>默认值：false</li>
<li>参数说明：如果使用HashShuffleManager，该参数有效。如果设置为true，那么就会开启consolidate机制，会大幅度合并shuffle write的输出文件，对于shuffle read task数量特别多的情况下，这种方法可以极大地减少磁盘IO开销，提升性能。</li>
<li>调优建议：如果的确不需要SortShuffleManager的排序机制，那么除了使用bypass机制，还可以尝试将spark.shffle.manager参数手动指定为hash，使用HashShuffleManager，同时开启consolidate机制。在实践中尝试过，发现其性能比开启了bypass机制的SortShuffleManager要高出10%~30%</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/09/08/4-sparkSql/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/08/4-sparkSql/" class="post-title-link" itemprop="url">4.sparkSql</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-08 14:08:52" itemprop="dateCreated datePublished" datetime="2021-09-08T14:08:52+08:00">2021-09-08</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-09-10 21:07:40" itemprop="dateModified" datetime="2021-09-10T21:07:40+08:00">2021-09-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h2><p>Spark SQL 是 Spark 用于结构化数据(structured data)处理的 Spark 模块</p>
<h2 id="Hive-and-SparkSQL"><a href="#Hive-and-SparkSQL" class="headerlink" title="Hive and SparkSQL"></a>Hive and SparkSQL</h2><ul>
<li>数据兼容方面 SparkSQL 不但兼容 Hive，还可以从 RDD、parquet 文件、JSON 文件中获取数据，未来版本甚至支持获取 RDBMS 数据以及 cassandra 等 NOSQL 数据；</li>
<li>性能优化方面 除了采取 In-Memory Columnar Storage、byte-code generation 等优化技术外、将会引进 Cost Model 对查询进行动态评估、获取最佳物理计划等等；</li>
<li>组件扩展方面 无论是 SQL 的语法解析器、分析器还是优化器都可以重新定义，进行扩展</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/09/07/3-spark%E6%A0%B8%E5%BF%83/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/07/3-spark%E6%A0%B8%E5%BF%83/" class="post-title-link" itemprop="url">3.spark核心</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-07 11:49:13" itemprop="dateCreated datePublished" datetime="2021-09-07T11:49:13+08:00">2021-09-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-09-10 21:07:40" itemprop="dateModified" datetime="2021-09-10T21:07:40+08:00">2021-09-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="核心数据结构"><a href="#核心数据结构" class="headerlink" title="核心数据结构"></a>核心数据结构</h2><p>Spark 计算框架为了能够进行高并发和高吞吐的数据处理，封装了三大数据结构，用于处理不同的应用场景。三大数据结构分别是：</p>
<ul>
<li>RDD : 弹性分布式数据集</li>
<li>累加器：分布式共享只写变量</li>
<li>广播变量：分布式共享只读变量</li>
</ul>
<h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行计算的集合</p>
<ul>
<li>弹性<ul>
<li>存储的弹性：内存与磁盘的自动切换；</li>
<li>容错的弹性：数据丢失可以自动恢复；</li>
<li>计算的弹性：计算出错重试机制；</li>
<li>分片的弹性：可根据需要重新分片。</li>
</ul>
</li>
<li>分布式：数据存储在大数据集群不同节点上</li>
<li>数据集：RDD 封装了计算逻辑，并不保存数据</li>
<li>数据抽象：RDD 是一个抽象类，需要子类具体实现</li>
<li>不可变：RDD 封装了计算逻辑，是不可以改变的，想要改变，只能产生新的 RDD，在新的 RDD 里面封装计算逻辑</li>
<li>可分区、并行计算</li>
</ul>
<h3 id="核心属性"><a href="#核心属性" class="headerlink" title="核心属性"></a>核心属性</h3><img src="/2021/09/07/3-spark%E6%A0%B8%E5%BF%83/%E6%A0%B8%E5%BF%83%E5%B1%9E%E6%80%A7.png" class="">

<h4 id="分区列表"><a href="#分区列表" class="headerlink" title="分区列表"></a>分区列表</h4><p>RDD 数据结构中存在分区列表，用于执行任务时并行计算，是实现分布式计算的重要属性。</p>
<h4 id="分区计算函数"><a href="#分区计算函数" class="headerlink" title="分区计算函数"></a>分区计算函数</h4><p>Spark 在计算时，是使用分区函数对每一个分区进行计算</p>
<h4 id="RDD-之间的依赖关系"><a href="#RDD-之间的依赖关系" class="headerlink" title="RDD 之间的依赖关系"></a>RDD 之间的依赖关系</h4><p>RDD 是计算模型的封装，当需求中需要将多个计算模型进行组合时，就需要将多个 RDD 建立依赖关系</p>
<h4 id="分区器（可选）"><a href="#分区器（可选）" class="headerlink" title="分区器（可选）"></a>分区器（可选）</h4><p>当数据为 KV 类型数据时，可以通过设定分区器自定义数据的分区</p>
<h4 id="首选位置（可选）"><a href="#首选位置（可选）" class="headerlink" title="首选位置（可选）"></a>首选位置（可选）</h4><p>计算数据时，可以根据计算节点的状态选择不同的节点位置进行计算</p>
<h3 id="执行原理"><a href="#执行原理" class="headerlink" title="执行原理"></a>执行原理</h3><ul>
<li>从计算的角度来讲，数据处理过程中需要计算资源（内存 &amp; CPU）和计算模型（逻辑）。执行时，需要将计算资源和计算模型进行协调和整合。</li>
<li>Spark 框架在执行时，先申请资源，然后将应用程序的数据处理逻辑分解成一个一个的计算任务。然后将任务发到已经分配资源的计算节点上, 按照指定的计算模型进行数据计算。最后得到计算结果</li>
</ul>
<p>RDD 是 Spark 框架中用于数据处理的核心模型，接下来我们看看，在 Yarn 环境中，RDD的工作原理:</p>
<ul>
<li>启动 Yarn 集群环境。（包括ResourceManager,NodeManager）</li>
<li>Spark 通过申请资源创建调度节点和计算节点。(启动ApplycationMaster,driver,excutor)</li>
<li>Spark 框架根据需求将计算逻辑根据分区划分成不同的任务</li>
<li>调度节点将任务根据计算节点状态发送到对应的计算节点进行计算</li>
</ul>
<p>从以上流程可以看出 RDD 在整个流程中主要用于将逻辑进行封装，并生成 Task 发送给Executor 节点执行计算，接下来我们就一起看看 Spark 框架中 RDD 是具体是如何进行数据处理的。</p>
<h3 id="基础编程"><a href="#基础编程" class="headerlink" title="基础编程"></a>基础编程</h3><h4 id="创建RDD"><a href="#创建RDD" class="headerlink" title="创建RDD"></a>创建RDD</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">&quot;local[*]&quot;</span>).setAppName(<span class="string">&quot;spark&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> sparkContext = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> rdd1 = sparkContext.parallelize(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> rdd2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">rdd1.collect().foreach(println)</span><br><span class="line">rdd2.collect().foreach(println)</span><br><span class="line">sparkContext.stop()</span><br><span class="line">从底层代码实现来讲，makeRDD 方法其实就是 parallelize 方法</span><br></pre></td></tr></table></figure>

<h4 id="map"><a href="#map" class="headerlink" title="map"></a>map</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>]</span><br><span class="line"><span class="keyword">val</span> dataRDD: <span class="type">RDD</span>[<span class="type">Int</span>] = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD1: <span class="type">RDD</span>[<span class="type">Int</span>] = dataRDD.map(num =&gt; &#123;num * <span class="number">2</span>&#125;)</span><br></pre></td></tr></table></figure>

<h4 id="mapPartitions"><a href="#mapPartitions" class="headerlink" title="mapPartitions"></a>mapPartitions</h4><p>将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据。</p>
<h4 id="map与mapPartitions-区别"><a href="#map与mapPartitions-区别" class="headerlink" title="map与mapPartitions 区别"></a>map与mapPartitions 区别</h4><ul>
<li>数据处理角度<ul>
<li>Map 算子是分区内一个数据一个数据的执行，类似于串行操作。而 mapPartitions 算子是以分区为单位进行批处理操作。</li>
</ul>
</li>
<li>功能的角度<ul>
<li>Map 算子主要目的将数据源中的数据进行转换和改变。但是不会减少或增多数据。MapPartitions 算子需要传递一个迭代器，返回一个迭代器，没有要求的元素的个数保持不变，所以可以增加或减少数据</li>
</ul>
</li>
<li>性能的角度<ul>
<li>Map 算子因为类似于串行操作，所以性能比较低，而是 mapPartitions 算子类似于批处理，所以性能较高。但是 mapPartitions 算子会长时间占用内存，那么这样会导致内存可能不够用，出现内存溢出的错误。所以在内存有限的情况下，不推荐使用。使用 map 操作</li>
</ul>
</li>
</ul>
<h4 id="mapPartitionsWithIndex"><a href="#mapPartitionsWithIndex" class="headerlink" title="mapPartitionsWithIndex"></a>mapPartitionsWithIndex</h4><p>将待处理的数据以分区为单位发送到计算节点进行处理，这里的处理是指可以进行任意的处理，哪怕是过滤数据，在处理时同时可以获取当前分区索引。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.mapPartitionsWithIndex((index, datas) =&gt; &#123;datas.map(index, _)&#125;)</span><br></pre></td></tr></table></figure>

<h4 id="flatMap"><a href="#flatMap" class="headerlink" title="flatMap"></a>flatMap</h4><p>将处理的数据进行扁平化后再进行映射处理，所以算子也称之为扁平映射</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>),<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>) ),<span class="number">1</span>) <span class="keyword">val</span> dataRDD1 = dataRDD.flatMap(list =&gt; list)</span><br></pre></td></tr></table></figure>

<h4 id="glom"><a href="#glom" class="headerlink" title="glom"></a>glom</h4><p>将同一个分区的数据直接转换为相同类型的内存数组进行处理，分区不变</p>
<h4 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h4><p>将数据根据指定的规则进行分组, 分区默认不变，但是数据会被打乱重新组合，我们将这样的操作称之为 shuffle。极限情况下，数据可能被分在同一个分区中 一个组的数据在一个分区中，但是并不是说一个分区中只有一个组</p>
<h4 id="filter"><a href="#filter" class="headerlink" title="filter"></a>filter</h4><p>将数据根据指定的规则进行筛选过滤，符合规则的数据保留，不符合规则的数据丢弃。 当数据进行筛选过滤后，分区不变，但是分区内的数据可能不均衡，生产环境下，可能会出现数据倾斜</p>
<h4 id="sample"><a href="#sample" class="headerlink" title="sample"></a>sample</h4><p>根据指定的规则从数据集中抽取数据</p>
<h4 id="distinct"><a href="#distinct" class="headerlink" title="distinct"></a>distinct</h4><p>将数据集中重复的数据去重</p>
<h4 id="coalesce"><a href="#coalesce" class="headerlink" title="coalesce"></a>coalesce</h4><p>根据数据量缩减分区，用于大数据集过滤后，提高小数据集的执行效率 当 spark 程序中，存在过多的小任务的时候，可以通过 coalesce 方法，收缩合并分区，减少分区的个数，减小任务调度成本</p>
<h4 id="repartition"><a href="#repartition" class="headerlink" title="repartition"></a>repartition</h4><p>该操作内部其实执行的是 coalesce 操作，参数 shuffle 的默认值为 true。无论是将分区数多的RDD 转换为分区数少的 RDD，还是将分区数少的 RDD 转换为分区数多的 RDD，repartition操作都可以完成，因为无论如何都会经 shuffle 过程。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span> ),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.repartition(<span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<h4 id="sortBy"><a href="#sortBy" class="headerlink" title="sortBy"></a>sortBy</h4><p>该操作用于排序数据。在排序之前，可以将数据通过 f 函数进行处理，之后按照 f 函数处理的结果进行排序，默认为升序排列。排序后新产生的 RDD 的分区数与原 RDD 的分区数一致。中间存在 shuffle 的过程</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">2</span> ),<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD1 = dataRDD.sortBy(num=&gt;num, <span class="literal">false</span>, <span class="number">4</span>)</span><br></pre></td></tr></table></figure>

<h4 id="intersection"><a href="#intersection" class="headerlink" title="intersection"></a>intersection</h4><p>对源 RDD 和参数 RDD 求交集后返回一个新的 RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.intersection(dataRDD2)</span><br></pre></td></tr></table></figure>

<h4 id="union"><a href="#union" class="headerlink" title="union"></a>union</h4><p>对源 RDD 和参数 RDD 求并集后返回一个新的 RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.union(dataRDD2)</span><br></pre></td></tr></table></figure>

<h4 id="subtract"><a href="#subtract" class="headerlink" title="subtract"></a>subtract</h4><p>以一个 RDD 元素为主，去除两个 RDD 中重复元素，将其他元素保留下来。求差集</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.subtract(dataRDD2)</span><br></pre></td></tr></table></figure>

<h4 id="zip"><a href="#zip" class="headerlink" title="zip"></a>zip</h4><p>将两个 RDD 中的元素，以键值对的形式进行合并。其中，键值对中的 Key 为第 1 个 RDD中的元素，Value 为第 2 个 RDD 中的相同位置的元素。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>(<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>))</span><br><span class="line"><span class="keyword">val</span> dataRDD = dataRDD1.zip(dataRDD2)</span><br></pre></td></tr></table></figure>

<h4 id="partitionBy"><a href="#partitionBy" class="headerlink" title="partitionBy"></a>partitionBy</h4><p>将数据按照指定 Partitioner 重新进行分区。Spark 默认的分区器是 HashPartitioner</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = sc.makeRDD(<span class="type">Array</span>((<span class="number">1</span>,<span class="string">&quot;aaa&quot;</span>),(<span class="number">2</span>,<span class="string">&quot;bbb&quot;</span>),(<span class="number">3</span>,<span class="string">&quot;ccc&quot;</span>)),<span class="number">3</span>)</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.<span class="type">HashPartitioner</span></span><br><span class="line"><span class="keyword">val</span> rdd2: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = rdd.partitionBy(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<h4 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a><strong>reduceByKey</strong></h4><p>可以将数据按照相同的 Key 对 Value 进行聚合</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD1.reduceByKey(_+_)</span><br><span class="line"><span class="keyword">val</span> dataRDD3 = dataRDD1.reduceByKey(_+_, <span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<h4 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h4><p>将数据源的数据根据 key 对 value 进行分组</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD1.groupByKey()</span><br><span class="line"><span class="keyword">val</span> dataRDD3 = dataRDD1.groupByKey(<span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> dataRDD4 = dataRDD1.groupByKey(<span class="keyword">new</span> <span class="type">HashPartitioner</span>(<span class="number">2</span>))</span><br></pre></td></tr></table></figure>

<h4 id="reduceByKey与groupByKey区别"><a href="#reduceByKey与groupByKey区别" class="headerlink" title="reduceByKey与groupByKey区别"></a>reduceByKey与groupByKey区别</h4><p>从 shuffle 的角度：reduceByKey 和 groupByKey 都存在 shuffle 的操作，但是 reduceByKey可以在 shuffle 前对分区内相同 key 的数据进行预聚合（combine）功能，这样会减少落盘的数据量，而 groupByKey 只是进行分组，不存在数据量减少的问题，reduceByKey 性能比较高。</p>
<p>从功能的角度：reduceByKey 其实包含分组和聚合的功能。GroupByKey 只能分组，不能聚合，所以在分组聚合的场合下，推荐使用 reduceByKey，如果仅仅是分组而不需要聚合。那么还是只能使用 groupByKey</p>
<h4 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h4><p>将数据根据不同的规则进行分区内计算和分区间计算</p>
<h4 id="foldByKey"><a href="#foldByKey" class="headerlink" title="foldByKey"></a>foldByKey</h4><p>当分区内计算规则和分区间计算规则相同时，aggregateByKey 就可以简化为 foldByKey</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = dataRDD1.foldByKey(<span class="number">0</span>)(_+_)</span><br></pre></td></tr></table></figure>

<h4 id="combineByKey"><a href="#combineByKey" class="headerlink" title="combineByKey"></a>combineByKey</h4><p>最通用的对 key-value 型 rdd 进行聚集操作的聚集函数（aggregation function）。类似于aggregate()，combineByKey()允许用户返回值的类型与输入不一致</p>
<h4 id="reduceByKey、foldByKey、aggregateByKey、combineByKey-的区别"><a href="#reduceByKey、foldByKey、aggregateByKey、combineByKey-的区别" class="headerlink" title="reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别"></a>reduceByKey、foldByKey、aggregateByKey、combineByKey 的区别</h4><ul>
<li>reduceByKey: 相同 key 的第一个数据不进行任何计算，分区内和分区间计算规则相同</li>
<li>FoldByKey: 相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则相同</li>
<li>AggregateByKey：相同 key 的第一个数据和初始值进行分区内计算，分区内和分区间计算规则可以不相同</li>
<li>CombineByKey:当计算时，发现数据结构不满足要求时，可以让第一个数据转换结构。分区内和分区间计算规则不相同。</li>
</ul>
<h4 id="sortByKey"><a href="#sortByKey" class="headerlink" title="sortByKey"></a>sortByKey</h4><p>在一个(K,V)的 RDD 上调用，K 必须实现 Ordered 接口(特质)，返回一个按照 key 进行排序的</p>
<h4 id="join"><a href="#join" class="headerlink" title="join"></a>join</h4><p>在类型为(K,V)和(K,W)的 RDD 上调用，返回一个相同 key 对应的所有元素连接在一起的(K,(V,W))的 RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">String</span>)] = sc.makeRDD(<span class="type">Array</span>((<span class="number">1</span>, <span class="string">&quot;a&quot;</span>), (<span class="number">2</span>, <span class="string">&quot;b&quot;</span>), (<span class="number">3</span>, <span class="string">&quot;c&quot;</span>)))</span><br><span class="line"><span class="keyword">val</span> rdd1: <span class="type">RDD</span>[(<span class="type">Int</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">Array</span>((<span class="number">1</span>, <span class="number">4</span>), (<span class="number">2</span>, <span class="number">5</span>), (<span class="number">3</span>, <span class="number">6</span>)))</span><br><span class="line">rdd.join(rdd1).collect().foreach(println)</span><br></pre></td></tr></table></figure>

<h4 id="leftOuterJoin"><a href="#leftOuterJoin" class="headerlink" title="leftOuterJoin"></a>leftOuterJoin</h4><p>类似于 SQL 语句的左外连接</p>
<h4 id="cogroup"><a href="#cogroup" class="headerlink" title="cogroup"></a>cogroup</h4><p>在类型为(K,V)和(K,W)的 RDD 上调用，返回一个(K,(Iterable&lt; V &gt;,Iterable&lt; W &gt;))类型的 RDD</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> dataRDD1 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;a&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> dataRDD2 = sparkContext.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">2</span>),(<span class="string">&quot;c&quot;</span>,<span class="number">3</span>)))</span><br><span class="line"><span class="keyword">val</span> value: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">Iterable</span>[<span class="type">Int</span>], <span class="type">Iterable</span>[<span class="type">Int</span>]))] = dataRDD1.cogroup(dataRDD2)</span><br></pre></td></tr></table></figure>

<p>———————RDD 行动算子———————————-</p>
<h4 id="reduce"><a href="#reduce" class="headerlink" title="reduce"></a>reduce</h4><p>聚集 RDD 中的所有元素，先聚合分区内数据，再聚合分区间数据</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment">// 聚合数据</span></span><br><span class="line"><span class="keyword">val</span> reduceResult: <span class="type">Int</span> = rdd.reduce(_+_)</span><br></pre></td></tr></table></figure>

<h4 id="collect"><a href="#collect" class="headerlink" title="collect"></a>collect</h4><p>在驱动程序中，以数组 Array 的形式返回数据集的所有元素</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment">// 收集数据到</span></span><br><span class="line"><span class="type">Driver</span> rdd.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<h4 id="count"><a href="#count" class="headerlink" title="count"></a>count</h4><p>返回 RDD 中元素的个数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment">// 返回 RDD 中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> countResult: <span class="type">Long</span> = rdd.count()</span><br></pre></td></tr></table></figure>

<h4 id="first"><a href="#first" class="headerlink" title="first"></a>first</h4><p>返回 RDD 中的第一个元素</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment">// 返回 RDD 中元素的个数</span></span><br><span class="line"><span class="keyword">val</span> firstResult: <span class="type">Int</span> = rdd.first() println(firstResult)</span><br></pre></td></tr></table></figure>

<h4 id="take-takeOrdered"><a href="#take-takeOrdered" class="headerlink" title="take takeOrdered"></a>take takeOrdered</h4><p>返回一个由 RDD 的前 n 个元素组成的数组</p>
<p>返回该 RDD 排序后的前 n 个元素组成的数组</p>
<h4 id="save-相关算子"><a href="#save-相关算子" class="headerlink" title="save 相关算子"></a>save 相关算子</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 保存成 Text 文件 rdd.saveAsTextFile(&quot;output&quot;)</span></span><br><span class="line"><span class="comment">// 序列化成对象保存到文件 rdd.saveAsObjectFile(&quot;output1&quot;)</span></span><br><span class="line"><span class="comment">// 保存成 Sequencefile 文件 rdd.map((_,1)).saveAsSequenceFile(&quot;output2&quot;)</span></span><br></pre></td></tr></table></figure>

<h4 id="foreach"><a href="#foreach" class="headerlink" title="foreach"></a>foreach</h4><p>分布式遍历 RDD 中的每一个元素，调用指定函数</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line"><span class="comment">// 收集后打印</span></span><br><span class="line">rdd.map(num=&gt;num).collect().foreach(println)</span><br><span class="line">println(<span class="string">&quot;****************&quot;</span>)</span><br><span class="line"><span class="comment">// 分布式打印</span></span><br><span class="line">rdd.foreach(println)</span><br></pre></td></tr></table></figure>

<h3 id="闭包检测"><a href="#闭包检测" class="headerlink" title="闭包检测"></a>闭包检测</h3><ul>
<li>从计算的角度, 算子以外的代码都是在 Driver 端执行, 算子里面的代码都是在 Executor端执行。</li>
<li>那么在 scala 的函数式编程中，就会导致算子内经常会用到算子外的数据，这样就形成了闭包的效果</li>
<li>如果使用的算子外的数据无法序列化，就意味着无法传值给 Executor端执行，就会发生错误，所以需要在执行任务计算前，检测闭包内的对象是否可以进行序列化，这个操作我们称之为闭包检测。Scala2.12 版本后闭包编译方式发生了改变</li>
</ul>
<h3 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h3><p>通过继承Serializable 使用kryo序列化</p>
<h3 id="RDD-依赖"><a href="#RDD-依赖" class="headerlink" title="RDD 依赖"></a>RDD 依赖</h3><h4 id="RDD-血缘关系"><a href="#RDD-血缘关系" class="headerlink" title="RDD 血缘关系"></a>RDD 血缘关系</h4><p>lineage血统，以便恢复丢失分区</p>
<h4 id="RDD-依赖关系"><a href="#RDD-依赖关系" class="headerlink" title="RDD 依赖关系"></a>RDD 依赖关系</h4><p>这里所谓的依赖关系，其实就是两个相邻 RDD 之间的关系</p>
<h4 id="RDD-窄依赖"><a href="#RDD-窄依赖" class="headerlink" title="RDD 窄依赖"></a>RDD 窄依赖</h4><p>窄依赖表示每一个父(上游)RDD 的 Partition 最多被子（下游）RDD 的一个 Partition 使用，窄依赖我们形象的比喻为独生子女</p>
<p>class OneToOneDependency[T](rdd: RDD[T]) extends NarrowDependency<a href="rdd">T</a></p>
<h4 id="RDD-宽依赖"><a href="#RDD-宽依赖" class="headerlink" title="RDD 宽依赖"></a>RDD 宽依赖</h4><p>宽依赖表示同一个父（上游）RDD 的 Partition 被多个子（下游）RDD 的 Partition 依赖，会引起 Shuffle，总结：宽依赖我们形象的比喻为多生。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ShuffleDependency</span>[<span class="type">K</span>: <span class="type">ClassTag</span>, <span class="type">V</span>: <span class="type">ClassTag</span>, <span class="type">C</span>: <span class="type">ClassTag</span>](<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">    @transient private val _rdd: <span class="type">RDD</span>[_ &lt;: <span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]],</span></span></span><br><span class="line"><span class="class"><span class="params">    val partitioner: <span class="type">Partitioner</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val serializer: <span class="type">Serializer</span> = <span class="type">SparkEnv</span>.get.serializer,</span></span></span><br><span class="line"><span class="class"><span class="params">    val keyOrdering: <span class="type">Option</span>[<span class="type">Ordering</span>[<span class="type">K</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val aggregator: <span class="type">Option</span>[<span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>]] = <span class="type">None</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">    val mapSideCombine: <span class="type">Boolean</span> = false</span>)</span></span><br><span class="line"><span class="class"><span class="keyword">extends</span> <span class="title">Dependency</span>[<span class="type">Product2</span>[<span class="type">K</span>, <span class="type">V</span>]]</span></span><br></pre></td></tr></table></figure>

<h4 id="RDD-阶段划分"><a href="#RDD-阶段划分" class="headerlink" title="RDD 阶段划分"></a>RDD 阶段划分</h4><p>DAG（Directed Acyclic Graph）有向无环图是由点和线组成的拓扑图形，该图形具有方向，不会闭环。例如，DAG 记录了 RDD 的转换过程和任务的阶段。</p>
<h4 id="RDD-任务划分"><a href="#RDD-任务划分" class="headerlink" title="RDD 任务划分"></a>RDD 任务划分</h4><p>RDD 任务切分中间分为：Application、Job、Stage 和 Task</p>
<ul>
<li>Application：初始化一个 SparkContext 即生成一个 Application；</li>
<li>Job：一个 Action 算子就会生成一个 Job；</li>
<li>Stage：Stage 等于宽依赖(ShuffleDependency)的个数加 1；</li>
<li>Task：一个 Stage 阶段中，最后一个 RDD 的分区个数就是 Task 的个数。</li>
</ul>
<h3 id="RDD-持久化"><a href="#RDD-持久化" class="headerlink" title="RDD 持久化"></a>RDD 持久化</h3><p>cache persist</p>
<p>mapRdd.persist(StorageLevel.MEMORY_AND_DISK_2)</p>
<h3 id="检查点"><a href="#检查点" class="headerlink" title="检查点"></a>检查点</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 设置检查点路径 sc.setCheckpointDir(&quot;./checkpoint1&quot;)</span></span><br><span class="line"><span class="comment">// 创建一个 RDD，读取指定位置文件:hello atguigu atguigu val lineRdd: RDD[String] = sc.textFile(&quot;input/1.txt&quot;)</span></span><br><span class="line"><span class="comment">// 业务逻辑 val wordRdd: RDD[String] = lineRdd.flatMap(line =&gt; line.split(&quot; &quot;))</span></span><br><span class="line"><span class="keyword">val</span> wordToOneRdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Long</span>)] = wordRdd.map &#123;     word =&gt; &#123;         (word, <span class="type">System</span>.currentTimeMillis())     &#125; &#125;</span><br><span class="line"><span class="comment">// 增加缓存,避免再重新跑一个 job 做 checkpoint wordToOneRdd.cache() // 数据检查点：针对 wordToOneRdd 做检查点计算 wordToOneRdd.checkpoint()</span></span><br><span class="line"><span class="comment">// 触发执行逻辑 wordToOneRdd.collect().foreach(println)</span></span><br></pre></td></tr></table></figure>

<h4 id="缓存与检查点区别"><a href="#缓存与检查点区别" class="headerlink" title="缓存与检查点区别"></a>缓存与检查点区别</h4><ul>
<li>Cache 缓存只是将数据保存起来，不切断血缘依赖。Checkpoint 检查点切断血缘依赖<br>Cache 缓存的数据通常存储在磁盘、内存等地方，可靠性低。Checkpoint 的数据通常存储在 HDFS 等容错、高可用的文件系统，可靠性高</li>
<li>建议对 checkpoint()的 RDD 使用 Cache 缓存，这样 checkpoint 的 job 只需从 Ca### RDD 分区器<br>che 缓存中读取数据即可，否则需要再从头计算一次 RDD</li>
</ul>
<h3 id="RDD-分区器"><a href="#RDD-分区器" class="headerlink" title="RDD 分区器"></a>RDD 分区器</h3><p>Spark 目前支持 Hash 分区和 Range 分区，和用户自定义分区。Hash 分区为当前的默认分区。分区器直接决定了 RDD 中分区的个数、RDD 中每条数据经过 Shuffle 后进入哪个分区，进而决定了 Reduce 的个数。</p>
<ul>
<li><p>只有 Key-Value 类型的 RDD 才有分区器，非 Key-Value 类型的 RDD 分区的值是 None</p>
</li>
<li><p>每个 RDD 的分区 ID 范围：0 ~ (numPartitions - 1)，决定这个值是属于那个分区的</p>
</li>
<li><p>Hash 分区：对于给定的 key，计算其 hashCode,并除以分区个数取余</p>
</li>
<li><p>Range 分区：将一定范围内的数据映射到一个分区中，尽量保证每个分区数据均匀，而且分区间有序</p>
</li>
</ul>
<h2 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h2><h3 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h3><p>累加器用来把 Executor 端变量信息聚合到 Driver 端。在 Driver 程序中定义的变量，在Executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 Driver 端进行 merge。</p>
<h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><h3 id="广播变量实现原理"><a href="#广播变量实现原理" class="headerlink" title="广播变量实现原理"></a>广播变量实现原理</h3><p>广播变量用来高效分发较大的对象。向所有工作节点发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有节点发送一个较大的只读查询表，广播变量用起来都很顺手。在多个并行操作中使用同一个变量，但是 Spark 会为每个任务分别发送</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/09/07/2-spark%E6%9E%B6%E6%9E%84/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/09/07/2-spark%E6%9E%B6%E6%9E%84/" class="post-title-link" itemprop="url">2.spark架构</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-09-07 10:45:18" itemprop="dateCreated datePublished" datetime="2021-09-07T10:45:18+08:00">2021-09-07</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-02-04 10:33:07" itemprop="dateModified" datetime="2023-02-04T10:33:07+08:00">2023-02-04</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="运行架构"><a href="#运行架构" class="headerlink" title="运行架构"></a>运行架构</h2><p>Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。 如下图所示，它展示了一个 Spark 执行时的基本结构。图形中的 Driver 表示 master，负责管理整个集群中的作业任务调度。图形中的 Executor 则是 slave，负责实际执行任务</p>
<img src="/2021/09/07/2-spark%E6%9E%B6%E6%9E%84/spark%E6%9E%B6%E6%9E%84.png" class="">

<h2 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h2><h3 id="Driver"><a href="#Driver" class="headerlink" title="Driver"></a>Driver</h3><p>Spark 驱动器节点，用于执行 Spark 任务中的 main 方法，负责实际代码的执行工作</p>
<p>Driver 在 Spark 作业执行时主要负责：</p>
<ul>
<li>将用户程序转化为作业（job）</li>
<li>在 Executor 之间调度任务(task)</li>
<li>跟踪 Executor 的执行情况</li>
<li>通过 UI 展示查询运行情</li>
</ul>
<p>实际上，我们无法准确地描述 Driver 的定义，因为在整个的编程过程中没有看到任何有关Driver 的字眼。所以简单理解，所谓的 Driver 就是驱使整个应用运行起来的程序，也称之为Driver 类</p>
<h3 id="Executor"><a href="#Executor" class="headerlink" title="Executor"></a>Executor</h3><p>Spark Executor 是集群中工作节点（Worker）中的一个 JVM 进程，负责在 Spark 作业中运行具体任务（Task），任务彼此之间相互独立。Spark 应用启动时，Executor 节点被同时启动，并且始终伴随着整个 Spark 应用的生命周期而存在。如果有 Executor 节点发生了故障或崩溃，Spark 应用也可以继续执行，会将出错节点上的任务调度到其他 Executor 节点上继续运行</p>
<p>Executor 有两个核心功能：</p>
<ul>
<li>负责运行组成 Spark 应用的任务，并将结果返回给驱动器进程</li>
<li>它们通过自身的块管理器（Block Manager）为用户程序中要求缓存的 RDD 提供内存式存储。RDD 是直接缓存在 Executor 进程内的，因此任务可以在运行时充分利用缓存数据加速运算。</li>
</ul>
<h3 id="Master-amp-Worker"><a href="#Master-amp-Worker" class="headerlink" title="Master &amp; Worker"></a>Master &amp; Worker</h3><p>Spark 集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调度的功能，所以环境中还有其他两个核心组件：Master 和 Worker，这里的 Master 是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于 Yarn 环境中的 RM, 而Worker 呢，也是进程，一个 Worker 运行在集群中的一台服务器上，由 Master 分配资源对数据进行并行的处理和计算，类似于 Yarn环境中 NM。</p>
<h3 id="ApplicationMaster"><a href="#ApplicationMaster" class="headerlink" title="ApplicationMaster"></a>ApplicationMaster</h3><p>Hadoop 用户向 YARN 集群提交应用程序时,提交程序中应该包含 ApplicationMaster，用于向资源调度器申请执行任务的资源容器 Container，运行用户自己的程序任务 job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。 说的简单点就是，ResourceManager（资源）和 Driver（计算）之间的解耦合靠的就是ApplicationMaster。</p>
<h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><h3 id="Executor-与-Core"><a href="#Executor-与-Core" class="headerlink" title="Executor 与 Core"></a>Executor 与 Core</h3><p>Spark Executor 是集群中运行在工作节点（Worker）中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核（Core）数量。</p>
<h3 id="并行度（Parallelism）与分区"><a href="#并行度（Parallelism）与分区" class="headerlink" title="并行度（Parallelism）与分区"></a>并行度（Parallelism）与分区</h3><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们将整个集群并行执行任务的数量称之为并行度。那么一个作业到底并行度是多少呢？这个取决于框架的默认配置。应用程序也可以在运行过程中动态修改。</p>
<p>默认情况下，Spark 可以将一个作业切分多个任务后，发送给 Executor 节点并行计算，而能够并行计算的任务数量我们称之为并行度。这个数量可以在构建 RDD 时指定。记住，这里的并行执行的任务数量，并不是指的切分任务的数量，不要混淆了。</p>
<h2 id="提交流程"><a href="#提交流程" class="headerlink" title="提交流程"></a>提交流程</h2><p>所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过 Spark 客户端提交给 Spark 运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又有细微的区别，我们这里不进行详细的比较，但是因为国内工作中，将 Spark 引用部署到Yarn 环境中会更多一些，所以本课程中的提交流程是基于 Yarn 环境的。</p>
<img src="/2021/09/07/2-spark%E6%9E%B6%E6%9E%84/%E6%8F%90%E4%BA%A4%E6%B5%81%E7%A8%8B.png" class="">

<p>Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式：Client和 Cluster。两种模式主要区别在于：Driver 程序的运行节点位置。</p>
<img src="/2021/09/07/2-spark%E6%9E%B6%E6%9E%84/yarn_client.png" class="">
<img src="/2021/09/07/2-spark%E6%9E%B6%E6%9E%84/yarn_cluster%E6%A8%A1%E5%BC%8F.png" class="">

<h3 id="Yarn-Client-模式"><a href="#Yarn-Client-模式" class="headerlink" title="Yarn Client 模式"></a>Yarn Client 模式</h3><p>Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一般用于测试</p>
<ul>
<li>Driver 在任务提交的本地机器上运行</li>
<li>Driver 启动后会和 ResourceManager 通讯申请启动 ApplicationMaster</li>
<li>ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，负责向 ResourceManager 申请 Executor 内存</li>
<li>ResourceManager 接到 ApplicationMaster 的资源申请后会分配 container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程</li>
<li>Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数</li>
<li>之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行</li>
</ul>
<h3 id="Yarn-Cluster-模式"><a href="#Yarn-Cluster-模式" class="headerlink" title="Yarn Cluster 模式"></a>Yarn Cluster 模式</h3><p>Cluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于实际生产环境。</p>
<ul>
<li>在 YARN Cluster 模式下，任务提交后会和 ResourceManager 通讯申请启动ApplicationMaster</li>
<li>随后 ResourceManager 分配 container，在合适的 NodeManager 上启动 ApplicationMaster，此时的 ApplicationMaster 就是 Driver。</li>
<li>Driver 启动后向 ResourceManager 申请 Executor 内存，ResourceManager 接到ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动Executor 进程</li>
<li>Executor 进程启动后会向 Driver 反向注册，Executor 全部注册完成后 Driver 开始执行main 函数</li>
<li>之后执行到 Action 算子时，触发一个 Job，并根据宽依赖开始划分 stage，每个 stage 生成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/default-index/page/3/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/default-index/">1</a><span class="space">&hellip;</span><a class="page-number" href="/default-index/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/default-index/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/default-index/page/15/">15</a><a class="extend next" rel="next" href="/default-index/page/5/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">sk-xinye</p>
  <div class="site-description" itemprop="description">愿所有努力都不被辜负</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">142</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">sk-xinye</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
