<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sk-xinye.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.yml"};
  </script>

  <meta name="description" content="愿所有努力都不被辜负">
<meta property="og:type" content="website">
<meta property="og:title" content="sk-xinyeの博客">
<meta property="og:url" content="https://sk-xinye.github.io/default-index/page/6/index.html">
<meta property="og:site_name" content="sk-xinyeの博客">
<meta property="og:description" content="愿所有努力都不被辜负">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="sk-xinye">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://sk-xinye.github.io/default-index/page/6/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>sk-xinyeの博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">sk-xinyeの博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">记录学习的脚步</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-fa fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签<span class="badge">1</span></a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类<span class="badge">18</span></a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档<span class="badge">142</span></a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/30/3-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/30/3-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F/" class="post-title-link" itemprop="url">3.深入理解容器镜像</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-30 09:44:21" itemprop="dateCreated datePublished" datetime="2021-06-30T09:44:21+08:00">2021-06-30</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-07-01 20:45:07" itemprop="dateModified" datetime="2021-07-01T20:45:07+08:00">2021-07-01</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>Namespace 的作用是“隔离”，它让应用进程只能看到该 Namespace 内的“世界”；而 Cgroups 的作用是“限制”，它给这个“世界”围上了一圈看不见的墙。</p>
<h2 id="容器里的进程看到的文件系统又是什么样子的呢"><a href="#容器里的进程看到的文件系统又是什么样子的呢" class="headerlink" title="容器里的进程看到的文件系统又是什么样子的呢"></a>容器里的进程看到的文件系统又是什么样子的呢</h2><h3 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h3><ul>
<li>Mount Namespace 修改的，是容器进程对文件系统“挂载点”的认知。</li>
<li>这也就意味着，只有在“挂载”这个操作发生之后，进程的视图才会被改变。</li>
<li>而在此之前，新创建的容器会直接继承宿主机的各个挂载点。</li>
</ul>
<p>这就是 Mount Namespace 跟其他 Namespace 的使用略有不同的地方：<strong>它对容器进程视图的改变，一定是伴随着挂载操作（mount）才能生效。</strong></p>
<p>挂载“/”目录，使容器内的文件系统单一</p>
<ul>
<li><p>在 Linux 操作系统里，有一个名为 chroot 的命令change root file system 即改变进程的根目录到你指定的位置。</p>
<ul>
<li><p>假设，我们现在有一个 $HOME/test 目录，想要把它作为一个 /bin/bash 进程的根目录。</p>
</li>
<li><p>首先，创建一个 test 目录和几个 lib 文件夹：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p <span class="variable">$HOME</span>/<span class="built_in">test</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir -p <span class="variable">$HOME</span>/<span class="built_in">test</span>/&#123;bin,lib64,lib&#125;</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">cd</span> <span class="variable">$T</span></span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>然后，把 bash 命令拷贝到 test 目录对应的 bin 路径下：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cp -v /bin/&#123;bash,ls&#125; <span class="variable">$HOME</span>/<span class="built_in">test</span>/bin</span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>接下来，把 bash 命令需要的所有 so 文件，也拷贝到 test 目录对应的 lib 路径下。找到 so 文件可以用 ldd 命令：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> T=<span class="variable">$HOME</span>/<span class="built_in">test</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> list=<span class="string">&quot;<span class="subst">$(ldd /bin/ls | egrep -o &#x27;/lib.*\.[0-9]&#x27;)</span>&quot;</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="variable">$list</span>; <span class="keyword">do</span> cp -v <span class="string">&quot;<span class="variable">$i</span>&quot;</span> <span class="string">&quot;<span class="variable">$&#123;T&#125;</span><span class="variable">$&#123;i&#125;</span>&quot;</span>; <span class="keyword">done</span></span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>最后，执行 chroot 命令，告诉操作系统，我们将使用 $HOME/test 目录作为 /bin/bash 进程的根目录：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> chroot <span class="variable">$HOME</span>/<span class="built_in">test</span> /bin/bash</span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>这时，你如果执行 “ls /“，就会看到，它返回的都是 $HOME/test 目录下面的内容，而不是宿主机的内容。</p>
</li>
</ul>
</li>
<li><p>实际上，Mount Namespace 正是基于对 chroot 的不断改良才被发明出来的，它也是 Linux 操作系统里的第一个 Namespace。</p>
</li>
<li><p>我们一般会在这个容器的根目录下挂载一个完整操作系统的文件系统，比如 Ubuntu16.04 的 ISO。这样，在容器启动之后，我们在容器里通过执行 “ls /“ 查看根目录下的内容，就是 Ubuntu 16.04 的所有目录和文件。</p>
</li>
<li><p>而这个挂载在容器根目录上、用来为容器进程提供隔离后执行环境的文件系统，就是所谓的“容器镜像”。它还有一个更为专业的名字，叫作：rootfs（根文件系统）。</p>
</li>
</ul>
<p>对 Docker 项目来说，它最核心的原理实际上就是为待创建的用户进程：</p>
<ul>
<li>启用 Linux Namespace 配置；</li>
<li>设置指定的 Cgroups 参数；</li>
<li>切换进程的根目录（Change Root）。</li>
<li>不过，Docker 项目在最后一步的切换上会优先使用 pivot_root 系统调用，如果系统不支持，才会使用 chroot。</li>
</ul>
<p>需要明确的是，rootfs 只是一个操作系统所包含的文件、配置和目录，并不包括操作系统内核。在 Linux 操作系统中，这两部分是分开存放的，操作系统只有在开机启动时才会加载指定版本的内核镜像。</p>
<ul>
<li>/usr/src/一般用于存放内核源代码</li>
<li>/boot一般用于存放可引导内核</li>
<li>/usr/lib/modules/kernel/存放内核内置的已编译好的驱动程序</li>
<li>所以说，rootfs 只包括了操作系统的“躯壳”，并没有包括操作系统的“灵魂”。</li>
</ul>
<h3 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h3><p>容器相比于虚拟机的主要缺陷之一：毕竟后者不仅有模拟出来的硬件机器充当沙盒，而且每个沙盒里还运行着一个完整的 Guest OS 给应用随便折腾。</p>
<ul>
<li>由于 rootfs 里打包的不只是应用，而是整个操作系统的文件和目录，也就意味着，应用以及它运行所需要的所有依赖，都被封装在了一起。</li>
<li>对一个应用来说，操作系统本身才是它运行所需要的最完整的“依赖库”。</li>
</ul>
<h3 id="联合文件系统（Union-File-System）"><a href="#联合文件系统（Union-File-System）" class="headerlink" title="联合文件系统（Union File System）"></a>联合文件系统（Union File System）</h3><pre><code>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> tree</span></span><br><span class="line">.</span><br><span class="line">├── A</span><br><span class="line">│  ├── a</span><br><span class="line">│  └── x</span><br><span class="line">└── B</span><br><span class="line">├── b</span><br><span class="line">└── x</span><br><span class="line"><span class="meta">$</span><span class="bash"> mkdir C</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> mount -t aufs -o <span class="built_in">dirs</span>=./A:./B none ./C</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> tree ./C</span></span><br><span class="line">./C</span><br><span class="line">├── a</span><br><span class="line">├── b</span><br><span class="line">└── x</span><br></pre></td></tr></table></figure>
</code></pre>
<p>层的概念就是在联合操作系统中</p>
<ul>
<li>镜像的层都放置在 /var/lib/docker/image/overlay2 目录下，然后被联合挂载在 /var/lib/docker/aufs/mnt 里面。</li>
<li>在/sys/fs/aufs 有个si=972c6d361e6b32ba。</li>
<li>然后使用这个 ID，你就可以在 /sys/fs/aufs 下查看被联合挂载在一起的各个层的信息：</li>
</ul>
<h3 id="分层"><a href="#分层" class="headerlink" title="分层"></a>分层</h3><ul>
<li>第一部分，只读层。<ul>
<li>它是这个容器的 rootfs 最下面的五层，对应的正是 ubuntu:latest 镜像的五层。可以看到，它们的挂载方式都是只读的（ro+wh，即 readonly+whiteout，至于什么是 whiteout，我下面马上会讲到）。</li>
<li>为了实现这样的删除操作，AuFS 会在可读写层创建一个 whiteout 文件，把只读层里的文件“遮挡”起来。</li>
</ul>
</li>
<li>第二部分，可读写层。<ul>
<li>它是这个容器的 rootfs 最上面的一层（6e3be5d2ecccae7cc），它的挂载方式为：rw，即 read write。在没有写入文件之前，这个目录是空的。而一旦在容器里做了写操作，你修改产生的内容就会以增量的方式出现在这个层中。</li>
</ul>
</li>
<li>第三部分，Init 层。<ul>
<li>它是一个以“-init”结尾的层，夹在只读层和读写层之间。Init 层是 Docker 项目单独生成的一个内部层，专门用来存放 /etc/hosts、/etc/resolv.conf 等信息。</li>
</ul>
</li>
</ul>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ul>
<li>rootfs。它只是一个操作系统的所有文件和目录，并不包含内核，最多也就几百兆。而相比之下，传统虚拟机的镜像大多是一个磁盘的“快照”，磁盘有多大，镜像就至少有多大。</li>
<li>通过结合使用 Mount Namespace 和 rootfs，容器就能够为进程构建出一个完善的文件系统隔离环境。当然，这个功能的实现还必须感谢 chroot 和 pivot_root 这两个系统调用切换进程根目录的能力。</li>
<li>而在 rootfs 的基础上，Docker 公司创新性地提出了使用多个增量 rootfs 联合挂载一个完整 rootfs 的方案，这就是容器镜像中“层”的概念。</li>
<li>通过“分层镜像”的设计，以 Docker 镜像为核心，来自不同公司、不同团队的技术人员被紧密地联系在了一起。而且，由于容器镜像的操作是增量式的，这样每次镜像拉取、推送的内容，比原本多个完整的操作系统的大小要小得多；而共享层的存在，可以使得所有这些容器镜像需要的总空间，也比每个镜像的总和要小。这样就使得基于容器镜像的团队协作，要比基于动则几个 GB 的虚拟机磁盘镜像的协作要敏捷得多。</li>
<li>更重要的是，一旦这个镜像被发布，那么你在全世界的任何一个地方下载这个镜像，得到的内容都完全一致，可以完全复现这个镜像制作者当初的完整环境。这，就是容器技术“强一致性”的重要体现。</li>
</ul>
<h2 id="制作"><a href="#制作" class="headerlink" title="制作"></a>制作</h2><ul>
<li><p>安装docker</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sSL https://get.daocloud.io/docker | sh</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>用 Docker 部署一个用 Python 编写的 Web 应用</p>
<ul>
<li><p>app.py</p>
<pre><code>    <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> flask <span class="keyword">import</span> Flask</span><br><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line">app = Flask(__name__)</span><br><span class="line"></span><br><span class="line"><span class="meta">@app.route(<span class="params"><span class="string">&#x27;/&#x27;</span></span>)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hello</span>():</span></span><br><span class="line">html = <span class="string">&quot;&lt;h3&gt;Hello &#123;name&#125;!&lt;/h3&gt;&quot;</span> \</span><br><span class="line">      <span class="string">&quot;&lt;b&gt;Hostname:&lt;/b&gt; &#123;hostname&#125;&lt;br/&gt;&quot;</span></span><br><span class="line"><span class="keyword">return</span> html.<span class="built_in">format</span>(name=os.getenv(<span class="string">&quot;NAME&quot;</span>, <span class="string">&quot;world&quot;</span>), hostname=socket.gethostname())</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">app.run(host=<span class="string">&#x27;0.0.0.0&#x27;</span>, port=<span class="number">80</span>)</span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>requirements</p>
<pre><code>    <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> cat requirements.txt Flask</span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>制作容器镜像</p>
<pre><code>    <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 使用官方提供的 Python 开发镜像作为基础镜像</span></span><br><span class="line">FROM python:2.7-slim</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将工作目录切换为 /app</span></span><br><span class="line">WORKDIR /app</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将当前目录下的所有内容复制到 /app 下</span></span><br><span class="line">ADD . /app</span><br><span class="line"><span class="meta">#</span><span class="bash"> 使用 pip 命令安装这个应用所需要的依赖</span></span><br><span class="line">RUN pip install --trusted-host pypi.python.org -r requirements.txt</span><br><span class="line"><span class="meta">#</span><span class="bash"> 允许外界访问容器的 80 端口</span></span><br><span class="line">EXPOSE 80</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置环境变量</span></span><br><span class="line">ENV NAME World</span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置容器进程为：python app.py，即：这个 Python 应用的启动命令</span></span><br><span class="line">CMD [&quot;python&quot;, &quot;app.py&quot;]</span><br></pre></td></tr></table></figure>
</code></pre>
<ul>
<li><p>这里，app.py 的实际路径是 /app/app.py。所以，CMD [“python”, “app.py”] 等价于 “docker run python app.py”。</p>
</li>
<li><p>另外，在使用 Dockerfile 时，你可能还会看到一个叫作 ENTRYPOINT 的原语。实际上，它和 CMD 都是 Docker 容器进程启动所必需的参数，完整执行格式是：“ENTRYPOINT CMD”。</p>
</li>
<li><p>默认情况下，Docker 会为你提供一个隐含的 ENTRYPOINT，即：/bin/sh -c。所以，在不指定 ENTRYPOINT 时，比如在我们这个例子里，实际上运行在容器里的完整进程是：/bin/sh -c “python app.py”，即 CMD 的内容就是 ENTRYPOINT 的参数。</p>
</li>
<li><p>Dockerfile 里的原语并不都是指对容器内部的操作。就比如 ADD，它指的是把当前目录（即 Dockerfile 所在的目录）里的文件，复制到指定容器内的目录当中。</p>
</li>
<li><p>读懂这个 Dockerfile 之后，我再把上述内容，保存到当前目录里一个名叫“Dockerfile”的文件中：$ ls  Dockerfile  app.py   requirements.txt</p>
</li>
<li><p>接下来，我就可以让 Docker 制作这个镜像了，在当前目录执行：$ docker build -t helloworld .</p>
</li>
<li><p>其中，-t 的作用是给这个镜像加一个 Tag，即：起一个好听的名字。docker build 会自动加载当前目录下的 Dockerfile 文件，然后按照顺序，执行文件中的原语。</p>
</li>
<li><p>需要注意的是，Dockerfile 中的每个原语执行后，都会生成一个对应的镜像层。即使原语本身并没有明显地修改文件的操作（比如，ENV 原语），它对应的层也会存在。只不过在外界看来，这个层是空的。</p>
</li>
<li><p>接下来，我使用这个镜像，通过 docker run 命令启动容器：$ docker run -p 4000:80 helloworld</p>
</li>
<li><p>在这一句命令中，镜像名 helloworld 后面，我什么都不用写，因为在 Dockerfile 中已经指定了 CMD。</p>
</li>
<li><p>我已经通过 -p 4000:80 告诉了 Docker，请把容器内的 80 端口映射在宿主机的 4000 端口上。</p>
</li>
<li><p>用 docker tag 命令给容器镜像起一个完整的名字：$ docker tag helloworld geektime/helloworld:v1</p>
</li>
<li><p>然后，我执行 docker push：$ docker push geektime/helloworld:v1</p>
</li>
<li><p>我还可以使用 docker commit 指令，把一个正在运行的容器，直接提交为一个镜像。</p>
</li>
<li><p>$ docker push geektime/helloworld:v2</p>
<pre><code>  <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker <span class="built_in">exec</span> -it 4ddf4638572d /bin/sh</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在容器内部新建了一个文件</span></span><br><span class="line">root@4ddf4638572d:/app# touch test.txt</span><br><span class="line">root@4ddf4638572d:/app# exit</span><br><span class="line"><span class="meta">#</span><span class="bash"> 将这个新建的文件提交到镜像中保存</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> docker commit 4ddf4638572d geektime/helloworld:v2</span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="docker-exec-是怎么做到进入容器里的呢"><a href="#docker-exec-是怎么做到进入容器里的呢" class="headerlink" title="docker exec 是怎么做到进入容器里的呢"></a>docker exec 是怎么做到进入容器里的呢</h3><ul>
<li><p>实际上，Linux Namespace 创建的隔离空间虽然看不见摸不着，但一个进程的 Namespace 信息在宿主机上是确确实实存在的，并且是以一个文件的方式存在。</p>
</li>
<li><p>通过如下指令，你可以看到当前正在运行的 Docker 容器的进程号（PID）是 25686：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> docker inspect --format <span class="string">&#x27;&#123;&#123; .State.Pid &#125;&#125;&#x27;</span>  4ddf4638572d</span></span><br></pre></td></tr></table></figure>
</code></pre>
</li>
<li><p>这时，你可以通过查看宿主机的 proc 文件，看到这个 25686 进程的所有 Namespace 对应的文件 $ ls -l  /proc/25686/ns</p>
</li>
<li><p>这也就意味着：一个进程，可以选择加入到某个进程已有的 Namespace 当中，从而达到“进入”这个进程所在容器的目的，这正是 docker exec 的实现原理。</p>
</li>
<li><p>而这个操作所依赖的，乃是一个名叫 setns() 的 Linux 系统调用。</p>
</li>
<li><p>$ docker run -it –net container:4ddf4638572d busybox ifconfig  共享network namespace</p>
</li>
<li><p>而如果我指定–net=host，就意味着这个容器不会为进程启用 Network Namespace。共享主机网络</p>
</li>
</ul>
<h3 id="docker-commit"><a href="#docker-commit" class="headerlink" title="docker commit"></a>docker commit</h3><ul>
<li>docker commit，实际上就是在容器运行起来后，把最上层的“可读写层”，加上原先容器镜像的只读层，打包组成了一个新的镜像。当然，下面这些只读层在宿主机上是共享的，不会占用额外的空间。</li>
<li>而由于使用了联合文件系统，你在容器里对镜像 rootfs 所做的任何修改，都会被操作系统先复制到这个可读写层，然后再修改。这就是所谓的：Copy-on-Write。</li>
<li>而正如前所说，Init 层的存在，就是为了避免你执行 docker commit 时，把 Docker 自己对 /etc/hosts 等文件做的修改，也一起提交掉。</li>
</ul>
<h3 id="Volume（数据卷）"><a href="#Volume（数据卷）" class="headerlink" title="Volume（数据卷）"></a>Volume（数据卷）</h3><ul>
<li>Volume 机制，允许你将宿主机上指定的目录或者文件，挂载到容器里面进行读取和修改操作。</li>
<li>宿主机文件映射到容器中 $ docker run -v /test …   $ docker run -v /home:/test …<ul>
<li>在第一种情况下，由于你并没有显示声明宿主机目录，那么 Docker 就会默认在宿主机上创建一个临时目录 /var/lib/docker/volumes/[VOLUME_ID]/_data，然后把它挂载到容器的 /test 目录上。</li>
<li>而在第二种情况下，Docker 就直接把宿主机的 /home 目录挂载到容器的 /test 目录上。</li>
</ul>
</li>
</ul>
<h3 id="总结2"><a href="#总结2" class="headerlink" title="总结2"></a>总结2</h3><img src="/2021/06/30/3-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E5%AE%B9%E5%99%A8%E9%95%9C%E5%83%8F/%E5%AE%B9%E5%99%A8%E6%80%BB%E8%A7%88.png" class="">

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/29/2-%E9%99%90%E5%88%B6%E4%B8%8E%E9%9A%94%E7%A6%BB/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/29/2-%E9%99%90%E5%88%B6%E4%B8%8E%E9%9A%94%E7%A6%BB/" class="post-title-link" itemprop="url">2.限制与隔离</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-29 21:42:37" itemprop="dateCreated datePublished" datetime="2021-06-29T21:42:37+08:00">2021-06-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-06-30 21:08:13" itemprop="dateModified" datetime="2021-06-30T21:08:13+08:00">2021-06-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h2><p>一旦“程序”被执行起来，它就从磁盘上的二进制文件，变成了计算机内存中的数据、寄存器里的值、堆栈中的指令、被打开的文件，以及各种设备的状态信息的一个集合。</p>
<h2 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h2><p>Cgroups 技术是用来制造约束的主要手段，而Namespace 技术则是用来修改进程视图的主要方法。</p>
<h3 id="隔离"><a href="#隔离" class="headerlink" title="隔离"></a>隔离</h3><ul>
<li>通过int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL) 系统调用，创建进程号为“1”的独立隔离进程空间，达到隔离的作用</li>
<li>除了PID Namespace，Linux 操作系统还提供了 Mount、UTS、IPC、Network 和 User 这些 Namespace，用来对各种不同的进程上下文进行“障眼法”操作。<ul>
<li>Mount Namespace，用于让被隔离进程只看到当前 Namespace 里的挂载点信息；Network Namespace，用于让被隔离进程看到当前 Namespace 里的网络设备和配置。</li>
</ul>
</li>
</ul>
<p>以上就是 Linux 容器最基本的实现原理了。</p>
<p>Docker 容器这个听起来玄而又玄的概念，实际上是在创建容器进程时，指定了这个进程所需要启用的一组 Namespace 参数。这样，容器就只能“看”到当前 Namespace 所限定的资源、文件、设备、状态，或者配置。而对于宿主机以及其他不相关的程序，它就完全看不到了。</p>
<p>所以说，容器，其实是一种特殊的进程而已。</p>
<p>使用虚拟化技术作为应用沙盒，就必须要由 Hypervisor 来负责创建虚拟机，这个虚拟机是真实存在的，并且它里面必须运行一个完整的 Guest OS 才能执行用户的应用进程。这就不可避免地带来了额外的资源消耗和占用。</p>
<p>隔离得不彻底</p>
<ul>
<li>首先，既然容器只是运行在宿主机上的一种特殊的进程，那么多个容器之间使用的就还是同一个宿主机的操作系统内核。<ul>
<li>尽管你可以在容器里通过 Mount Namespace 单独挂载其他不同版本的操作系统文件，比如 CentOS 或者 Ubuntu，但这并不能改变共享宿主机内核的事实。</li>
<li>这意味着，如果你要在 Windows 宿主机上运行 Linux 容器，或者在低版本的 Linux 宿主机上运行高版本的 Linux 容器，都是行不通的。</li>
</ul>
</li>
<li>其次，在 Linux 内核中，有很多资源和对象是不能被 Namespace 化的，最典型的例子就是：时间。<ul>
<li>如果你的容器中的程序使用 settimeofday(2) 系统调用修改了时间，整个宿主机的时间都会被随之修改，这显然不符合用户的预期。</li>
</ul>
</li>
</ul>
<h3 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h3><ul>
<li>Linux Cgroups 就是 Linux 内核中用来为进程设置资源限制的一个重要功能。</li>
<li>Linux Cgroups 的全称是 Linux Control Group。它最主要的作用，就是限制一个进程组能够使用的资源上限，包括 CPU、内存、磁盘、网络带宽等等。</li>
<li>Cgroups 还能够对进程进行优先级设置、审计，以及将进程挂起和恢复等操作。</li>
<li>在 Linux 中，Cgroups 给用户暴露出来的操作接口是文件系统，即它以文件和目录的方式组织在操作系统的 /sys/fs/cgroup 路径下</li>
</ul>
<p> /sys/fs/cgroup 下面有很多诸如 cpuset、cpu、 memory 这样的子目录，也叫子系统。</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> ls /sys/fs/cgroup/cpu</span></span><br><span class="line">cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release</span><br><span class="line">cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks</span><br></pre></td></tr></table></figure>

可以用来限制进程在长度为 cfs_period 的一段时间内，只能被分配到总量为 cfs_quota 的 CPU 时间。
</code></pre>
<p>使用：</p>
<pre><code><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">root@ubuntu:/sys/fs/cgroup/cpu$ mkdir container</span><br><span class="line">root@ubuntu:/sys/fs/cgroup/cpu$ ls container/</span><br><span class="line">cgroup.clone_children cpu.cfs_period_us cpu.rt_period_us  cpu.shares notify_on_release</span><br><span class="line">cgroup.procs      cpu.cfs_quota_us  cpu.rt_runtime_us cpu.stat  tasks</span><br></pre></td></tr></table></figure>
这个目录就称为一个“控制组”。你会发现，操作系统会在你新创建的 container 目录下，自动生成该子系统对应的资源限制文件。
$ echo 20000 &gt; /sys/fs/cgroup/cpu/container/cpu.cfs_quota_us
</code></pre>
<p>其他：</p>
<pre><code>blkio，为​​​块​​​设​​​备​​​设​​​定​​​I/O 限​​​制，一般用于磁盘等设备；
cpuset，为进程分配单独的 CPU 核和对应的内存节点；
memory，为进程设定内存使用的限制。
</code></pre>
<p>docker run -it –cpu-period=100000 –cpu-quota=20000 ubuntu /bin/bash  就是对资源使用的限制</p>
<p>Cgroups 对资源的限制能力也有很多不完善的地方，被提及最多的自然是 /proc 文件系统的问题。</p>
<ul>
<li>Linux 下的 /proc 目录存储的是记录当前内核运行状态的一系列特殊文件，用户可以通过访问这些文件，查看系统以及当前正在运行的进程的信息，比如 CPU 使用情况、内存占用率等，这些文件也是 top 指令查看系统信息的主要数据来源。</li>
<li>但是，你如果在容器里执行 top 指令，就会发现，它显示的信息居然是宿主机的 CPU 和内存数据，而不是当前容器的数据。</li>
<li>造成这个问题的原因就是，/proc 文件系统并不知道用户通过 Cgroups 给这个容器做了什么样的资源限制，即：/proc 文件系统不了解 Cgroups 限制的存在。</li>
<li>top 是从 /prof/stats 目录下获取数据，所以道理上来讲，容器不挂载宿主机的该目录就可以了。lxcfs就是来实现这个功能的，做法是把宿主机的 /var/lib/lxcfs/proc/memoinfo 文件挂载到Docker容器的/proc/meminfo位置后。容器中进程读取相应文件内容时，LXCFS的FUSE实现会从容器对应的Cgroup中读取正确的内存限制。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/29/1-%E5%8F%91%E5%B1%95%E5%8F%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/29/1-%E5%8F%91%E5%B1%95%E5%8F%B2/" class="post-title-link" itemprop="url">1.发展史</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-29 21:33:16" itemprop="dateCreated datePublished" datetime="2021-06-29T21:33:16+08:00">2021-06-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-06-30 08:10:35" itemprop="dateModified" datetime="2021-06-30T08:10:35+08:00">2021-06-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/docker/" itemprop="url" rel="index"><span itemprop="name">docker</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ul>
<li>2013~2014 年，以 Cloud Foundry 为代表的 PaaS 项目，逐渐完成了教育用户和开拓市场的艰巨任务，也正是在这个将概念逐渐落地的过程中，应用“打包”困难这个问题，成了整个后端技术圈子的一块心病。</li>
<li>2013年Docker 项目的出现，则为这个根本性的问题提供了一个近乎完美的解决方案。dotCloud 公司则在 2013 年底大胆改名为 Docker 公司。</li>
<li>Fig 项目之所以受欢迎，在于它在开发者面前第一次提出了“容器编排”（Container Orchestration）的概念，后Fig 项目被收购后改名为 Compose。</li>
<li>谷歌开源Kubernetes，成为受欢迎项目</li>
<li>容器技术的兴起源于 PaaS 技术的普及；</li>
<li>Docker 公司发布的 Docker 项目具有里程碑式的意义；</li>
<li>Docker 项目通过“容器镜像”，解决了应用打包这个根本性难题。</li>
<li>容器本身没有价值，有价值的是“容器编排”</li>
<li>最终以 Kubernetes 项目和 CNCF 社区的胜利而告终</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/29/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/29/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/" class="post-title-link" itemprop="url">常见问题</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-29 16:00:43" itemprop="dateCreated datePublished" datetime="2021-06-29T16:00:43+08:00">2021-06-29</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2023-02-04 10:33:07" itemprop="dateModified" datetime="2023-02-04T10:33:07+08:00">2023-02-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kafka/" itemprop="url" rel="index"><span itemprop="name">kafka</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <ol>
<li><p>Kafka 如何做到高吞吐、低延迟的呢？</p>
<p> 这里提下 Kafka 写数据的大致方式：先写操作系统的页缓存（Page Cache）,然后由操作系统自行决定何时刷到磁盘。</p>
<p> 因此 Kafka 达到高吞吐、低延迟的原因主要有以下 4 点：</p>
<ul>
<li>页缓存是在内存中分配的，所以消息写入的速度很快。</li>
<li>Kafka 不必和底层的文件系统进行交互，所有繁琐的 I/O 操作都由操作系统来处理。</li>
<li>Kafka 采用追加写的方式，避免了磁盘随机写操作。</li>
<li>使用以 sendfile 为代表的零拷贝技术提高了读取数据的效率。</li>
<li>PS: 使用页缓存而非堆内存还有一个好处，就是当 Kafka broker 的进程崩溃时，堆内存的数据会丢失，但是页缓存的数据依然存在，重启 Kafka broker 后可以继续提供服务。</li>
</ul>
</li>
<li><p>Kafka 的 producer 工作流程？</p>
 <img src="/2021/06/29/%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/%E7%94%9F%E4%BA%A7%E8%80%85%E5%86%99%E5%85%A5.png" class="">

<ul>
<li>封装为 ProducerRecord 实例</li>
<li>序列化</li>
<li>由 partitioner 确定具体分区</li>
<li>发送到内存缓冲区</li>
<li>由 producer 的一个专属 I/O 线程去取消息，并将其封装到一个批次 ，发送给对应分区的 kafka broker</li>
<li>leader 将消息写入本地 log</li>
<li>followers 从 leader pull 消息，写入本地 log 后 leader 发送 ACK</li>
<li>leader 收到所有 ISR 中的 replica 的 ACK 后，增加 HW（high watermark，最后 commit 的 offset） 并向 producer 发送 ACK</li>
</ul>
</li>
<li><p>Kafka 的 consumer 工作流程？</p>
<ul>
<li>连接 ZK 集群，拿到对应 topic 的 partition 信息和 partition 的 leader 的相关信息</li>
<li>连接到对应 leader 对应的 broker</li>
<li>consumer 将自己保存的 offset 发送给 leader</li>
<li>leader 根据 offset 等信息定位到 segment（索引文件和日志文件）</li>
<li>根据索引文件中的内容，定位到日志文件中该偏移量对应的开始位置读取相应长度的数据并返回给 consumer</li>
</ul>
</li>
<li><p>重要参数有哪些？</p>
<ul>
<li>acks<ul>
<li>acks = 0 : 不接收发送结果</li>
<li>acks = all 或者 -1: 表示发送消息时，不仅要写入本地日志，还要等待所有副本写入成功。</li>
<li>acks = 1: 写入本地日志即可，是上述二者的折衷方案，也是默认值。</li>
</ul>
</li>
<li>retries<ul>
<li>默认为 0，即不重试，立即失败。</li>
<li>一个大于 0 的值，表示重试次数。</li>
</ul>
</li>
<li>buffer.memory<ul>
<li>指定 producer 端用于缓存消息的缓冲区的大小，默认 32M；</li>
<li>适当提升该参数值，可以增加一定的吞吐量。</li>
</ul>
</li>
<li>batch.size<ul>
<li>producer 会将发送分区的多条数据封装在一个 batch 中进行发送，这里的参数指的就是 batch 的大小。</li>
<li>该参数值过小的话，会降低吞吐量，过大的话，会带来较大的内存压力。</li>
<li>默认为 16K，建议合理增加该值。</li>
</ul>
</li>
</ul>
</li>
<li><p>丢失数据的场景？</p>
<ul>
<li>consumer 端：不是严格意义的丢失，其实只是漏消费了。<ul>
<li>设置了 auto.commit.enable=true ，当 consumer fetch 了一些数据但还没有完全处理掉的时候，刚好到 commit interval 触发了提交 offset 操作，接着 consumer 挂掉。这时已经fetch的数据还没有处理完成但已经被commit掉，因此没有机会再次被处理，数据丢失。</li>
</ul>
</li>
<li>producer 端：<ul>
<li>I/O 线程发送消息之前，producer 崩溃， 则 producer 的内存缓冲区的数据将丢失。</li>
</ul>
</li>
</ul>
</li>
<li><p>producer 端丢失数据如何解决？</p>
<ul>
<li>同步发送，性能差，不推荐。</li>
<li>仍然异步发送，通过“无消息丢失配置”（来自胡夕的《Apache Kafka 实战》）极大降低丢失的可能性：<ul>
<li>block.on.buffer.full = true 尽管该参数在0.9.0.0已经被标记为“deprecated”，但鉴于它的含义非常直观，所以这里还是显式设置它为true，使得producer将一直等待缓冲区直至其变为可用。否则如果producer生产速度过快耗尽了缓冲区，producer将抛出异常</li>
<li>acks=all 很好理解，所有follower都响应了才认为消息提交成功，即”committed”</li>
<li>retries = MAX 无限重试，直到你意识到出现了问题:)</li>
<li>max.in.flight.requests.per.connection = 1 限制客户端在单个连接上能够发送的未响应请求的个数。设置此值是1表示kafka broker在响应请求之前client不能再向同一个broker发送请求。注意：设置此参数是为了避免消息乱序</li>
<li>使用KafkaProducer.send(record, callback)而不是send(record)方法 自定义回调逻辑处理消息发送失败</li>
<li>callback逻辑中最好显式关闭producer：close(0) 注意：设置此参数是为了避免消息乱序</li>
<li>unclean.leader.election.enable=false 关闭unclean leader选举，即不允许非ISR中的副本被选举为leader，以避免数据丢失</li>
<li>replication.factor &gt;= 3 这个完全是个人建议了，参考了Hadoop及业界通用的三备份原则</li>
<li>min.insync.replicas &gt; 1 消息至少要被写入到这么多副本才算成功，也是提升数据持久性的一个参数。与acks配合使用</li>
<li>保证replication.factor &gt; min.insync.replicas 如果两者相等，当一个副本挂掉了分区也就没法正常工作了。通常设置replication.factor = min.insync.replicas + 1即可</li>
</ul>
</li>
</ul>
</li>
<li><p>consumer 端丢失数据如何解决？</p>
<ul>
<li>enable.auto.commit=false 关闭自动提交位移，在消息被完整处理之后再手动提交位移</li>
</ul>
</li>
<li><p>重复数据的场景？</p>
<ul>
<li>网络抖动导致 producer 误以为发送错误，导致重试，从而产生重复数据，可以通过幂等性配置避免。</li>
</ul>
</li>
<li><p>分区策略（即生产消息时如何选择哪个具体的分区）？</p>
<ul>
<li>指定了 key ，相同的 key 会被发送到相同的分区，通过key计算哈希值，（采用MurmurHash2算法，具备高运算性能及低碰撞率）；</li>
<li>没有指定 key，通过轮询保证各个分区上的均匀分配。</li>
</ul>
</li>
<li><p>乱序的场景？</p>
<ul>
<li>消息重试发送</li>
</ul>
</li>
<li><p>乱序如何解决？</p>
<ul>
<li>参数配置 max.in.flight.requests.per.connection = 1 ，但同时会限制 producer 未响应请求的数量，即造成在 broker 响应之前，producer 无法再向该 broker 发送数据。</li>
</ul>
</li>
<li><p>如何选择 Partiton 的数量？</p>
<ul>
<li>在创建 Topic 的时候可以指定 Partiton 数量，也可以在创建完后手动修改。但 Partiton 数量只能增加不能减少。中途增加 Partiton 会导致各个 Partiton 之间数据量的不平等。</li>
<li>Partition 的数量直接决定了该 Topic 的并发处理能力。但也并不是越多越好。Partition 的数量对消息延迟性会产生影响。</li>
<li>一般建议选择 Broker Num * Consumer Num ，这样平均每个 Consumer 会同时读取 Broker 数目个 Partition ， 这些 Partition 压力可以平摊到每台 Broker 上。</li>
</ul>
</li>
<li><p>可重试的异常情况有哪些？</p>
<ul>
<li>分区的 leader 副本不可用，一般发生再 leader 换届选举时。</li>
<li>controller 当前不可用，一般是 controller 在经历新一轮的选举。</li>
<li>网络瞬时故障。</li>
</ul>
</li>
<li><p>controller 的职责有哪些？</p>
<p>在 kafka 集群中，某个 broker 会被选举承担特殊的角色，即控制器（controller），用于管理和协调 kafka 集群，具体职责如下：</p>
<ul>
<li>管理副本和分区的状态</li>
<li>更新集群元数据信息</li>
<li>创建、删除 topic</li>
<li>分区重分配</li>
<li>leader 副本选举</li>
<li>topic 分区扩展</li>
<li>broker 加入、退出集群</li>
<li>受控关闭</li>
<li>controller leader 选举</li>
</ul>
</li>
<li><p>leader 挂了会怎样？（leader failover）</p>
<ul>
<li>当 leader 挂了之后，controller 默认会从 ISR 中选择一个 replica 作为 leader 继续工作，条件是新 leader 必须有挂掉 leader 的所有数据。</li>
<li>如果为了系统的可用性，而容忍降低数据的一致性的话，可以将 unclean.leader.election.enable = true ，开启 kafka 的”脏 leader 选举”。当 ISR 中没有 replica，则会从 OSR 中选择一个 replica 作为 leader 继续响应请求，如此操作提高了 Kafka 的分区容忍度，但是数据一致性降低了。</li>
</ul>
</li>
<li><p>broker 挂了会怎样？（broker failover）</p>
<p>broker上面有很多 partition 和多个 leader 。因此至少需要处理如下内容：</p>
<ul>
<li>更新该 broker 上所有 follower 的状态</li>
<li>从新给 leader 在该 broker 上的 partition 选举 leader</li>
<li>选举完成后，要更新 partition 的状态，比如谁是 leader 等<br>kafka 集群启动后，所有的 broker 都会被 controller 监控，一旦有 broker 宕机，ZK 的监听机制会通知到 controller， controller 拿到挂掉 broker 中所有的 partition，以及它上面的存在的 leader，然后从 partition的 ISR 中选择一个 follower 作为 leader，更改 partition 的 follower 和 leader 状态。</li>
</ul>
</li>
<li><p>controller 挂了会怎样？（controller failover）</p>
<ul>
<li>由于每个 broker 都会在 zookeeper 的 “/controller” 节点注册 watcher，当 controller 宕机时 zookeeper 中的临时节点消失</li>
<li>所有存活的 broker 收到 fire 的通知，每个 broker 都尝试创建新的 controller path，只有一个竞选成功并当选为 controller。</li>
</ul>
</li>
<li><p>Zookeeper 为 Kafka 做了什么？</p>
<ul>
<li>管理 broker 与 consumer 的动态加入与离开。（Producer 不需要管理，随便一台计算机都可以作为Producer 向 Kakfa Broker 发消息）</li>
<li>触发负载均衡，当 broker 或 consumer 加入或离开时会触发负载均衡算法，使得一个 consumer group 内的多个 consumer 的消费负载平衡。（因为一个 comsumer 消费一个或多个partition，一个 partition 只能被一个 consumer 消费）</li>
<li>维护消费关系及每个 partition 的消费信息。</li>
</ul>
</li>
<li><p>Page Cache 带来的好处。</p>
<ul>
<li>Linux 总会把系统中还没被应用使用的内存挪来给 Page Cache，在命令行输入free，或者 cat /proc/meminfo ，“Cached”的部分就是 Page Cache。</li>
<li>Page Cache 中每个文件是一棵 Radix 树（又称 PAT 位树, 一种多叉搜索树），节点由 4k 大小的 Page 组成，可以通过文件的偏移量（如 0x1110001）快速定位到某个Page。</li>
<li>当写操作发生时，它只是将数据写入 Page Cache 中，并将该页置上 dirty 标志。</li>
<li>当读操作发生时，它会首先在 Page Cache 中查找，如果有就直接返回，没有的话就会从磁盘读取文件写入 Page Cache 再读取。</li>
<li>可见，只要生产者与消费者的速度相差不大，消费者会直接读取之前生产者写入Page Cache的数据，大家在内存里完成接力，根本没有磁盘访问。</li>
<li>而比起在内存中维护一份消息数据的传统做法，这既不会重复浪费一倍的内存，Page Cache 又不需要 GC （可以放心使用60G内存了），而且即使 Kafka 重启了，Page Cache 还依然在。</li>
</ul>
</li>
<li><p>我感觉kafka和es的原理其实差不多，并不能控制文件系统顺序写入，都是尽量只能保证对一个文件追加写入，由文件系统去优化这个部分。可能磁盘里面不是很干净的话（包含很多其他文件），顺序写入的效果就会差很多</p>
<ul>
<li>你理解的没问题，不过es的file都比较小，所以在访问多个数据项的时候更像是在随机读写</li>
<li>而一般kafka的segment files都比较大</li>
<li>还有一点是kafka用了zero copy，这一点也比es快的多。</li>
<li>es读取索引 用的数据还挺多的，比如.fdt .fdx .cfs还有很多类似_lock的东西</li>
<li>所以一般不认为es是sequential disk access</li>
</ul>
</li>
<li><p>kafka reassign 方案</p>
<ul>
<li>通过客户端获取kafka集群元数据信息。通过kafka-python SimpleClient对象获取元数据信息</li>
<li>拿到所有需要重新分配的分区信息。通过元数据信息拿到要被删除节点的node_id，最终的node_ids并筛选出元数据分区信息中副本包含该node_id的分区，即要被reassign的分区。数据格式为：{“str_replicas”:[“partition_num”,”replicas”,”topic_name”]}即{“[1,2]”:[0,[1,2],”test_topic”]}</li>
<li>对上述结果进行遍历，将str_replicas 类中的每一个分区中副本中轮询添加node_ids中的一个node，保证数据均匀</li>
</ul>
</li>
<li><p>客户端管理工具Offset Explorer ：<a target="_blank" rel="noopener" href="http://www.ibloger.net/article/3497.html">http://www.ibloger.net/article/3497.html</a></p>
</li>
<li><p>kafka连接zookeeper maxbuffer 问题，可以在KAFKA_OPTS中添加 “-Djute.maxbuffer=10485760” zookeeper通过java.env 文件 export JVMFLAGS=”-Xms512m -Xmx2048m -Djute.maxbuffer=10485760” 映射到/conf/java.env中即可</p>
</li>
<li><p>kafka leader为None  deleteall /brokers/topics/__consumer_offsets 停止kafka执行删除逻辑   或者删除 rmr /controller</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/27/8-%E5%8F%AF%E9%9D%A0%E6%80%A7%E6%8E%A2%E7%A9%B6/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/27/8-%E5%8F%AF%E9%9D%A0%E6%80%A7%E6%8E%A2%E7%A9%B6/" class="post-title-link" itemprop="url">8.可靠性探究</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-27 19:08:21" itemprop="dateCreated datePublished" datetime="2021-06-27T19:08:21+08:00">2021-06-27</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-06-29 21:22:13" itemprop="dateModified" datetime="2021-06-29T21:22:13+08:00">2021-06-29</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kafka/" itemprop="url" rel="index"><span itemprop="name">kafka</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="副本剖析"><a href="#副本剖析" class="headerlink" title="副本剖析"></a>副本剖析</h2><h3 id="回忆概念"><a href="#回忆概念" class="headerlink" title="回忆概念"></a>回忆概念</h3><ul>
<li>副本是相对于分区而言的，即副本是特定分区的副本。</li>
<li>一个分区中包含一个或多个副本，其中一个为leader副本，其余为follower副本，各个副本位于不同的broker节点中。只有leader副本对外提供服务，follower副本只负责数据同步。</li>
<li>分区中的所有副本统称为 AR，而ISR 是指与leader 副本保持同步状态的副本集合，当然leader副本本身也是这个集合中的一员。</li>
<li>LEO标识每个分区中最后一条消息的下一个位置，分区的每个副本都有自己的LEO，ISR中最小的LEO即为HW，俗称高水位，消费者只能拉取到HW之前的消息。</li>
<li>从生产者发出的一条消息首先会被写入分区的leader副本，不过还需要等待ISR集合中的所有 follower 副本都同步完之后才能被认为已经提交，之后才会更新分区的 HW，进而消费者可以消费到这条消息。</li>
</ul>
<h3 id="失效副本"><a href="#失效副本" class="headerlink" title="失效副本"></a>失效副本</h3><p>replica.lag.time.max.ms来抉择，当ISR集合中的一个follower副本滞后leader副本的时间超过此参数指定的值时则判定为同步失败，需要将此follower副本剔除出ISR集合，replica.lag.time.max.ms参数的默认值为10000。</p>


<ul>
<li>follower副本进程卡住，在一段时间内根本没有向leader副本发起同步请求，比如频繁的Full GC。</li>
<li>follower副本进程同步过慢，在一段时间内都无法追赶上leader副本，比如I/O开销过大。</li>
</ul>
<h3 id="ISR的伸缩"><a href="#ISR的伸缩" class="headerlink" title="ISR的伸缩"></a>ISR的伸缩</h3><h3 id="LEO与HW"><a href="#LEO与HW" class="headerlink" title="LEO与HW"></a>LEO与HW</h3><p>对于副本而言，还有两个概念：本地副本（Local Replica）和远程副本（Remote Replica），本地副本是指对应的Log分配在当前的broker节点上，远程副本是指对应的Log分配在其他的broker节点上。</p>
<p>整个消息追加的过程可以概括如下：</p>
<ul>
<li>生产者客户端发送消息至leader副本（副本1）中。</li>
<li>消息被追加到leader副本的本地日志，并且会更新日志的偏移量。</li>
<li>follower副本（副本2和副本3）向leader副本请求同步数据。</li>
<li>leader副本所在的服务器读取本地日志，并更新对应拉取的follower副本的信息。</li>
<li>leader副本所在的服务器将拉取结果返回给follower副本。</li>
<li>follower副本收到leader副本返回的拉取结果，将消息追加到本地日志中，并更新日志的偏移量信息。</li>
</ul>
<h3 id="为什么不支持读写分离"><a href="#为什么不支持读写分离" class="headerlink" title="为什么不支持读写分离"></a><strong>为什么不支持读写分离</strong></h3><p>可以实现，但是主写从读也有2个很明显的缺点：</p>
<ul>
<li>数据一致性问题。数据从主节点转到从节点必然会有一个延时的时间窗口，这个时间窗口会导致主从节点之间的数据不一致。某一时刻，在主节点和从节点中A数据的值都为X，之后将主节点中A的值修改为Y，那么在这个变更通知到从节点之前，应用读取从节点中的A数据的值并不为最新的Y，由此便产生了数据不一致的问题。</li>
<li>延时问题。类似Redis这种组件，数据从写入主节点到同步至从节点中的过程需要经历网络→主节点内存→网络→从节点内存这几个阶段，整个过程会耗费一定的时间。而在Kafka中，主从同步会比 Redis 更加耗时，它需要经历网络→主节点内存→主节点磁盘→网络→从节点内存→从节点磁盘这几个阶段。对延时敏感的应用而言，主写从读的功能并不太适用。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/26/7-%E6%B7%B1%E5%85%A5%E5%AE%A2%E6%88%B7%E7%AB%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/26/7-%E6%B7%B1%E5%85%A5%E5%AE%A2%E6%88%B7%E7%AB%AF/" class="post-title-link" itemprop="url">7.深入客户端</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-26 09:38:04" itemprop="dateCreated datePublished" datetime="2021-06-26T09:38:04+08:00">2021-06-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-06-27 21:44:56" itemprop="dateModified" datetime="2021-06-27T21:44:56+08:00">2021-06-27</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kafka/" itemprop="url" rel="index"><span itemprop="name">kafka</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="分区分配策略"><a href="#分区分配策略" class="headerlink" title="分区分配策略"></a>分区分配策略</h2><ul>
<li>Kafka提供了消费者客户端参数partition.assignment.strategy来设置消费者与订阅主题之间的分区分配策略。</li>
<li>默认情况下，此参数的值为 org.apache.kafka.clients.consumer.RangeAssignor，即采用RangeAssignor分配策略。</li>
<li>除此之外，Kafka还提供了另外两种分配策略：RoundRobinAssignor 和 StickyAssignor。</li>
<li>消费者客户端参数 partition.assignment.strategy可以配置多个分配策略，彼此之间以逗号分隔。</li>
</ul>
<h3 id="RangeAssignor分配策略"><a href="#RangeAssignor分配策略" class="headerlink" title="RangeAssignor分配策略"></a>RangeAssignor分配策略</h3><p>假设n=分区数/消费者数量，m=分区数%消费者数量，那么前m个消费者每个分配n+1个分区，后面的（消费者数量-m）个消费者每个分配n个分区。</p>
<p>假设消费组内有2个消费者C0和C1，都订阅了主题t0和t1，并且每个主题都有4个分区，那么订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t0p3、t1p0、t1p1、t1p2、t1p3。最终的分配结果为：</p>
<ul>
<li>消费者c0:t0p0、t0p1、t1p0、t1p1</li>
<li>消费者c1:t0p2、t0p3、t1p2、t1p3</li>
</ul>
<p>假设上面例子中2个主题都只有3个分区，那么订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为：</p>
<ul>
<li>消费者c0:t0p0、t0p1、t1p0、t1p1</li>
<li>消费者c1:t0p2、t1p2</li>
</ul>
<p>可以明显地看到这样的分配并不均匀，如果将类似的情形扩大，则有可能出现部分消费者过载的情况。</p>
<h3 id="RoundRobinAssignor分配策略"><a href="#RoundRobinAssignor分配策略" class="headerlink" title="RoundRobinAssignor分配策略"></a>RoundRobinAssignor分配策略</h3><ul>
<li>RoundRobinAssignor分配策略的原理是将消费组内所有消费者及消费者订阅的所有主题的分区按照字典序排序，然后通过轮询方式逐个将分区依次分配给每个消费者。</li>
<li>RoundRobinAssignor分配策略对应的 partition.assignment.strategy 参数值为 org.apache.kafka.clients.consumer.RoundRobinAssignor。</li>
</ul>
<p>假设消费组中有2个消费者C0和C1，都订阅了主题t0和t1，并且每个主题都有3个分区，那么订阅的所有分区可以标识为：t0p0、t0p1、t0p2、t1p0、t1p1、t1p2。最终的分配结果为：</p>
<ul>
<li>消费者c0:t0p0、t0p2、t1p1</li>
<li>消费者c1:t0p1、t1p0、t1p2</li>
</ul>
<p>当也会有缺点：假设消费组内有3个消费者（C0、C1和C2），它们共订阅了3个主题（t0、t1、t2），这3个主题分别有1、2、3个分区，即整个消费组订阅了t0p0、t1p0、t1p1、t2p0、t2p1、t2p2这6个分区。具体而言，消费者C0订阅的是主题t0，消费者C1订阅的是主题t0和t1，消费者C2订阅的是主题t0、t1和t2</p>
<ul>
<li>消费者c0:t0p0</li>
<li>消费者c1:t1p0</li>
<li>消费者c0:t1p1,t2p0,t2p1,t2p2</li>
<li>可以看到RoundRobinAssignor策略也不是十分完美，这样分配其实并不是最优解，因为完全可以将分区t1p1分配给消费者C1。</li>
</ul>
<h3 id="StickyAssignor分配策略"><a href="#StickyAssignor分配策略" class="headerlink" title="StickyAssignor分配策略"></a>StickyAssignor分配策略</h3><p>“sticky”这个单词可以翻译为“黏性的”，Kafka从0.11.x版本开始引入这种分配策略，它主要有两个目的</p>
<ul>
<li>分区的分配要尽可能均匀。</li>
<li>分区的分配尽可能与上次分配的保持相同。</li>
<li>当两者发生冲突时，第一个目标优先于第二个目标。鉴于这两个目标，StickyAssignor分配策略的具体实现要比RangeAssignor和RoundRobinAssignor这两种分配策略要复杂得多。我们举例来看一下StickyAssignor分配策略的实际效果。</li>
</ul>
<p>假设消费组内有3个消费者（C0、C1和C2），它们都订阅了4个主题（t0、t1、t2、t3），并且每个主题有2个分区。也就是说，整个消费组订阅了t0p0、t0p1、t1p0、t1p1、t2p0、t2p1、t3p0、t3p1这8个分区。最终的分配结果如下：</p>
<ul>
<li>消费者c0:t0p0,t1p1,t3p0</li>
<li>消费者c1:t0p1,t2p0,t3p1</li>
<li>消费者c2:t1p0,t2p1</li>
</ul>
<p>这样初看上去似乎与采用RoundRobinAssignor分配策略所分配的结果相同，但事实是否真的如此呢？再假设此时消费者 C1 脱离了消费组，那么消费组就会执行再均衡操作，进而消费分区会重新分配。如果采用RoundRobinAssignor分配策略，那么此时的分配结果如下:</p>
<ul>
<li>消费者c0:t0p0,t1p0,t2p0,t3p0</li>
<li>消费者c2:t0p1,t1p1,t2p1,t3p1</li>
</ul>
<p>如分配结果所示，RoundRobinAssignor分配策略会按照消费者C0和C2进行重新轮询分配。如果此时使用的是StickyAssignor分配策略，那么分配结果为：</p>
<ul>
<li>消费者c0:t0p0,t1p1,t3p0,t3p0</li>
<li>消费者c2:t1p0,t2p1,t0p1,t3p1</li>
</ul>
<p>使用StickyAssignor分配策略的一个优点就是可以使分区重分配具备“黏性”，减少不必要的分区移动（即一个分区剥离之前的消费者，转而分配给另一个新的消费者）。</p>
<h3 id="自定义分区分配策略"><a href="#自定义分区分配策略" class="headerlink" title="自定义分区分配策略"></a>自定义分区分配策略</h3><h2 id="消费者协调器和组协调器"><a href="#消费者协调器和组协调器" class="headerlink" title="消费者协调器和组协调器"></a>消费者协调器和组协调器</h2><p>多个消费者之间的分区分配是需要协同的，那么这个协同的过程又是怎样的呢？这一切都是交由消费者协调器（ConsumerCoordinator）和组协调器（GroupCoordinator）来完成的，它们之间使用一套组协调协议进行交互。</p>
<h3 id="再均衡的原理"><a href="#再均衡的原理" class="headerlink" title="再均衡的原理"></a>再均衡的原理</h3><p>新版的消费者客户端对此进行了重新设计，将全部消费组分成多个子集，每个消费组的子集在服务端对应一个GroupCoordinator对其进行管理，GroupCoordinator是Kafka服务端中用于管理消费组的组件。而消费者客户端中的ConsumerCoordinator组件负责与GroupCoordinator进行交互。</p>
<p>ConsumerCoordinator与GroupCoordinator之间最重要的职责就是负责执行消费者再均衡的操作，包括前面提及的分区分配的工作也是在再均衡期间完成的。就目前而言，一共有如下几种情形会触发再均衡的操作</p>
<ul>
<li>有新的消费者加入消费组</li>
<li>有消费者宕机下线。消费者并不一定需要真正下线，例如遇到长时间的 GC、网络延迟导致消费者长时间未向GroupCoordinator发送心跳等情况时，GroupCoordinator会认为消费者已经下线。</li>
<li>有消费者主动退出消费组（发送 LeaveGroupRequest 请求）。比如客户端调用了unsubscrible（）方法取消对某些主题的订阅。</li>
<li>消费组所对应的GroupCoorinator节点发生了变更。</li>
<li>消费组内所订阅的任一主题或者主题的分区数量发生变化。</li>
</ul>
<p>当有消费者加入消费组时，消费者、消费组及组协调器之间会经历一下几个阶段。</p>
<ul>
<li>第一阶段（FIND_COORDINATOR）<ul>
<li>消费者需要确定它所属的消费组对应的GroupCoordinator所在的broker，并创建与该broker相互通信的网络连接。</li>
<li>如果消费者已经保存了与消费组对应的 GroupCoordinator 节点的信息，并且与它之间的网络连接是正常的，那么就可以进入第二阶段。</li>
<li>否则，就需要向集群中的某个节点发送FindCoordinatorRequest请求来查找对应的GroupCoordinator，这里的“某个节点”并非是集群中的任意节点，而是负载最小的节点，即2.2.2节中的leastLoadedNode。</li>
<li>Kafka 在收到 FindCoordinatorRequest 请求之后，会根据 coordinator_key（也就是groupId）查找对应的GroupCoordinator节点，如果找到对应的GroupCoordinator则会返回其相对应的node_id、host和port信息。</li>
<li>具体查找GroupCoordinator的方式是先根据消费组groupId的哈希值计算__consumer_offsets中的分区编号</li>
<li>中 groupId.hashCode 就是使用 Java 中 String 类的 hashCode（）方法获得的，groupMetadataTopicPartitionCount 为主题__consumer_offsets 的分区个数，这个可以通过broker端参数offsets.topic.num.partitions来配置，默认值为50。</li>
<li>找到对应的__consumer_offsets中的分区之后，再寻找此分区leader副本所在的broker节点，该broker节点即为这个groupId所对应的GroupCoordinator节点。</li>
<li>消费者groupId最终的分区分配方案及组内消费者所提交的消费位移信息都会发送给此分区leader副本所在的broker节点，让此broker节点既扮演GroupCoordinator的角色，又扮演保存分区分配方案和组内消费者位移的角色，这样可以省去很多不必要的中间轮转所带来的开销。</li>
</ul>
</li>
<li>第二阶段（JOIN_GROUP）<ul>
<li>在成功找到消费组所对应的 GroupCoordinator 之后就进入加入消费组的阶段，在此阶段的消费者会向GroupCoordinator发送JoinGroupRequest请求，并处理响应。</li>
<li>如果是原有的消费者重新加入消费组，那么在真正发送JoinGroupRequest 请求之前还要执行一些准备工作<ul>
<li>如果消费端参数enable.auto.commit设置为true（默认值也为true），即开启自动提交位移功能，那么在请求加入消费组之前需要向 GroupCoordinator 提交消费位移。这个过程是阻塞执行的，要么成功提交消费位移，要么超时。</li>
<li>如果消费者添加了自定义的再均衡监听器（ConsumerRebalanceListener），那么此时会调用onPartitionsRevoked（）方法在重新加入消费组之前实施自定义的规则逻辑，比如清除一些状态，或者提交消费位移等。</li>
<li>因为是重新加入消费组，之前与GroupCoordinator节点之间的心跳检测也就不需要了，所以在成功地重新加入消费组之前需要禁止心跳检测的运作。</li>
</ul>
</li>
<li>选举消费组的leader<ul>
<li>GroupCoordinator需要为消费组内的消费者选举出一个消费组的leader，这个选举的算法也很简单，分两种情况分析。如果消费组内还没有 leader，那么第一个加入消费组的消费者即为消费组的 leader。很随机</li>
</ul>
</li>
<li>选举分区分配策略<ul>
<li>收集各个消费者支持的所有分配策略，组成候选集candidates。</li>
<li>每个消费者从候选集candidates中找出第一个自身支持的策略，为这个策略投上一票。</li>
<li>计算候选集中各个策略的选票数，选票数最多的策略即为当前消费组的分配策略。</li>
</ul>
</li>
<li>在此之后，Kafka服务端就要发送JoinGroupResponse响应给各个消费者，leader消费者和其他普通消费者收到的响应内容并不相同，</li>
<li><img src="/2021/06/26/7-%E6%B7%B1%E5%85%A5%E5%AE%A2%E6%88%B7%E7%AB%AF/%E5%AE%A2%E6%88%B7%E7%AB%AF%E5%8F%91%E9%80%81%E5%8A%A0%E5%85%A5%E7%BB%84%E8%AF%B7%E6%B1%82.png" class=""></li>
<li></li>
</ul>
</li>
<li>第三阶段（SYNC_GROUP）<ul>
<li>leader 消费者根据在第二阶段中选举出来的分区分配策略来实施具体的分区分配，在此之后需要将分配的方案同步给各个消费者，此时leader消费者并不是直接和其余的普通消费者同步分配方案，而是通过 GroupCoordinator 这个“中间人”来负责转发同步分配方案的。</li>
<li><img src="/2021/06/26/7-%E6%B7%B1%E5%85%A5%E5%AE%A2%E6%88%B7%E7%AB%AF/%E5%88%86%E5%8C%BA%E5%88%86%E9%85%8D.png" class=""></li>
<li>服务端在收到消费者发送的SyncGroupRequest请求之后会交由GroupCoordinator来负责具体的逻辑处理。</li>
<li>GroupCoordinator同样会先对SyncGroupRequest请求做合法性校验，在此之后会将从 leader 消费者发送过来的分配方案提取出来，连同整个消费组的元数据信息一起存入Kafka的__consumer_offsets主题中，最后发送响应给各个消费者以提供给各个消费者各自所属的分配方案。</li>
<li>当消费者收到所属的分配方案之后会调用PartitionAssignor中的onAssignment（）方法。随后再调用ConsumerRebalanceListener中的OnPartitionAssigned（）方法。之后开启心跳任务，消费者定期向服务端的GroupCoordinator发送HeartbeatRequest来确定彼此在线。</li>
<li>消费组元数据信息</li>
</ul>
</li>
<li>第四阶段（HEARTBEAT）<ul>
<li>进入这个阶段之后，消费组中的所有消费者就会处于正常工作状态。</li>
<li>由参数heartbeat.interval.ms指定，默认值为3000，即3秒，</li>
</ul>
</li>
</ul>
<h2 id="consumer-offsets剖析"><a href="#consumer-offsets剖析" class="headerlink" title="__consumer_offsets剖析"></a>__consumer_offsets剖析</h2><h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><p>At Least Once + 幂等性 = Exactly Once，但是 PID 重启就会变化，同时不同的 Partition 也具有不同主键，所以幂等性无法保证跨分区跨会话的 Exactly Once。<br>事务</p>
<p>事务可以保证 Kafka 在 Exactly Once 语义的基础上，生产和消费可以跨分区和会话，要么全部成功，要么全部失败。</p>
<ul>
<li>为了实现跨分区跨会话的事务，需要引入一个全局唯一的 Transaction ID，并将 Producer获得的PID和Transaction ID绑定。这样当Producer重启后就可以通过正在进行的TransactionID 获得原来的 PID。</li>
<li>为了管理 Transaction，Kafka 引入了一个新的组件 Transaction Coordinator。Producer 就是通过和 Transaction Coordinator 交互获得 Transaction ID 对应的任务状态。TransactionCoordinator 还负责将事务所有写入 Kafka 的一个内部 Topic，这样即使整个服务重启，由于事务状态得到保存，进行中的事务状态可以得到恢复，从而继续进行。</li>
</ul>
<h3 id="消息传输保障"><a href="#消息传输保障" class="headerlink" title="消息传输保障"></a>消息传输保障</h3><p>一般而言，消息中间件的消息传输保障有3个层级，分别如下。</p>
<ol>
<li>at most once：至多一次。消息可能会丢失，但绝对不会重复传输。</li>
<li>at least once：最少一次。消息绝不会丢失，但可能会重复传输。</li>
<li>exactly once：恰好一次。每条消息肯定会被传输一次且仅传输一次。</li>
</ol>
<p>生产者提供的消息保证是最少一次，对消费者而言，消费者处理消息和提交消费位移的顺序在很大程度上决定了消费者提供哪一种消息传输保障。</p>
<ul>
<li>如果消费者在拉取完消息之后，应用逻辑先处理消息后提交消费位移，那么在消息处理之后且在位移提交之前消费者宕机了，待它重新上线之后会从上一次位移提交的位置拉取，这样就出现了重复消费，</li>
<li>如果消费者在拉完消息之后，应用逻辑先提交消费位移后进行消息处理，那么在位移提交之后且在消息处理完成之前消费者宕机了，待它重新上线之后，会从已经提交的位移处开始重新消费，但之前尚有部分消息未进行消费，如此就会发生消息丢失，此时就对应at most once。</li>
</ul>
<p>Kafka从0.11.0.0版本开始引入了<strong>幂等和事务</strong>这两个特性，以此来实现EOS（exactly once semantics，精确一次处理语义）。</p>
<h3 id="幂等"><a href="#幂等" class="headerlink" title="幂等"></a>幂等</h3><ul>
<li>所谓的幂等，简单地说就是对接口的多次调用所产生的结果和调用一次是一致的。</li>
<li>开启幂等性功能的方式很简单，只需要显式地将生产者客户端参数enable.idempotence设置为true即可</li>
<li>不过如果要确保幂等性功能正常，还需要确保生产者客户端的 retries、acks、max.in.flight.requests.per.connection这几个参数不被配置错。实际上在使用幂等性功能的时候，用户完全可以不用配置（也不建议配置）这几个参数。<ul>
<li>如果用户显式地指定了 retries 参数，那么这个参数的值必须大于 0，否则会报出ConfigException：</li>
<li>如果用户没有显式地指定 retries 参数，那么 KafkaProducer 会将它置为 Integer.MAX_VALUE。</li>
<li>同时还需要保证max.in.flight.requests.per.connection参数的值不能大于5（这个参数的值默认为5，在2.2.1节中有相关的介绍），否则也会报出ConfigException</li>
<li>如果用户还显式地指定了 acks 参数，那么还需要保证这个参数的值为-1（all），如果不为-1（这个参数的值默认为1，2.3节中有相关的介绍），那么也会报出ConfigException：</li>
</ul>
</li>
<li>为了实现生产者的幂等性，Kafka为此引入了producer id（以下简称PID）和序列号（sequence number）这两个概念</li>
<li>对于每个PID，消息发送到的每一个分区都有对应的序列号，这些序列号从0开始单调递增。生产者每发送一条消息就会将＜PID，分区＞对应的序列号的值加1</li>
<li>broker端会在内存中为每一对＜PID，分区＞维护一个序列号。对于收到的每一条消息，只有当它的序列号的值（SN_new）比broker端中维护的对应的序列号的值（SN_old）大1（即SN_new=SN_old+1）时，broker才会接收它。如果SN_new＜SN_old+1，那么说明消息被重复写入，broker可以直接将其丢弃。如果SN_new＞SN_old+1，那么说明中间有数据尚未写入，出现了乱序，暗示可能有消息丢失，对应的生产者会抛出OutOfOrderSequenceException，这个异常是一个严重的异常，后续的诸如 send（）、beginTransaction（）、commitTransaction（）等方法的调用都会抛出IllegalStateException的异常。</li>
<li>引入序列号来实现幂等也只是针对每一对＜PID，分区＞而言的，也就是说，Kafka的幂等只能保证单个生产者会话（session）中单分区的幂等。</li>
</ul>
<h3 id="事务——"><a href="#事务——" class="headerlink" title="事务——"></a>事务——</h3><p>幂等性并不能跨多个分区运作，而事务[1]可以弥补这个缺陷。事务可以保证对多个分区写入操作的原子性。</p>
<ul>
<li>对流式应用（Stream Processing Applications）而言，一个典型的应用模式为“consume-transform-produce”。在这种模式下消费和生产并存，</li>
<li>Kafka 中的事务可以使应用程序将消费消息、生产消息、提交消费位移当作原子操作来处理，同时成功或失败，即使该生产或消费会跨多个分区。</li>
<li>为了实现事务，应用程序必须提供唯一的 transactionalId，这个 transactionalId通过客户端参数transactional.id来显式设置</li>
<li>事务要求生产者开启幂等特性，因此通过将transactional.id参数设置为非空从而开启事务特性的同时需要将 enable.idempotence 设置为 true</li>
<li>transactionalId与PID一一对应，两者之间所不同的是transactionalId由用户显式设置，而PID是由Kafka内部分配的。</li>
<li>为了保证新的生产者启动后具有相同transactionalId的旧生产者能够立即失效，每个生产者通过transactionalId获取PID的同时，还会获取一个单调递增的producer epoch</li>
<li>如果使用同一个transactionalId开启两个生产者，那么前一个开启的生产者会报出如下的错误</li>
</ul>
<p>从生产者的角度分析，通过事务，Kafka 可以保证跨生产者会话的消息幂等发送，以及跨生产者会话的事务恢复。</p>
<ul>
<li>前者表示具有相同 transactionalId 的新生产者实例被创建且工作的时候，旧的且拥有相同transactionalId的生产者实例将不再工作。</li>
<li>后者指当某个生产者实例宕机后，新的生产者实例可以保证任何未完成的旧事务要么被提交（Commit），要么被中止（Abort），如此可以使新的生产者实例从一个正常的状态开始工作。</li>
</ul>
<p>而从消费者的角度分析，事务能保证的语义相对偏弱。出于以下原因，Kafka 并不能保证已提交的事务中的所有消息都能够被消费：</p>
<ul>
<li>对采用日志压缩策略的主题而言，事务中的某些消息有可能被清理（相同key的消息，后写入的消息会覆盖前面写入的消息）</li>
<li>事务中消息可能分布在同一个分区的多个日志分段（LogSegment）中，当老的日志分段被删除时，对应的消息可能会丢失。</li>
<li>消费者可以通过seek（）方法访问任意offset的消息，从而可能遗漏事务中的部分消息。</li>
<li>消费者在消费时可能没有分配到事务内的所有分区，如此它也就不能读取事务中的所有消息。</li>
</ul>
<img src="/2021/06/26/7-%E6%B7%B1%E5%85%A5%E5%AE%A2%E6%88%B7%E7%AB%AF/%E4%BA%8B%E5%8A%A1%E7%9B%B8%E5%85%B3.png" class="">

<ul>
<li>initTransactions（）方法用来初始化事务，这个方法能够执行的前提是配置了transactionalId</li>
<li>beginTransaction（）方法用来开启事务；</li>
<li>sendOffsetsToTransaction（）方法为消费者提供在事务内的位移提交的操作；</li>
<li>commitTransaction（）方法用来提交事务；</li>
<li>abortTransaction（）方法用来中止事务，类似于事务回滚。</li>
</ul>
<p>在消费端有一个参数isolation.level，与事务有着莫大的关联，这个参数的默认值为“read_uncommitted”</p>
<ul>
<li>设置为“read_committed”的消费端应用是消费不到这些消息的，不过在KafkaConsumer内部会缓存这些消息，直到生产者执行 commitTransaction（）方法之后它才能将这些消息推送给消费端应用。反之，如果生产者执行了 abortTransaction（）方法，那么 KafkaConsumer 会将这些缓存的消息丢弃而不推送给消费端应用。</li>
</ul>
<p>日志文件中除了普通的消息，还有一种消息专门用来标志一个事务的结束，它就是控制消息（ControlBatch）。</p>
<ul>
<li>控制消息一共有两种类型：COMMIT和ABORT，分别用来表征事务已经成功提交或已经被成功中止。</li>
<li>KafkaConsumer 可以通过这个控制消息来判断对应的事务是被提交了还是被中止了，然后结合参数isolation.level配置的隔离级别来决定是否将相应的消息返回给消费端应用，</li>
</ul>
<img src="/2021/06/26/7-%E6%B7%B1%E5%85%A5%E5%AE%A2%E6%88%B7%E7%AB%AF/%E6%B6%88%E8%B4%B9%E7%94%9F%E4%BA%A7.png" class="">

<ul>
<li>查找TransactionCoordinator<ul>
<li>TransactionCoordinator负责分配PID和管理事务，因此生产者要做的第一件事情就是找出对应的TransactionCoordinator所在的broker节点。</li>
<li>与查找GroupCoordinator节点一样，也是通过FindCoordinatorRequest请求来实现的，只不过FindCoordinatorRequest中的coordinator_type就由原来的0变成了1，由此来表示与事务相关联</li>
<li>Kafka 在收到 FindCoorinatorRequest 请求之后，会根据 coordinator_key （也就是transactionalId）查找对应的TransactionCoordinator节点。</li>
<li>如果找到，则会返回其相对应的node_id、host和port信息。</li>
<li>具体查找TransactionCoordinator的方式是根据transactionalId的哈希值计算主题__transaction_state中的分区编号，</li>
<li>找到对应的分区之后，再寻找此分区leader副本所在的broker节点，该broker节点即为这个transactionalId对应的TransactionCoordinator节点。</li>
</ul>
</li>
<li>获取PID<ul>
<li>在找到TransactionCoordinator节点之后，就需要为当前生产者分配一个PID了。凡是开启了幂等性功能的生产者都必须执行这个操作，不需要考虑该生产者是否还开启了事务。</li>
<li>生产者获取PID的操作是通过InitProducerIdRequest请求来实现的</li>
<li>生产者的InitProducerIdRequest请求会被发送给TransactionCoordinator。</li>
<li>如果未开启事务特性而只开启幂等特性，那么 InitProducerIdRequest 请求可以发送给任意的 broker。</li>
<li>当TransactionCoordinator第一次收到包含该transactionalId的InitProducerIdRequest请求时，它会把transactionalId和对应的PID以消息（我们习惯性地把这类消息称为“事务日志消息”）的形式保存到主题__transaction_state中</li>
<li>与InitProducerIdRequest对应的InitProducerIdResponse响应体结构如图7-24所示，除了返回PID，InitProducerIdRequest还会触发执行以下任务<ul>
<li>增加该 PID 对应的 producer_epoch。具有相同 PID 但 producer_epoch 小于该producer_epoch的其他生产者新开启的事务将被拒绝。</li>
<li>恢复（Commit）或中止（Abort）之前的生产者未完成的事务</li>
</ul>
</li>
</ul>
</li>
<li>开启事务<ul>
<li>通过KafkaProducer的beginTransaction（）方法可以开启一个事务，调用该方法后，生产者本地会标记已经开启了一个新的事务，只有在生产者发送第一条消息之后 TransactionCoordinator才会认为该事务已经开启。</li>
</ul>
</li>
<li>Consume-Transform-Produce<ul>
<li>AddPartitionsToTxnRequest<ul>
<li>当生产者给一个新的分区（TopicPartition）发送数据前，它需要先向TransactionCoordinator发送AddPartitionsToTxnRequest请求（AddPartitionsToTxnRequest请求体结构如图7-25所示），这个请求会让 TransactionCoordinator 将＜transactionId，TopicPartition＞的对应关系存储在主题__transaction_state中</li>
</ul>
</li>
<li>ProduceRequest<ul>
<li>这一步骤很容易理解，生产者通过ProduceRequest 请求发送消息（ProducerBatch）到用户自定义主题中，这一点和发送普通消息时相同，</li>
</ul>
</li>
<li>AddOffsetsToTxnRequest<ul>
<li>通过KafkaProducer的sendOffsetsToTransaction（）方法可以在一个事务批次里处理消息的消费和发送，方法中包含2个参数：Map＜TopicPartition，OffsetAndMetadata＞ offsets和groupId。</li>
</ul>
</li>
<li>TxnOffsetCommitRequest<ul>
<li>这个请求也是sendOffsetsToTransaction（）方法中的一部分，在处理完AddOffsetsToTxnRequest之后，生产者还会发送 TxnOffsetCommitRequest 请求给 GroupCoordinator，从而将本次事务中包含的消费位移信息offsets存储到主题__consumer_offsets中</li>
</ul>
</li>
</ul>
</li>
<li>提交或者中止事务<ul>
<li>一旦数据被写入成功，我们就可以调用 KafkaProducer 的 commitTransaction（）方法或abortTransaction（）方法来结束当前的事务。</li>
</ul>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/" class="post-title-link" itemprop="url">6.深入服务端</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-25 13:57:01" itemprop="dateCreated datePublished" datetime="2021-06-25T13:57:01+08:00">2021-06-25</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-08-08 21:43:29" itemprop="dateModified" datetime="2021-08-08T21:43:29+08:00">2021-08-08</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kafka/" itemprop="url" rel="index"><span itemprop="name">kafka</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>本章涉及协议设计、时间轮、延迟操作、控制器及参数解密，尤其是协议设计和控制器的介绍，这些是深入了解Kafka的必备知识点。</p>
<h2 id="协议设计"><a href="#协议设计" class="headerlink" title="协议设计"></a>协议设计</h2><p>Kafka自定义了一组基于TCP的二进制协议，只要遵守这组协议的格式，就可以向Kafka发送消息，也可以从Kafka中拉取消息，或者做一些其他的事情，比如提交消费位移等。</p>
<p>一共包含了 43 种协议类型</p>
<ul>
<li>每种类型的Request都包含相同结构的协议请求头（RequestHeader）和不同结构的协议请求体（RequestBody）<img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/%E5%8D%8F%E8%AE%AE%E8%AF%B7%E6%B1%82%E5%A4%B4.png" class=""><img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/%E8%AF%B7%E6%B1%82%E5%A4%B4%E8%A7%A3%E9%87%8A.png" class=""></li>
<li>每种类型的Response也包含相同结构的协议响应头（ResponseHeader）和不同结构的响应体（ResponseBody）<img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/%E5%93%8D%E5%BA%94%E8%AF%B7%E6%B1%82%E5%A4%B4.png" class=""><img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B.png" class=""><img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/request%E7%BB%93%E6%9E%84.png" class=""></li>
</ul>
<p>消息累加器 RecordAccumulator 中的消息是以＜分区，Deque＜ProducerBatch＞＞的形式进行缓存的，之后由Sender线程转变成＜Node，List＜ProducerBatch＞＞的形式，针对每个Node，Sender线程在发送消息前会将对应的List＜ProducerBatch＞形式的内容转变成 ProduceRequest 的具体结构。List＜ProducerBatch＞中的内容首先会按照主题名称进行分类（对应ProduceRequest中的域topic），然后按照分区编号进行分类（对应ProduceRequest中的域partition），分类之后的ProducerBatch集合就对应ProduceRequest中的域record_set。</p>
<h2 id="时间轮"><a href="#时间轮" class="headerlink" title="时间轮"></a>时间轮</h2><p>Kafka中存在大量的延时操作，比如延时生产、延时拉取和延时删除等。Kafka并没有使用JDK自带的Timer或DelayQueue（O(nlogn)）来实现延时的功能，而是基于时间轮的概念自定义实现了一个用于延时功能的定时器,而基于时间轮可以将插入和删除操作的时间复杂度都降为O（1）（SystemTimer）</p>
<img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/%E6%97%B6%E9%97%B4%E8%BD%AE.png" class="">

<ul>
<li>Kafka中的时间轮（TimingWheel）是一个存储定时任务的环形队列，底层采用数组实现，数组中的每个元素可以存放一个定时任务列表（TimerTaskList）</li>
<li>TimerTaskList是一个环形的双向链表，链表中的每一项表示的都是定时任务项（TimerTaskEntry），其中封装了真正的定时任务（TimerTask）。</li>
<li>时间轮由多个时间格组成，每个时间格代表当前时间轮的基本时间跨度（tickMs）。时间轮的时间格个数是固定的，可用wheelSize来表示，</li>
<li>那么整个时间轮的总体时间跨度（interval）可以通过公式 tickMs×wheelSize计算得出。</li>
<li>时间轮还有一个表盘指针（currentTime），用来表示时间轮当前所处的时间，currentTime是tickMs的整数倍</li>
<li>currentTime可以将整个时间轮划分为到期部分和未到期部分，currentTime当前指向的时间格也属于到期部分，表示刚好到期，需要处理此时间格所对应的TimerTaskList中的所有任务。</li>
</ul>
<p>工作流程：</p>
<ul>
<li>若时间轮的tickMs为1ms且wheelSize等于20，那么可以计算得出总体时间跨度interval为20ms。</li>
<li>初始情况下表盘指针currentTime指向时间格0，此时有一个定时为2ms的任务插进来会存放到时间格为2的TimerTaskList中。</li>
<li>随着时间的不断推移，指针currentTime不断向前推进，过了2ms之后，当到达时间格2时，就需要将时间格2对应的TimeTaskList中的任务进行相应的到期操作。</li>
<li>此时若又有一个定时为 8ms 的任务插进来，则会存放到时间格 10 中，currentTime再过8ms后会指向时间格10。</li>
<li>如果同时有一个定时为19ms的任务插进来怎么办？新来的TimerTaskEntry会复用原来的TimerTaskList，所以它会插入原本已经到期的时间格1。</li>
<li>总之，整个时间轮的总体跨度是不变的，随着指针currentTime的不断推进，当前时间轮所能处理的时间段也在不断后移，总体时间范围在currentTime和currentTime+interval之间。</li>
<li>Kafka中不乏几万甚至几十万毫秒的定时任务，这个wheelSize的扩充没有底线，就算将所有的定时任务的到期时间都设定一个上限，比如100万毫秒，那么这个wheelSize为100万毫秒的时间轮不仅占用很大的内存空间，而且也会拉低效率。</li>
<li>Kafka 为此引入了<strong>层级时间轮的概念</strong>，当任务的到期时间超过了当前时间轮所表示的时间范围时，就会尝试添加到上层时间轮中。<img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/%E5%B1%82%E7%BA%A7%E6%97%B6%E9%97%B4%E8%BD%AE.png" class=""></li>
<li>第一层的时间轮tickMs=1ms、wheelSize=20、interval=20ms。</li>
<li>第二层的时间轮的tickMs为第一层时间轮的interval，即20ms。每一层时间轮的wheelSize是固定的，都是20，那么第二层的时间轮的总体时间跨度interval为400ms。</li>
<li>以此类推，这个400ms也是第三层的tickMs的大小，第三层的时间轮的总体时间跨度为8000ms。</li>
<li>同理，也会有降级操作</li>
</ul>
<p>TimingWheel时还有一些小细节</p>
<ul>
<li>TimingWheel中的每个双向环形链表TimerTaskList都会有一个哨兵节点（sentinel），引入哨兵节点可以简化边界条件。哨兵节点也称为哑元节点（dummy node），它是一个附加的链表节点，该节点作为第一个节点，</li>
<li>Kafka 中的定时器只需持有 TimingWheel 的第一层时间轮的引用，并不会直接持有其他高层的时间轮，但每一层时间轮都会有一个引用（overflowWheel）指向更高一层的应用，以此层级调用可以实现定时器间接持有各个层级时间轮的引用。</li>
<li>并且会配合DelayQueue 完成工作，其中用TimingWheel做最擅长的任务添加和删除操作，而用DelayQueue做最擅长的时间推进工作</li>
<li>会有线程专门拿取DelayQueue中的到期的任务列表，推进时间轮，降级时间轮，处理操作</li>
</ul>
<h2 id="延时操作"><a href="#延时操作" class="headerlink" title="延时操作"></a>延时操作</h2><p>如果在使用生产者客户端发送消息的时候将 acks 参数设置为-1，那么就意味着需要等待ISR集合中的所有副本都确认收到消息之后才能正确地收到响应的结果，或者捕获超时异常。</p>
<p>在Kafka中有多种延时操作，比如前面提及的延时生产，还有延时拉取（DelayedFetch）、延时数据删除（DelayedDeleteRecords）等。</p>
<ul>
<li>延时操作创建之后会被加入延时操作管理器（DelayedOperationPurgatory）来做专门的处理。延时操作有可能会超时，每个延时操作管理器都会配备一个定时器（SystemTimer）来做超时管理，定时器的底层就是采用时间轮（TimingWheel）实现的</li>
<li>时间轮的轮转是靠“收割机”线程ExpiredOperationReaper来驱动的，这里的“收割机”线程就是由延时操作管理器启动的。</li>
<li>定时器、“收割机”线程和延时操作管理器都是一一对应的。</li>
<li>延时操作需要支持外部事件的触发，所以还要配备一个监听池来负责监听每个分区的外部事件—查看是否有分区的HW发生了增长。另外需要补充的是，ExpiredOperationReaper不仅可以推进时间轮，还会定期清理监听池中已完成的延时操作。<img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/%E5%BB%B6%E6%97%B6%E7%94%9F%E4%BA%A7%E7%BB%86%E8%8A%82.png" class=""></li>
</ul>
<p>延时拉取</p>
<p>Kafka在处理拉取请求时，会先读取一次日志文件，如果收集不到足够多（fetchMinBytes，由参数fetch.min.bytes配置，默认值为1）的消息，那么就会创建一个延时拉取操作（DelayedFetch）以等待拉取到足够数量的消息。当延时拉取操作执行时，会再读取一次日志文件，然后将拉取结果返回给 follower 副本。延时拉取操作也会有一个专门的延时操作管理器负责管理，大体的脉络与延时生产操作相同，不再赘述。如果拉取进度一直没有追赶上leader副本，那么在拉取leader副本的消息时一般拉取的消息大小都会不小于fetchMinBytes，这样Kafka也就不会创建相应的延时拉取操作，而是立即返回拉取结果。如果是follower副本的延时拉取，它的外部事件就是消息追加到了leader副本的本地日志文件中；如果是消费者客户端的延时拉取，它的外部事件可以简单地理解为HW的增长。</p>
<p>目前版本的Kafka还引入了事务的概念，对于消费者或follower副本而言，其默认的事务隔离级别为“read_uncommitted”。不过消费者可以通过客户端参数isolation.level将事务隔离级别设置为“read_committed”（注意：follower副本不可以将事务隔离级别修改为这个值），这样消费者拉取不到生产者已经写入却尚未提交的消息。对应的消费者的延时拉取，它的外部事件实际上会切换为由LSO<br>（LastStableOffset）的增长来触发。LSO是HW之前除去未提交的事务消息的最大偏移量，LSO≤HW，有关事务和LSO的内容可以分别参考7.4节和10.2节。</p>
<h2 id="控制器"><a href="#控制器" class="headerlink" title="控制器"></a>控制器</h2><ul>
<li>在 Kafka 集群中会有一个或多个 broker，其中有一个 broker 会被选举为控制器（Kafka Controller），它负责管理整个集群中所有分区和副本的状态。</li>
<li>当某个分区的leader副本出现故障时，由控制器负责为该分区选举新的leader副本。</li>
<li>当检测到某个分区的ISR集合发生变化时，由控制器负责通知所有broker更新其元数据信息。</li>
<li>当使用kafka-topics.sh脚本为某个topic增加分区数量时，同样还是由控制器负责分区的重新分配。</li>
</ul>
<h3 id="控制器的选举及异常恢复"><a href="#控制器的选举及异常恢复" class="headerlink" title="控制器的选举及异常恢复"></a>控制器的选举及异常恢复</h3><p>Kafka中的控制器选举工作依赖于ZooKeeper，成功竞选为控制器的broker会在ZooKeeper中创建/controller这个临时（EPHEMERAL）节点，此临时节点的内容参考如下：</p>
<ul>
<li>在任意时刻，集群中有且仅有一个控制器。每个 broker 启动的时候会去尝试读取/controller节点的brokerid的值，如果读取到brokerid的值不为-1，则表示已经有其他 broker 节点成功竞选为控制器，所以当前 broker 就会放弃竞选</li>
<li>如果 ZooKeeper 中不存在/controller节点，或者这个节点中的数据异常，那么就会尝试去创建/controller节点。</li>
<li>当前broker去创建节点的时候，也有可能其他broker同时去尝试创建这个节点，只有创建成功的那个broker才会成为控制器，而创建失败的broker竞选失败</li>
<li>每个broker都会在内存中保存当前控制器的brokerid值，这个值可以标识为activeControllerId。</li>
<li>ZooKeeper 中还有一个与控制器有关的/controller_epoch 节点，这个节点是持久（PERSISTENT）节点，节点中存放的是一个整型的controller_epoch值。controller_epoch用于记录控制器发生变更的次数，即记录当前的控制器是第几代控制器，我们也可以称之为“控制器的纪元”。</li>
<li>由此可见，Kafka 通过 controller_epoch 来保证控制器的唯一性，进而保证相关操作的一致性。</li>
</ul>
<p>具备控制器身份的broker需要比其他普通的broker多一份职责，具体细节如下</p>
<ul>
<li>监听分区相关的变化。为ZooKeeper中的/admin/reassign_partitions 节点注册 PartitionReassignmentHandler，用来处理分区重分配的动作。为 ZooKeeper 中的/isr_change_notification节点注册IsrChangeNotificetionHandler，用来处理ISR集合变更的动作。为ZooKeeper中的/admin/preferred-replica-election节点添加PreferredReplicaElectionHandler，用来处理优先副本的选举动作。</li>
<li>监听主题相关的变化。为 ZooKeeper 中的/brokers/topics 节点添加TopicChangeHandler，用来处理主题增减的变化；为 ZooKeeper 中的/admin/delete_topics节点添加TopicDeletionHandler，用来处理删除主题的动作。</li>
<li>监听broker相关的变化。为ZooKeeper中的/brokers/ids节点添加BrokerChangeHandler，用来处理broker增减的变化。</li>
<li>从ZooKeeper中读取获取当前所有与主题、分区及broker有关的信息并进行相应的管理。对所有主题对应的 ZooKeeper 中的/brokers/topics/＜topic＞节点添加PartitionModificationsHandler，用来监听主题中的分区分配变化。</li>
<li>启动并管理分区状态机和副本状态机。</li>
<li>更新集群的元数据信息。</li>
<li>如果参数 auto.leader.rebalance.enable 设置为 true，则还会开启一个名为“auto-leader-rebalance-task”的定时任务来负责维护分区的优先副本的均衡。<img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/%E6%8E%A7%E5%88%B6%E5%99%A8.png" class=""></li>
<li>在目前的新版本的设计中，只有Kafka Controller在ZooKeeper上注册相应的监听器，其他的broker极少需要再监听ZooKeeper中的数据变化，这样省去了很多不必要的麻烦。不过每个broker还是会对/controller节点添加监听器，以此来监听此节点的数据变化（ControllerChangeHandler）。</li>
<li>当/controller 节点的数据发生变化时，每个 broker 都会更新自身内存中保存的activeControllerId。如果broker 在数据变更前是控制器，在数据变更后自身的 brokerid 值与新的 activeControllerId 值不一致，那么就需要“退位”，关闭相应的资源，比如关闭状态机、注销相应的监听器等。</li>
<li>当/controller节点被删除时，每个broker都会进行选举，如果broker在节点被删除前是控制器，那么在选举前还需要有一个“退位”的动作。如果有特殊需要，则可以手动删除/controller 节点来触发新一轮的选举。当然关闭控制器所对应的 broker，以及手动向/controller节点写入新的brokerid的所对应的数据，同样可以触发新一轮的选举。</li>
</ul>
<h3 id="优雅关闭"><a href="#优雅关闭" class="headerlink" title="优雅关闭"></a>优雅关闭</h3><ol>
<li>获取Kafka的服务进程号PIDS。可以使用Java中的jps命令或使用Linux系统中的ps命令来查看。</li>
<li>使用 kill-s TERM $PIDS 或 kill-15 $PIDS 的方式来关闭进程，注意千万不要使用kill-9的方式。<img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/%E5%85%B3%E9%97%AD.png" class=""></li>
</ol>
<h3 id="分区leader的选举"><a href="#分区leader的选举" class="headerlink" title="分区leader的选举"></a>分区leader的选举</h3><p>分区leader副本的选举由控制器负责具体实施。当创建分区（创建主题或增加分区都有创建分区的动作）或分区上线（比如分区中原先的leader副本下线，此时分区需要选举一个新的leader 上线来对外提供服务）的时候都需要执行 leader 的选举动作，对应的选举策略为OfflinePartitionLeaderElectionStrategy。</p>
<ul>
<li>这种策略的基本思路是按照 AR 集合中副本的顺序查找第一个存活的副本，并且这个副本在ISR集合中。</li>
<li>一个分区的AR集合在分配的时候就被指定，并且只要不发生重分配的情况，集合内部副本的顺序是保持不变的，而分区的ISR集合中副本的顺序可能会改变。</li>
<li>注意这里是根据AR的顺序而不是ISR的顺序进行选举的。</li>
<li>如果ISR集合中没有可用的副本，那么此时还要再检查一下所配置的unclean.leader.election.enable参数（默认值为false）。如果这个参数配置为true，那么表示允许从非ISR列表中的选举leader，从AR列表中找到第一个存活的副本即为leader。</li>
</ul>
<p>当分区进行重分配（可以先回顾一下4.3.2节的内容）的时候也需要执行leader的选举动作，对应的选举策略为 ReassignPartitionLeaderElectionStrategy。</p>
<ul>
<li>从重分配的AR列表中找到第一个存活的副本，且这个副本在目前的ISR列表中。</li>
<li>当发生优先副本（可以先回顾一下4.3.1节的内容）的选举时，直接将优先副本设置为leader即可，AR集合中的第一个副本即为优先副本（PreferredReplicaPartitionLeaderElectionStrategy）。</li>
</ul>
<p>还有一种情况会发生 leader 的选举，当某节点被优雅地关闭（也就是执行ControlledShutdown）时，位于这个节点上的leader副本都会下线，所以与此对应的分区需要执行leader的选举。</p>
<ul>
<li>从AR列表中找到第一个存活的副本，且这个副本在目前的ISR列表中，与此同时还要确保这个副本不处于正在被关闭的节点上。</li>
</ul>
<h2 id="参数解密"><a href="#参数解密" class="headerlink" title="参数解密"></a>参数解密</h2><h3 id="broker-id"><a href="#broker-id" class="headerlink" title="broker.id"></a>broker.id</h3><ul>
<li>broker.id是broker在启动之前必须设定的参数之一，在Kafka集群中，每个broker都有唯一的 id（也可以记作 brokerId）值用来区分彼此。</li>
<li>broker 在启动时会在 ZooKeeper 中的/brokers/ids路径下创建一个以当前brokerId为名称的虚节点，broker的健康状态检查就依赖于此虚节点。</li>
<li>当 broker 下线时，该虚节点会自动删除，其他 broker 节点或客户端通过判断/brokers/ids路径下是否有此broker的brokerId节点来确定该broker的健康状态。</li>
<li>config/server.properties 里的 broker.id 参数来配置brokerId，默认情况下broker.id值为-1。</li>
<li>在Kafka中，brokerId值必须大于等于0才有可能正常启动，但这里并不是只能通过配置文件config/server.properties来设定这个值，还可以通过meta.properties文件或自动生成功能来实现。</li>
</ul>
<p>meta.properties文件与broker.id的关联如下</p>
<ul>
<li>如果 log.dir 或 log.dirs 中配置了多个日志根目录，这些日志根目录中的meta.properties文件所配置的broker.id不一致则会抛出InconsistentBrokerIdException的异常。</li>
<li>如果config/server.properties配置文件里配置的broker.id的值和meta.properties文件里的broker.id值不一致，那么同样会抛出InconsistentBrokerIdException的异常。</li>
<li>如果 config/server.properties 配置文件中并未配置 broker.id 的值，那么就以meta.properties文件中的broker.id值为准。</li>
<li>如果没有meta.properties文件，那么在获取合适的broker.id值之后会创建一个新的meta.properties文件并将broker.id值存入其中。</li>
<li>如果两个文件中都没有broker.id，Kafka 还提供了另外两个 broker 端参数：broker.id.generation.enable 和reserved.broker.max.id来配合生成新的brokerId。<ul>
<li>broker.id.generation.enable参数用来配置是否开启自动生成 brokerId 的功能，默认情况下为 true，即开启此功能。</li>
<li>自动生成的 brokerId 有一个基准值，即自动生成的 brokerId 必须超过这个基准值，这个基准值通过reserverd.broker.max.id参数配置，默认值为1000。也就是说，默认情况下自动生成的brokerId从1001开始。</li>
<li>自动生成的brokerId的原理是先往ZooKeeper中的/brokers/seqid节点中写入一个空字符 串，然 后 获 取 返 回 的 Stat 信 息 中 的 version 值，进 而 将 version 的 值 和reserved.broker.max.id参数配置的值相加。先往节点中写入数据再获取Stat信息，这样可以确保返回的 version 值大于 0，进而就可以确保生成的 brokerId 值大于reserved.broker.max.id 参数配置的值，符合非自动生成的 broker.id 的值在[0，reserved.broker.max.id]区间设定。</li>
</ul>
</li>
</ul>
<h3 id="bootstrap-servers"><a href="#bootstrap-servers" class="headerlink" title="bootstrap.servers"></a>bootstrap.servers</h3><ul>
<li>我们一般可以简单地认为 bootstrap.servers 这个参数所要指定的就是将要连接的Kafka集群的broker地址列表。</li>
<li>不过从深层次的意义上来讲，这个参数配置的是用来发现Kafka集群元数据信息的服务地址。<img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/bootstrap%E5%8E%9F%E7%90%86.png" class=""></li>
</ul>
<p>客户端KafkaProducer1与Kafka Cluster直连，这是客户端给我们的既定印象，而事实上客户端连接Kafka集群要经历以下3个过程</p>
<ul>
<li>客户端KafkaProducer2与bootstrap.servers参数所指定的Server连接，并发送MetadataRequest请求来获取集群的元数据信息。</li>
<li>Server在收到MetadataRequest请求之后，返回MetadataResponse给KafkaProducer2，在MetadataResponse中包含了集群的元数据信息。</li>
<li>客户端KafkaProducer2收到的MetadataResponse之后解析出其中包含的集群元数据信息，然后与集群中的各个节点建立连接，之后就可以发送消息了。</li>
</ul>
<p>在绝大多数情况下，Kafka 本身就扮演着第一步和第二步中的 Server 角色，我们完全可以将这个Server的角色从Kafka中剥离出来。我们可以在这个Server的角色上大做文章，比如添加一些路由的功能、负载均衡的功能。</p>
<p>bootstrap.servers、metadata.broker.list、zookeeper.connect 参数往往不是很清楚。这一现象还存在Kafka所提供的诸多脚本之中，在这些脚本中连接Kafka采用的选项参数有–bootstrap-server、–broker-list和–zookeeper</p>
<ul>
<li>bootstrap-server是broker-list 的替代品，但是kafka-console-producer.sh 还在使用</li>
<li>zookeeper 命令是kafka-topics.sh脚本实际上操纵的就是ZooKeeper中的节点，而不是Kafka本身，它并没有被替代的必要。</li>
</ul>
<h3 id="服务端参数列表"><a href="#服务端参数列表" class="headerlink" title="服务端参数列表"></a>服务端参数列表</h3>  <img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%8F%82%E6%95%B01.png" class="">  <img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%8F%82%E6%95%B02.png" class="">  <img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%8F%82%E6%95%B03.png" class="">  <img src="/2021/06/25/6-%E6%B7%B1%E5%85%A5%E6%9C%8D%E5%8A%A1%E7%AB%AF/%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%8F%82%E6%95%B04.png" class="">

<h2 id="元数据更新"><a href="#元数据更新" class="headerlink" title="元数据更新"></a>元数据更新</h2><p>broker是有状态的服务：每台broker在内存中都维护了集群上所有节点和topic分区的状态信息——Kafka称这部分状态信息为元数据缓存(metadata cache)。本文就将讨论一下这个metadata cache的设计与实现。</p>
<h3 id="cache里面存了什么"><a href="#cache里面存了什么" class="headerlink" title="cache里面存了什么"></a>cache里面存了什么</h3><p>首先，我们来看下cache里面都存了什么，我们以Kafka 1.0.0版本作为分析对象。Metadata cache中保存的信息十分丰富，几乎囊括了Kafka集群的各个方面，它包含了：</p>
<ul>
<li>controller所在的broker ID，即保存了当前集群中controller是哪台broker</li>
<li>集群中所有broker的信息：比如每台broker的ID、机架信息以及配置的若干组连接信息(比如配置了PLAINTEXT和SASL监听器就有两套连接信息，分别使用不同的安全协议和端口，甚至主机名都可能不同)</li>
<li>集群中所有节点的信息：严格来说，它和上一个有些重复，不过此项是按照broker ID和监听器类型进行分组的。对于超大集群来说，使用这一项缓存可以快速地定位和查找给定节点信息，而无需遍历上一项中的内容，算是一个优化吧</li>
<li>集群中所有分区的信息：所谓分区信息指的是分区的leader、ISR和AR信息以及当前处于offline状态的副本集合。这部分数据按照topic和分区ID进行分组，可以快速地查找到每个分区的当前状态。（注：AR表示assigned replicas，即创建topic时为该分区分配的副本集合）</li>
</ul>
<h3 id="每台broker都保存相同的cache吗"><a href="#每台broker都保存相同的cache吗" class="headerlink" title="每台broker都保存相同的cache吗"></a>每台broker都保存相同的cache吗</h3><p>　　是的，至少Kafka在设计时的确是这样的愿景：每台Kafka broker都要维护相同的缓存，这样客户端程序(clients)随意地给任何一个broker发送请求都能够获取相同的数据，这也是为什么任何一个broker都能处理clients发来的Metadata请求的原因：因为每个broker上都有这些数据！要知道目前Kafka共有38种请求类型，能做到这一点的可谓少之又少。每个broker都能处理的能力可以缩短请求被处理的延时从而提高整体clients端的吞吐，因此用空间去换一些时间的做法是值得的。</p>
<h3 id="cache是怎么更新的"><a href="#cache是怎么更新的" class="headerlink" title="cache是怎么更新的"></a>cache是怎么更新的</h3><p>　　如前所述，用空间去换时间，好处是降低了延时，提升了吞吐，但劣势就在于你需要处理cache的更新并且维护一致性。目前Kafka是怎么更新cache的？简单来说，就是通过发送异步更新请求(UpdateMetadata request)来维护一致性的。既然是异步的，那么在某一个时间点集群上所有broker的cache信息就未必是严格相同的。只不过在实际使用场景中，这种弱一致性似乎并没有太大的问题。原因如下：</p>
<ol>
<li>clients并不是时刻都需要去请求元数据的，且会缓存到本地；</li>
<li>即使获取的元数据无效或者过期了，clients通常都有重试机制，可以去其他broker上再次获取元数据;</li>
<li>cache更新是很轻量级的，仅仅是更新一些内存中的数据结构，不会有太大的成本。因此我们还是可以安全地认为每台broker上都有相同的cache信息。</li>
</ol>
<p>　　具体的更新操作实际上是由controller来完成的。controller会在一定场景下向特定broker发送UpdateMetadata请求令这些broker去更新它们各自的cache，这些broker一旦接收到请求便开始全量更新——即清空当前所有cache信息，使用UpdateMetadata请求中的数据来重新填充cache。</p>
<h3 id="cache什么时候更新"><a href="#cache什么时候更新" class="headerlink" title="cache什么时候更新"></a>cache什么时候更新</h3><p>　　实际上这个问题等同于：controller何时向特定broker发送UpdateMetadata请求？ 如果从源码开始分析，那么涉及到的场景太多了，比如controller启动时、新broker启动时、更新broker时、副本重分配时等等。我们只需要记住：<strong>只要集群中有broker或分区数据发生了变更就需要更新这些cache</strong></p>
<p>　　举个经常有人问的例子：集群中新增加的broker是如何获取这些cache，并且其他broker是如何知晓它的？当有新broker启动时，它会在Zookeeper中进行注册，此时监听Zookeeper的controller就会立即感知这台新broker的加入，此时controller会更新它自己的缓存（注意：这是controller自己的缓存，不是本文讨论的metadata cache）把这台broker加入到当前broker列表中，之后它会发送UpdateMetadata请求给集群中所有的broker(也包括那台新加入的broker)让它们去更新metadata cache。一旦这些broker更新cache完成，它们就知道了这台新broker的存在，同时由于新broker也更新了cache，故现在它也有了集群所有的状态信息。</p>
<h3 id="目前的问题"><a href="#目前的问题" class="headerlink" title="目前的问题"></a>目前的问题</h3><p>　　前面说过了，现在更新cache完全由controller来驱动，故controller所在broker的负载会极大地影响这部分操作（实际上，它会影响所有的controller操作）。根据目前的设计，controller所在broker依然作为一个普通broker执行其他的clients请求处理逻辑，所以如果controller broker一旦忙于各种clients请求(比如生产消息或消费消息)，那么这种更新操作的请求就会积压起来(backlog)，造成了更新操作的延缓甚至是被取消。究其根本原因在于当前controller对待数据类请求和控制类请求并无任何优先级化处理——controller一视同仁地对待这些请求，而实际上我们更希望controller能否赋予控制类请求更高的优先级。社区目前已经开始着手改造当前的设计，相信在未来的版本中此问题可以得到解决。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/25/5-%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/25/5-%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/" class="post-title-link" itemprop="url">5.日志存储</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2021-06-25 10:04:59 / 修改时间：20:42:44" itemprop="dateCreated datePublished" datetime="2021-06-25T10:04:59+08:00">2021-06-25</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kafka/" itemprop="url" rel="index"><span itemprop="name">kafka</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="文件目录布局"><a href="#文件目录布局" class="headerlink" title="文件目录布局"></a>文件目录布局</h2><ul>
<li>主题，分区是逻辑上的概念，物理上没有对应的东西</li>
<li>log或者说副本在物理存储上对应了目录，例如topic-log-0 ，其是＜topic＞-＜partition＞的文件夹</li>
<li>每个log中有多个logsegment 概念，多个分段，其实就是数据的拆分</li>
<li>每个分段又包含了.log .index .timeindex 等文件，还可能包含“.deleted”“.cleaned”“.swap”等临时文件，以及可能的“.snapshot”“.txnindex”“leader-epoch-checkpoint”等文件。<img src="/2021/06/25/5-%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8.png" class="">
<img src="/2021/06/25/5-%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/%E5%B8%83%E5%B1%80.png" class=""></li>
</ul>
<h2 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h2><p>其实和redis 类似，减少了一些元数据信息,并增加了其他可变数据类型</p>
<h2 id="消息格式v2版本"><a href="#消息格式v2版本" class="headerlink" title="消息格式v2版本"></a>消息格式v2版本</h2><ul>
<li>v2版本中消息集称为Record Batch，而不是先前的Message Set，其内部也包含了一条或多条消息，</li>
<li>生产者客户端中的ProducerBatch对应这里的RecordBatch，而ProducerRecord对应这里的Record。<img src="/2021/06/25/5-%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/v2.png" class="">
详细见深入理解kafka</li>
</ul>
<h2 id="日志索引"><a href="#日志索引" class="headerlink" title="日志索引"></a>日志索引</h2><ul>
<li>Kafka 中的索引文件以稀疏索引（sparse index）的方式构造消息的索引，它并不保证每个消息在索引文件中都有对应的索引项。</li>
<li>稀疏索引通过MappedByteBuffer将索引文件映射到内存中，以加快索引的查询速度。</li>
<li>使用二分查找法来快速定位偏移量的位置，如果指定的偏移量不在索引文件中，则会返回小于指定偏移量的最大偏移量。时间戳索引文件中的时间戳也保持严格的单调递增，查询指定时间戳时，也根据二分查找法来查找不大于该时间戳的最大偏移量，至于要找到对应的物理文件位置还需要根据偏移量索引文件来进行再次定位。</li>
</ul>
<p>日志切分</p>
<ol>
<li>当前日志分段文件的大小超过了 broker 端参数 log.segment.bytes 配置的值。log.segment.bytes参数的默认值为1073741824，即1GB。</li>
<li>当前日志分段中消息的最大时间戳与当前系统的时间戳的差值大于 log.roll.ms或log.roll.hours参数配置的值。如果同时配置了log.roll.ms和log.roll.hours参数，那么log.roll.ms的优先级高。默认情况下，只配置了log.roll.hours参数，其值为168，即7天。</li>
<li>偏移量索引文件或时间戳索引文件的大小达到broker端参数log.index.size.max.bytes配置的值。log.index.size.max.bytes的默认值为10485760，即10MB。</li>
<li>追加的消息的偏移量与当前日志分段的偏移量之间的差值大于Integer.MAX_VALUE，即要追加的消息的偏移量不能转变为相对偏移量（offset-baseOffset＞Integer.MAX_VALUE）。</li>
</ol>
<p>位置索引和时间索引都是为了加速查找</p>
<img src="/2021/06/25/5-%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/%E6%97%B6%E9%97%B4%E4%BD%8D%E7%BD%AE%E7%B4%A2%E5%BC%95.png" class="">

<h2 id="日志清理"><a href="#日志清理" class="headerlink" title="日志清理"></a>日志清理</h2><ol>
<li>日志删除（Log Retention）：按照一定的保留策略直接删除不符合条件的日志分段。、<ul>
<li>基于时间，log.retention.hours参数，其值为168，故默认情况下日志分段文件的保留时间为7天。</li>
<li>基于日志大小。retentionSize可以通过broker端参数log.retention.bytes来配置，默认值为-1，表示无穷大。</li>
<li>基于日志起始偏移量。</li>
</ul>
</li>
<li>日志压缩（Log Compaction）：针对每个消息的key进行整合，对于有相同key的不同value值，只保留最后一个版本。</li>
</ol>
<h2 id="磁盘存储"><a href="#磁盘存储" class="headerlink" title="磁盘存储"></a>磁盘存储</h2><p>Kafka 在设计时采用了文件追加的方式来写入消息，即只能在日志文件的尾部追加新的消息，并且也不允许修改已写入的消息，这种方式属于典型的顺序写盘的操作，所以就算 Kafka使用磁盘作为存储介质，它所能承载的吞吐量也不容小觑。</p>
<h3 id="页缓存"><a href="#页缓存" class="headerlink" title="页缓存"></a>页缓存</h3><ul>
<li>当一个进程准备读取磁盘上的文件内容时，操作系统会先查看待读取的数据所在的页（page）是否在页缓存（pagecache）中，如果存在（命中）则直接返回数据，从而避免了对物理磁盘的 I/O 操作；</li>
<li>如果没有命中，则操作系统会向磁盘发起读取请求并将读取的数据页存入页缓存，之后再将数据返回给进程。</li>
<li>如果一个进程需要将数据写入磁盘，那么操作系统也会检测数据对应的页是否在页缓存中，如果不存在，则会先在页缓存中添加相应的页，最后将数据写入对应的页。被修改过后的页也就变成了脏页，操作系统会在合适的时间把脏页中的数据写入磁盘，以保持数据的一致性。</li>
<li>Linux操作系统中的vm.dirty_background_ratio参数用来指定当脏页数量达到系统内存的百分之多少之后就会触发 pdflush/flush/kdmflush 等后台回写进程的运行来处理脏页，一般设置为小于10的值即可</li>
<li>在Kafka中同样提供了同步刷盘及间断性强制刷盘（fsync）的功能，这些功能可以通过 log.flush.interval.messages、log.flush.interval.ms 等参数来控制。</li>
<li>最好关闭swap vm.swappiness 建议将这个参数的值设置为 1（1-100），这样保留了swap的机制而又最大限度地限制了它对Kafka性能的影响</li>
</ul>
<h3 id="磁盘I-O流程"><a href="#磁盘I-O流程" class="headerlink" title="磁盘I/O流程"></a>磁盘I/O流程</h3><p>从编程角度而言，一般磁盘I/O的场景有以下四种</p>
<ul>
<li>用户调用标准C库进行I/O操作，数据流为：应用程序buffer→C库标准IObuffer→文件系统页缓存→通过具体文件系统到磁盘。</li>
<li>用户调用文件 I/O，数据流为：应用程序buffer→文件系统页缓存→通过具体文件系统到磁盘。</li>
<li>用户打开文件时使用O_DIRECT，绕过页缓存直接读写磁盘。</li>
<li>用户使用类似dd工具，并使用direct参数，绕过系统cache与文件系统直接写磁盘。</li>
</ul>
<ol>
<li>写操作：<ul>
<li>用户调用fwrite把数据写入C库标准IObuffer后就返回，即写操作通常是异步操作；</li>
<li>数据写入C库标准IObuffer后，不会立即刷新到磁盘，会将多次小数据量相邻写操作先缓存起来合并，最终调用write函数一次性写入（或者将大块数据分解多次write 调用）页缓存；</li>
<li>数据到达页缓存后也不会立即刷新到磁盘，内核有 pdflush 线程在不停地检测脏页，判断是否要写回到磁盘，如果是则发起磁盘I/O请求。</li>
</ul>
</li>
<li>读操作：<ul>
<li>用户调用fread到C库标准IObuffer中读取数据，如果成功则返回，否则继续；</li>
<li>到页缓存中读取数据，如果成功则返回，否则继续；</li>
<li>发起 I/O 请求，读取数据后缓存buffer和C库标准IObuffer并返回。可以看出，读操作是同步请求。</li>
</ul>
</li>
<li>I/O请求处理：<ul>
<li>通用块层根据I/O请求构造一个或多个bio结构并提交给调度层；</li>
<li>调度器将 bio 结构进行排序和合并组织成队列且确保读写操作尽可能理想：将一个或多个进程的读操作合并到一起读，将一个或多个进程的写操作合并到一起写，尽可能变随机为顺序（因为随机读写比顺序读写要慢），读必须优先满足，而写也不能等太久。<img src="/2021/06/25/5-%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/io.png" class=""></li>
</ul>
</li>
</ol>
<p>针对不同的应用场景，I/O调度策略也会影响I/O的读写性能，目前Linux系统中的I/O调度策略有4种，分别为NOOP、CFQ、DEADLINE和ANTICIPATORY，默认为CFQ。</p>
<ul>
<li>NOOP NOOP算法的全写为No Operation。该算法实现了最简单的FIFO队列，所有I/O请求大致按照先来后到的顺序进行操作。之所以说“大致”，原因是NOOP在FIFO的基础上还做了相邻I/O请求的合并，并不是完全按照先进先出的规则满足I/O请求。</li>
<li>CFQ CFQ算法的全写为Completely Fair Queuing。该算法的特点是按照I/O请求的地址进行排序，而不是按照先来后到的顺序进行响应。相比于NOOP的缺点是，先来的I/O请求并不一定能被满足，可能会出现“饿死”的情况。</li>
<li>DEADLINE DEADLINE在CFQ的基础上，解决了I/O请求“饿死”的极端情况。除了CFQ本身具有的I/O排序队列，DEADLINE额外分别为读I/O和写I/O提供了FIFO队列。读FIFO队列的最大等待时间为500ms，写FIFO队列的最大等待时间为5s。FIFO队列内的I/O请求优先级要比CFQ队列中的高，而读FIFO队列的优先级又比写FIFO队列的优先级高。</li>
<li>ANTICIPATORY ANTICIPATORY在DEADLINE的基础上，为每个读I/O都设置了6ms的等待时间窗口。如果在6ms内OS收到了相邻位置的读I/O请求，就可以立即满足</li>
</ul>
<h3 id="零拷贝"><a href="#零拷贝" class="headerlink" title="零拷贝"></a>零拷贝</h3><p>将文件传出出去时，文件A经历了4次复制的过程：</p>
<img src="/2021/06/25/5-%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/%E9%9D%9E%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" class="">

<ul>
<li>调用read（）时，文件A中的内容被复制到了内核模式下的Read Buffer中。</li>
<li>CPU控制将内核模式数据复制到用户模式下。</li>
<li>调用write（）时，将用户模式下的内容复制到内核模式下的Socket Buffer中。</li>
<li>将内核模式下的Socket Buffer的数据复制到网卡设备中传送。</li>
</ul>
<p>从上面的过程可以看出，数据平白无故地从内核模式到用户模式“走了一圈”，浪费了 2次复制过程：第一次是从内核模式复制到用户模式；第二次是从用户模式再复制回内核模式，即上面4次过程中的第2步和第3步。而且在上面的过程中，内核和用户模式的上下文的切换也是4次。</p>
<img src="/2021/06/25/5-%E6%97%A5%E5%BF%97%E5%AD%98%E5%82%A8/%E9%9B%B6%E6%8B%B7%E8%B4%9D.png" class="">

<ul>
<li>零拷贝技术通过DMA（Direct Memory Access）技术将文件内容复制到内核模式下的Read Buffer 中。不过没有数据被复制到 Socket Buffer，相反只有包含数据的位置和长度的信息的文件描述符被加到Socket Buffer</li>
<li>DMA引擎直接将数据从内核模式中传递到网卡设备（协议引擎）。这里数据只经历了2次复制就从磁盘中传送出去了，并且上下文切换也变成了2次。零拷贝是针对内核模式而言的，数据在内核模式下实现了零拷贝。</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/24/4-%E4%B8%BB%E9%A2%98%E4%B8%8E%E5%88%86%E5%8C%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/24/4-%E4%B8%BB%E9%A2%98%E4%B8%8E%E5%88%86%E5%8C%BA/" class="post-title-link" itemprop="url">4.主题与分区</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-24 18:54:23" itemprop="dateCreated datePublished" datetime="2021-06-24T18:54:23+08:00">2021-06-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-06-25 20:42:44" itemprop="dateModified" datetime="2021-06-25T20:42:44+08:00">2021-06-25</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kafka/" itemprop="url" rel="index"><span itemprop="name">kafka</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>主题作为消息的归类，可以再细分为一个或多个分区，分区也可以看作对消息的二次归类。从Kafka的底层实现来说，主题和分区都是逻辑上的概念，分区可以有一至多个副本，每个副本对应一个日志文件，每个日志文件对应一至多个日志分段（LogSegment），每个日志分段还可以细分为索引文件、日志存储文件和快照文件等。</p>
<p>主题和分区都是提供给上层用户的抽象，而在副本层面或更加确切地说是Log层面才有实际物理上的存在。</p>
<h2 id="主题管理"><a href="#主题管理" class="headerlink" title="主题管理"></a>主题管理</h2><ul>
<li>可以通过 Kafka提供的 kafka-topics.sh 脚本来执行这些操作<ul>
<li>其实质上是调用了kafka.admin.TopicCommand类来执行主题管理的操作。</li>
</ul>
</li>
<li>还可以通过KafkaAdminClient 的方式实现</li>
<li>直接操纵日志文件和ZooKeeper节点来实现。</li>
</ul>
<h3 id="创建主题"><a href="#创建主题" class="headerlink" title="创建主题"></a>创建主题</h3><ul>
<li>auto.create.topics.enable设置为true</li>
<li>那么当生产者向一个尚未创建的主题发送消息时，会自动创建一个分区数为num.partitions （默认值为1）、副本因子为default.replication.factor（默认值为1）的主题。</li>
<li>当一个消费者开始从未知主题中读取消息时，或者当任意一个客户端向未知主题发送元数据请求时，都会按照配置参数num.partitions和default.replication.factor的值来创建一个相应的主题。</li>
<li>不建议将auto.create.topics.enable参数设置为true</li>
<li>通过zookeeper /brokers/topics  可以查看主题分区情况</li>
<li>kafka-topics.sh脚本在创建主题时还会检测是否包含“.”或“_”字符。为什么要检测这两个字符呢？因为在Kafka的内部做埋点时会根据主题的名称来命名metrics的名称，并且会将点号“.”改成下画线“_”。假设遇到一个名称为“topic.1_2”的主题，还有一个名称为“topic_1.2”的主题，那么最后的metrics的名称都会为“topic_1_2”，这样就发生了名称冲突。</li>
</ul>
<h3 id="分区副本的分配"><a href="#分区副本的分配" class="headerlink" title="分区副本的分配"></a>分区副本的分配</h3><ul>
<li>生产者的分区分配是指为每条消息指定其所要发往的分区，</li>
<li>消费者中的分区分配是指为消费者指定其可以消费消息的分区</li>
<li>在创建主题时，如果使用了replica-assignment参数，那么就按照指定的方案来进行分区副本的创建</li>
</ul>
<h3 id="查看主题"><a href="#查看主题" class="headerlink" title="查看主题"></a>查看主题</h3><p>create list describe alter delete</p>
<h3 id="修改主题"><a href="#修改主题" class="headerlink" title="修改主题"></a>修改主题</h3><p>通过kafka-topics.sh 脚本中alter 指令提供</p>
<h2 id="分区管理"><a href="#分区管理" class="headerlink" title="分区管理"></a>分区管理</h2><h3 id="优先副本的选举"><a href="#优先副本的选举" class="headerlink" title="优先副本的选举"></a>优先副本的选举</h3><ul>
<li>分区使用多副本机制来提升可靠性，但只有leader副本对外提供读写服务，而follower副本只负责在内部进行消息的同步。</li>
<li>如果一个分区的leader副本不可用，那么就意味着整个分区变得不可用，此时就需要Kafka从剩余的follower副本中挑选一个新的leader副本来继续对外提供服务。</li>
<li>leader 副本个数的多少决定了这个节点负载的高低。</li>
<li>针对同一个分区而言，同一个broker节点中不可能出现它的多个副本，即Kafka集群的一个broker中最多只能有它的一个副本</li>
<li>我们可以将leader副本所在的broker节点叫作分区的leader节点，而follower副本所在的broker节点叫作分区的follower节点。</li>
<li>当原来的leader节点恢复之后重新加入集群时，它只能成为一个新的follower节点而不再对外提供服务。</li>
</ul>
<p>Kafka引入了优先副本（preferred replica）的概念。所谓的优先副本是指在 AR 集合列表中的第一个副本。</p>
<p>在 Kafka 中可以提供分区自动平衡的功能，与此对应的 broker 端参数是 auto.leader.rebalance.enable，此参数的默认值为true，即默认情况下此功能是开启的。</p>
<ul>
<li>如果开启分区自动平衡的功能，则 Kafka 的控制器会启动一个定时任务，这个定时任务会轮询所有的 broker节点，计算每个broker节点的分区不平衡率（broker中的不平衡率=非优先副本的leader个数/分区总数）是否超过leader.imbalance.per.broker.percentage参数配置的比值，默认值为 10%，如果超过设定的比值则会自动执行优先副本的选举动作以求分区平衡。</li>
<li>执行周期由参数leader.imbalance.check.interval.seconds控制，默认值为300秒，即5分钟。</li>
</ul>
<h3 id="分区重分配"><a href="#分区重分配" class="headerlink" title="分区重分配"></a>分区重分配</h3><p>当集群中加入节点或者减少节点时，需要重新分配分区，以达到负载均衡的目的</p>
<p>Kafka提供了 kafka-reassign-partitions.sh 脚本来执行分区重分配的工作，它可以在集群扩容、broker节点失效的场景下对分区进行迁移。</p>
<ul>
<li>首先创建需要一个包含主题清单的JSON 文件，</li>
<li>其次根据主题清单和 broker 节点清单生成一份重分配方案，</li>
<li>最后根据这份方案执行具体的重分配动作。</li>
<li>分区重分配的基本原理是先通过<strong>控制器</strong>为每个分区添加新副本（增加副本因子），</li>
<li>新的副本将从分区的leader副本那里复制所有的数据。</li>
<li>根据分区的大小不同，复制过程可能需要花一些时间，因为数据是通过网络复制到新副本上的。</li>
<li>在复制完成之后，控制器将旧副本从副本清单里移除（恢复为原先的副本因子数）。</li>
<li>注意在重分配的过程中要确保有足够的空间。</li>
</ul>
<h3 id="复制限流"><a href="#复制限流" class="headerlink" title="复制限流"></a>复制限流</h3><p>kafka-config.sh脚本主要以动态配置的方式来达到限流的目的，在broker级别有两个与复制限流相关的配置参数：follower.replication.throttled.rate和leader.replication.throttled.rate，前者用于设置follower副本复制的速度，后者用于设置leader副本传输的速度，它们的单位都是B/s。</p>
<h3 id="修改副本因子"><a href="#修改副本因子" class="headerlink" title="修改副本因子"></a>修改副本因子</h3><p>创建主题之后我们还可以修改分区的个数，同样可以修改副本因子（副本数）。</p>
<h2 id="如何选择合适的分区数"><a href="#如何选择合适的分区数" class="headerlink" title="如何选择合适的分区数"></a>如何选择合适的分区数</h2><h3 id="性能测试工具"><a href="#性能测试工具" class="headerlink" title="性能测试工具"></a>性能测试工具</h3><p>Kafka 本身提供的用于生产者性能测试的 kafka-producer-perf-test.sh和用于消费者性能测试的kafka-consumer-perf-test.sh。</p>
<h3 id="分区数越多吞吐量就越高吗"><a href="#分区数越多吞吐量就越高吗" class="headerlink" title="分区数越多吞吐量就越高吗"></a>分区数越多吞吐量就越高吗</h3><p>分区是Kafka 中最小的并行操作单元，对生产者而言，每一个分区的数据写入是完全可以并行化的；对消费者而言，Kafka 只允许单个分区中的消息被一个消费者线程消费，一个消费组的消费并行度完全依赖于所消费的分区数。</p>
<p>本次案例中使用的测试环境为一个由3台普通云主机组成的3节点的Kafka集群，每台云主机的内存大小为8GB、磁盘大小为40GB、4核CPU的主频为2600MHz。JVM版本为1.8.0_112，Linux系统版本为2.6.32-504.23.4.el6.x86_64。</p>
<h3 id="分区数上限"><a href="#分区数上限" class="headerlink" title="分区数上限"></a>分区数上限</h3><p>由于文件描述符限制 ulimit -n，不能超过该值，会报错</p>
<h3 id="考量因素"><a href="#考量因素" class="headerlink" title="考量因素"></a>考量因素</h3><p>一个“恰如其分”的答案就是视具体情况而定。Kafka本身、业务应用、硬件资源、环境配置等多方面的考量而做出的选择。在设定完分区数，或者更确切地说是创建主题之后，还要对其追踪、监控、调优以求更好地利用它。</p>
<p>如果一定要给一个准则，则建议将分区数设定为集群中broker的倍数，即假定集群中有3个broker节点，可以设定分区数为3、6、9等</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sk-xinye.github.io/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sk-xinye">
      <meta itemprop="description" content="愿所有努力都不被辜负">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="sk-xinyeの博客">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/" class="post-title-link" itemprop="url">3.消费者</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-06-22 17:11:12" itemprop="dateCreated datePublished" datetime="2021-06-22T17:11:12+08:00">2021-06-22</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-06-24 21:03:12" itemprop="dateModified" datetime="2021-06-24T21:03:12+08:00">2021-06-24</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/kafka/" itemprop="url" rel="index"><span itemprop="name">kafka</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h2 id="消费者与消费组"><a href="#消费者与消费组" class="headerlink" title="消费者与消费组"></a>消费者与消费组</h2><p>消费者（Consumer）负责订阅Kafka中的主题（Topic），并且从订阅的主题上拉取消息。<strong>与其他一些消息中间件不同的是：在Kafka的消费理念中还有一层消费组（Consumer Group）的概念</strong>，每个消费者都有一个对应的消费组。当消息发布到主题后，只会被投递给订阅它的每个消费组中的一个消费者。</p>
<ul>
<li>每一个分区只能被一个消费组中的一个消费者所消费。</li>
<li>但是一味增加消费者个数，并不一定会增加消费能力，对于分区数固定的主题，当消费者个数大于分区数，就会有消费者不能分配到任何分区的情况</li>
<li>默认分区分配策略partition.assignment.strategy</li>
<li>消费组是一个逻辑上的概念，它将旗下的消费者归为一类，每一个消费者只隶属于一个消费组。每一个消费组都会有一个固定的名称，消费者在进行消费前需要指定其所属消费组的名称，这个可以通过消费者客户端参数group.id来配置，默认值为空字符串。</li>
<li>消费者并非逻辑上的概念，它是实际的应用实例，它可以是一个线程，也可以是一个进程。同一个消费组内的消费者既可以部署在同一台机器上，也可以部署在不同的机器上。<img src="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/%E6%B6%88%E8%B4%B9%E8%80%85%E7%BB%84.png" class=""></li>
</ul>
<h3 id="消息投递模式"><a href="#消息投递模式" class="headerlink" title="消息投递模式"></a>消息投递模式</h3><p>对于消息中间件而言，一般有两种消息投递模式：点对点（P2P，Point-to-Point）模式和发布/订阅（Pub/Sub）模式。kafka都支持，得益于消费者组</p>
<ul>
<li>如果所有的消费者都隶属于同一个消费组，那么所有的消息都会被均衡地投递给每一个消费者，即每条消息只会被一个消费者处理，这就相当于点对点模式的应用。</li>
<li>如果所有的消费者都隶属于不同的消费组，那么所有的消息都会被广播给所有的消费者，即每条消息会被所有的消费者处理，这就相当于发布/订阅模式的应用。</li>
</ul>
<h2 id="客户端开发"><a href="#客户端开发" class="headerlink" title="客户端开发"></a>客户端开发</h2><h3 id="主要步骤"><a href="#主要步骤" class="headerlink" title="主要步骤"></a>主要步骤</h3><ul>
<li>配置消费者客户端参数及创建相应的消费者实例。</li>
<li>订阅主题。</li>
<li>拉取消息并消费。</li>
<li>提交消费位移。</li>
<li>关闭消费者实例。</li>
</ul>
<img src="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/%E6%B6%88%E8%B4%B9%E8%80%85%E4%BB%A3%E7%A0%81.png" class=""> <img src="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/%E6%B6%88%E8%B4%B9%E8%80%85%E4%BB%A3%E7%A0%812.png" class="">

<h3 id="必要的参数配置"><a href="#必要的参数配置" class="headerlink" title="必要的参数配置"></a>必要的参数配置</h3><ul>
<li>在initConfig（）方法，在Kafka消费者客户端KafkaConsumer中有4个参数是必填的。<ul>
<li>bootstrap.servers：该参数的释义和生产者客户端 KafkaProducer 中的相同，用来 指 定 连 接 Kafka 集 群 所 需 的 broker 地 址 清 单，具 体 内 容 形 式 为host1：port1，host2：post，</li>
<li>group.id：消费者隶属的消费组的名称，默认值为“”。如果设置为空，则会报出异常：Exception in thread “main” org.apache.kafka.common.errors.InvalidGroupIdException：The configured groupId is invalid。一般而言，这个参数需要设置成具有一定的业务意义的名称。</li>
<li>key.deserializer 和 value.deserializer：与生产者客户端 KafkaProducer中的key.serializer和value.serializer参数对应。</li>
</ul>
</li>
</ul>
<h3 id="订阅主题与分区"><a href="#订阅主题与分区" class="headerlink" title="订阅主题与分区"></a>订阅主题与分区</h3><ul>
<li>订阅主题既可以以集合方式，也可以以正则表达式方式订阅（consumer.subscribe(Pattern.compile(“topic-.*”))），如图的构造函数<img src="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/%E8%AE%A2%E9%98%85%E4%B8%BB%E9%A2%98.png" class=""></li>
<li>如果前后两次订阅了不同的主题，那么消费者以最后一次的为准。</li>
<li>其中一个构造参数ConsumerRebalance-Listener，这个是用来设置相应的再均衡监听器的</li>
<li>消费者不仅可以通过KafkaConsumer.subscribe（）方法订阅主题，还可以直接订阅某些主题的特定分区，在KafkaConsumer中还提供了一个assign（）方法来实现这些功能<ul>
<li>public void assign(Collection&lt; TopicPartition &gt; partitions)</li>
<li>这个方法只接受一个参数partitions，用来指定需要订阅的分区集合。这里补充说明一下TopicPartition类，在Kafka的客户端中，它用来表示分区</li>
<li><img src="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/topicpatition.png" class=""></li>
<li>TopicPartition类只有2个属性：topic和partition，分别代表分区所属的主题和自身的分区编号，这个类可以和我们通常所说的主题—分区的概念映射起来。</li>
<li>可以将subscribe 替换为assign consumer.assign(Arrays.asList(new TopicPrtition(“topic-demo”,0)))</li>
</ul>
</li>
<li>KafkaConsumer 中的partitionsFor（）方法可以用来查询指定主题的元数据信息<ul>
<li>public List&lt; PartitionInfo &gt; partitionsFor(String topic)</li>
<li><img src="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/partitionfor.png" class=""></li>
<li>PartitionInfo类中的属性topic表示主题名称，partition代表分区编号，leader代表分区的leader副本所在的位置，replicas代表分区的AR集合，inSyncReplicas代表分区的ISR集合，offlineReplicas代表分区的OSR集合。</li>
</ul>
</li>
<li>既然有订阅，那么就有取消订阅，可以使用 KafkaConsumer 中的 unsubscribe（）方法来取消主题的订阅。</li>
</ul>
<h3 id="反序列化"><a href="#反序列化" class="headerlink" title="反序列化"></a>反序列化</h3><ul>
<li>Kafka所提供的反序列化器有ByteBufferDeserializer、ByteArrayDeserializer、BytesDeserializer、DoubleDeserializer、FloatDeserializer、IntegerDeserializer、LongDeserializer、ShortDeserializer、StringDeserializer，它们分别用于ByteBuffer、ByteArray、Bytes、Double、Float、Integer、Long、Short 及String类型的反序列化，这些序列化器也都实现了 Deserializer 接口</li>
<li>与KafkaProducer中提及的Serializer接口一样，Deserializer接口也有三个方法<ul>
<li>public void configure（Map＜String，？＞ configs，boolean isKey）：用来配置当前类。</li>
<li>public byte[] serialize（String topic，T data）：用来执行反序列化。如果data为null，那么处理的时候直接返回null而不是抛出一个异常。</li>
<li>public void close（）：用来关闭当前序列化器。</li>
</ul>
</li>
</ul>
<h3 id="消息消费"><a href="#消息消费" class="headerlink" title="消息消费"></a>消息消费</h3><ul>
<li>Kafka中的消费是基于拉模式的。</li>
<li>Kafka中的消息消费是一个不断轮询的过程，消费者所要做的就是重复地调用poll（）方法，而poll（）方法返回的是所订阅的主题（分区）上的一组消息。</li>
<li>public ConsumerRecords&lt; k,v &gt; poll(final Duration timeout)</li>
<li>线程不安全的 <a target="_blank" rel="noopener" href="https://www.pianshen.com/article/88711219115/">https://www.pianshen.com/article/88711219115/</a></li>
</ul>
<h4 id="ConsumerRecords"><a href="#ConsumerRecords" class="headerlink" title="ConsumerRecords"></a>ConsumerRecords</h4><img src="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/consumerrecords.png" class="">

<ul>
<li>ConsumerRecords类提供了一个records（TopicPartition）方法来获取消息集中指定分区的消息</li>
<li>ConsumerRecords 类中并没提供与 partitions（）类似的 topics（）方法来查看拉取的消息集中所包含的主题列表，如果要按照主题维度来进行消费，那么只能根据消费者订阅主题时的列表来进行逻辑处理了。下面的示例演示了如何使用ConsumerRecords中的record（String topic）方法：</li>
</ul>
<h3 id="位移提交"><a href="#位移提交" class="headerlink" title="位移提交"></a>位移提交</h3><ul>
<li><p>对于消息在分区中的位置，我们将offset称为“偏移量”；对于消费者消费到的位置，将 offset 称为“位移”，有时候也会更明确地称之为“消费位移”</p>
<img src="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/%E6%B6%88%E8%B4%B9%E4%BD%8D%E7%A7%BB.png" class=""></li>
<li><p>在 Kafka 中默认的消费位移的提交方式是自动提交 enable.auto.commit 配置，默认值为 true</p>
</li>
<li><p>当然这个默认的自动提交不是每消费一条消息就提交一次，而是定期提交，这个定期的周期时间由客户端参数auto.commit.interval.ms配置，默认值为5秒，此参数生效的前提是enable.auto.commit参数为true。</p>
</li>
<li><p>自动提交缺点：</p>
<ul>
<li>自动位移提交的方式在正常情况下不会发生消息丢失或重复消费的现象，但是在编程的世界里异常无可避免，与此同时，自动位移提交也无法做到精确的位移管理。</li>
</ul>
</li>
<li><p>手动提交</p>
<ul>
<li>enable.auto.commit配置为false</li>
<li>手动提交可以细分为同步提交和异步提交，对应于 KafkaConsumer 中的 commitSync（）和commitAsync（）两种类型的方法</li>
<li>commitSync（）commitAsync（） 有重复消费问题</li>
</ul>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(isRunning.get()):</span><br><span class="line">  ConsumerRecords&lt;String,String&gt; records = consumer.poll(<span class="number">1000</span>)；</span><br><span class="line">  <span class="keyword">for</span> (ConsumerRecord&lt;String,String&gt; record: records)&#123;</span><br><span class="line">    <span class="comment">//do some  logical processing</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="控制或关闭消费"><a href="#控制或关闭消费" class="headerlink" title="控制或关闭消费"></a>控制或关闭消费</h3><ul>
<li>KafkaConsumer中使用pause（）和resume（）方法来分别实现暂停某些分区在拉取操作时返回数据给客户端和恢复某些分区向客户端返回数据的操作</li>
</ul>
<h3 id="指定位移消费"><a href="#指定位移消费" class="headerlink" title="指定位移消费"></a>指定位移消费</h3><ul>
<li>如果将auto.offset.reset参数配置为“earliest”，那么消费者会从起始处，也就是0开始消费</li>
</ul>
<h3 id="再均衡"><a href="#再均衡" class="headerlink" title="再均衡"></a>再均衡</h3><ul>
<li>再均衡是指分区的所属权从一个消费者转移到另一消费者的行为，它为消费组具备高可用性和伸缩性提供保障，使我们可以既方便又安全地删除消费组内的消费者或往消费组内添加消费者。</li>
<li>不过在再均衡发生期间，消费组内的消费者是无法读取消息的。也就是说，在再均衡发生期间的这一小段时间内，消费组会变得不可用。</li>
</ul>
<h3 id="消费者拦截器"><a href="#消费者拦截器" class="headerlink" title="消费者拦截器"></a>消费者拦截器</h3><h3 id="多线程实现"><a href="#多线程实现" class="headerlink" title="多线程实现"></a>多线程实现</h3><ul>
<li>为了加速消费者的消费能力，我们可以通过多线程的方式实现消息消费。</li>
<li>KafkaProducer是线程安全的，然而KafkaConsumer却是非线程安全的。</li>
</ul>
<h4 id="第一种也是最常见的方式：线程封闭，即为每个线程实例化一个KafkaConsumer对象"><a href="#第一种也是最常见的方式：线程封闭，即为每个线程实例化一个KafkaConsumer对象" class="headerlink" title="第一种也是最常见的方式：线程封闭，即为每个线程实例化一个KafkaConsumer对象"></a>第一种也是最常见的方式：线程封闭，即为每个线程实例化一个KafkaConsumer对象</h4><img src="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/%E5%A4%9A%E7%BA%BF%E7%A8%8B1.png" class="">

<ul>
<li>一个线程对应一个KafkaConsumer实例，我们可以称之为消费线程。</li>
<li>一个消费线程可以消费一个或多个分区中的消息，所有的消费线程都隶属于同一个消费组。</li>
<li>这种实现方式的并发度受限于分区的实际个数当消费线程的个数大于分区数时，就有部分消费线程一直处于空闲的状态。</li>
<li>上面这种多线程的实现方式和开启多个消费进程的方式没有本质上的区别，它的优点是每个线程可以按顺序消费各个分区中的消息。</li>
<li>缺点也很明显，每个消费线程都要维护一个独立的TCP连接，如果分区数和consumerThreadNum的值都很大，那么会造成不小的系统开销。</li>
</ul>
<h4 id="第二种方式是多个消费线程同时消费同一个分区"><a href="#第二种方式是多个消费线程同时消费同一个分区" class="headerlink" title="第二种方式是多个消费线程同时消费同一个分区"></a>第二种方式是多个消费线程同时消费同一个分区</h4><ul>
<li>通过 assign（）、seek（）等方法实现,这样可以打破原有的消费线程的个数不能超过分区数的限制，进一步提高了消费的能力。</li>
<li>不过这种实现方式对于位移提交和顺序控制的处理就会变得非常复杂，实际应用中使用得极少，笔者也并不推荐。</li>
<li>一般而言，分区是消费线程的最小划分单位。</li>
</ul>
<h4 id="第三种实现方式，将处理消息模块改成多线程的实现方式"><a href="#第三种实现方式，将处理消息模块改成多线程的实现方式" class="headerlink" title="第三种实现方式，将处理消息模块改成多线程的实现方式"></a>第三种实现方式，将处理消息模块改成多线程的实现方式</h4><p>一般而言，poll（）拉取消息的速度是相当快的，而整体消费的瓶颈也正是在处理消息这一块，如果我们通过一定的方式来改进这一部分，那么我们就能带动整体消费性能的提升。</p>
<img src="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/%E5%A4%9A%E7%BA%BF%E7%A8%8B3.png" class="">

<ul>
<li>KafkaConsumerThread类对应的是一个消费线程，里面通过线程池的方式来调用 RecordHandler 处理一批批的消息。</li>
<li>第三种实现方式还可以横向扩展，通过开启多个 KafkaConsumerThread 实例来进一步提升整体的消费能力。</li>
<li>第三种实现方式相比第一种实现方式而言，除了横向扩展的能力，还可以减少TCP连接对系统资源的消耗，不过缺点就是对于消息的顺序处理就比较困难了。</li>
</ul>
<p>对于消费位移的处理可以通过滑动窗口来解决</p>
<img src="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3.png" class="">

<ul>
<li>每一个方格代表一个批次的消息，一个滑动窗口包含若干方格，startOffset标注的是当前滑动窗口的起始位置，endOffset标注的是末尾位置。</li>
<li>每当startOffset指向的方格中的消息被消费完成，就可以提交这部分的位移，与此同时，窗口向前滑动一格，删除原来startOffset所指方格中对应的消息，并且拉取新的消息进入窗口。</li>
<li>滑动窗口的大小固定，所对应的用来暂存消息的缓存大小也就固定了，这部分内存开销可控。</li>
<li>方格大小和滑动窗口的大小同时决定了消费线程的并发数：一个方格对应一个消费线程，对于窗口大小固定的情况，方格越小并行度越高；</li>
<li>对于方格大小固定的情况，窗口越大并行度越高。</li>
<li>不过，若窗口设置得过大，不仅会增大内存的开销，而且在发生异常（比如Crash）的情况下也会引起大量的重复消费，同时还考虑线程切换的开销，建议根据实际情况设置一个合理的值，不管是对于方格还是窗口而言，过大或过小都不合适。</li>
<li>如果一个方格内的消息无法被标记为消费完成，重试失败就转入重试队列，如果还不奏效就转入死信队列</li>
</ul>
<h2 id="重要的消费者参数"><a href="#重要的消费者参数" class="headerlink" title="重要的消费者参数"></a>重要的消费者参数</h2><ol>
<li>fetch.min.bytes<ul>
<li>该参数用来配置Consumer在一次拉取请求（调用poll（）方法）中能从Kafka中拉取的最小数据量，默认值为1（B）。Kafka在收到Consumer的拉取请求时，如果返回给Consumer的数据量小于这个参数所配置的值，那么它就需要进行等待，直到数据量满足这个参数的配置大小。可以适当调大这个参数的值以提高一定的吞吐量，不过也会造成额外的延迟（latency），对于延迟敏感的应用可能就不可取了。</li>
</ul>
</li>
<li>fetch.max.bytes<ul>
<li>该参数与fetch.max.bytes参数对应，它用来配置Consumer在一次拉取请求中从Kafka中拉取的最大数据量，默认值为 52428800（B），也就是 50MB。</li>
<li>与此相关的，Kafka中所能接收的最大消息的大小通过服务端参数message.max.bytes（对应于主题端参数max.message.bytes）来设置。</li>
</ul>
</li>
<li>fetch.max.wait.ms<ul>
<li>fetch.max.wait.ms参数用于指定Kafka的等待时间，默认值为500（ms）。</li>
<li>如果Kafka中没有足够多的消息而满足不了fetch.min.bytes参数的要求，那么最终会等待500ms。</li>
<li>这个参数的设定和Consumer与Kafka之间的延迟也有关系，如果业务应用对延迟敏感，那么可以适当调小这个参数。</li>
</ul>
</li>
<li>max.partition.fetch.bytes<ul>
<li>这个参数用来配置从每个分区里返回给Consumer的最大数据量，默认值为1048576（B），即1MB。</li>
<li>这个参数与 fetch.max.bytes 参数相似，只不过前者用来限制一次拉取中每个分区的消息大小，而后者用来限制一次拉取中整体消息的大小。</li>
<li>同样，如果这个参数设定的值比消息的大小要小，那么也不会造成无法消费，Kafka 为了保持消费逻辑的正常运转不会对此做强硬的限制。</li>
</ul>
</li>
<li>max.poll.records<ul>
<li>这个参数用来配置Consumer在一次拉取请求中拉取的最大消息数，默认值为500（条）。如果消息的大小都比较小，则可以适当调大这个参数值来提升一定的消费速度。</li>
</ul>
</li>
<li>connections.max.idle.ms<ul>
<li>这个参数用来指定在多久之后关闭限制的连接，默认值是540000（ms），即9分钟</li>
</ul>
</li>
<li>exclude.internal.topics<ul>
<li>Kafka中有两个内部的主题：__consumer_offsets和__transaction_state</li>
<li>exclude.internal.topics用来指定Kafka中的内部主题是否可以向消费者公开，默认值为true。</li>
<li>如果设置为true，那么只能使用subscribe（Collection）的方式而不能使用subscribe（Pattern）的方式来订阅内部主题，设置为false则没有这个限制。</li>
</ul>
</li>
<li>receive.buffer.bytes<ul>
<li>这个参数用来设置Socket接收消息缓冲区（SO_RECBUF）的大小，默认值为65536（B），即64KB。</li>
<li>如果设置为-1，则使用操作系统的默认值。</li>
<li>如果Consumer与Kafka处于不同的机房，则可以适当调大这个参数值。</li>
</ul>
</li>
<li>send.buffer.bytes<ul>
<li>这个参数用来设置Socket发送消息缓冲区（SO_SNDBUF）的大小，默认值为131072（B），即128KB。</li>
<li>与receive.buffer.bytes参数一样，如果设置为-1，则使用操作系统的默认值。</li>
</ul>
</li>
<li>request.timeout.ms<ul>
<li>这个参数用来配置Consumer等待请求响应的最长时间，默认值为30000（ms）。</li>
</ul>
</li>
<li>metadata.max.age.ms<ul>
<li>这个参数用来配置元数据的过期时间，默认值为300000（ms），即5分钟。</li>
<li>如果元数据在此参数所限定的时间范围内没有进行更新，则会被强制更新，即使没有任何分区变化或有新的broker加入。</li>
</ul>
</li>
<li>reconnect.backoff.ms<ul>
<li>这个参数用来配置尝试重新连接指定主机之前的等待时间（也称为退避时间），避免频繁地连接主机，默认值为50（ms）。</li>
<li>这种机制适用于消费者向broker发送的所有请求。</li>
</ul>
</li>
<li>retry.backoff.ms<ul>
<li>个参数用来配置尝试重新发送失败的请求到指定的主题分区之前的等待（退避）时间，避免在某些故障情况下频繁地重复发送，默认值为100（ms）。</li>
</ul>
</li>
<li>isolation.level<ul>
<li>这个参数用来配置消费者的事务隔离级别。</li>
<li>字符串类型，有效值为“read_uncommitted”和“read_committed”，表示消费者所消费到的位置，如果设置为“read_committed”，那么消费者就会忽略事务未提交的消息，即只能消费到 LSO（LastStableOffset）的位置，默认情况下为“read_uncommitted”，即可以消费到HW（High Watermark）处的位置。</li>
</ul>
</li>
</ol>
<img src="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/%E5%85%B6%E4%BB%96%E5%8F%82%E6%95%B01.png" class="">
<img src="/2021/06/22/3-%E6%B6%88%E8%B4%B9%E8%80%85/%E5%85%B6%E4%BB%96%E5%8F%82%E6%95%B02.png" class="">

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/default-index/page/5/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/default-index/">1</a><span class="space">&hellip;</span><a class="page-number" href="/default-index/page/5/">5</a><span class="page-number current">6</span><a class="page-number" href="/default-index/page/7/">7</a><span class="space">&hellip;</span><a class="page-number" href="/default-index/page/15/">15</a><a class="extend next" rel="next" href="/default-index/page/7/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">sk-xinye</p>
  <div class="site-description" itemprop="description">愿所有努力都不被辜负</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">142</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">18</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2023</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">sk-xinye</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
